{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-09T13:38:20.480016300Z",
     "start_time": "2023-11-09T13:33:06.210556400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/8.44k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ddf48baa628f4d13b072dff935327731"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading metadata:   0%|          | 0.00/18.5k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fafc1dad6209496a8a99e9196606c439"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/12.9k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b77f8c77c36b442f916c655ec0acf46f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8977527ae26d4de28285a8f57fa2c1ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/941M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6251305eb084b43a0595dbce98c3ba9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ba8133eb4d5423cb961e697dacc8be4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8bf3acc22fb04500907f9e8924b19ae0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/412178 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "006bf4085cca4cb2978ce2ef34f15520"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/22176 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e3a006daf8524c15b7b367dd4d934071"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating validation split:   0%|          | 0/23107 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "629611a1fbf24ea080d33bc1effe91de"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Software\\anaconda3\\envs\\llmnote\\lib\\site-packages\\datasets\\table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\", cache_dir=\"../dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['train', 'test', 'validation'])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T14:02:53.213692600Z",
     "start_time": "2023-11-09T14:02:53.197692400Z"
    }
   },
   "id": "59f979799889d279"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n    num_rows: 412178\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T14:02:59.805176700Z",
     "start_time": "2023-11-09T14:02:59.785176700Z"
    }
   },
   "id": "33ce58273033dedc"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "{'repository_name': 'ageitgey/face_recognition',\n 'func_path_in_repository': 'examples/face_recognition_knn.py',\n 'func_name': 'train',\n 'whole_func_string': 'def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo=\\'ball_tree\\', verbose=False):\\n    \"\"\"\\n    Trains a k-nearest neighbors classifier for face recognition.\\n\\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\\n\\n     (View in source code to see train_dir example tree structure)\\n\\n     Structure:\\n        <train_dir>/\\n        ├── <person1>/\\n        │   ├── <somename1>.jpeg\\n        │   ├── <somename2>.jpeg\\n        │   ├── ...\\n        ├── <person2>/\\n        │   ├── <somename1>.jpeg\\n        │   └── <somename2>.jpeg\\n        └── ...\\n\\n    :param model_save_path: (optional) path to save model on disk\\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\\n    :param verbose: verbosity of training\\n    :return: returns knn classifier that was trained on the given data.\\n    \"\"\"\\n    X = []\\n    y = []\\n\\n    # Loop through each person in the training set\\n    for class_dir in os.listdir(train_dir):\\n        if not os.path.isdir(os.path.join(train_dir, class_dir)):\\n            continue\\n\\n        # Loop through each training image for the current person\\n        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\\n            image = face_recognition.load_image_file(img_path)\\n            face_bounding_boxes = face_recognition.face_locations(image)\\n\\n            if len(face_bounding_boxes) != 1:\\n                # If there are no people (or too many people) in a training image, skip the image.\\n                if verbose:\\n                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn\\'t find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\\n            else:\\n                # Add face encoding for current image to the training set\\n                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\\n                y.append(class_dir)\\n\\n    # Determine how many neighbors to use for weighting in the KNN classifier\\n    if n_neighbors is None:\\n        n_neighbors = int(round(math.sqrt(len(X))))\\n        if verbose:\\n            print(\"Chose n_neighbors automatically:\", n_neighbors)\\n\\n    # Create and train the KNN classifier\\n    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights=\\'distance\\')\\n    knn_clf.fit(X, y)\\n\\n    # Save the trained KNN classifier\\n    if model_save_path is not None:\\n        with open(model_save_path, \\'wb\\') as f:\\n            pickle.dump(knn_clf, f)\\n\\n    return knn_clf',\n 'language': 'python',\n 'func_code_string': 'def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo=\\'ball_tree\\', verbose=False):\\n    \"\"\"\\n    Trains a k-nearest neighbors classifier for face recognition.\\n\\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\\n\\n     (View in source code to see train_dir example tree structure)\\n\\n     Structure:\\n        <train_dir>/\\n        ├── <person1>/\\n        │   ├── <somename1>.jpeg\\n        │   ├── <somename2>.jpeg\\n        │   ├── ...\\n        ├── <person2>/\\n        │   ├── <somename1>.jpeg\\n        │   └── <somename2>.jpeg\\n        └── ...\\n\\n    :param model_save_path: (optional) path to save model on disk\\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\\n    :param verbose: verbosity of training\\n    :return: returns knn classifier that was trained on the given data.\\n    \"\"\"\\n    X = []\\n    y = []\\n\\n    # Loop through each person in the training set\\n    for class_dir in os.listdir(train_dir):\\n        if not os.path.isdir(os.path.join(train_dir, class_dir)):\\n            continue\\n\\n        # Loop through each training image for the current person\\n        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\\n            image = face_recognition.load_image_file(img_path)\\n            face_bounding_boxes = face_recognition.face_locations(image)\\n\\n            if len(face_bounding_boxes) != 1:\\n                # If there are no people (or too many people) in a training image, skip the image.\\n                if verbose:\\n                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn\\'t find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\\n            else:\\n                # Add face encoding for current image to the training set\\n                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\\n                y.append(class_dir)\\n\\n    # Determine how many neighbors to use for weighting in the KNN classifier\\n    if n_neighbors is None:\\n        n_neighbors = int(round(math.sqrt(len(X))))\\n        if verbose:\\n            print(\"Chose n_neighbors automatically:\", n_neighbors)\\n\\n    # Create and train the KNN classifier\\n    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights=\\'distance\\')\\n    knn_clf.fit(X, y)\\n\\n    # Save the trained KNN classifier\\n    if model_save_path is not None:\\n        with open(model_save_path, \\'wb\\') as f:\\n            pickle.dump(knn_clf, f)\\n\\n    return knn_clf',\n 'func_code_tokens': ['def',\n  'train',\n  '(',\n  'train_dir',\n  ',',\n  'model_save_path',\n  '=',\n  'None',\n  ',',\n  'n_neighbors',\n  '=',\n  'None',\n  ',',\n  'knn_algo',\n  '=',\n  \"'ball_tree'\",\n  ',',\n  'verbose',\n  '=',\n  'False',\n  ')',\n  ':',\n  'X',\n  '=',\n  '[',\n  ']',\n  'y',\n  '=',\n  '[',\n  ']',\n  '# Loop through each person in the training set',\n  'for',\n  'class_dir',\n  'in',\n  'os',\n  '.',\n  'listdir',\n  '(',\n  'train_dir',\n  ')',\n  ':',\n  'if',\n  'not',\n  'os',\n  '.',\n  'path',\n  '.',\n  'isdir',\n  '(',\n  'os',\n  '.',\n  'path',\n  '.',\n  'join',\n  '(',\n  'train_dir',\n  ',',\n  'class_dir',\n  ')',\n  ')',\n  ':',\n  'continue',\n  '# Loop through each training image for the current person',\n  'for',\n  'img_path',\n  'in',\n  'image_files_in_folder',\n  '(',\n  'os',\n  '.',\n  'path',\n  '.',\n  'join',\n  '(',\n  'train_dir',\n  ',',\n  'class_dir',\n  ')',\n  ')',\n  ':',\n  'image',\n  '=',\n  'face_recognition',\n  '.',\n  'load_image_file',\n  '(',\n  'img_path',\n  ')',\n  'face_bounding_boxes',\n  '=',\n  'face_recognition',\n  '.',\n  'face_locations',\n  '(',\n  'image',\n  ')',\n  'if',\n  'len',\n  '(',\n  'face_bounding_boxes',\n  ')',\n  '!=',\n  '1',\n  ':',\n  '# If there are no people (or too many people) in a training image, skip the image.',\n  'if',\n  'verbose',\n  ':',\n  'print',\n  '(',\n  '\"Image {} not suitable for training: {}\"',\n  '.',\n  'format',\n  '(',\n  'img_path',\n  ',',\n  '\"Didn\\'t find a face\"',\n  'if',\n  'len',\n  '(',\n  'face_bounding_boxes',\n  ')',\n  '<',\n  '1',\n  'else',\n  '\"Found more than one face\"',\n  ')',\n  ')',\n  'else',\n  ':',\n  '# Add face encoding for current image to the training set',\n  'X',\n  '.',\n  'append',\n  '(',\n  'face_recognition',\n  '.',\n  'face_encodings',\n  '(',\n  'image',\n  ',',\n  'known_face_locations',\n  '=',\n  'face_bounding_boxes',\n  ')',\n  '[',\n  '0',\n  ']',\n  ')',\n  'y',\n  '.',\n  'append',\n  '(',\n  'class_dir',\n  ')',\n  '# Determine how many neighbors to use for weighting in the KNN classifier',\n  'if',\n  'n_neighbors',\n  'is',\n  'None',\n  ':',\n  'n_neighbors',\n  '=',\n  'int',\n  '(',\n  'round',\n  '(',\n  'math',\n  '.',\n  'sqrt',\n  '(',\n  'len',\n  '(',\n  'X',\n  ')',\n  ')',\n  ')',\n  ')',\n  'if',\n  'verbose',\n  ':',\n  'print',\n  '(',\n  '\"Chose n_neighbors automatically:\"',\n  ',',\n  'n_neighbors',\n  ')',\n  '# Create and train the KNN classifier',\n  'knn_clf',\n  '=',\n  'neighbors',\n  '.',\n  'KNeighborsClassifier',\n  '(',\n  'n_neighbors',\n  '=',\n  'n_neighbors',\n  ',',\n  'algorithm',\n  '=',\n  'knn_algo',\n  ',',\n  'weights',\n  '=',\n  \"'distance'\",\n  ')',\n  'knn_clf',\n  '.',\n  'fit',\n  '(',\n  'X',\n  ',',\n  'y',\n  ')',\n  '# Save the trained KNN classifier',\n  'if',\n  'model_save_path',\n  'is',\n  'not',\n  'None',\n  ':',\n  'with',\n  'open',\n  '(',\n  'model_save_path',\n  ',',\n  \"'wb'\",\n  ')',\n  'as',\n  'f',\n  ':',\n  'pickle',\n  '.',\n  'dump',\n  '(',\n  'knn_clf',\n  ',',\n  'f',\n  ')',\n  'return',\n  'knn_clf'],\n 'func_documentation_string': 'Trains a k-nearest neighbors classifier for face recognition.\\n\\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\\n\\n     (View in source code to see train_dir example tree structure)\\n\\n     Structure:\\n        <train_dir>/\\n        ├── <person1>/\\n        │   ├── <somename1>.jpeg\\n        │   ├── <somename2>.jpeg\\n        │   ├── ...\\n        ├── <person2>/\\n        │   ├── <somename1>.jpeg\\n        │   └── <somename2>.jpeg\\n        └── ...\\n\\n    :param model_save_path: (optional) path to save model on disk\\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\\n    :param verbose: verbosity of training\\n    :return: returns knn classifier that was trained on the given data.',\n 'func_documentation_tokens': ['Trains',\n  'a',\n  'k',\n  '-',\n  'nearest',\n  'neighbors',\n  'classifier',\n  'for',\n  'face',\n  'recognition',\n  '.'],\n 'split_name': 'train',\n 'func_code_url': 'https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/examples/face_recognition_knn.py#L46-L108'}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T14:03:09.621634200Z",
     "start_time": "2023-11-09T14:03:09.555063Z"
    }
   },
   "id": "1df0740b033aa41b"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo=\\'ball_tree\\', verbose=False):\\n    \"\"\"\\n    Trains a k-nearest neighbors classifier for face recognition.\\n\\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\\n\\n     (View in source code to see train_dir example tree structure)\\n\\n     Structure:\\n        <train_dir>/\\n        ├── <person1>/\\n        │   ├── <somename1>.jpeg\\n        │   ├── <somename2>.jpeg\\n        │   ├── ...\\n        ├── <person2>/\\n        │   ├── <somename1>.jpeg\\n        │   └── <somename2>.jpeg\\n        └── ...\\n\\n    :param model_save_path: (optional) path to save model on disk\\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\\n    :param verbose: verbosity of training\\n    :return: returns knn classifier that was trained on the given data.\\n    \"\"\"\\n    X = []\\n    y = []\\n\\n    # Loop through each person in the training set\\n    for class_dir in os.listdir(train_dir):\\n        if not os.path.isdir(os.path.join(train_dir, class_dir)):\\n            continue\\n\\n        # Loop through each training image for the current person\\n        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\\n            image = face_recognition.load_image_file(img_path)\\n            face_bounding_boxes = face_recognition.face_locations(image)\\n\\n            if len(face_bounding_boxes) != 1:\\n                # If there are no people (or too many people) in a training image, skip the image.\\n                if verbose:\\n                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn\\'t find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\\n            else:\\n                # Add face encoding for current image to the training set\\n                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\\n                y.append(class_dir)\\n\\n    # Determine how many neighbors to use for weighting in the KNN classifier\\n    if n_neighbors is None:\\n        n_neighbors = int(round(math.sqrt(len(X))))\\n        if verbose:\\n            print(\"Chose n_neighbors automatically:\", n_neighbors)\\n\\n    # Create and train the KNN classifier\\n    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights=\\'distance\\')\\n    knn_clf.fit(X, y)\\n\\n    # Save the trained KNN classifier\\n    if model_save_path is not None:\\n        with open(model_save_path, \\'wb\\') as f:\\n            pickle.dump(knn_clf, f)\\n\\n    return knn_clf'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0][\"whole_func_string\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T14:04:11.716160900Z",
     "start_time": "2023-11-09T14:04:11.690790100Z"
    }
   },
   "id": "fab39d4e4d5a7212"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "training_corpus = (\n",
    "    raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "    for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T14:06:49.607533Z",
     "start_time": "2023-11-09T14:06:49.589797200Z"
    }
   },
   "id": "552f6b3d51b97be0"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<generator object <genexpr> at 0x000002145F9742E0>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_corpus"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T14:06:53.585903100Z",
     "start_time": "2023-11-09T14:06:53.567995500Z"
    }
   },
   "id": "b34a7121ece1d9ce"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "batch_0 = next(training_corpus)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T14:07:30.216977700Z",
     "start_time": "2023-11-09T14:07:30.098009600Z"
    }
   },
   "id": "2870585388568e23"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "1000"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T14:07:46.830770400Z",
     "start_time": "2023-11-09T14:07:46.817131400Z"
    }
   },
   "id": "68540a9cac8bb239"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "['def concat_same_type(self, to_concat, placement=None):\\n        \"\"\"\\n        Concatenate list of single blocks of the same type.\\n        \"\"\"\\n        values = self._concatenator([blk.values for blk in to_concat],\\n                                    axis=self.ndim - 1)\\n        return self.make_block_same_class(\\n            values, placement=placement or slice(0, len(values), 1))',\n 'def delete(self, loc):\\n        \"\"\"\\n        Delete given loc(-s) from block in-place.\\n        \"\"\"\\n        self.values = np.delete(self.values, loc, 0)\\n        self.mgr_locs = self.mgr_locs.delete(loc)',\n 'def apply(self, func, **kwargs):\\n        \"\"\" apply the function to my values; return a block if we are not\\n        one\\n        \"\"\"\\n        with np.errstate(all=\\'ignore\\'):\\n            result = func(self.values, **kwargs)\\n        if not isinstance(result, Block):\\n            result = self.make_block(values=_block_shape(result,\\n                                                         ndim=self.ndim))\\n\\n        return result',\n 'def fillna(self, value, limit=None, inplace=False, downcast=None):\\n        \"\"\" fillna on the block with the value. If we fail, then convert to\\n        ObjectBlock and try again\\n        \"\"\"\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n\\n        if not self._can_hold_na:\\n            if inplace:\\n                return self\\n            else:\\n                return self.copy()\\n\\n        mask = isna(self.values)\\n        if limit is not None:\\n            if not is_integer(limit):\\n                raise ValueError(\\'Limit must be an integer\\')\\n            if limit < 1:\\n                raise ValueError(\\'Limit must be greater than 0\\')\\n            if self.ndim > 2:\\n                raise NotImplementedError(\"number of dimensions for \\'fillna\\' \"\\n                                          \"is currently limited to 2\")\\n            mask[mask.cumsum(self.ndim - 1) > limit] = False\\n\\n        # fillna, but if we cannot coerce, then try again as an ObjectBlock\\n        try:\\n            values, _ = self._try_coerce_args(self.values, value)\\n            blocks = self.putmask(mask, value, inplace=inplace)\\n            blocks = [b.make_block(values=self._try_coerce_result(b.values))\\n                      for b in blocks]\\n            return self._maybe_downcast(blocks, downcast)\\n        except (TypeError, ValueError):\\n\\n            # we can\\'t process the value, but nothing to do\\n            if not mask.any():\\n                return self if inplace else self.copy()\\n\\n            # operate column-by-column\\n            def f(m, v, i):\\n                block = self.coerce_to_target_dtype(value)\\n\\n                # slice out our block\\n                if i is not None:\\n                    block = block.getitem_block(slice(i, i + 1))\\n                return block.fillna(value,\\n                                    limit=limit,\\n                                    inplace=inplace,\\n                                    downcast=None)\\n\\n            return self.split_and_operate(mask, f, inplace)',\n 'def split_and_operate(self, mask, f, inplace):\\n        \"\"\"\\n        split the block per-column, and apply the callable f\\n        per-column, return a new block for each. Handle\\n        masking which will not change a block unless needed.\\n\\n        Parameters\\n        ----------\\n        mask : 2-d boolean mask\\n        f : callable accepting (1d-mask, 1d values, indexer)\\n        inplace : boolean\\n\\n        Returns\\n        -------\\n        list of blocks\\n        \"\"\"\\n\\n        if mask is None:\\n            mask = np.ones(self.shape, dtype=bool)\\n        new_values = self.values\\n\\n        def make_a_block(nv, ref_loc):\\n            if isinstance(nv, Block):\\n                block = nv\\n            elif isinstance(nv, list):\\n                block = nv[0]\\n            else:\\n                # Put back the dimension that was taken from it and make\\n                # a block out of the result.\\n                try:\\n                    nv = _block_shape(nv, ndim=self.ndim)\\n                except (AttributeError, NotImplementedError):\\n                    pass\\n                block = self.make_block(values=nv,\\n                                        placement=ref_loc)\\n            return block\\n\\n        # ndim == 1\\n        if self.ndim == 1:\\n            if mask.any():\\n                nv = f(mask, new_values, None)\\n            else:\\n                nv = new_values if inplace else new_values.copy()\\n            block = make_a_block(nv, self.mgr_locs)\\n            return [block]\\n\\n        # ndim > 1\\n        new_blocks = []\\n        for i, ref_loc in enumerate(self.mgr_locs):\\n            m = mask[i]\\n            v = new_values[i]\\n\\n            # need a new block\\n            if m.any():\\n                nv = f(m, v, i)\\n            else:\\n                nv = v if inplace else v.copy()\\n\\n            block = make_a_block(nv, [ref_loc])\\n            new_blocks.append(block)\\n\\n        return new_blocks',\n 'def downcast(self, dtypes=None):\\n        \"\"\" try to downcast each item to the dict of dtypes if present \"\"\"\\n\\n        # turn it off completely\\n        if dtypes is False:\\n            return self\\n\\n        values = self.values\\n\\n        # single block handling\\n        if self._is_single_block:\\n\\n            # try to cast all non-floats here\\n            if dtypes is None:\\n                dtypes = \\'infer\\'\\n\\n            nv = maybe_downcast_to_dtype(values, dtypes)\\n            return self.make_block(nv)\\n\\n        # ndim > 1\\n        if dtypes is None:\\n            return self\\n\\n        if not (dtypes == \\'infer\\' or isinstance(dtypes, dict)):\\n            raise ValueError(\"downcast must have a dictionary or \\'infer\\' as \"\\n                             \"its argument\")\\n\\n        # operate column-by-column\\n        # this is expensive as it splits the blocks items-by-item\\n        def f(m, v, i):\\n\\n            if dtypes == \\'infer\\':\\n                dtype = \\'infer\\'\\n            else:\\n                raise AssertionError(\"dtypes as dict is not supported yet\")\\n\\n            if dtype is not None:\\n                v = maybe_downcast_to_dtype(v, dtype)\\n            return v\\n\\n        return self.split_and_operate(None, f, False)',\n 'def _astype(self, dtype, copy=False, errors=\\'raise\\', values=None,\\n                **kwargs):\\n        \"\"\"Coerce to the new type\\n\\n        Parameters\\n        ----------\\n        dtype : str, dtype convertible\\n        copy : boolean, default False\\n            copy if indicated\\n        errors : str, {\\'raise\\', \\'ignore\\'}, default \\'ignore\\'\\n            - ``raise`` : allow exceptions to be raised\\n            - ``ignore`` : suppress exceptions. On error return original object\\n\\n        Returns\\n        -------\\n        Block\\n        \"\"\"\\n        errors_legal_values = (\\'raise\\', \\'ignore\\')\\n\\n        if errors not in errors_legal_values:\\n            invalid_arg = (\"Expected value of kwarg \\'errors\\' to be one of {}. \"\\n                           \"Supplied value is \\'{}\\'\".format(\\n                               list(errors_legal_values), errors))\\n            raise ValueError(invalid_arg)\\n\\n        if (inspect.isclass(dtype) and\\n                issubclass(dtype, (PandasExtensionDtype, ExtensionDtype))):\\n            msg = (\"Expected an instance of {}, but got the class instead. \"\\n                   \"Try instantiating \\'dtype\\'.\".format(dtype.__name__))\\n            raise TypeError(msg)\\n\\n        # may need to convert to categorical\\n        if self.is_categorical_astype(dtype):\\n\\n            # deprecated 17636\\n            if (\\'categories\\' in kwargs or \\'ordered\\' in kwargs):\\n                if isinstance(dtype, CategoricalDtype):\\n                    raise TypeError(\\n                        \"Cannot specify a CategoricalDtype and also \"\\n                        \"`categories` or `ordered`. Use \"\\n                        \"`dtype=CategoricalDtype(categories, ordered)`\"\\n                        \" instead.\")\\n                warnings.warn(\"specifying \\'categories\\' or \\'ordered\\' in \"\\n                              \".astype() is deprecated; pass a \"\\n                              \"CategoricalDtype instead\",\\n                              FutureWarning, stacklevel=7)\\n\\n            categories = kwargs.get(\\'categories\\', None)\\n            ordered = kwargs.get(\\'ordered\\', None)\\n            if com._any_not_none(categories, ordered):\\n                dtype = CategoricalDtype(categories, ordered)\\n\\n            if is_categorical_dtype(self.values):\\n                # GH 10696/18593: update an existing categorical efficiently\\n                return self.make_block(self.values.astype(dtype, copy=copy))\\n\\n            return self.make_block(Categorical(self.values, dtype=dtype))\\n\\n        dtype = pandas_dtype(dtype)\\n\\n        # astype processing\\n        if is_dtype_equal(self.dtype, dtype):\\n            if copy:\\n                return self.copy()\\n            return self\\n\\n        try:\\n            # force the copy here\\n            if values is None:\\n\\n                if self.is_extension:\\n                    values = self.values.astype(dtype)\\n                else:\\n                    if issubclass(dtype.type, str):\\n\\n                        # use native type formatting for datetime/tz/timedelta\\n                        if self.is_datelike:\\n                            values = self.to_native_types()\\n\\n                        # astype formatting\\n                        else:\\n                            values = self.get_values()\\n\\n                    else:\\n                        values = self.get_values(dtype=dtype)\\n\\n                    # _astype_nansafe works fine with 1-d only\\n                    values = astype_nansafe(values.ravel(), dtype, copy=True)\\n\\n                # TODO(extension)\\n                # should we make this attribute?\\n                try:\\n                    values = values.reshape(self.shape)\\n                except AttributeError:\\n                    pass\\n\\n            newb = make_block(values, placement=self.mgr_locs,\\n                              ndim=self.ndim)\\n        except Exception:  # noqa: E722\\n            if errors == \\'raise\\':\\n                raise\\n            newb = self.copy() if copy else self\\n\\n        if newb.is_numeric and self.is_numeric:\\n            if newb.shape != self.shape:\\n                raise TypeError(\\n                    \"cannot set astype for copy = [{copy}] for dtype \"\\n                    \"({dtype} [{shape}]) to different shape \"\\n                    \"({newb_dtype} [{newb_shape}])\".format(\\n                        copy=copy, dtype=self.dtype.name,\\n                        shape=self.shape, newb_dtype=newb.dtype.name,\\n                        newb_shape=newb.shape))\\n        return newb',\n 'def _can_hold_element(self, element):\\n        \"\"\" require the same dtype as ourselves \"\"\"\\n        dtype = self.values.dtype.type\\n        tipo = maybe_infer_dtype_type(element)\\n        if tipo is not None:\\n            return issubclass(tipo.type, dtype)\\n        return isinstance(element, dtype)',\n 'def _try_cast_result(self, result, dtype=None):\\n        \"\"\" try to cast the result to our original type, we may have\\n        roundtripped thru object in the mean-time\\n        \"\"\"\\n        if dtype is None:\\n            dtype = self.dtype\\n\\n        if self.is_integer or self.is_bool or self.is_datetime:\\n            pass\\n        elif self.is_float and result.dtype == self.dtype:\\n\\n            # protect against a bool/object showing up here\\n            if isinstance(dtype, str) and dtype == \\'infer\\':\\n                return result\\n            if not isinstance(dtype, type):\\n                dtype = dtype.type\\n            if issubclass(dtype, (np.bool_, np.object_)):\\n                if issubclass(dtype, np.bool_):\\n                    if isna(result).all():\\n                        return result.astype(np.bool_)\\n                    else:\\n                        result = result.astype(np.object_)\\n                        result[result == 1] = True\\n                        result[result == 0] = False\\n                        return result\\n                else:\\n                    return result.astype(np.object_)\\n\\n            return result\\n\\n        # may need to change the dtype here\\n        return maybe_downcast_to_dtype(result, dtype)',\n 'def _try_coerce_args(self, values, other):\\n        \"\"\" provide coercion to our input arguments \"\"\"\\n\\n        if np.any(notna(other)) and not self._can_hold_element(other):\\n            # coercion issues\\n            # let higher levels handle\\n            raise TypeError(\"cannot convert {} to an {}\".format(\\n                type(other).__name__,\\n                type(self).__name__.lower().replace(\\'Block\\', \\'\\')))\\n\\n        return values, other',\n 'def to_native_types(self, slicer=None, na_rep=\\'nan\\', quoting=None,\\n                        **kwargs):\\n        \"\"\" convert to our native types format, slicing if desired \"\"\"\\n\\n        values = self.get_values()\\n\\n        if slicer is not None:\\n            values = values[:, slicer]\\n        mask = isna(values)\\n\\n        if not self.is_object and not quoting:\\n            values = values.astype(str)\\n        else:\\n            values = np.array(values, dtype=\\'object\\')\\n\\n        values[mask] = na_rep\\n        return values',\n 'def copy(self, deep=True):\\n        \"\"\" copy constructor \"\"\"\\n        values = self.values\\n        if deep:\\n            values = values.copy()\\n        return self.make_block_same_class(values, ndim=self.ndim)',\n 'def replace(self, to_replace, value, inplace=False, filter=None,\\n                regex=False, convert=True):\\n        \"\"\"replace the to_replace value with value, possible to create new\\n        blocks here this is just a call to putmask. regex is not used here.\\n        It is used in ObjectBlocks.  It is here for API compatibility.\\n        \"\"\"\\n\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        original_to_replace = to_replace\\n\\n        # try to replace, if we raise an error, convert to ObjectBlock and\\n        # retry\\n        try:\\n            values, to_replace = self._try_coerce_args(self.values,\\n                                                       to_replace)\\n            mask = missing.mask_missing(values, to_replace)\\n            if filter is not None:\\n                filtered_out = ~self.mgr_locs.isin(filter)\\n                mask[filtered_out.nonzero()[0]] = False\\n\\n            blocks = self.putmask(mask, value, inplace=inplace)\\n            if convert:\\n                blocks = [b.convert(by_item=True, numeric=False,\\n                                    copy=not inplace) for b in blocks]\\n            return blocks\\n        except (TypeError, ValueError):\\n            # GH 22083, TypeError or ValueError occurred within error handling\\n            # causes infinite loop. Cast and retry only if not objectblock.\\n            if is_object_dtype(self):\\n                raise\\n\\n            # try again with a compatible block\\n            block = self.astype(object)\\n            return block.replace(to_replace=original_to_replace,\\n                                 value=value,\\n                                 inplace=inplace,\\n                                 filter=filter,\\n                                 regex=regex,\\n                                 convert=convert)',\n 'def setitem(self, indexer, value):\\n        \"\"\"Set the value inplace, returning a a maybe different typed block.\\n\\n        Parameters\\n        ----------\\n        indexer : tuple, list-like, array-like, slice\\n            The subset of self.values to set\\n        value : object\\n            The value being set\\n\\n        Returns\\n        -------\\n        Block\\n\\n        Notes\\n        -----\\n        `indexer` is a direct slice/positional indexer. `value` must\\n        be a compatible shape.\\n        \"\"\"\\n        # coerce None values, if appropriate\\n        if value is None:\\n            if self.is_numeric:\\n                value = np.nan\\n\\n        # coerce if block dtype can store value\\n        values = self.values\\n        try:\\n            values, value = self._try_coerce_args(values, value)\\n            # can keep its own dtype\\n            if hasattr(value, \\'dtype\\') and is_dtype_equal(values.dtype,\\n                                                          value.dtype):\\n                dtype = self.dtype\\n            else:\\n                dtype = \\'infer\\'\\n\\n        except (TypeError, ValueError):\\n            # current dtype cannot store value, coerce to common dtype\\n            find_dtype = False\\n\\n            if hasattr(value, \\'dtype\\'):\\n                dtype = value.dtype\\n                find_dtype = True\\n\\n            elif lib.is_scalar(value):\\n                if isna(value):\\n                    # NaN promotion is handled in latter path\\n                    dtype = False\\n                else:\\n                    dtype, _ = infer_dtype_from_scalar(value,\\n                                                       pandas_dtype=True)\\n                    find_dtype = True\\n            else:\\n                dtype = \\'infer\\'\\n\\n            if find_dtype:\\n                dtype = find_common_type([values.dtype, dtype])\\n                if not is_dtype_equal(self.dtype, dtype):\\n                    b = self.astype(dtype)\\n                    return b.setitem(indexer, value)\\n\\n        # value must be storeable at this moment\\n        arr_value = np.array(value)\\n\\n        # cast the values to a type that can hold nan (if necessary)\\n        if not self._can_hold_element(value):\\n            dtype, _ = maybe_promote(arr_value.dtype)\\n            values = values.astype(dtype)\\n\\n        transf = (lambda x: x.T) if self.ndim == 2 else (lambda x: x)\\n        values = transf(values)\\n\\n        # length checking\\n        check_setitem_lengths(indexer, value, values)\\n\\n        def _is_scalar_indexer(indexer):\\n            # return True if we are all scalar indexers\\n\\n            if arr_value.ndim == 1:\\n                if not isinstance(indexer, tuple):\\n                    indexer = tuple([indexer])\\n                    return any(isinstance(idx, np.ndarray) and len(idx) == 0\\n                               for idx in indexer)\\n            return False\\n\\n        def _is_empty_indexer(indexer):\\n            # return a boolean if we have an empty indexer\\n\\n            if is_list_like(indexer) and not len(indexer):\\n                return True\\n            if arr_value.ndim == 1:\\n                if not isinstance(indexer, tuple):\\n                    indexer = tuple([indexer])\\n                return any(isinstance(idx, np.ndarray) and len(idx) == 0\\n                           for idx in indexer)\\n            return False\\n\\n        # empty indexers\\n        # 8669 (empty)\\n        if _is_empty_indexer(indexer):\\n            pass\\n\\n        # setting a single element for each dim and with a rhs that could\\n        # be say a list\\n        # GH 6043\\n        elif _is_scalar_indexer(indexer):\\n            values[indexer] = value\\n\\n        # if we are an exact match (ex-broadcasting),\\n        # then use the resultant dtype\\n        elif (len(arr_value.shape) and\\n              arr_value.shape[0] == values.shape[0] and\\n              np.prod(arr_value.shape) == np.prod(values.shape)):\\n            values[indexer] = value\\n            try:\\n                values = values.astype(arr_value.dtype)\\n            except ValueError:\\n                pass\\n\\n        # set\\n        else:\\n            values[indexer] = value\\n\\n        # coerce and try to infer the dtypes of the result\\n        values = self._try_coerce_and_cast_result(values, dtype)\\n        block = self.make_block(transf(values))\\n        return block',\n 'def putmask(self, mask, new, align=True, inplace=False, axis=0,\\n                transpose=False):\\n        \"\"\" putmask the data to the block; it is possible that we may create a\\n        new dtype of block\\n\\n        return the resulting block(s)\\n\\n        Parameters\\n        ----------\\n        mask  : the condition to respect\\n        new : a ndarray/object\\n        align : boolean, perform alignment on other/cond, default is True\\n        inplace : perform inplace modification, default is False\\n        axis : int\\n        transpose : boolean\\n            Set to True if self is stored with axes reversed\\n\\n        Returns\\n        -------\\n        a list of new blocks, the result of the putmask\\n        \"\"\"\\n\\n        new_values = self.values if inplace else self.values.copy()\\n\\n        new = getattr(new, \\'values\\', new)\\n        mask = getattr(mask, \\'values\\', mask)\\n\\n        # if we are passed a scalar None, convert it here\\n        if not is_list_like(new) and isna(new) and not self.is_object:\\n            new = self.fill_value\\n\\n        if self._can_hold_element(new):\\n            _, new = self._try_coerce_args(new_values, new)\\n\\n            if transpose:\\n                new_values = new_values.T\\n\\n            # If the default repeat behavior in np.putmask would go in the\\n            # wrong direction, then explicitly repeat and reshape new instead\\n            if getattr(new, \\'ndim\\', 0) >= 1:\\n                if self.ndim - 1 == new.ndim and axis == 1:\\n                    new = np.repeat(\\n                        new, new_values.shape[-1]).reshape(self.shape)\\n                new = new.astype(new_values.dtype)\\n\\n            # we require exact matches between the len of the\\n            # values we are setting (or is compat). np.putmask\\n            # doesn\\'t check this and will simply truncate / pad\\n            # the output, but we want sane error messages\\n            #\\n            # TODO: this prob needs some better checking\\n            # for 2D cases\\n            if ((is_list_like(new) and\\n                 np.any(mask[mask]) and\\n                 getattr(new, \\'ndim\\', 1) == 1)):\\n\\n                if not (mask.shape[-1] == len(new) or\\n                        mask[mask].shape[-1] == len(new) or\\n                        len(new) == 1):\\n                    raise ValueError(\"cannot assign mismatch \"\\n                                     \"length to masked array\")\\n\\n            np.putmask(new_values, mask, new)\\n\\n        # maybe upcast me\\n        elif mask.any():\\n            if transpose:\\n                mask = mask.T\\n                if isinstance(new, np.ndarray):\\n                    new = new.T\\n                axis = new_values.ndim - axis - 1\\n\\n            # Pseudo-broadcast\\n            if getattr(new, \\'ndim\\', 0) >= 1:\\n                if self.ndim - 1 == new.ndim:\\n                    new_shape = list(new.shape)\\n                    new_shape.insert(axis, 1)\\n                    new = new.reshape(tuple(new_shape))\\n\\n            # operate column-by-column\\n            def f(m, v, i):\\n\\n                if i is None:\\n                    # ndim==1 case.\\n                    n = new\\n                else:\\n\\n                    if isinstance(new, np.ndarray):\\n                        n = np.squeeze(new[i % new.shape[0]])\\n                    else:\\n                        n = np.array(new)\\n\\n                    # type of the new block\\n                    dtype, _ = maybe_promote(n.dtype)\\n\\n                    # we need to explicitly astype here to make a copy\\n                    n = n.astype(dtype)\\n\\n                nv = _putmask_smart(v, m, n)\\n                return nv\\n\\n            new_blocks = self.split_and_operate(mask, f, inplace)\\n            return new_blocks\\n\\n        if inplace:\\n            return [self]\\n\\n        if transpose:\\n            new_values = new_values.T\\n\\n        return [self.make_block(new_values)]',\n 'def coerce_to_target_dtype(self, other):\\n        \"\"\"\\n        coerce the current block to a dtype compat for other\\n        we will return a block, possibly object, and not raise\\n\\n        we can also safely try to coerce to the same dtype\\n        and will receive the same block\\n        \"\"\"\\n\\n        # if we cannot then coerce to object\\n        dtype, _ = infer_dtype_from(other, pandas_dtype=True)\\n\\n        if is_dtype_equal(self.dtype, dtype):\\n            return self\\n\\n        if self.is_bool or is_object_dtype(dtype) or is_bool_dtype(dtype):\\n            # we don\\'t upcast to bool\\n            return self.astype(object)\\n\\n        elif ((self.is_float or self.is_complex) and\\n              (is_integer_dtype(dtype) or is_float_dtype(dtype))):\\n            # don\\'t coerce float/complex to int\\n            return self\\n\\n        elif (self.is_datetime or\\n              is_datetime64_dtype(dtype) or\\n              is_datetime64tz_dtype(dtype)):\\n\\n            # not a datetime\\n            if not ((is_datetime64_dtype(dtype) or\\n                     is_datetime64tz_dtype(dtype)) and self.is_datetime):\\n                return self.astype(object)\\n\\n            # don\\'t upcast timezone with different timezone or no timezone\\n            mytz = getattr(self.dtype, \\'tz\\', None)\\n            othertz = getattr(dtype, \\'tz\\', None)\\n\\n            if str(mytz) != str(othertz):\\n                return self.astype(object)\\n\\n            raise AssertionError(\"possible recursion in \"\\n                                 \"coerce_to_target_dtype: {} {}\".format(\\n                                     self, other))\\n\\n        elif (self.is_timedelta or is_timedelta64_dtype(dtype)):\\n\\n            # not a timedelta\\n            if not (is_timedelta64_dtype(dtype) and self.is_timedelta):\\n                return self.astype(object)\\n\\n            raise AssertionError(\"possible recursion in \"\\n                                 \"coerce_to_target_dtype: {} {}\".format(\\n                                     self, other))\\n\\n        try:\\n            return self.astype(dtype)\\n        except (ValueError, TypeError, OverflowError):\\n            pass\\n\\n        return self.astype(object)',\n 'def _interpolate_with_fill(self, method=\\'pad\\', axis=0, inplace=False,\\n                               limit=None, fill_value=None, coerce=False,\\n                               downcast=None):\\n        \"\"\" fillna but using the interpolate machinery \"\"\"\\n\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n\\n        # if we are coercing, then don\\'t force the conversion\\n        # if the block can\\'t hold the type\\n        if coerce:\\n            if not self._can_hold_na:\\n                if inplace:\\n                    return [self]\\n                else:\\n                    return [self.copy()]\\n\\n        values = self.values if inplace else self.values.copy()\\n        values, fill_value = self._try_coerce_args(values, fill_value)\\n        values = missing.interpolate_2d(values, method=method, axis=axis,\\n                                        limit=limit, fill_value=fill_value,\\n                                        dtype=self.dtype)\\n        values = self._try_coerce_result(values)\\n\\n        blocks = [self.make_block_same_class(values, ndim=self.ndim)]\\n        return self._maybe_downcast(blocks, downcast)',\n 'def _interpolate(self, method=None, index=None, values=None,\\n                     fill_value=None, axis=0, limit=None,\\n                     limit_direction=\\'forward\\', limit_area=None,\\n                     inplace=False, downcast=None, **kwargs):\\n        \"\"\" interpolate using scipy wrappers \"\"\"\\n\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        data = self.values if inplace else self.values.copy()\\n\\n        # only deal with floats\\n        if not self.is_float:\\n            if not self.is_integer:\\n                return self\\n            data = data.astype(np.float64)\\n\\n        if fill_value is None:\\n            fill_value = self.fill_value\\n\\n        if method in (\\'krogh\\', \\'piecewise_polynomial\\', \\'pchip\\'):\\n            if not index.is_monotonic:\\n                raise ValueError(\"{0} interpolation requires that the \"\\n                                 \"index be monotonic.\".format(method))\\n        # process 1-d slices in the axis direction\\n\\n        def func(x):\\n\\n            # process a 1-d slice, returning it\\n            # should the axis argument be handled below in apply_along_axis?\\n            # i.e. not an arg to missing.interpolate_1d\\n            return missing.interpolate_1d(index, x, method=method, limit=limit,\\n                                          limit_direction=limit_direction,\\n                                          limit_area=limit_area,\\n                                          fill_value=fill_value,\\n                                          bounds_error=False, **kwargs)\\n\\n        # interp each column independently\\n        interp_values = np.apply_along_axis(func, axis, data)\\n\\n        blocks = [self.make_block_same_class(interp_values)]\\n        return self._maybe_downcast(blocks, downcast)',\n 'def take_nd(self, indexer, axis, new_mgr_locs=None, fill_tuple=None):\\n        \"\"\"\\n        Take values according to indexer and return them as a block.bb\\n\\n        \"\"\"\\n\\n        # algos.take_nd dispatches for DatetimeTZBlock, CategoricalBlock\\n        # so need to preserve types\\n        # sparse is treated like an ndarray, but needs .get_values() shaping\\n\\n        values = self.values\\n        if self.is_sparse:\\n            values = self.get_values()\\n\\n        if fill_tuple is None:\\n            fill_value = self.fill_value\\n            new_values = algos.take_nd(values, indexer, axis=axis,\\n                                       allow_fill=False, fill_value=fill_value)\\n        else:\\n            fill_value = fill_tuple[0]\\n            new_values = algos.take_nd(values, indexer, axis=axis,\\n                                       allow_fill=True, fill_value=fill_value)\\n\\n        if new_mgr_locs is None:\\n            if axis == 0:\\n                slc = libinternals.indexer_as_slice(indexer)\\n                if slc is not None:\\n                    new_mgr_locs = self.mgr_locs[slc]\\n                else:\\n                    new_mgr_locs = self.mgr_locs[indexer]\\n            else:\\n                new_mgr_locs = self.mgr_locs\\n\\n        if not is_dtype_equal(new_values.dtype, self.dtype):\\n            return self.make_block(new_values, new_mgr_locs)\\n        else:\\n            return self.make_block_same_class(new_values, new_mgr_locs)',\n 'def diff(self, n, axis=1):\\n        \"\"\" return block for the diff of the values \"\"\"\\n        new_values = algos.diff(self.values, n, axis=axis)\\n        return [self.make_block(values=new_values)]',\n 'def shift(self, periods, axis=0, fill_value=None):\\n        \"\"\" shift the block by periods, possibly upcast \"\"\"\\n\\n        # convert integer to float if necessary. need to do a lot more than\\n        # that, handle boolean etc also\\n        new_values, fill_value = maybe_upcast(self.values, fill_value)\\n\\n        # make sure array sent to np.roll is c_contiguous\\n        f_ordered = new_values.flags.f_contiguous\\n        if f_ordered:\\n            new_values = new_values.T\\n            axis = new_values.ndim - axis - 1\\n\\n        if np.prod(new_values.shape):\\n            new_values = np.roll(new_values, ensure_platform_int(periods),\\n                                 axis=axis)\\n\\n        axis_indexer = [slice(None)] * self.ndim\\n        if periods > 0:\\n            axis_indexer[axis] = slice(None, periods)\\n        else:\\n            axis_indexer[axis] = slice(periods, None)\\n        new_values[tuple(axis_indexer)] = fill_value\\n\\n        # restore original order\\n        if f_ordered:\\n            new_values = new_values.T\\n\\n        return [self.make_block(new_values)]',\n 'def where(self, other, cond, align=True, errors=\\'raise\\',\\n              try_cast=False, axis=0, transpose=False):\\n        \"\"\"\\n        evaluate the block; return result block(s) from the result\\n\\n        Parameters\\n        ----------\\n        other : a ndarray/object\\n        cond  : the condition to respect\\n        align : boolean, perform alignment on other/cond\\n        errors : str, {\\'raise\\', \\'ignore\\'}, default \\'raise\\'\\n            - ``raise`` : allow exceptions to be raised\\n            - ``ignore`` : suppress exceptions. On error return original object\\n\\n        axis : int\\n        transpose : boolean\\n            Set to True if self is stored with axes reversed\\n\\n        Returns\\n        -------\\n        a new block(s), the result of the func\\n        \"\"\"\\n        import pandas.core.computation.expressions as expressions\\n        assert errors in [\\'raise\\', \\'ignore\\']\\n\\n        values = self.values\\n        orig_other = other\\n        if transpose:\\n            values = values.T\\n\\n        other = getattr(other, \\'_values\\', getattr(other, \\'values\\', other))\\n        cond = getattr(cond, \\'values\\', cond)\\n\\n        # If the default broadcasting would go in the wrong direction, then\\n        # explicitly reshape other instead\\n        if getattr(other, \\'ndim\\', 0) >= 1:\\n            if values.ndim - 1 == other.ndim and axis == 1:\\n                other = other.reshape(tuple(other.shape + (1, )))\\n            elif transpose and values.ndim == self.ndim - 1:\\n                cond = cond.T\\n\\n        if not hasattr(cond, \\'shape\\'):\\n            raise ValueError(\"where must have a condition that is ndarray \"\\n                             \"like\")\\n\\n        # our where function\\n        def func(cond, values, other):\\n            if cond.ravel().all():\\n                return values\\n\\n            values, other = self._try_coerce_args(values, other)\\n\\n            try:\\n                return self._try_coerce_result(expressions.where(\\n                    cond, values, other))\\n            except Exception as detail:\\n                if errors == \\'raise\\':\\n                    raise TypeError(\\n                        \\'Could not operate [{other!r}] with block values \\'\\n                        \\'[{detail!s}]\\'.format(other=other, detail=detail))\\n                else:\\n                    # return the values\\n                    result = np.empty(values.shape, dtype=\\'float64\\')\\n                    result.fill(np.nan)\\n                    return result\\n\\n        # see if we can operate on the entire block, or need item-by-item\\n        # or if we are a single block (ndim == 1)\\n        try:\\n            result = func(cond, values, other)\\n        except TypeError:\\n\\n            # we cannot coerce, return a compat dtype\\n            # we are explicitly ignoring errors\\n            block = self.coerce_to_target_dtype(other)\\n            blocks = block.where(orig_other, cond, align=align,\\n                                 errors=errors,\\n                                 try_cast=try_cast, axis=axis,\\n                                 transpose=transpose)\\n            return self._maybe_downcast(blocks, \\'infer\\')\\n\\n        if self._can_hold_na or self.ndim == 1:\\n\\n            if transpose:\\n                result = result.T\\n\\n            # try to cast if requested\\n            if try_cast:\\n                result = self._try_cast_result(result)\\n\\n            return self.make_block(result)\\n\\n        # might need to separate out blocks\\n        axis = cond.ndim - 1\\n        cond = cond.swapaxes(axis, 0)\\n        mask = np.array([cond[i].all() for i in range(cond.shape[0])],\\n                        dtype=bool)\\n\\n        result_blocks = []\\n        for m in [mask, ~mask]:\\n            if m.any():\\n                r = self._try_cast_result(result.take(m.nonzero()[0],\\n                                                      axis=axis))\\n                result_blocks.append(\\n                    self.make_block(r.T, placement=self.mgr_locs[m]))\\n\\n        return result_blocks',\n 'def _unstack(self, unstacker_func, new_columns, n_rows, fill_value):\\n        \"\"\"Return a list of unstacked blocks of self\\n\\n        Parameters\\n        ----------\\n        unstacker_func : callable\\n            Partially applied unstacker.\\n        new_columns : Index\\n            All columns of the unstacked BlockManager.\\n        n_rows : int\\n            Only used in ExtensionBlock.unstack\\n        fill_value : int\\n            Only used in ExtensionBlock.unstack\\n\\n        Returns\\n        -------\\n        blocks : list of Block\\n            New blocks of unstacked values.\\n        mask : array_like of bool\\n            The mask of columns of `blocks` we should keep.\\n        \"\"\"\\n        unstacker = unstacker_func(self.values.T)\\n        new_items = unstacker.get_new_columns()\\n        new_placement = new_columns.get_indexer(new_items)\\n        new_values, mask = unstacker.get_new_values()\\n\\n        mask = mask.any(0)\\n        new_values = new_values.T[mask]\\n        new_placement = new_placement[mask]\\n\\n        blocks = [make_block(new_values, placement=new_placement)]\\n        return blocks, mask',\n 'def quantile(self, qs, interpolation=\\'linear\\', axis=0):\\n        \"\"\"\\n        compute the quantiles of the\\n\\n        Parameters\\n        ----------\\n        qs: a scalar or list of the quantiles to be computed\\n        interpolation: type of interpolation, default \\'linear\\'\\n        axis: axis to compute, default 0\\n\\n        Returns\\n        -------\\n        Block\\n        \"\"\"\\n        if self.is_datetimetz:\\n            # TODO: cleanup this special case.\\n            # We need to operate on i8 values for datetimetz\\n            # but `Block.get_values()` returns an ndarray of objects\\n            # right now. We need an API for \"values to do numeric-like ops on\"\\n            values = self.values.asi8\\n\\n            # TODO: NonConsolidatableMixin shape\\n            # Usual shape inconsistencies for ExtensionBlocks\\n            if self.ndim > 1:\\n                values = values[None, :]\\n        else:\\n            values = self.get_values()\\n            values, _ = self._try_coerce_args(values, values)\\n\\n        is_empty = values.shape[axis] == 0\\n        orig_scalar = not is_list_like(qs)\\n        if orig_scalar:\\n            # make list-like, unpack later\\n            qs = [qs]\\n\\n        if is_empty:\\n            if self.ndim == 1:\\n                result = self._na_value\\n            else:\\n                # create the array of na_values\\n                # 2d len(values) * len(qs)\\n                result = np.repeat(np.array([self.fill_value] * len(qs)),\\n                                   len(values)).reshape(len(values),\\n                                                        len(qs))\\n        else:\\n            # asarray needed for Sparse, see GH#24600\\n            # TODO: Why self.values and not values?\\n            mask = np.asarray(isna(self.values))\\n            result = nanpercentile(values, np.array(qs) * 100,\\n                                   axis=axis, na_value=self.fill_value,\\n                                   mask=mask, ndim=self.ndim,\\n                                   interpolation=interpolation)\\n\\n            result = np.array(result, copy=False)\\n            if self.ndim > 1:\\n                result = result.T\\n\\n        if orig_scalar and not lib.is_scalar(result):\\n            # result could be scalar in case with is_empty and self.ndim == 1\\n            assert result.shape[-1] == 1, result.shape\\n            result = result[..., 0]\\n            result = lib.item_from_zerodim(result)\\n\\n        ndim = getattr(result, \\'ndim\\', None) or 0\\n        result = self._try_coerce_result(result)\\n        return make_block(result,\\n                          placement=np.arange(len(result)),\\n                          ndim=ndim)',\n 'def _replace_coerce(self, to_replace, value, inplace=True, regex=False,\\n                        convert=False, mask=None):\\n        \"\"\"\\n        Replace value corresponding to the given boolean array with another\\n        value.\\n\\n        Parameters\\n        ----------\\n        to_replace : object or pattern\\n            Scalar to replace or regular expression to match.\\n        value : object\\n            Replacement object.\\n        inplace : bool, default False\\n            Perform inplace modification.\\n        regex : bool, default False\\n            If true, perform regular expression substitution.\\n        convert : bool, default True\\n            If true, try to coerce any object types to better types.\\n        mask : array-like of bool, optional\\n            True indicate corresponding element is ignored.\\n\\n        Returns\\n        -------\\n        A new block if there is anything to replace or the original block.\\n        \"\"\"\\n\\n        if mask.any():\\n            if not regex:\\n                self = self.coerce_to_target_dtype(value)\\n                return self.putmask(mask, value, inplace=inplace)\\n            else:\\n                return self._replace_single(to_replace, value, inplace=inplace,\\n                                            regex=regex,\\n                                            convert=convert,\\n                                            mask=mask)\\n        return self',\n 'def putmask(self, mask, new, align=True, inplace=False, axis=0,\\n                transpose=False):\\n        \"\"\"\\n        putmask the data to the block; we must be a single block and not\\n        generate other blocks\\n\\n        return the resulting block\\n\\n        Parameters\\n        ----------\\n        mask  : the condition to respect\\n        new : a ndarray/object\\n        align : boolean, perform alignment on other/cond, default is True\\n        inplace : perform inplace modification, default is False\\n\\n        Returns\\n        -------\\n        a new block, the result of the putmask\\n        \"\"\"\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n\\n        # use block\\'s copy logic.\\n        # .values may be an Index which does shallow copy by default\\n        new_values = self.values if inplace else self.copy().values\\n        new_values, new = self._try_coerce_args(new_values, new)\\n\\n        if isinstance(new, np.ndarray) and len(new) == len(mask):\\n            new = new[mask]\\n\\n        mask = _safe_reshape(mask, new_values.shape)\\n\\n        new_values[mask] = new\\n        new_values = self._try_coerce_result(new_values)\\n        return [self.make_block(values=new_values)]',\n 'def _get_unstack_items(self, unstacker, new_columns):\\n        \"\"\"\\n        Get the placement, values, and mask for a Block unstack.\\n\\n        This is shared between ObjectBlock and ExtensionBlock. They\\n        differ in that ObjectBlock passes the values, while ExtensionBlock\\n        passes the dummy ndarray of positions to be used by a take\\n        later.\\n\\n        Parameters\\n        ----------\\n        unstacker : pandas.core.reshape.reshape._Unstacker\\n        new_columns : Index\\n            All columns of the unstacked BlockManager.\\n\\n        Returns\\n        -------\\n        new_placement : ndarray[int]\\n            The placement of the new columns in `new_columns`.\\n        new_values : Union[ndarray, ExtensionArray]\\n            The first return value from _Unstacker.get_new_values.\\n        mask : ndarray[bool]\\n            The second return value from _Unstacker.get_new_values.\\n        \"\"\"\\n        # shared with ExtensionBlock\\n        new_items = unstacker.get_new_columns()\\n        new_placement = new_columns.get_indexer(new_items)\\n        new_values, mask = unstacker.get_new_values()\\n\\n        mask = mask.any(0)\\n        return new_placement, new_values, mask',\n 'def _maybe_coerce_values(self, values):\\n        \"\"\"Unbox to an extension array.\\n\\n        This will unbox an ExtensionArray stored in an Index or Series.\\n        ExtensionArrays pass through. No dtype coercion is done.\\n\\n        Parameters\\n        ----------\\n        values : Index, Series, ExtensionArray\\n\\n        Returns\\n        -------\\n        ExtensionArray\\n        \"\"\"\\n        if isinstance(values, (ABCIndexClass, ABCSeries)):\\n            values = values._values\\n        return values',\n 'def setitem(self, indexer, value):\\n        \"\"\"Set the value inplace, returning a same-typed block.\\n\\n        This differs from Block.setitem by not allowing setitem to change\\n        the dtype of the Block.\\n\\n        Parameters\\n        ----------\\n        indexer : tuple, list-like, array-like, slice\\n            The subset of self.values to set\\n        value : object\\n            The value being set\\n\\n        Returns\\n        -------\\n        Block\\n\\n        Notes\\n        -----\\n        `indexer` is a direct slice/positional indexer. `value` must\\n        be a compatible shape.\\n        \"\"\"\\n        if isinstance(indexer, tuple):\\n            # we are always 1-D\\n            indexer = indexer[0]\\n\\n        check_setitem_lengths(indexer, value, self.values)\\n        self.values[indexer] = value\\n        return self',\n 'def take_nd(self, indexer, axis=0, new_mgr_locs=None, fill_tuple=None):\\n        \"\"\"\\n        Take values according to indexer and return them as a block.\\n        \"\"\"\\n        if fill_tuple is None:\\n            fill_value = None\\n        else:\\n            fill_value = fill_tuple[0]\\n\\n        # axis doesn\\'t matter; we are really a single-dim object\\n        # but are passed the axis depending on the calling routing\\n        # if its REALLY axis 0, then this will be a reindex and not a take\\n        new_values = self.values.take(indexer, fill_value=fill_value,\\n                                      allow_fill=True)\\n\\n        if self.ndim == 1 and new_mgr_locs is None:\\n            new_mgr_locs = [0]\\n        else:\\n            if new_mgr_locs is None:\\n                new_mgr_locs = self.mgr_locs\\n\\n        return self.make_block_same_class(new_values, new_mgr_locs)',\n 'def _slice(self, slicer):\\n        \"\"\" return a slice of my values \"\"\"\\n\\n        # slice the category\\n        # return same dims as we currently have\\n\\n        if isinstance(slicer, tuple) and len(slicer) == 2:\\n            if not com.is_null_slice(slicer[0]):\\n                raise AssertionError(\"invalid slicing for a 1-ndim \"\\n                                     \"categorical\")\\n            slicer = slicer[1]\\n\\n        return self.values[slicer]',\n 'def concat_same_type(self, to_concat, placement=None):\\n        \"\"\"\\n        Concatenate list of single blocks of the same type.\\n        \"\"\"\\n        values = self._holder._concat_same_type(\\n            [blk.values for blk in to_concat])\\n        placement = placement or slice(0, len(values), 1)\\n        return self.make_block_same_class(values, ndim=self.ndim,\\n                                          placement=placement)',\n 'def shift(self,\\n              periods: int,\\n              axis: libinternals.BlockPlacement = 0,\\n              fill_value: Any = None) -> List[\\'ExtensionBlock\\']:\\n        \"\"\"\\n        Shift the block by `periods`.\\n\\n        Dispatches to underlying ExtensionArray and re-boxes in an\\n        ExtensionBlock.\\n        \"\"\"\\n        return [\\n            self.make_block_same_class(\\n                self.values.shift(periods=periods, fill_value=fill_value),\\n                placement=self.mgr_locs, ndim=self.ndim)\\n        ]',\n 'def to_native_types(self, slicer=None, na_rep=\\'\\', float_format=None,\\n                        decimal=\\'.\\', quoting=None, **kwargs):\\n        \"\"\" convert to our native types format, slicing if desired \"\"\"\\n\\n        values = self.values\\n        if slicer is not None:\\n            values = values[:, slicer]\\n\\n        # see gh-13418: no special formatting is desired at the\\n        # output (important for appropriate \\'quoting\\' behaviour),\\n        # so do not pass it through the FloatArrayFormatter\\n        if float_format is None and decimal == \\'.\\':\\n            mask = isna(values)\\n\\n            if not quoting:\\n                values = values.astype(str)\\n            else:\\n                values = np.array(values, dtype=\\'object\\')\\n\\n            values[mask] = na_rep\\n            return values\\n\\n        from pandas.io.formats.format import FloatArrayFormatter\\n        formatter = FloatArrayFormatter(values, na_rep=na_rep,\\n                                        float_format=float_format,\\n                                        decimal=decimal, quoting=quoting,\\n                                        fixed_width=False)\\n        return formatter.get_result_as_array()',\n 'def get_values(self, dtype=None):\\n        \"\"\"\\n        return object dtype as boxed values, such as Timestamps/Timedelta\\n        \"\"\"\\n        if is_object_dtype(dtype):\\n            values = self.values.ravel()\\n            result = self._holder(values).astype(object)\\n            return result.reshape(self.values.shape)\\n        return self.values',\n 'def _maybe_coerce_values(self, values):\\n        \"\"\"Input validation for values passed to __init__. Ensure that\\n        we have datetime64ns, coercing if necessary.\\n\\n        Parameters\\n        ----------\\n        values : array-like\\n            Must be convertible to datetime64\\n\\n        Returns\\n        -------\\n        values : ndarray[datetime64ns]\\n\\n        Overridden by DatetimeTZBlock.\\n        \"\"\"\\n        if values.dtype != _NS_DTYPE:\\n            values = conversion.ensure_datetime64ns(values)\\n\\n        if isinstance(values, DatetimeArray):\\n            values = values._data\\n\\n        assert isinstance(values, np.ndarray), type(values)\\n        return values',\n 'def _astype(self, dtype, **kwargs):\\n        \"\"\"\\n        these automatically copy, so copy=True has no effect\\n        raise on an except if raise == True\\n        \"\"\"\\n        dtype = pandas_dtype(dtype)\\n\\n        # if we are passed a datetime64[ns, tz]\\n        if is_datetime64tz_dtype(dtype):\\n            values = self.values\\n            if getattr(values, \\'tz\\', None) is None:\\n                values = DatetimeIndex(values).tz_localize(\\'UTC\\')\\n            values = values.tz_convert(dtype.tz)\\n            return self.make_block(values)\\n\\n        # delegate\\n        return super()._astype(dtype=dtype, **kwargs)',\n 'def _try_coerce_args(self, values, other):\\n        \"\"\"\\n        Coerce values and other to dtype \\'i8\\'. NaN and NaT convert to\\n        the smallest i8, and will correctly round-trip to NaT if converted\\n        back in _try_coerce_result. values is always ndarray-like, other\\n        may not be\\n\\n        Parameters\\n        ----------\\n        values : ndarray-like\\n        other : ndarray-like or scalar\\n\\n        Returns\\n        -------\\n        base-type values, base-type other\\n        \"\"\"\\n\\n        values = values.view(\\'i8\\')\\n\\n        if isinstance(other, bool):\\n            raise TypeError\\n        elif is_null_datetimelike(other):\\n            other = tslibs.iNaT\\n        elif isinstance(other, (datetime, np.datetime64, date)):\\n            other = self._box_func(other)\\n            if getattr(other, \\'tz\\') is not None:\\n                raise TypeError(\"cannot coerce a Timestamp with a tz on a \"\\n                                \"naive Block\")\\n            other = other.asm8.view(\\'i8\\')\\n        elif hasattr(other, \\'dtype\\') and is_datetime64_dtype(other):\\n            other = other.astype(\\'i8\\', copy=False).view(\\'i8\\')\\n        else:\\n            # coercion issues\\n            # let higher levels handle\\n            raise TypeError(other)\\n\\n        return values, other',\n 'def _try_coerce_result(self, result):\\n        \"\"\" reverse of try_coerce_args \"\"\"\\n        if isinstance(result, np.ndarray):\\n            if result.dtype.kind in [\\'i\\', \\'f\\']:\\n                result = result.astype(\\'M8[ns]\\')\\n\\n        elif isinstance(result, (np.integer, np.float, np.datetime64)):\\n            result = self._box_func(result)\\n        return result',\n 'def to_native_types(self, slicer=None, na_rep=None, date_format=None,\\n                        quoting=None, **kwargs):\\n        \"\"\" convert to our native types format, slicing if desired \"\"\"\\n\\n        values = self.values\\n        i8values = self.values.view(\\'i8\\')\\n\\n        if slicer is not None:\\n            values = values[..., slicer]\\n            i8values = i8values[..., slicer]\\n\\n        from pandas.io.formats.format import _get_format_datetime64_from_values\\n        fmt = _get_format_datetime64_from_values(values, date_format)\\n\\n        result = tslib.format_array_from_datetime(\\n            i8values.ravel(), tz=getattr(self.values, \\'tz\\', None),\\n            format=fmt, na_rep=na_rep).reshape(i8values.shape)\\n        return np.atleast_2d(result)',\n 'def set(self, locs, values):\\n        \"\"\"\\n        Modify Block in-place with new item value\\n\\n        Returns\\n        -------\\n        None\\n        \"\"\"\\n        values = conversion.ensure_datetime64ns(values, copy=False)\\n\\n        self.values[locs] = values',\n 'def _maybe_coerce_values(self, values):\\n        \"\"\"Input validation for values passed to __init__. Ensure that\\n        we have datetime64TZ, coercing if necessary.\\n\\n        Parametetrs\\n        -----------\\n        values : array-like\\n            Must be convertible to datetime64\\n\\n        Returns\\n        -------\\n        values : DatetimeArray\\n        \"\"\"\\n        if not isinstance(values, self._holder):\\n            values = self._holder(values)\\n\\n        if values.tz is None:\\n            raise ValueError(\"cannot create a DatetimeTZBlock without a tz\")\\n\\n        return values',\n 'def get_values(self, dtype=None):\\n        \"\"\"\\n        Returns an ndarray of values.\\n\\n        Parameters\\n        ----------\\n        dtype : np.dtype\\n            Only `object`-like dtypes are respected here (not sure\\n            why).\\n\\n        Returns\\n        -------\\n        values : ndarray\\n            When ``dtype=object``, then and object-dtype ndarray of\\n            boxed values is returned. Otherwise, an M8[ns] ndarray\\n            is returned.\\n\\n            DatetimeArray is always 1-d. ``get_values`` will reshape\\n            the return value to be the same dimensionality as the\\n            block.\\n        \"\"\"\\n        values = self.values\\n        if is_object_dtype(dtype):\\n            values = values._box_values(values._data)\\n\\n        values = np.asarray(values)\\n\\n        if self.ndim == 2:\\n            # Ensure that our shape is correct for DataFrame.\\n            # ExtensionArrays are always 1-D, even in a DataFrame when\\n            # the analogous NumPy-backed column would be a 2-D ndarray.\\n            values = values.reshape(1, -1)\\n        return values',\n 'def _slice(self, slicer):\\n        \"\"\" return a slice of my values \"\"\"\\n        if isinstance(slicer, tuple):\\n            col, loc = slicer\\n            if not com.is_null_slice(col) and col != 0:\\n                raise IndexError(\"{0} only contains one item\".format(self))\\n            return self.values[loc]\\n        return self.values[slicer]',\n 'def _try_coerce_args(self, values, other):\\n        \"\"\"\\n        localize and return i8 for the values\\n\\n        Parameters\\n        ----------\\n        values : ndarray-like\\n        other : ndarray-like or scalar\\n\\n        Returns\\n        -------\\n        base-type values, base-type other\\n        \"\"\"\\n        # asi8 is a view, needs copy\\n        values = _block_shape(values.view(\"i8\"), ndim=self.ndim)\\n\\n        if isinstance(other, ABCSeries):\\n            other = self._holder(other)\\n\\n        if isinstance(other, bool):\\n            raise TypeError\\n        elif is_datetime64_dtype(other):\\n            # add the tz back\\n            other = self._holder(other, dtype=self.dtype)\\n\\n        elif is_null_datetimelike(other):\\n            other = tslibs.iNaT\\n        elif isinstance(other, self._holder):\\n            if other.tz != self.values.tz:\\n                raise ValueError(\"incompatible or non tz-aware value\")\\n            other = _block_shape(other.asi8, ndim=self.ndim)\\n        elif isinstance(other, (np.datetime64, datetime, date)):\\n            other = tslibs.Timestamp(other)\\n            tz = getattr(other, \\'tz\\', None)\\n\\n            # test we can have an equal time zone\\n            if tz is None or str(tz) != str(self.values.tz):\\n                raise ValueError(\"incompatible or non tz-aware value\")\\n            other = other.value\\n        else:\\n            raise TypeError(other)\\n\\n        return values, other',\n 'def _try_coerce_result(self, result):\\n        \"\"\" reverse of try_coerce_args \"\"\"\\n        if isinstance(result, np.ndarray):\\n            if result.dtype.kind in [\\'i\\', \\'f\\']:\\n                result = result.astype(\\'M8[ns]\\')\\n\\n        elif isinstance(result, (np.integer, np.float, np.datetime64)):\\n            result = self._box_func(result)\\n\\n        if isinstance(result, np.ndarray):\\n            # allow passing of > 1dim if its trivial\\n\\n            if result.ndim > 1:\\n                result = result.reshape(np.prod(result.shape))\\n            # GH#24096 new values invalidates a frequency\\n            result = self._holder._simple_new(result, freq=None,\\n                                              dtype=self.values.dtype)\\n\\n        return result',\n 'def diff(self, n, axis=0):\\n        \"\"\"1st discrete difference\\n\\n        Parameters\\n        ----------\\n        n : int, number of periods to diff\\n        axis : int, axis to diff upon. default 0\\n\\n        Return\\n        ------\\n        A list with a new TimeDeltaBlock.\\n\\n        Note\\n        ----\\n        The arguments here are mimicking shift so they are called correctly\\n        by apply.\\n        \"\"\"\\n        if axis == 0:\\n            # Cannot currently calculate diff across multiple blocks since this\\n            # function is invoked via apply\\n            raise NotImplementedError\\n        new_values = (self.values - self.shift(n, axis=axis)[0].values).asi8\\n\\n        # Reshape the new_values like how algos.diff does for timedelta data\\n        new_values = new_values.reshape(1, len(new_values))\\n        new_values = new_values.astype(\\'timedelta64[ns]\\')\\n        return [TimeDeltaBlock(new_values, placement=self.mgr_locs.indexer)]',\n 'def _try_coerce_args(self, values, other):\\n        \"\"\"\\n        Coerce values and other to int64, with null values converted to\\n        iNaT. values is always ndarray-like, other may not be\\n\\n        Parameters\\n        ----------\\n        values : ndarray-like\\n        other : ndarray-like or scalar\\n\\n        Returns\\n        -------\\n        base-type values, base-type other\\n        \"\"\"\\n        values = values.view(\\'i8\\')\\n\\n        if isinstance(other, bool):\\n            raise TypeError\\n        elif is_null_datetimelike(other):\\n            other = tslibs.iNaT\\n        elif isinstance(other, (timedelta, np.timedelta64)):\\n            other = Timedelta(other).value\\n        elif hasattr(other, \\'dtype\\') and is_timedelta64_dtype(other):\\n            other = other.astype(\\'i8\\', copy=False).view(\\'i8\\')\\n        else:\\n            # coercion issues\\n            # let higher levels handle\\n            raise TypeError(other)\\n\\n        return values, other',\n 'def _try_coerce_result(self, result):\\n        \"\"\" reverse of try_coerce_args / try_operate \"\"\"\\n        if isinstance(result, np.ndarray):\\n            mask = isna(result)\\n            if result.dtype.kind in [\\'i\\', \\'f\\']:\\n                result = result.astype(\\'m8[ns]\\')\\n            result[mask] = tslibs.iNaT\\n\\n        elif isinstance(result, (np.integer, np.float)):\\n            result = self._box_func(result)\\n\\n        return result',\n 'def to_native_types(self, slicer=None, na_rep=None, quoting=None,\\n                        **kwargs):\\n        \"\"\" convert to our native types format, slicing if desired \"\"\"\\n\\n        values = self.values\\n        if slicer is not None:\\n            values = values[:, slicer]\\n        mask = isna(values)\\n\\n        rvalues = np.empty(values.shape, dtype=object)\\n        if na_rep is None:\\n            na_rep = \\'NaT\\'\\n        rvalues[mask] = na_rep\\n        imask = (~mask).ravel()\\n\\n        # FIXME:\\n        # should use the formats.format.Timedelta64Formatter here\\n        # to figure what format to pass to the Timedelta\\n        # e.g. to not show the decimals say\\n        rvalues.flat[imask] = np.array([Timedelta(val)._repr_base(format=\\'all\\')\\n                                        for val in values.ravel()[imask]],\\n                                       dtype=object)\\n        return rvalues',\n 'def convert(self, *args, **kwargs):\\n        \"\"\" attempt to coerce any object types to better types return a copy of\\n        the block (if copy = True) by definition we ARE an ObjectBlock!!!!!\\n\\n        can return multiple blocks!\\n        \"\"\"\\n\\n        if args:\\n            raise NotImplementedError\\n        by_item = kwargs.get(\\'by_item\\', True)\\n\\n        new_inputs = [\\'coerce\\', \\'datetime\\', \\'numeric\\', \\'timedelta\\']\\n        new_style = False\\n        for kw in new_inputs:\\n            new_style |= kw in kwargs\\n\\n        if new_style:\\n            fn = soft_convert_objects\\n            fn_inputs = new_inputs\\n        else:\\n            fn = maybe_convert_objects\\n            fn_inputs = [\\'convert_dates\\', \\'convert_numeric\\',\\n                         \\'convert_timedeltas\\']\\n        fn_inputs += [\\'copy\\']\\n\\n        fn_kwargs = {key: kwargs[key] for key in fn_inputs if key in kwargs}\\n\\n        # operate column-by-column\\n        def f(m, v, i):\\n            shape = v.shape\\n            values = fn(v.ravel(), **fn_kwargs)\\n            try:\\n                values = values.reshape(shape)\\n                values = _block_shape(values, ndim=self.ndim)\\n            except (AttributeError, NotImplementedError):\\n                pass\\n\\n            return values\\n\\n        if by_item and not self._is_single_block:\\n            blocks = self.split_and_operate(None, f, False)\\n        else:\\n            values = f(None, self.values.ravel(), None)\\n            blocks = [make_block(values, ndim=self.ndim,\\n                                 placement=self.mgr_locs)]\\n\\n        return blocks',\n 'def set(self, locs, values):\\n        \"\"\"\\n        Modify Block in-place with new item value\\n\\n        Returns\\n        -------\\n        None\\n        \"\"\"\\n        try:\\n            self.values[locs] = values\\n        except (ValueError):\\n\\n            # broadcasting error\\n            # see GH6171\\n            new_shape = list(values.shape)\\n            new_shape[0] = len(self.items)\\n            self.values = np.empty(tuple(new_shape), dtype=self.dtype)\\n            self.values.fill(np.nan)\\n            self.values[locs] = values',\n 'def _try_coerce_args(self, values, other):\\n        \"\"\" provide coercion to our input arguments \"\"\"\\n\\n        if isinstance(other, ABCDatetimeIndex):\\n            # May get a DatetimeIndex here. Unbox it.\\n            other = other.array\\n\\n        if isinstance(other, DatetimeArray):\\n            # hit in pandas/tests/indexing/test_coercion.py\\n            # ::TestWhereCoercion::test_where_series_datetime64[datetime64tz]\\n            # when falling back to ObjectBlock.where\\n            other = other.astype(object)\\n\\n        return values, other',\n 'def _replace_single(self, to_replace, value, inplace=False, filter=None,\\n                        regex=False, convert=True, mask=None):\\n        \"\"\"\\n        Replace elements by the given value.\\n\\n        Parameters\\n        ----------\\n        to_replace : object or pattern\\n            Scalar to replace or regular expression to match.\\n        value : object\\n            Replacement object.\\n        inplace : bool, default False\\n            Perform inplace modification.\\n        filter : list, optional\\n        regex : bool, default False\\n            If true, perform regular expression substitution.\\n        convert : bool, default True\\n            If true, try to coerce any object types to better types.\\n        mask : array-like of bool, optional\\n            True indicate corresponding element is ignored.\\n\\n        Returns\\n        -------\\n        a new block, the result after replacing\\n        \"\"\"\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n\\n        # to_replace is regex compilable\\n        to_rep_re = regex and is_re_compilable(to_replace)\\n\\n        # regex is regex compilable\\n        regex_re = is_re_compilable(regex)\\n\\n        # only one will survive\\n        if to_rep_re and regex_re:\\n            raise AssertionError(\\'only one of to_replace and regex can be \\'\\n                                 \\'regex compilable\\')\\n\\n        # if regex was passed as something that can be a regex (rather than a\\n        # boolean)\\n        if regex_re:\\n            to_replace = regex\\n\\n        regex = regex_re or to_rep_re\\n\\n        # try to get the pattern attribute (compiled re) or it\\'s a string\\n        try:\\n            pattern = to_replace.pattern\\n        except AttributeError:\\n            pattern = to_replace\\n\\n        # if the pattern is not empty and to_replace is either a string or a\\n        # regex\\n        if regex and pattern:\\n            rx = re.compile(to_replace)\\n        else:\\n            # if the thing to replace is not a string or compiled regex call\\n            # the superclass method -> to_replace is some kind of object\\n            return super().replace(to_replace, value, inplace=inplace,\\n                                   filter=filter, regex=regex)\\n\\n        new_values = self.values if inplace else self.values.copy()\\n\\n        # deal with replacing values with objects (strings) that match but\\n        # whose replacement is not a string (numeric, nan, object)\\n        if isna(value) or not isinstance(value, str):\\n\\n            def re_replacer(s):\\n                try:\\n                    return value if rx.search(s) is not None else s\\n                except TypeError:\\n                    return s\\n        else:\\n            # value is guaranteed to be a string here, s can be either a string\\n            # or null if it\\'s null it gets returned\\n            def re_replacer(s):\\n                try:\\n                    return rx.sub(value, s)\\n                except TypeError:\\n                    return s\\n\\n        f = np.vectorize(re_replacer, otypes=[self.dtype])\\n\\n        if filter is None:\\n            filt = slice(None)\\n        else:\\n            filt = self.mgr_locs.isin(filter).nonzero()[0]\\n\\n        if mask is None:\\n            new_values[filt] = f(new_values[filt])\\n        else:\\n            new_values[filt][mask] = f(new_values[filt][mask])\\n\\n        # convert\\n        block = self.make_block(new_values)\\n        if convert:\\n            block = block.convert(by_item=True, numeric=False)\\n        return block',\n 'def _replace_coerce(self, to_replace, value, inplace=True, regex=False,\\n                        convert=False, mask=None):\\n        \"\"\"\\n        Replace value corresponding to the given boolean array with another\\n        value.\\n\\n        Parameters\\n        ----------\\n        to_replace : object or pattern\\n            Scalar to replace or regular expression to match.\\n        value : object\\n            Replacement object.\\n        inplace : bool, default False\\n            Perform inplace modification.\\n        regex : bool, default False\\n            If true, perform regular expression substitution.\\n        convert : bool, default True\\n            If true, try to coerce any object types to better types.\\n        mask : array-like of bool, optional\\n            True indicate corresponding element is ignored.\\n\\n        Returns\\n        -------\\n        A new block if there is anything to replace or the original block.\\n        \"\"\"\\n        if mask.any():\\n            block = super()._replace_coerce(\\n                to_replace=to_replace, value=value, inplace=inplace,\\n                regex=regex, convert=convert, mask=mask)\\n            if convert:\\n                block = [b.convert(by_item=True, numeric=False, copy=True)\\n                         for b in block]\\n            return block\\n        return self',\n 'def _try_coerce_result(self, result):\\n        \"\"\" reverse of try_coerce_args \"\"\"\\n\\n        # GH12564: CategoricalBlock is 1-dim only\\n        # while returned results could be any dim\\n        if ((not is_categorical_dtype(result)) and\\n                isinstance(result, np.ndarray)):\\n            result = _block_shape(result, ndim=self.ndim)\\n\\n        return result',\n 'def to_native_types(self, slicer=None, na_rep=\\'\\', quoting=None, **kwargs):\\n        \"\"\" convert to our native types format, slicing if desired \"\"\"\\n\\n        values = self.values\\n        if slicer is not None:\\n            # Categorical is always one dimension\\n            values = values[slicer]\\n        mask = isna(values)\\n        values = np.array(values, dtype=\\'object\\')\\n        values[mask] = na_rep\\n\\n        # we are expected to return a 2-d ndarray\\n        return values.reshape(1, len(values))',\n 'def _style_to_xlwt(cls, item, firstlevel=True, field_sep=\\',\\',\\n                       line_sep=\\';\\'):\\n        \"\"\"helper which recursively generate an xlwt easy style string\\n        for example:\\n\\n            hstyle = {\"font\": {\"bold\": True},\\n            \"border\": {\"top\": \"thin\",\\n                    \"right\": \"thin\",\\n                    \"bottom\": \"thin\",\\n                    \"left\": \"thin\"},\\n            \"align\": {\"horiz\": \"center\"}}\\n            will be converted to\\n            font: bold on; \\\\\\n                    border: top thin, right thin, bottom thin, left thin; \\\\\\n                    align: horiz center;\\n        \"\"\"\\n        if hasattr(item, \\'items\\'):\\n            if firstlevel:\\n                it = [\"{key}: {val}\"\\n                      .format(key=key, val=cls._style_to_xlwt(value, False))\\n                      for key, value in item.items()]\\n                out = \"{sep} \".format(sep=(line_sep).join(it))\\n                return out\\n            else:\\n                it = [\"{key} {val}\"\\n                      .format(key=key, val=cls._style_to_xlwt(value, False))\\n                      for key, value in item.items()]\\n                out = \"{sep} \".format(sep=(field_sep).join(it))\\n                return out\\n        else:\\n            item = \"{item}\".format(item=item)\\n            item = item.replace(\"True\", \"on\")\\n            item = item.replace(\"False\", \"off\")\\n            return item',\n 'def _convert_to_style(cls, style_dict, num_format_str=None):\\n        \"\"\"\\n        converts a style_dict to an xlwt style object\\n        Parameters\\n        ----------\\n        style_dict : style dictionary to convert\\n        num_format_str : optional number format string\\n        \"\"\"\\n        import xlwt\\n\\n        if style_dict:\\n            xlwt_stylestr = cls._style_to_xlwt(style_dict)\\n            style = xlwt.easyxf(xlwt_stylestr, field_sep=\\',\\', line_sep=\\';\\')\\n        else:\\n            style = xlwt.XFStyle()\\n        if num_format_str is not None:\\n            style.num_format_str = num_format_str\\n\\n        return style',\n 'def convert(cls, style_dict, num_format_str=None):\\n        \"\"\"\\n        converts a style_dict to an xlsxwriter format dict\\n\\n        Parameters\\n        ----------\\n        style_dict : style dictionary to convert\\n        num_format_str : optional number format string\\n        \"\"\"\\n\\n        # Create a XlsxWriter format object.\\n        props = {}\\n\\n        if num_format_str is not None:\\n            props[\\'num_format\\'] = num_format_str\\n\\n        if style_dict is None:\\n            return props\\n\\n        if \\'borders\\' in style_dict:\\n            style_dict = style_dict.copy()\\n            style_dict[\\'border\\'] = style_dict.pop(\\'borders\\')\\n\\n        for style_group_key, style_group in style_dict.items():\\n            for src, dst in cls.STYLE_MAPPING.get(style_group_key, []):\\n                # src is a sequence of keys into a nested dict\\n                # dst is a flat key\\n                if dst in props:\\n                    continue\\n                v = style_group\\n                for k in src:\\n                    try:\\n                        v = v[k]\\n                    except (KeyError, TypeError):\\n                        break\\n                else:\\n                    props[dst] = v\\n\\n        if isinstance(props.get(\\'pattern\\'), str):\\n            # TODO: support other fill patterns\\n            props[\\'pattern\\'] = 0 if props[\\'pattern\\'] == \\'none\\' else 1\\n\\n        for k in [\\'border\\', \\'top\\', \\'right\\', \\'bottom\\', \\'left\\']:\\n            if isinstance(props.get(k), str):\\n                try:\\n                    props[k] = [\\'none\\', \\'thin\\', \\'medium\\', \\'dashed\\', \\'dotted\\',\\n                                \\'thick\\', \\'double\\', \\'hair\\', \\'mediumDashed\\',\\n                                \\'dashDot\\', \\'mediumDashDot\\', \\'dashDotDot\\',\\n                                \\'mediumDashDotDot\\',\\n                                \\'slantDashDot\\'].index(props[k])\\n                except ValueError:\\n                    props[k] = 2\\n\\n        if isinstance(props.get(\\'font_script\\'), str):\\n            props[\\'font_script\\'] = [\\'baseline\\', \\'superscript\\',\\n                                    \\'subscript\\'].index(props[\\'font_script\\'])\\n\\n        if isinstance(props.get(\\'underline\\'), str):\\n            props[\\'underline\\'] = {\\'none\\': 0, \\'single\\': 1, \\'double\\': 2,\\n                                  \\'singleAccounting\\': 33,\\n                                  \\'doubleAccounting\\': 34}[props[\\'underline\\']]\\n\\n        return props',\n 'def _unstack_extension_series(series, level, fill_value):\\n    \"\"\"\\n    Unstack an ExtensionArray-backed Series.\\n\\n    The ExtensionDtype is preserved.\\n\\n    Parameters\\n    ----------\\n    series : Series\\n        A Series with an ExtensionArray for values\\n    level : Any\\n        The level name or number.\\n    fill_value : Any\\n        The user-level (not physical storage) fill value to use for\\n        missing values introduced by the reshape. Passed to\\n        ``series.values.take``.\\n\\n    Returns\\n    -------\\n    DataFrame\\n        Each column of the DataFrame will have the same dtype as\\n        the input Series.\\n    \"\"\"\\n    # Implementation note: the basic idea is to\\n    # 1. Do a regular unstack on a dummy array of integers\\n    # 2. Followup with a columnwise take.\\n    # We use the dummy take to discover newly-created missing values\\n    # introduced by the reshape.\\n    from pandas.core.reshape.concat import concat\\n\\n    dummy_arr = np.arange(len(series))\\n    # fill_value=-1, since we will do a series.values.take later\\n    result = _Unstacker(dummy_arr, series.index,\\n                        level=level, fill_value=-1).get_result()\\n\\n    out = []\\n    values = extract_array(series, extract_numpy=False)\\n\\n    for col, indices in result.iteritems():\\n        out.append(Series(values.take(indices.values,\\n                                      allow_fill=True,\\n                                      fill_value=fill_value),\\n                          name=col, index=result.index))\\n    return concat(out, axis=\\'columns\\', copy=False, keys=result.columns)',\n 'def stack(frame, level=-1, dropna=True):\\n    \"\"\"\\n    Convert DataFrame to Series with multi-level Index. Columns become the\\n    second level of the resulting hierarchical index\\n\\n    Returns\\n    -------\\n    stacked : Series\\n    \"\"\"\\n    def factorize(index):\\n        if index.is_unique:\\n            return index, np.arange(len(index))\\n        codes, categories = _factorize_from_iterable(index)\\n        return categories, codes\\n\\n    N, K = frame.shape\\n\\n    # Will also convert negative level numbers and check if out of bounds.\\n    level_num = frame.columns._get_level_number(level)\\n\\n    if isinstance(frame.columns, MultiIndex):\\n        return _stack_multi_columns(frame, level_num=level_num, dropna=dropna)\\n    elif isinstance(frame.index, MultiIndex):\\n        new_levels = list(frame.index.levels)\\n        new_codes = [lab.repeat(K) for lab in frame.index.codes]\\n\\n        clev, clab = factorize(frame.columns)\\n        new_levels.append(clev)\\n        new_codes.append(np.tile(clab, N).ravel())\\n\\n        new_names = list(frame.index.names)\\n        new_names.append(frame.columns.name)\\n        new_index = MultiIndex(levels=new_levels, codes=new_codes,\\n                               names=new_names, verify_integrity=False)\\n    else:\\n        levels, (ilab, clab) = zip(*map(factorize, (frame.index,\\n                                                    frame.columns)))\\n        codes = ilab.repeat(K), np.tile(clab, N).ravel()\\n        new_index = MultiIndex(levels=levels, codes=codes,\\n                               names=[frame.index.name, frame.columns.name],\\n                               verify_integrity=False)\\n\\n    if frame._is_homogeneous_type:\\n        # For homogeneous EAs, frame.values will coerce to object. So\\n        # we concatenate instead.\\n        dtypes = list(frame.dtypes.values)\\n        dtype = dtypes[0]\\n\\n        if is_extension_array_dtype(dtype):\\n            arr = dtype.construct_array_type()\\n            new_values = arr._concat_same_type([\\n                col._values for _, col in frame.iteritems()\\n            ])\\n            new_values = _reorder_for_extension_array_stack(new_values, N, K)\\n        else:\\n            # homogeneous, non-EA\\n            new_values = frame.values.ravel()\\n\\n    else:\\n        # non-homogeneous\\n        new_values = frame.values.ravel()\\n\\n    if dropna:\\n        mask = notna(new_values)\\n        new_values = new_values[mask]\\n        new_index = new_index[mask]\\n\\n    return frame._constructor_sliced(new_values, index=new_index)',\n 'def get_dummies(data, prefix=None, prefix_sep=\\'_\\', dummy_na=False,\\n                columns=None, sparse=False, drop_first=False, dtype=None):\\n    \"\"\"\\n    Convert categorical variable into dummy/indicator variables.\\n\\n    Parameters\\n    ----------\\n    data : array-like, Series, or DataFrame\\n        Data of which to get dummy indicators.\\n    prefix : str, list of str, or dict of str, default None\\n        String to append DataFrame column names.\\n        Pass a list with length equal to the number of columns\\n        when calling get_dummies on a DataFrame. Alternatively, `prefix`\\n        can be a dictionary mapping column names to prefixes.\\n    prefix_sep : str, default \\'_\\'\\n        If appending prefix, separator/delimiter to use. Or pass a\\n        list or dictionary as with `prefix`.\\n    dummy_na : bool, default False\\n        Add a column to indicate NaNs, if False NaNs are ignored.\\n    columns : list-like, default None\\n        Column names in the DataFrame to be encoded.\\n        If `columns` is None then all the columns with\\n        `object` or `category` dtype will be converted.\\n    sparse : bool, default False\\n        Whether the dummy-encoded columns should be backed by\\n        a :class:`SparseArray` (True) or a regular NumPy array (False).\\n    drop_first : bool, default False\\n        Whether to get k-1 dummies out of k categorical levels by removing the\\n        first level.\\n\\n        .. versionadded:: 0.18.0\\n\\n    dtype : dtype, default np.uint8\\n        Data type for new columns. Only a single dtype is allowed.\\n\\n        .. versionadded:: 0.23.0\\n\\n    Returns\\n    -------\\n    DataFrame\\n        Dummy-coded data.\\n\\n    See Also\\n    --------\\n    Series.str.get_dummies : Convert Series to dummy codes.\\n\\n    Examples\\n    --------\\n    >>> s = pd.Series(list(\\'abca\\'))\\n\\n    >>> pd.get_dummies(s)\\n       a  b  c\\n    0  1  0  0\\n    1  0  1  0\\n    2  0  0  1\\n    3  1  0  0\\n\\n    >>> s1 = [\\'a\\', \\'b\\', np.nan]\\n\\n    >>> pd.get_dummies(s1)\\n       a  b\\n    0  1  0\\n    1  0  1\\n    2  0  0\\n\\n    >>> pd.get_dummies(s1, dummy_na=True)\\n       a  b  NaN\\n    0  1  0    0\\n    1  0  1    0\\n    2  0  0    1\\n\\n    >>> df = pd.DataFrame({\\'A\\': [\\'a\\', \\'b\\', \\'a\\'], \\'B\\': [\\'b\\', \\'a\\', \\'c\\'],\\n    ...                    \\'C\\': [1, 2, 3]})\\n\\n    >>> pd.get_dummies(df, prefix=[\\'col1\\', \\'col2\\'])\\n       C  col1_a  col1_b  col2_a  col2_b  col2_c\\n    0  1       1       0       0       1       0\\n    1  2       0       1       1       0       0\\n    2  3       1       0       0       0       1\\n\\n    >>> pd.get_dummies(pd.Series(list(\\'abcaa\\')))\\n       a  b  c\\n    0  1  0  0\\n    1  0  1  0\\n    2  0  0  1\\n    3  1  0  0\\n    4  1  0  0\\n\\n    >>> pd.get_dummies(pd.Series(list(\\'abcaa\\')), drop_first=True)\\n       b  c\\n    0  0  0\\n    1  1  0\\n    2  0  1\\n    3  0  0\\n    4  0  0\\n\\n    >>> pd.get_dummies(pd.Series(list(\\'abc\\')), dtype=float)\\n         a    b    c\\n    0  1.0  0.0  0.0\\n    1  0.0  1.0  0.0\\n    2  0.0  0.0  1.0\\n    \"\"\"\\n    from pandas.core.reshape.concat import concat\\n    from itertools import cycle\\n\\n    dtypes_to_encode = [\\'object\\', \\'category\\']\\n\\n    if isinstance(data, DataFrame):\\n        # determine columns being encoded\\n        if columns is None:\\n            data_to_encode = data.select_dtypes(\\n                include=dtypes_to_encode)\\n        else:\\n            data_to_encode = data[columns]\\n\\n        # validate prefixes and separator to avoid silently dropping cols\\n        def check_len(item, name):\\n            len_msg = (\"Length of \\'{name}\\' ({len_item}) did not match the \"\\n                       \"length of the columns being encoded ({len_enc}).\")\\n\\n            if is_list_like(item):\\n                if not len(item) == data_to_encode.shape[1]:\\n                    len_msg = len_msg.format(name=name, len_item=len(item),\\n                                             len_enc=data_to_encode.shape[1])\\n                    raise ValueError(len_msg)\\n\\n        check_len(prefix, \\'prefix\\')\\n        check_len(prefix_sep, \\'prefix_sep\\')\\n\\n        if isinstance(prefix, str):\\n            prefix = cycle([prefix])\\n        if isinstance(prefix, dict):\\n            prefix = [prefix[col] for col in data_to_encode.columns]\\n\\n        if prefix is None:\\n            prefix = data_to_encode.columns\\n\\n        # validate separators\\n        if isinstance(prefix_sep, str):\\n            prefix_sep = cycle([prefix_sep])\\n        elif isinstance(prefix_sep, dict):\\n            prefix_sep = [prefix_sep[col] for col in data_to_encode.columns]\\n\\n        if data_to_encode.shape == data.shape:\\n            # Encoding the entire df, do not prepend any dropped columns\\n            with_dummies = []\\n        elif columns is not None:\\n            # Encoding only cols specified in columns. Get all cols not in\\n            # columns to prepend to result.\\n            with_dummies = [data.drop(columns, axis=1)]\\n        else:\\n            # Encoding only object and category dtype columns. Get remaining\\n            # columns to prepend to result.\\n            with_dummies = [data.select_dtypes(exclude=dtypes_to_encode)]\\n\\n        for (col, pre, sep) in zip(data_to_encode.iteritems(), prefix,\\n                                   prefix_sep):\\n            # col is (column_name, column), use just column data here\\n            dummy = _get_dummies_1d(col[1], prefix=pre, prefix_sep=sep,\\n                                    dummy_na=dummy_na, sparse=sparse,\\n                                    drop_first=drop_first, dtype=dtype)\\n            with_dummies.append(dummy)\\n        result = concat(with_dummies, axis=1)\\n    else:\\n        result = _get_dummies_1d(data, prefix, prefix_sep, dummy_na,\\n                                 sparse=sparse,\\n                                 drop_first=drop_first,\\n                                 dtype=dtype)\\n    return result',\n 'def make_axis_dummies(frame, axis=\\'minor\\', transform=None):\\n    \"\"\"\\n    Construct 1-0 dummy variables corresponding to designated axis\\n    labels\\n\\n    Parameters\\n    ----------\\n    frame : DataFrame\\n    axis : {\\'major\\', \\'minor\\'}, default \\'minor\\'\\n    transform : function, default None\\n        Function to apply to axis labels first. For example, to\\n        get \"day of week\" dummies in a time series regression\\n        you might call::\\n\\n            make_axis_dummies(panel, axis=\\'major\\',\\n                              transform=lambda d: d.weekday())\\n    Returns\\n    -------\\n    dummies : DataFrame\\n        Column names taken from chosen axis\\n    \"\"\"\\n    numbers = {\\'major\\': 0, \\'minor\\': 1}\\n    num = numbers.get(axis, axis)\\n\\n    items = frame.index.levels[num]\\n    codes = frame.index.codes[num]\\n    if transform is not None:\\n        mapped_items = items.map(transform)\\n        codes, items = _factorize_from_iterable(mapped_items.take(codes))\\n\\n    values = np.eye(len(items), dtype=float)\\n    values = values.take(codes, axis=0)\\n\\n    return DataFrame(values, columns=items, index=frame.index)',\n 'def _reorder_for_extension_array_stack(arr, n_rows, n_columns):\\n    \"\"\"\\n    Re-orders the values when stacking multiple extension-arrays.\\n\\n    The indirect stacking method used for EAs requires a followup\\n    take to get the order correct.\\n\\n    Parameters\\n    ----------\\n    arr : ExtensionArray\\n    n_rows, n_columns : int\\n        The number of rows and columns in the original DataFrame.\\n\\n    Returns\\n    -------\\n    taken : ExtensionArray\\n        The original `arr` with elements re-ordered appropriately\\n\\n    Examples\\n    --------\\n    >>> arr = np.array([\\'a\\', \\'b\\', \\'c\\', \\'d\\', \\'e\\', \\'f\\'])\\n    >>> _reorder_for_extension_array_stack(arr, 2, 3)\\n    array([\\'a\\', \\'c\\', \\'e\\', \\'b\\', \\'d\\', \\'f\\'], dtype=\\'<U1\\')\\n\\n    >>> _reorder_for_extension_array_stack(arr, 3, 2)\\n    array([\\'a\\', \\'d\\', \\'b\\', \\'e\\', \\'c\\', \\'f\\'], dtype=\\'<U1\\')\\n    \"\"\"\\n    # final take to get the order correct.\\n    # idx is an indexer like\\n    # [c0r0, c1r0, c2r0, ...,\\n    #  c0r1, c1r1, c2r1, ...]\\n    idx = np.arange(n_rows * n_columns).reshape(n_columns, n_rows).T.ravel()\\n    return arr.take(idx)',\n 'def _split_line(s, parts):\\n    \"\"\"\\n    Parameters\\n    ----------\\n    s: string\\n        Fixed-length string to split\\n    parts: list of (name, length) pairs\\n        Used to break up string, name \\'_\\' will be filtered from output.\\n\\n    Returns\\n    -------\\n    Dict of name:contents of string at given location.\\n    \"\"\"\\n    out = {}\\n    start = 0\\n    for name, length in parts:\\n        out[name] = s[start:start + length].strip()\\n        start += length\\n    del out[\\'_\\']\\n    return out',\n 'def _parse_float_vec(vec):\\n    \"\"\"\\n    Parse a vector of float values representing IBM 8 byte floats into\\n    native 8 byte floats.\\n    \"\"\"\\n\\n    dtype = np.dtype(\\'>u4,>u4\\')\\n    vec1 = vec.view(dtype=dtype)\\n    xport1 = vec1[\\'f0\\']\\n    xport2 = vec1[\\'f1\\']\\n\\n    # Start by setting first half of ieee number to first half of IBM\\n    # number sans exponent\\n    ieee1 = xport1 & 0x00ffffff\\n\\n    # The fraction bit to the left of the binary point in the ieee\\n    # format was set and the number was shifted 0, 1, 2, or 3\\n    # places. This will tell us how to adjust the ibm exponent to be a\\n    # power of 2 ieee exponent and how to shift the fraction bits to\\n    # restore the correct magnitude.\\n    shift = np.zeros(len(vec), dtype=np.uint8)\\n    shift[np.where(xport1 & 0x00200000)] = 1\\n    shift[np.where(xport1 & 0x00400000)] = 2\\n    shift[np.where(xport1 & 0x00800000)] = 3\\n\\n    # shift the ieee number down the correct number of places then\\n    # set the second half of the ieee number to be the second half\\n    # of the ibm number shifted appropriately, ored with the bits\\n    # from the first half that would have been shifted in if we\\n    # could shift a double. All we are worried about are the low\\n    # order 3 bits of the first half since we\\'re only shifting by\\n    # 1, 2, or 3.\\n    ieee1 >>= shift\\n    ieee2 = (xport2 >> shift) | ((xport1 & 0x00000007) << (29 + (3 - shift)))\\n\\n    # clear the 1 bit to the left of the binary point\\n    ieee1 &= 0xffefffff\\n\\n    # set the exponent of the ieee number to be the actual exponent\\n    # plus the shift count + 1023. Or this into the first half of the\\n    # ieee number. The ibm exponent is excess 64 but is adjusted by 65\\n    # since during conversion to ibm format the exponent is\\n    # incremented by 1 and the fraction bits left 4 positions to the\\n    # right of the radix point.  (had to add >> 24 because C treats &\\n    # 0x7f as 0x7f000000 and Python doesn\\'t)\\n    ieee1 |= ((((((xport1 >> 24) & 0x7f) - 65) << 2) +\\n               shift + 1023) << 20) | (xport1 & 0x80000000)\\n\\n    ieee = np.empty((len(ieee1),), dtype=\\'>u4,>u4\\')\\n    ieee[\\'f0\\'] = ieee1\\n    ieee[\\'f1\\'] = ieee2\\n    ieee = ieee.view(dtype=\\'>f8\\')\\n    ieee = ieee.astype(\\'f8\\')\\n\\n    return ieee',\n 'def _record_count(self):\\n        \"\"\"\\n        Get number of records in file.\\n\\n        This is maybe suboptimal because we have to seek to the end of\\n        the file.\\n\\n        Side effect: returns file position to record_start.\\n        \"\"\"\\n\\n        self.filepath_or_buffer.seek(0, 2)\\n        total_records_length = (self.filepath_or_buffer.tell() -\\n                                self.record_start)\\n\\n        if total_records_length % 80 != 0:\\n            warnings.warn(\"xport file may be corrupted\")\\n\\n        if self.record_length > 80:\\n            self.filepath_or_buffer.seek(self.record_start)\\n            return total_records_length // self.record_length\\n\\n        self.filepath_or_buffer.seek(-80, 2)\\n        last_card = self.filepath_or_buffer.read(80)\\n        last_card = np.frombuffer(last_card, dtype=np.uint64)\\n\\n        # 8 byte blank\\n        ix = np.flatnonzero(last_card == 2314885530818453536)\\n\\n        if len(ix) == 0:\\n            tail_pad = 0\\n        else:\\n            tail_pad = 8 * len(ix)\\n\\n        self.filepath_or_buffer.seek(self.record_start)\\n\\n        return (total_records_length - tail_pad) // self.record_length',\n 'def get_chunk(self, size=None):\\n        \"\"\"\\n        Reads lines from Xport file and returns as dataframe\\n\\n        Parameters\\n        ----------\\n        size : int, defaults to None\\n            Number of lines to read.  If None, reads whole file.\\n\\n        Returns\\n        -------\\n        DataFrame\\n        \"\"\"\\n        if size is None:\\n            size = self._chunksize\\n        return self.read(nrows=size)',\n 'def construction_error(tot_items, block_shape, axes, e=None):\\n    \"\"\" raise a helpful message about our construction \"\"\"\\n    passed = tuple(map(int, [tot_items] + list(block_shape)))\\n    # Correcting the user facing error message during dataframe construction\\n    if len(passed) <= 2:\\n        passed = passed[::-1]\\n\\n    implied = tuple(len(ax) for ax in axes)\\n    # Correcting the user facing error message during dataframe construction\\n    if len(implied) <= 2:\\n        implied = implied[::-1]\\n\\n    if passed == implied and e is not None:\\n        raise e\\n    if block_shape[0] == 0:\\n        raise ValueError(\"Empty data passed with indices specified.\")\\n    raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\\n        passed, implied))',\n 'def _simple_blockify(tuples, dtype):\\n    \"\"\" return a single array of a block that has a single dtype; if dtype is\\n    not None, coerce to this dtype\\n    \"\"\"\\n    values, placement = _stack_arrays(tuples, dtype)\\n\\n    # CHECK DTYPE?\\n    if dtype is not None and values.dtype != dtype:  # pragma: no cover\\n        values = values.astype(dtype)\\n\\n    block = make_block(values, placement=placement)\\n    return [block]',\n 'def _multi_blockify(tuples, dtype=None):\\n    \"\"\" return an array of blocks that potentially have different dtypes \"\"\"\\n\\n    # group by dtype\\n    grouper = itertools.groupby(tuples, lambda x: x[2].dtype)\\n\\n    new_blocks = []\\n    for dtype, tup_block in grouper:\\n\\n        values, placement = _stack_arrays(list(tup_block), dtype)\\n\\n        block = make_block(values, placement=placement)\\n        new_blocks.append(block)\\n\\n    return new_blocks',\n 'def _sparse_blockify(tuples, dtype=None):\\n    \"\"\" return an array of blocks that potentially have different dtypes (and\\n    are sparse)\\n    \"\"\"\\n\\n    new_blocks = []\\n    for i, names, array in tuples:\\n        array = _maybe_to_sparse(array)\\n        block = make_block(array, placement=[i])\\n        new_blocks.append(block)\\n\\n    return new_blocks',\n 'def _interleaved_dtype(\\n        blocks: List[Block]\\n) -> Optional[Union[np.dtype, ExtensionDtype]]:\\n    \"\"\"Find the common dtype for `blocks`.\\n\\n    Parameters\\n    ----------\\n    blocks : List[Block]\\n\\n    Returns\\n    -------\\n    dtype : Optional[Union[np.dtype, ExtensionDtype]]\\n        None is returned when `blocks` is empty.\\n    \"\"\"\\n    if not len(blocks):\\n        return None\\n\\n    return find_common_type([b.dtype for b in blocks])',\n 'def _consolidate(blocks):\\n    \"\"\"\\n    Merge blocks having same dtype, exclude non-consolidating blocks\\n    \"\"\"\\n\\n    # sort by _can_consolidate, dtype\\n    gkey = lambda x: x._consolidate_key\\n    grouper = itertools.groupby(sorted(blocks, key=gkey), gkey)\\n\\n    new_blocks = []\\n    for (_can_consolidate, dtype), group_blocks in grouper:\\n        merged_blocks = _merge_blocks(list(group_blocks), dtype=dtype,\\n                                      _can_consolidate=_can_consolidate)\\n        new_blocks = _extend_blocks(merged_blocks, new_blocks)\\n    return new_blocks',\n 'def _compare_or_regex_search(a, b, regex=False):\\n    \"\"\"\\n    Compare two array_like inputs of the same shape or two scalar values\\n\\n    Calls operator.eq or re.search, depending on regex argument. If regex is\\n    True, perform an element-wise regex matching.\\n\\n    Parameters\\n    ----------\\n    a : array_like or scalar\\n    b : array_like or scalar\\n    regex : bool, default False\\n\\n    Returns\\n    -------\\n    mask : array_like of bool\\n    \"\"\"\\n    if not regex:\\n        op = lambda x: operator.eq(x, b)\\n    else:\\n        op = np.vectorize(lambda x: bool(re.search(b, x)) if isinstance(x, str)\\n                          else False)\\n\\n    is_a_array = isinstance(a, np.ndarray)\\n    is_b_array = isinstance(b, np.ndarray)\\n\\n    # numpy deprecation warning to have i8 vs integer comparisons\\n    if is_datetimelike_v_numeric(a, b):\\n        result = False\\n\\n    # numpy deprecation warning if comparing numeric vs string-like\\n    elif is_numeric_v_string_like(a, b):\\n        result = False\\n    else:\\n        result = op(a)\\n\\n    if is_scalar(result) and (is_a_array or is_b_array):\\n        type_names = [type(a).__name__, type(b).__name__]\\n\\n        if is_a_array:\\n            type_names[0] = \\'ndarray(dtype={dtype})\\'.format(dtype=a.dtype)\\n\\n        if is_b_array:\\n            type_names[1] = \\'ndarray(dtype={dtype})\\'.format(dtype=b.dtype)\\n\\n        raise TypeError(\\n            \"Cannot compare types {a!r} and {b!r}\".format(a=type_names[0],\\n                                                          b=type_names[1]))\\n    return result',\n 'def items_overlap_with_suffix(left, lsuffix, right, rsuffix):\\n    \"\"\"\\n    If two indices overlap, add suffixes to overlapping entries.\\n\\n    If corresponding suffix is empty, the entry is simply converted to string.\\n\\n    \"\"\"\\n    to_rename = left.intersection(right)\\n    if len(to_rename) == 0:\\n        return left, right\\n    else:\\n        if not lsuffix and not rsuffix:\\n            raise ValueError(\\'columns overlap but no suffix specified: \\'\\n                             \\'{rename}\\'.format(rename=to_rename))\\n\\n        def renamer(x, suffix):\\n            \"\"\"Rename the left and right indices.\\n\\n            If there is overlap, and suffix is not None, add\\n            suffix, otherwise, leave it as-is.\\n\\n            Parameters\\n            ----------\\n            x : original column name\\n            suffix : str or None\\n\\n            Returns\\n            -------\\n            x : renamed column name\\n            \"\"\"\\n            if x in to_rename and suffix is not None:\\n                return \\'{x}{suffix}\\'.format(x=x, suffix=suffix)\\n            return x\\n\\n        lrenamer = partial(renamer, suffix=lsuffix)\\n        rrenamer = partial(renamer, suffix=rsuffix)\\n\\n        return (_transform_index(left, lrenamer),\\n                _transform_index(right, rrenamer))',\n 'def _transform_index(index, func, level=None):\\n    \"\"\"\\n    Apply function to all values found in index.\\n\\n    This includes transforming multiindex entries separately.\\n    Only apply function to one level of the MultiIndex if level is specified.\\n\\n    \"\"\"\\n    if isinstance(index, MultiIndex):\\n        if level is not None:\\n            items = [tuple(func(y) if i == level else y\\n                           for i, y in enumerate(x)) for x in index]\\n        else:\\n            items = [tuple(func(y) for y in x) for x in index]\\n        return MultiIndex.from_tuples(items, names=index.names)\\n    else:\\n        items = [func(x) for x in index]\\n        return Index(items, name=index.name, tupleize_cols=False)',\n 'def _fast_count_smallints(arr):\\n    \"\"\"Faster version of set(arr) for sequences of small numbers.\"\"\"\\n    counts = np.bincount(arr.astype(np.int_))\\n    nz = counts.nonzero()[0]\\n    return np.c_[nz, counts[nz]]',\n 'def concatenate_block_managers(mgrs_indexers, axes, concat_axis, copy):\\n    \"\"\"\\n    Concatenate block managers into one.\\n\\n    Parameters\\n    ----------\\n    mgrs_indexers : list of (BlockManager, {axis: indexer,...}) tuples\\n    axes : list of Index\\n    concat_axis : int\\n    copy : bool\\n\\n    \"\"\"\\n    concat_plans = [get_mgr_concatenation_plan(mgr, indexers)\\n                    for mgr, indexers in mgrs_indexers]\\n    concat_plan = combine_concat_plans(concat_plans, concat_axis)\\n    blocks = []\\n\\n    for placement, join_units in concat_plan:\\n\\n        if len(join_units) == 1 and not join_units[0].indexers:\\n            b = join_units[0].block\\n            values = b.values\\n            if copy:\\n                values = values.copy()\\n            elif not copy:\\n                values = values.view()\\n            b = b.make_block_same_class(values, placement=placement)\\n        elif is_uniform_join_units(join_units):\\n            b = join_units[0].block.concat_same_type(\\n                [ju.block for ju in join_units], placement=placement)\\n        else:\\n            b = make_block(\\n                concatenate_join_units(join_units, concat_axis, copy=copy),\\n                placement=placement)\\n        blocks.append(b)\\n\\n    return BlockManager(blocks, axes)',\n 'def make_empty(self, axes=None):\\n        \"\"\" return an empty BlockManager with the items axis of len 0 \"\"\"\\n        if axes is None:\\n            axes = [ensure_index([])] + [ensure_index(a)\\n                                         for a in self.axes[1:]]\\n\\n        # preserve dtype if possible\\n        if self.ndim == 1:\\n            blocks = np.array([], dtype=self.array_dtype)\\n        else:\\n            blocks = []\\n        return self.__class__(blocks, axes)',\n 'def rename_axis(self, mapper, axis, copy=True, level=None):\\n        \"\"\"\\n        Rename one of axes.\\n\\n        Parameters\\n        ----------\\n        mapper : unary callable\\n        axis : int\\n        copy : boolean, default True\\n        level : int, default None\\n        \"\"\"\\n        obj = self.copy(deep=copy)\\n        obj.set_axis(axis, _transform_index(self.axes[axis], mapper, level))\\n        return obj',\n 'def _rebuild_blknos_and_blklocs(self):\\n        \"\"\"\\n        Update mgr._blknos / mgr._blklocs.\\n        \"\"\"\\n        new_blknos = np.empty(self.shape[0], dtype=np.int64)\\n        new_blklocs = np.empty(self.shape[0], dtype=np.int64)\\n        new_blknos.fill(-1)\\n        new_blklocs.fill(-1)\\n\\n        for blkno, blk in enumerate(self.blocks):\\n            rl = blk.mgr_locs\\n            new_blknos[rl.indexer] = blkno\\n            new_blklocs[rl.indexer] = np.arange(len(rl))\\n\\n        if (new_blknos == -1).any():\\n            raise AssertionError(\"Gaps in blk ref_locs\")\\n\\n        self._blknos = new_blknos\\n        self._blklocs = new_blklocs',\n 'def _get_counts(self, f):\\n        \"\"\" return a dict of the counts of the function in BlockManager \"\"\"\\n        self._consolidate_inplace()\\n        counts = dict()\\n        for b in self.blocks:\\n            v = f(b)\\n            counts[v] = counts.get(v, 0) + b.shape[0]\\n        return counts',\n 'def apply(self, f, axes=None, filter=None, do_integrity_check=False,\\n              consolidate=True, **kwargs):\\n        \"\"\"\\n        iterate over the blocks, collect and create a new block manager\\n\\n        Parameters\\n        ----------\\n        f : the callable or function name to operate on at the block level\\n        axes : optional (if not supplied, use self.axes)\\n        filter : list, if supplied, only call the block if the filter is in\\n                 the block\\n        do_integrity_check : boolean, default False. Do the block manager\\n            integrity check\\n        consolidate: boolean, default True. Join together blocks having same\\n            dtype\\n\\n        Returns\\n        -------\\n        Block Manager (new object)\\n\\n        \"\"\"\\n\\n        result_blocks = []\\n\\n        # filter kwarg is used in replace-* family of methods\\n        if filter is not None:\\n            filter_locs = set(self.items.get_indexer_for(filter))\\n            if len(filter_locs) == len(self.items):\\n                # All items are included, as if there were no filtering\\n                filter = None\\n            else:\\n                kwargs[\\'filter\\'] = filter_locs\\n\\n        if consolidate:\\n            self._consolidate_inplace()\\n\\n        if f == \\'where\\':\\n            align_copy = True\\n            if kwargs.get(\\'align\\', True):\\n                align_keys = [\\'other\\', \\'cond\\']\\n            else:\\n                align_keys = [\\'cond\\']\\n        elif f == \\'putmask\\':\\n            align_copy = False\\n            if kwargs.get(\\'align\\', True):\\n                align_keys = [\\'new\\', \\'mask\\']\\n            else:\\n                align_keys = [\\'mask\\']\\n        elif f == \\'fillna\\':\\n            # fillna internally does putmask, maybe it\\'s better to do this\\n            # at mgr, not block level?\\n            align_copy = False\\n            align_keys = [\\'value\\']\\n        else:\\n            align_keys = []\\n\\n        # TODO(EA): may interfere with ExtensionBlock.setitem for blocks\\n        # with a .values attribute.\\n        aligned_args = {k: kwargs[k]\\n                        for k in align_keys\\n                        if hasattr(kwargs[k], \\'values\\') and\\n                        not isinstance(kwargs[k], ABCExtensionArray)}\\n\\n        for b in self.blocks:\\n            if filter is not None:\\n                if not b.mgr_locs.isin(filter_locs).any():\\n                    result_blocks.append(b)\\n                    continue\\n\\n            if aligned_args:\\n                b_items = self.items[b.mgr_locs.indexer]\\n\\n                for k, obj in aligned_args.items():\\n                    axis = getattr(obj, \\'_info_axis_number\\', 0)\\n                    kwargs[k] = obj.reindex(b_items, axis=axis,\\n                                            copy=align_copy)\\n\\n            applied = getattr(b, f)(**kwargs)\\n            result_blocks = _extend_blocks(applied, result_blocks)\\n\\n        if len(result_blocks) == 0:\\n            return self.make_empty(axes or self.axes)\\n        bm = self.__class__(result_blocks, axes or self.axes,\\n                            do_integrity_check=do_integrity_check)\\n        bm._consolidate_inplace()\\n        return bm',\n 'def quantile(self, axis=0, consolidate=True, transposed=False,\\n                 interpolation=\\'linear\\', qs=None, numeric_only=None):\\n        \"\"\"\\n        Iterate over blocks applying quantile reduction.\\n        This routine is intended for reduction type operations and\\n        will do inference on the generated blocks.\\n\\n        Parameters\\n        ----------\\n        axis: reduction axis, default 0\\n        consolidate: boolean, default True. Join together blocks having same\\n            dtype\\n        transposed: boolean, default False\\n            we are holding transposed data\\n        interpolation : type of interpolation, default \\'linear\\'\\n        qs : a scalar or list of the quantiles to be computed\\n        numeric_only : ignored\\n\\n        Returns\\n        -------\\n        Block Manager (new object)\\n        \"\"\"\\n\\n        # Series dispatches to DataFrame for quantile, which allows us to\\n        #  simplify some of the code here and in the blocks\\n        assert self.ndim >= 2\\n\\n        if consolidate:\\n            self._consolidate_inplace()\\n\\n        def get_axe(block, qs, axes):\\n            from pandas import Float64Index\\n            if is_list_like(qs):\\n                ax = Float64Index(qs)\\n            elif block.ndim == 1:\\n                ax = Float64Index([qs])\\n            else:\\n                ax = axes[0]\\n            return ax\\n\\n        axes, blocks = [], []\\n        for b in self.blocks:\\n            block = b.quantile(axis=axis, qs=qs, interpolation=interpolation)\\n\\n            axe = get_axe(b, qs, axes=self.axes)\\n\\n            axes.append(axe)\\n            blocks.append(block)\\n\\n        # note that some DatetimeTZ, Categorical are always ndim==1\\n        ndim = {b.ndim for b in blocks}\\n        assert 0 not in ndim, ndim\\n\\n        if 2 in ndim:\\n\\n            new_axes = list(self.axes)\\n\\n            # multiple blocks that are reduced\\n            if len(blocks) > 1:\\n                new_axes[1] = axes[0]\\n\\n                # reset the placement to the original\\n                for b, sb in zip(blocks, self.blocks):\\n                    b.mgr_locs = sb.mgr_locs\\n\\n            else:\\n                new_axes[axis] = Index(np.concatenate(\\n                    [ax.values for ax in axes]))\\n\\n            if transposed:\\n                new_axes = new_axes[::-1]\\n                blocks = [b.make_block(b.values.T,\\n                                       placement=np.arange(b.shape[1])\\n                                       ) for b in blocks]\\n\\n            return self.__class__(blocks, new_axes)\\n\\n        # single block, i.e. ndim == {1}\\n        values = _concat._concat_compat([b.values for b in blocks])\\n\\n        # compute the orderings of our original data\\n        if len(self.blocks) > 1:\\n\\n            indexer = np.empty(len(self.axes[0]), dtype=np.intp)\\n            i = 0\\n            for b in self.blocks:\\n                for j in b.mgr_locs:\\n                    indexer[j] = i\\n                    i = i + 1\\n\\n            values = values.take(indexer)\\n\\n        return SingleBlockManager(\\n            [make_block(values,\\n                        ndim=1,\\n                        placement=np.arange(len(values)))],\\n            axes[0])',\n 'def replace_list(self, src_list, dest_list, inplace=False, regex=False):\\n        \"\"\" do a list replace \"\"\"\\n\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n\\n        # figure out our mask a-priori to avoid repeated replacements\\n        values = self.as_array()\\n\\n        def comp(s, regex=False):\\n            \"\"\"\\n            Generate a bool array by perform an equality check, or perform\\n            an element-wise regular expression matching\\n            \"\"\"\\n            if isna(s):\\n                return isna(values)\\n            if hasattr(s, \\'asm8\\'):\\n                return _compare_or_regex_search(maybe_convert_objects(values),\\n                                                getattr(s, \\'asm8\\'), regex)\\n            return _compare_or_regex_search(values, s, regex)\\n\\n        masks = [comp(s, regex) for i, s in enumerate(src_list)]\\n\\n        result_blocks = []\\n        src_len = len(src_list) - 1\\n        for blk in self.blocks:\\n\\n            # its possible to get multiple result blocks here\\n            # replace ALWAYS will return a list\\n            rb = [blk if inplace else blk.copy()]\\n            for i, (s, d) in enumerate(zip(src_list, dest_list)):\\n                new_rb = []\\n                for b in rb:\\n                    m = masks[i][b.mgr_locs.indexer]\\n                    convert = i == src_len\\n                    result = b._replace_coerce(mask=m, to_replace=s, value=d,\\n                                               inplace=inplace,\\n                                               convert=convert, regex=regex)\\n                    if m.any():\\n                        new_rb = _extend_blocks(result, new_rb)\\n                    else:\\n                        new_rb.append(b)\\n                rb = new_rb\\n            result_blocks.extend(rb)\\n\\n        bm = self.__class__(result_blocks, self.axes)\\n        bm._consolidate_inplace()\\n        return bm',\n 'def get_bool_data(self, copy=False):\\n        \"\"\"\\n        Parameters\\n        ----------\\n        copy : boolean, default False\\n            Whether to copy the blocks\\n        \"\"\"\\n        self._consolidate_inplace()\\n        return self.combine([b for b in self.blocks if b.is_bool], copy)',\n 'def get_numeric_data(self, copy=False):\\n        \"\"\"\\n        Parameters\\n        ----------\\n        copy : boolean, default False\\n            Whether to copy the blocks\\n        \"\"\"\\n        self._consolidate_inplace()\\n        return self.combine([b for b in self.blocks if b.is_numeric], copy)',\n 'def combine(self, blocks, copy=True):\\n        \"\"\" return a new manager with the blocks \"\"\"\\n        if len(blocks) == 0:\\n            return self.make_empty()\\n\\n        # FIXME: optimization potential\\n        indexer = np.sort(np.concatenate([b.mgr_locs.as_array\\n                                          for b in blocks]))\\n        inv_indexer = lib.get_reverse_indexer(indexer, self.shape[0])\\n\\n        new_blocks = []\\n        for b in blocks:\\n            b = b.copy(deep=copy)\\n            b.mgr_locs = algos.take_1d(inv_indexer, b.mgr_locs.as_array,\\n                                       axis=0, allow_fill=False)\\n            new_blocks.append(b)\\n\\n        axes = list(self.axes)\\n        axes[0] = self.items.take(indexer)\\n\\n        return self.__class__(new_blocks, axes, do_integrity_check=False)',\n 'def copy(self, deep=True):\\n        \"\"\"\\n        Make deep or shallow copy of BlockManager\\n\\n        Parameters\\n        ----------\\n        deep : boolean o rstring, default True\\n            If False, return shallow copy (do not copy data)\\n            If \\'all\\', copy data and a deep copy of the index\\n\\n        Returns\\n        -------\\n        copy : BlockManager\\n        \"\"\"\\n        # this preserves the notion of view copying of axes\\n        if deep:\\n            if deep == \\'all\\':\\n                copy = lambda ax: ax.copy(deep=True)\\n            else:\\n                copy = lambda ax: ax.view()\\n            new_axes = [copy(ax) for ax in self.axes]\\n        else:\\n            new_axes = list(self.axes)\\n        return self.apply(\\'copy\\', axes=new_axes, deep=deep,\\n                          do_integrity_check=False)',\n 'def as_array(self, transpose=False, items=None):\\n        \"\"\"Convert the blockmanager data into an numpy array.\\n\\n        Parameters\\n        ----------\\n        transpose : boolean, default False\\n            If True, transpose the return array\\n        items : list of strings or None\\n            Names of block items that will be included in the returned\\n            array. ``None`` means that all block items will be used\\n\\n        Returns\\n        -------\\n        arr : ndarray\\n        \"\"\"\\n        if len(self.blocks) == 0:\\n            arr = np.empty(self.shape, dtype=float)\\n            return arr.transpose() if transpose else arr\\n\\n        if items is not None:\\n            mgr = self.reindex_axis(items, axis=0)\\n        else:\\n            mgr = self\\n\\n        if self._is_single_block and mgr.blocks[0].is_datetimetz:\\n            # TODO(Block.get_values): Make DatetimeTZBlock.get_values\\n            # always be object dtype. Some callers seem to want the\\n            # DatetimeArray (previously DTI)\\n            arr = mgr.blocks[0].get_values(dtype=object)\\n        elif self._is_single_block or not self.is_mixed_type:\\n            arr = np.asarray(mgr.blocks[0].get_values())\\n        else:\\n            arr = mgr._interleave()\\n\\n        return arr.transpose() if transpose else arr',\n 'def _interleave(self):\\n        \"\"\"\\n        Return ndarray from blocks with specified item order\\n        Items must be contained in the blocks\\n        \"\"\"\\n        from pandas.core.dtypes.common import is_sparse\\n        dtype = _interleaved_dtype(self.blocks)\\n\\n        # TODO: https://github.com/pandas-dev/pandas/issues/22791\\n        # Give EAs some input on what happens here. Sparse needs this.\\n        if is_sparse(dtype):\\n            dtype = dtype.subtype\\n        elif is_extension_array_dtype(dtype):\\n            dtype = \\'object\\'\\n\\n        result = np.empty(self.shape, dtype=dtype)\\n\\n        itemmask = np.zeros(self.shape[0])\\n\\n        for blk in self.blocks:\\n            rl = blk.mgr_locs\\n            result[rl.indexer] = blk.get_values(dtype)\\n            itemmask[rl.indexer] = 1\\n\\n        if not itemmask.all():\\n            raise AssertionError(\\'Some items were not contained in blocks\\')\\n\\n        return result',\n 'def to_dict(self, copy=True):\\n        \"\"\"\\n        Return a dict of str(dtype) -> BlockManager\\n\\n        Parameters\\n        ----------\\n        copy : boolean, default True\\n\\n        Returns\\n        -------\\n        values : a dict of dtype -> BlockManager\\n\\n        Notes\\n        -----\\n        This consolidates based on str(dtype)\\n        \"\"\"\\n        self._consolidate_inplace()\\n\\n        bd = {}\\n        for b in self.blocks:\\n            bd.setdefault(str(b.dtype), []).append(b)\\n\\n        return {dtype: self.combine(blocks, copy=copy)\\n                for dtype, blocks in bd.items()}',\n 'def fast_xs(self, loc):\\n        \"\"\"\\n        get a cross sectional for a given location in the\\n        items ; handle dups\\n\\n        return the result, is *could* be a view in the case of a\\n        single block\\n        \"\"\"\\n        if len(self.blocks) == 1:\\n            return self.blocks[0].iget((slice(None), loc))\\n\\n        items = self.items\\n\\n        # non-unique (GH4726)\\n        if not items.is_unique:\\n            result = self._interleave()\\n            if self.ndim == 2:\\n                result = result.T\\n            return result[loc]\\n\\n        # unique\\n        dtype = _interleaved_dtype(self.blocks)\\n\\n        n = len(items)\\n        if is_extension_array_dtype(dtype):\\n            # we\\'ll eventually construct an ExtensionArray.\\n            result = np.empty(n, dtype=object)\\n        else:\\n            result = np.empty(n, dtype=dtype)\\n\\n        for blk in self.blocks:\\n            # Such assignment may incorrectly coerce NaT to None\\n            # result[blk.mgr_locs] = blk._slice((slice(None), loc))\\n            for i, rl in enumerate(blk.mgr_locs):\\n                result[rl] = blk._try_coerce_result(blk.iget((i, loc)))\\n\\n        if is_extension_array_dtype(dtype):\\n            result = dtype.construct_array_type()._from_sequence(\\n                result, dtype=dtype\\n            )\\n\\n        return result',\n 'def consolidate(self):\\n        \"\"\"\\n        Join together blocks having same dtype\\n\\n        Returns\\n        -------\\n        y : BlockManager\\n        \"\"\"\\n        if self.is_consolidated():\\n            return self\\n\\n        bm = self.__class__(self.blocks, self.axes)\\n        bm._is_consolidated = False\\n        bm._consolidate_inplace()\\n        return bm',\n 'def get(self, item, fastpath=True):\\n        \"\"\"\\n        Return values for selected item (ndarray or BlockManager).\\n        \"\"\"\\n        if self.items.is_unique:\\n\\n            if not isna(item):\\n                loc = self.items.get_loc(item)\\n            else:\\n                indexer = np.arange(len(self.items))[isna(self.items)]\\n\\n                # allow a single nan location indexer\\n                if not is_scalar(indexer):\\n                    if len(indexer) == 1:\\n                        loc = indexer.item()\\n                    else:\\n                        raise ValueError(\"cannot label index with a null key\")\\n\\n            return self.iget(loc, fastpath=fastpath)\\n        else:\\n\\n            if isna(item):\\n                raise TypeError(\"cannot label index with a null key\")\\n\\n            indexer = self.items.get_indexer_for([item])\\n            return self.reindex_indexer(new_axis=self.items[indexer],\\n                                        indexer=indexer, axis=0,\\n                                        allow_dups=True)',\n 'def iget(self, i, fastpath=True):\\n        \"\"\"\\n        Return the data as a SingleBlockManager if fastpath=True and possible\\n\\n        Otherwise return as a ndarray\\n        \"\"\"\\n        block = self.blocks[self._blknos[i]]\\n        values = block.iget(self._blklocs[i])\\n        if not fastpath or not block._box_to_block_values or values.ndim != 1:\\n            return values\\n\\n        # fastpath shortcut for select a single-dim from a 2-dim BM\\n        return SingleBlockManager(\\n            [block.make_block_same_class(values,\\n                                         placement=slice(0, len(values)),\\n                                         ndim=1)],\\n            self.axes[1])',\n 'def delete(self, item):\\n        \"\"\"\\n        Delete selected item (items if non-unique) in-place.\\n        \"\"\"\\n        indexer = self.items.get_loc(item)\\n\\n        is_deleted = np.zeros(self.shape[0], dtype=np.bool_)\\n        is_deleted[indexer] = True\\n        ref_loc_offset = -is_deleted.cumsum()\\n\\n        is_blk_deleted = [False] * len(self.blocks)\\n\\n        if isinstance(indexer, int):\\n            affected_start = indexer\\n        else:\\n            affected_start = is_deleted.nonzero()[0][0]\\n\\n        for blkno, _ in _fast_count_smallints(self._blknos[affected_start:]):\\n            blk = self.blocks[blkno]\\n            bml = blk.mgr_locs\\n            blk_del = is_deleted[bml.indexer].nonzero()[0]\\n\\n            if len(blk_del) == len(bml):\\n                is_blk_deleted[blkno] = True\\n                continue\\n            elif len(blk_del) != 0:\\n                blk.delete(blk_del)\\n                bml = blk.mgr_locs\\n\\n            blk.mgr_locs = bml.add(ref_loc_offset[bml.indexer])\\n\\n        # FIXME: use Index.delete as soon as it uses fastpath=True\\n        self.axes[0] = self.items[~is_deleted]\\n        self.blocks = tuple(b for blkno, b in enumerate(self.blocks)\\n                            if not is_blk_deleted[blkno])\\n        self._shape = None\\n        self._rebuild_blknos_and_blklocs()',\n 'def set(self, item, value):\\n        \"\"\"\\n        Set new item in-place. Does not consolidate. Adds new Block if not\\n        contained in the current set of items\\n        \"\"\"\\n        # FIXME: refactor, clearly separate broadcasting & zip-like assignment\\n        #        can prob also fix the various if tests for sparse/categorical\\n\\n        # TODO(EA): Remove an is_extension_ when all extension types satisfy\\n        # the interface\\n        value_is_extension_type = (is_extension_type(value) or\\n                                   is_extension_array_dtype(value))\\n\\n        # categorical/spares/datetimetz\\n        if value_is_extension_type:\\n\\n            def value_getitem(placement):\\n                return value\\n        else:\\n            if value.ndim == self.ndim - 1:\\n                value = _safe_reshape(value, (1,) + value.shape)\\n\\n                def value_getitem(placement):\\n                    return value\\n            else:\\n\\n                def value_getitem(placement):\\n                    return value[placement.indexer]\\n\\n            if value.shape[1:] != self.shape[1:]:\\n                raise AssertionError(\\'Shape of new values must be compatible \\'\\n                                     \\'with manager shape\\')\\n\\n        try:\\n            loc = self.items.get_loc(item)\\n        except KeyError:\\n            # This item wasn\\'t present, just insert at end\\n            self.insert(len(self.items), item, value)\\n            return\\n\\n        if isinstance(loc, int):\\n            loc = [loc]\\n\\n        blknos = self._blknos[loc]\\n        blklocs = self._blklocs[loc].copy()\\n\\n        unfit_mgr_locs = []\\n        unfit_val_locs = []\\n        removed_blknos = []\\n        for blkno, val_locs in libinternals.get_blkno_placements(blknos,\\n                                                                 self.nblocks,\\n                                                                 group=True):\\n            blk = self.blocks[blkno]\\n            blk_locs = blklocs[val_locs.indexer]\\n            if blk.should_store(value):\\n                blk.set(blk_locs, value_getitem(val_locs))\\n            else:\\n                unfit_mgr_locs.append(blk.mgr_locs.as_array[blk_locs])\\n                unfit_val_locs.append(val_locs)\\n\\n                # If all block items are unfit, schedule the block for removal.\\n                if len(val_locs) == len(blk.mgr_locs):\\n                    removed_blknos.append(blkno)\\n                else:\\n                    self._blklocs[blk.mgr_locs.indexer] = -1\\n                    blk.delete(blk_locs)\\n                    self._blklocs[blk.mgr_locs.indexer] = np.arange(len(blk))\\n\\n        if len(removed_blknos):\\n            # Remove blocks & update blknos accordingly\\n            is_deleted = np.zeros(self.nblocks, dtype=np.bool_)\\n            is_deleted[removed_blknos] = True\\n\\n            new_blknos = np.empty(self.nblocks, dtype=np.int64)\\n            new_blknos.fill(-1)\\n            new_blknos[~is_deleted] = np.arange(self.nblocks -\\n                                                len(removed_blknos))\\n            self._blknos = algos.take_1d(new_blknos, self._blknos, axis=0,\\n                                         allow_fill=False)\\n            self.blocks = tuple(blk for i, blk in enumerate(self.blocks)\\n                                if i not in set(removed_blknos))\\n\\n        if unfit_val_locs:\\n            unfit_mgr_locs = np.concatenate(unfit_mgr_locs)\\n            unfit_count = len(unfit_mgr_locs)\\n\\n            new_blocks = []\\n            if value_is_extension_type:\\n                # This code (ab-)uses the fact that sparse blocks contain only\\n                # one item.\\n                new_blocks.extend(\\n                    make_block(values=value.copy(), ndim=self.ndim,\\n                               placement=slice(mgr_loc, mgr_loc + 1))\\n                    for mgr_loc in unfit_mgr_locs)\\n\\n                self._blknos[unfit_mgr_locs] = (np.arange(unfit_count) +\\n                                                len(self.blocks))\\n                self._blklocs[unfit_mgr_locs] = 0\\n\\n            else:\\n                # unfit_val_locs contains BlockPlacement objects\\n                unfit_val_items = unfit_val_locs[0].append(unfit_val_locs[1:])\\n\\n                new_blocks.append(\\n                    make_block(values=value_getitem(unfit_val_items),\\n                               ndim=self.ndim, placement=unfit_mgr_locs))\\n\\n                self._blknos[unfit_mgr_locs] = len(self.blocks)\\n                self._blklocs[unfit_mgr_locs] = np.arange(unfit_count)\\n\\n            self.blocks += tuple(new_blocks)\\n\\n            # Newly created block\\'s dtype may already be present.\\n            self._known_consolidated = False',\n 'def insert(self, loc, item, value, allow_duplicates=False):\\n        \"\"\"\\n        Insert item at selected position.\\n\\n        Parameters\\n        ----------\\n        loc : int\\n        item : hashable\\n        value : array_like\\n        allow_duplicates: bool\\n            If False, trying to insert non-unique item will raise\\n\\n        \"\"\"\\n        if not allow_duplicates and item in self.items:\\n            # Should this be a different kind of error??\\n            raise ValueError(\\'cannot insert {}, already exists\\'.format(item))\\n\\n        if not isinstance(loc, int):\\n            raise TypeError(\"loc must be int\")\\n\\n        # insert to the axis; this could possibly raise a TypeError\\n        new_axis = self.items.insert(loc, item)\\n\\n        block = make_block(values=value, ndim=self.ndim,\\n                           placement=slice(loc, loc + 1))\\n\\n        for blkno, count in _fast_count_smallints(self._blknos[loc:]):\\n            blk = self.blocks[blkno]\\n            if count == len(blk.mgr_locs):\\n                blk.mgr_locs = blk.mgr_locs.add(1)\\n            else:\\n                new_mgr_locs = blk.mgr_locs.as_array.copy()\\n                new_mgr_locs[new_mgr_locs >= loc] += 1\\n                blk.mgr_locs = new_mgr_locs\\n\\n        if loc == self._blklocs.shape[0]:\\n            # np.append is a lot faster, let\\'s use it if we can.\\n            self._blklocs = np.append(self._blklocs, 0)\\n            self._blknos = np.append(self._blknos, len(self.blocks))\\n        else:\\n            self._blklocs = np.insert(self._blklocs, loc, 0)\\n            self._blknos = np.insert(self._blknos, loc, len(self.blocks))\\n\\n        self.axes[0] = new_axis\\n        self.blocks += (block,)\\n        self._shape = None\\n\\n        self._known_consolidated = False\\n\\n        if len(self.blocks) > 100:\\n            self._consolidate_inplace()',\n 'def reindex_axis(self, new_index, axis, method=None, limit=None,\\n                     fill_value=None, copy=True):\\n        \"\"\"\\n        Conform block manager to new index.\\n        \"\"\"\\n        new_index = ensure_index(new_index)\\n        new_index, indexer = self.axes[axis].reindex(new_index, method=method,\\n                                                     limit=limit)\\n\\n        return self.reindex_indexer(new_index, indexer, axis=axis,\\n                                    fill_value=fill_value, copy=copy)',\n 'def reindex_indexer(self, new_axis, indexer, axis, fill_value=None,\\n                        allow_dups=False, copy=True):\\n        \"\"\"\\n        Parameters\\n        ----------\\n        new_axis : Index\\n        indexer : ndarray of int64 or None\\n        axis : int\\n        fill_value : object\\n        allow_dups : bool\\n\\n        pandas-indexer with -1\\'s only.\\n        \"\"\"\\n        if indexer is None:\\n            if new_axis is self.axes[axis] and not copy:\\n                return self\\n\\n            result = self.copy(deep=copy)\\n            result.axes = list(self.axes)\\n            result.axes[axis] = new_axis\\n            return result\\n\\n        self._consolidate_inplace()\\n\\n        # some axes don\\'t allow reindexing with dups\\n        if not allow_dups:\\n            self.axes[axis]._can_reindex(indexer)\\n\\n        if axis >= self.ndim:\\n            raise IndexError(\"Requested axis not found in manager\")\\n\\n        if axis == 0:\\n            new_blocks = self._slice_take_blocks_ax0(indexer,\\n                                                     fill_tuple=(fill_value,))\\n        else:\\n            new_blocks = [blk.take_nd(indexer, axis=axis, fill_tuple=(\\n                fill_value if fill_value is not None else blk.fill_value,))\\n                for blk in self.blocks]\\n\\n        new_axes = list(self.axes)\\n        new_axes[axis] = new_axis\\n        return self.__class__(new_blocks, new_axes)',\n 'def _slice_take_blocks_ax0(self, slice_or_indexer, fill_tuple=None):\\n        \"\"\"\\n        Slice/take blocks along axis=0.\\n\\n        Overloaded for SingleBlock\\n\\n        Returns\\n        -------\\n        new_blocks : list of Block\\n\\n        \"\"\"\\n\\n        allow_fill = fill_tuple is not None\\n\\n        sl_type, slobj, sllen = _preprocess_slice_or_indexer(\\n            slice_or_indexer, self.shape[0], allow_fill=allow_fill)\\n\\n        if self._is_single_block:\\n            blk = self.blocks[0]\\n\\n            if sl_type in (\\'slice\\', \\'mask\\'):\\n                return [blk.getitem_block(slobj, new_mgr_locs=slice(0, sllen))]\\n            elif not allow_fill or self.ndim == 1:\\n                if allow_fill and fill_tuple[0] is None:\\n                    _, fill_value = maybe_promote(blk.dtype)\\n                    fill_tuple = (fill_value, )\\n\\n                return [blk.take_nd(slobj, axis=0,\\n                                    new_mgr_locs=slice(0, sllen),\\n                                    fill_tuple=fill_tuple)]\\n\\n        if sl_type in (\\'slice\\', \\'mask\\'):\\n            blknos = self._blknos[slobj]\\n            blklocs = self._blklocs[slobj]\\n        else:\\n            blknos = algos.take_1d(self._blknos, slobj, fill_value=-1,\\n                                   allow_fill=allow_fill)\\n            blklocs = algos.take_1d(self._blklocs, slobj, fill_value=-1,\\n                                    allow_fill=allow_fill)\\n\\n        # When filling blknos, make sure blknos is updated before appending to\\n        # blocks list, that way new blkno is exactly len(blocks).\\n        #\\n        # FIXME: mgr_groupby_blknos must return mgr_locs in ascending order,\\n        # pytables serialization will break otherwise.\\n        blocks = []\\n        for blkno, mgr_locs in libinternals.get_blkno_placements(blknos,\\n                                                                 self.nblocks,\\n                                                                 group=True):\\n            if blkno == -1:\\n                # If we\\'ve got here, fill_tuple was not None.\\n                fill_value = fill_tuple[0]\\n\\n                blocks.append(self._make_na_block(placement=mgr_locs,\\n                                                  fill_value=fill_value))\\n            else:\\n                blk = self.blocks[blkno]\\n\\n                # Otherwise, slicing along items axis is necessary.\\n                if not blk._can_consolidate:\\n                    # A non-consolidatable block, it\\'s easy, because there\\'s\\n                    # only one item and each mgr loc is a copy of that single\\n                    # item.\\n                    for mgr_loc in mgr_locs:\\n                        newblk = blk.copy(deep=True)\\n                        newblk.mgr_locs = slice(mgr_loc, mgr_loc + 1)\\n                        blocks.append(newblk)\\n\\n                else:\\n                    blocks.append(blk.take_nd(blklocs[mgr_locs.indexer],\\n                                              axis=0, new_mgr_locs=mgr_locs,\\n                                              fill_tuple=None))\\n\\n        return blocks',\n 'def take(self, indexer, axis=1, verify=True, convert=True):\\n        \"\"\"\\n        Take items along any axis.\\n        \"\"\"\\n        self._consolidate_inplace()\\n        indexer = (np.arange(indexer.start, indexer.stop, indexer.step,\\n                             dtype=\\'int64\\')\\n                   if isinstance(indexer, slice)\\n                   else np.asanyarray(indexer, dtype=\\'int64\\'))\\n\\n        n = self.shape[axis]\\n        if convert:\\n            indexer = maybe_convert_indices(indexer, n)\\n\\n        if verify:\\n            if ((indexer == -1) | (indexer >= n)).any():\\n                raise Exception(\\'Indices must be nonzero and less than \\'\\n                                \\'the axis length\\')\\n\\n        new_labels = self.axes[axis].take(indexer)\\n        return self.reindex_indexer(new_axis=new_labels, indexer=indexer,\\n                                    axis=axis, allow_dups=True)',\n 'def unstack(self, unstacker_func, fill_value):\\n        \"\"\"Return a blockmanager with all blocks unstacked.\\n\\n        Parameters\\n        ----------\\n        unstacker_func : callable\\n            A (partially-applied) ``pd.core.reshape._Unstacker`` class.\\n        fill_value : Any\\n            fill_value for newly introduced missing values.\\n\\n        Returns\\n        -------\\n        unstacked : BlockManager\\n        \"\"\"\\n        n_rows = self.shape[-1]\\n        dummy = unstacker_func(np.empty((0, 0)), value_columns=self.items)\\n        new_columns = dummy.get_new_columns()\\n        new_index = dummy.get_new_index()\\n        new_blocks = []\\n        columns_mask = []\\n\\n        for blk in self.blocks:\\n            blocks, mask = blk._unstack(\\n                partial(unstacker_func,\\n                        value_columns=self.items[blk.mgr_locs.indexer]),\\n                new_columns,\\n                n_rows,\\n                fill_value\\n            )\\n\\n            new_blocks.extend(blocks)\\n            columns_mask.extend(mask)\\n\\n        new_columns = new_columns[columns_mask]\\n\\n        bm = BlockManager(new_blocks, [new_columns, new_index])\\n        return bm',\n 'def delete(self, item):\\n        \"\"\"\\n        Delete single item from SingleBlockManager.\\n\\n        Ensures that self.blocks doesn\\'t become empty.\\n        \"\"\"\\n        loc = self.items.get_loc(item)\\n        self._block.delete(loc)\\n        self.axes[0] = self.axes[0].delete(loc)',\n 'def concat(self, to_concat, new_axis):\\n        \"\"\"\\n        Concatenate a list of SingleBlockManagers into a single\\n        SingleBlockManager.\\n\\n        Used for pd.concat of Series objects with axis=0.\\n\\n        Parameters\\n        ----------\\n        to_concat : list of SingleBlockManagers\\n        new_axis : Index of the result\\n\\n        Returns\\n        -------\\n        SingleBlockManager\\n\\n        \"\"\"\\n        non_empties = [x for x in to_concat if len(x) > 0]\\n\\n        # check if all series are of the same block type:\\n        if len(non_empties) > 0:\\n            blocks = [obj.blocks[0] for obj in non_empties]\\n            if len({b.dtype for b in blocks}) == 1:\\n                new_block = blocks[0].concat_same_type(blocks)\\n            else:\\n                values = [x.values for x in blocks]\\n                values = _concat._concat_compat(values)\\n                new_block = make_block(\\n                    values, placement=slice(0, len(values), 1))\\n        else:\\n            values = [x._block.values for x in to_concat]\\n            values = _concat._concat_compat(values)\\n            new_block = make_block(\\n                values, placement=slice(0, len(values), 1))\\n\\n        mgr = SingleBlockManager(new_block, new_axis)\\n        return mgr',\n 'def from_array(cls, arr, index=None, name=None, copy=False,\\n                   fill_value=None, fastpath=False):\\n        \"\"\"Construct SparseSeries from array.\\n\\n        .. deprecated:: 0.23.0\\n            Use the pd.SparseSeries(..) constructor instead.\\n        \"\"\"\\n        warnings.warn(\"\\'from_array\\' is deprecated and will be removed in a \"\\n                      \"future version. Please use the pd.SparseSeries(..) \"\\n                      \"constructor instead.\", FutureWarning, stacklevel=2)\\n        return cls(arr, index=index, name=name, copy=copy,\\n                   fill_value=fill_value, fastpath=fastpath)',\n 'def as_sparse_array(self, kind=None, fill_value=None, copy=False):\\n        \"\"\" return my self as a sparse array, do not copy by default \"\"\"\\n\\n        if fill_value is None:\\n            fill_value = self.fill_value\\n        if kind is None:\\n            kind = self.kind\\n        return SparseArray(self.values, sparse_index=self.sp_index,\\n                           fill_value=fill_value, kind=kind, copy=copy)',\n 'def _reduce(self, op, name, axis=0, skipna=True, numeric_only=None,\\n                filter_type=None, **kwds):\\n        \"\"\" perform a reduction operation \"\"\"\\n        return op(self.get_values(), skipna=skipna, **kwds)',\n 'def _ixs(self, i, axis=0):\\n        \"\"\"\\n        Return the i-th value or values in the SparseSeries by location\\n\\n        Parameters\\n        ----------\\n        i : int, slice, or sequence of integers\\n\\n        Returns\\n        -------\\n        value : scalar (int) or Series (slice, sequence)\\n        \"\"\"\\n        label = self.index[i]\\n        if isinstance(label, Index):\\n            return self.take(i, axis=axis)\\n        else:\\n            return self._get_val_at(i)',\n 'def abs(self):\\n        \"\"\"\\n        Return an object with absolute value taken. Only applicable to objects\\n        that are all numeric\\n\\n        Returns\\n        -------\\n        abs: same type as caller\\n        \"\"\"\\n        return self._constructor(np.abs(self.values),\\n                                 index=self.index).__finalize__(self)',\n 'def get(self, label, default=None):\\n        \"\"\"\\n        Returns value occupying requested label, default to specified\\n        missing value if not present. Analogous to dict.get\\n\\n        Parameters\\n        ----------\\n        label : object\\n            Label value looking for\\n        default : object, optional\\n            Value to return if label not in index\\n\\n        Returns\\n        -------\\n        y : scalar\\n        \"\"\"\\n        if label in self.index:\\n            loc = self.index.get_loc(label)\\n            return self._get_val_at(loc)\\n        else:\\n            return default',\n 'def get_value(self, label, takeable=False):\\n        \"\"\"\\n        Retrieve single value at passed index label\\n\\n        .. deprecated:: 0.21.0\\n\\n        Please use .at[] or .iat[] accessors.\\n\\n        Parameters\\n        ----------\\n        index : label\\n        takeable : interpret the index as indexers, default False\\n\\n        Returns\\n        -------\\n        value : scalar value\\n        \"\"\"\\n        warnings.warn(\"get_value is deprecated and will be removed \"\\n                      \"in a future release. Please use \"\\n                      \".at[] or .iat[] accessors instead\", FutureWarning,\\n                      stacklevel=2)\\n\\n        return self._get_value(label, takeable=takeable)',\n 'def set_value(self, label, value, takeable=False):\\n        \"\"\"\\n        Quickly set single value at passed label. If label is not contained, a\\n        new object is created with the label placed at the end of the result\\n        index\\n\\n        .. deprecated:: 0.21.0\\n\\n        Please use .at[] or .iat[] accessors.\\n\\n        Parameters\\n        ----------\\n        label : object\\n            Partial indexing with MultiIndex not allowed\\n        value : object\\n            Scalar value\\n        takeable : interpret the index as indexers, default False\\n\\n        Notes\\n        -----\\n        This method *always* returns a new object. It is not particularly\\n        efficient but is provided for API compatibility with Series\\n\\n        Returns\\n        -------\\n        series : SparseSeries\\n        \"\"\"\\n        warnings.warn(\"set_value is deprecated and will be removed \"\\n                      \"in a future release. Please use \"\\n                      \".at[] or .iat[] accessors instead\", FutureWarning,\\n                      stacklevel=2)\\n        return self._set_value(label, value, takeable=takeable)',\n 'def to_dense(self):\\n        \"\"\"\\n        Convert SparseSeries to a Series.\\n\\n        Returns\\n        -------\\n        s : Series\\n        \"\"\"\\n        return Series(self.values.to_dense(), index=self.index,\\n                      name=self.name)',\n 'def copy(self, deep=True):\\n        \"\"\"\\n        Make a copy of the SparseSeries. Only the actual sparse values need to\\n        be copied\\n        \"\"\"\\n        # TODO: https://github.com/pandas-dev/pandas/issues/22314\\n        # We skip the block manager till that is resolved.\\n        new_data = self.values.copy(deep=deep)\\n        return self._constructor(new_data, sparse_index=self.sp_index,\\n                                 fill_value=self.fill_value,\\n                                 index=self.index.copy(),\\n                                 name=self.name).__finalize__(self)',\n 'def sparse_reindex(self, new_index):\\n        \"\"\"\\n        Conform sparse values to new SparseIndex\\n\\n        Parameters\\n        ----------\\n        new_index : {BlockIndex, IntIndex}\\n\\n        Returns\\n        -------\\n        reindexed : SparseSeries\\n        \"\"\"\\n        if not isinstance(new_index, splib.SparseIndex):\\n            raise TypeError(\"new index must be a SparseIndex\")\\n        values = self.values\\n        values = values.sp_index.to_int_index().reindex(\\n            values.sp_values.astype(\\'float64\\'), values.fill_value, new_index)\\n        values = SparseArray(values,\\n                             sparse_index=new_index,\\n                             fill_value=self.values.fill_value)\\n        return self._constructor(values, index=self.index).__finalize__(self)',\n 'def cumsum(self, axis=0, *args, **kwargs):\\n        \"\"\"\\n        Cumulative sum of non-NA/null values.\\n\\n        When performing the cumulative summation, any non-NA/null values will\\n        be skipped. The resulting SparseSeries will preserve the locations of\\n        NaN values, but the fill value will be `np.nan` regardless.\\n\\n        Parameters\\n        ----------\\n        axis : {0}\\n\\n        Returns\\n        -------\\n        cumsum : SparseSeries\\n        \"\"\"\\n        nv.validate_cumsum(args, kwargs)\\n        # Validate axis\\n        if axis is not None:\\n            self._get_axis_number(axis)\\n\\n        new_array = self.values.cumsum()\\n\\n        return self._constructor(\\n            new_array, index=self.index,\\n            sparse_index=new_array.sp_index).__finalize__(self)',\n 'def dropna(self, axis=0, inplace=False, **kwargs):\\n        \"\"\"\\n        Analogous to Series.dropna. If fill_value=NaN, returns a dense Series\\n        \"\"\"\\n        # TODO: make more efficient\\n        # Validate axis\\n        self._get_axis_number(axis or 0)\\n        dense_valid = self.to_dense().dropna()\\n        if inplace:\\n            raise NotImplementedError(\"Cannot perform inplace dropna\"\\n                                      \" operations on a SparseSeries\")\\n        if isna(self.fill_value):\\n            return dense_valid\\n        else:\\n            dense_valid = dense_valid[dense_valid != self.fill_value]\\n            return dense_valid.to_sparse(fill_value=self.fill_value)',\n 'def combine_first(self, other):\\n        \"\"\"\\n        Combine Series values, choosing the calling Series\\'s values\\n        first. Result index will be the union of the two indexes\\n\\n        Parameters\\n        ----------\\n        other : Series\\n\\n        Returns\\n        -------\\n        y : Series\\n        \"\"\"\\n        if isinstance(other, SparseSeries):\\n            other = other.to_dense()\\n\\n        dense_combined = self.to_dense().combine_first(other)\\n        return dense_combined.to_sparse(fill_value=self.fill_value)',\n 'def _maybe_cache(arg, format, cache, convert_listlike):\\n    \"\"\"\\n    Create a cache of unique dates from an array of dates\\n\\n    Parameters\\n    ----------\\n    arg : integer, float, string, datetime, list, tuple, 1-d array, Series\\n    format : string\\n        Strftime format to parse time\\n    cache : boolean\\n        True attempts to create a cache of converted values\\n    convert_listlike : function\\n        Conversion function to apply on dates\\n\\n    Returns\\n    -------\\n    cache_array : Series\\n        Cache of converted, unique dates. Can be empty\\n    \"\"\"\\n    from pandas import Series\\n    cache_array = Series()\\n    if cache:\\n        # Perform a quicker unique check\\n        from pandas import Index\\n        unique_dates = Index(arg).unique()\\n        if len(unique_dates) < len(arg):\\n            cache_dates = convert_listlike(unique_dates.to_numpy(),\\n                                           True, format)\\n            cache_array = Series(cache_dates, index=unique_dates)\\n    return cache_array',\n 'def _convert_and_box_cache(arg, cache_array, box, errors, name=None):\\n    \"\"\"\\n    Convert array of dates with a cache and box the result\\n\\n    Parameters\\n    ----------\\n    arg : integer, float, string, datetime, list, tuple, 1-d array, Series\\n    cache_array : Series\\n        Cache of converted, unique dates\\n    box : boolean\\n        True boxes result as an Index-like, False returns an ndarray\\n    errors : string\\n        \\'ignore\\' plus box=True will convert result to Index\\n    name : string, default None\\n        Name for a DatetimeIndex\\n\\n    Returns\\n    -------\\n    result : datetime of converted dates\\n        Returns:\\n\\n        - Index-like if box=True\\n        - ndarray if box=False\\n    \"\"\"\\n    from pandas import Series, DatetimeIndex, Index\\n    result = Series(arg).map(cache_array)\\n    if box:\\n        if errors == \\'ignore\\':\\n            return Index(result, name=name)\\n        else:\\n            return DatetimeIndex(result, name=name)\\n    return result.values',\n 'def _return_parsed_timezone_results(result, timezones, box, tz, name):\\n    \"\"\"\\n    Return results from array_strptime if a %z or %Z directive was passed.\\n\\n    Parameters\\n    ----------\\n    result : ndarray\\n        int64 date representations of the dates\\n    timezones : ndarray\\n        pytz timezone objects\\n    box : boolean\\n        True boxes result as an Index-like, False returns an ndarray\\n    tz : object\\n        None or pytz timezone object\\n    name : string, default None\\n        Name for a DatetimeIndex\\n\\n    Returns\\n    -------\\n    tz_result : ndarray of parsed dates with timezone\\n        Returns:\\n\\n        - Index-like if box=True\\n        - ndarray of Timestamps if box=False\\n\\n    \"\"\"\\n    if tz is not None:\\n        raise ValueError(\"Cannot pass a tz argument when \"\\n                         \"parsing strings with timezone \"\\n                         \"information.\")\\n    tz_results = np.array([Timestamp(res).tz_localize(zone) for res, zone\\n                           in zip(result, timezones)])\\n    if box:\\n        from pandas import Index\\n        return Index(tz_results, name=name)\\n    return tz_results',\n 'def _convert_listlike_datetimes(arg, box, format, name=None, tz=None,\\n                                unit=None, errors=None,\\n                                infer_datetime_format=None, dayfirst=None,\\n                                yearfirst=None, exact=None):\\n    \"\"\"\\n    Helper function for to_datetime. Performs the conversions of 1D listlike\\n    of dates\\n\\n    Parameters\\n    ----------\\n    arg : list, tuple, ndarray, Series, Index\\n        date to be parced\\n    box : boolean\\n        True boxes result as an Index-like, False returns an ndarray\\n    name : object\\n        None or string for the Index name\\n    tz : object\\n        None or \\'utc\\'\\n    unit : string\\n        None or string of the frequency of the passed data\\n    errors : string\\n        error handing behaviors from to_datetime, \\'raise\\', \\'coerce\\', \\'ignore\\'\\n    infer_datetime_format : boolean\\n        inferring format behavior from to_datetime\\n    dayfirst : boolean\\n        dayfirst parsing behavior from to_datetime\\n    yearfirst : boolean\\n        yearfirst parsing behavior from to_datetime\\n    exact : boolean\\n        exact format matching behavior from to_datetime\\n\\n    Returns\\n    -------\\n    ndarray of parsed dates\\n        Returns:\\n\\n        - Index-like if box=True\\n        - ndarray of Timestamps if box=False\\n    \"\"\"\\n    from pandas import DatetimeIndex\\n    from pandas.core.arrays import DatetimeArray\\n    from pandas.core.arrays.datetimes import (\\n        maybe_convert_dtype, objects_to_datetime64ns)\\n\\n    if isinstance(arg, (list, tuple)):\\n        arg = np.array(arg, dtype=\\'O\\')\\n\\n    # these are shortcutable\\n    if is_datetime64tz_dtype(arg):\\n        if not isinstance(arg, (DatetimeArray, DatetimeIndex)):\\n            return DatetimeIndex(arg, tz=tz, name=name)\\n        if tz == \\'utc\\':\\n            arg = arg.tz_convert(None).tz_localize(tz)\\n        return arg\\n\\n    elif is_datetime64_ns_dtype(arg):\\n        if box and not isinstance(arg, (DatetimeArray, DatetimeIndex)):\\n            try:\\n                return DatetimeIndex(arg, tz=tz, name=name)\\n            except ValueError:\\n                pass\\n\\n        return arg\\n\\n    elif unit is not None:\\n        if format is not None:\\n            raise ValueError(\"cannot specify both format and unit\")\\n        arg = getattr(arg, \\'values\\', arg)\\n        result = tslib.array_with_unit_to_datetime(arg, unit,\\n                                                   errors=errors)\\n        if box:\\n            if errors == \\'ignore\\':\\n                from pandas import Index\\n                result = Index(result, name=name)\\n                # GH 23758: We may still need to localize the result with tz\\n                try:\\n                    return result.tz_localize(tz)\\n                except AttributeError:\\n                    return result\\n\\n            return DatetimeIndex(result, tz=tz, name=name)\\n        return result\\n    elif getattr(arg, \\'ndim\\', 1) > 1:\\n        raise TypeError(\\'arg must be a string, datetime, list, tuple, \\'\\n                        \\'1-d array, or Series\\')\\n\\n    # warn if passing timedelta64, raise for PeriodDtype\\n    # NB: this must come after unit transformation\\n    orig_arg = arg\\n    arg, _ = maybe_convert_dtype(arg, copy=False)\\n\\n    arg = ensure_object(arg)\\n    require_iso8601 = False\\n\\n    if infer_datetime_format and format is None:\\n        format = _guess_datetime_format_for_array(arg, dayfirst=dayfirst)\\n\\n    if format is not None:\\n        # There is a special fast-path for iso8601 formatted\\n        # datetime strings, so in those cases don\\'t use the inferred\\n        # format because this path makes process slower in this\\n        # special case\\n        format_is_iso8601 = _format_is_iso(format)\\n        if format_is_iso8601:\\n            require_iso8601 = not infer_datetime_format\\n            format = None\\n\\n    tz_parsed = None\\n    result = None\\n\\n    if format is not None:\\n        try:\\n            # shortcut formatting here\\n            if format == \\'%Y%m%d\\':\\n                try:\\n                    # pass orig_arg as float-dtype may have been converted to\\n                    # datetime64[ns]\\n                    orig_arg = ensure_object(orig_arg)\\n                    result = _attempt_YYYYMMDD(orig_arg, errors=errors)\\n                except (ValueError, TypeError, tslibs.OutOfBoundsDatetime):\\n                    raise ValueError(\"cannot convert the input to \"\\n                                     \"\\'%Y%m%d\\' date format\")\\n\\n            # fallback\\n            if result is None:\\n                try:\\n                    result, timezones = array_strptime(\\n                        arg, format, exact=exact, errors=errors)\\n                    if \\'%Z\\' in format or \\'%z\\' in format:\\n                        return _return_parsed_timezone_results(\\n                            result, timezones, box, tz, name)\\n                except tslibs.OutOfBoundsDatetime:\\n                    if errors == \\'raise\\':\\n                        raise\\n                    elif errors == \\'coerce\\':\\n                        result = np.empty(arg.shape, dtype=\\'M8[ns]\\')\\n                        iresult = result.view(\\'i8\\')\\n                        iresult.fill(tslibs.iNaT)\\n                    else:\\n                        result = arg\\n                except ValueError:\\n                    # if format was inferred, try falling back\\n                    # to array_to_datetime - terminate here\\n                    # for specified formats\\n                    if not infer_datetime_format:\\n                        if errors == \\'raise\\':\\n                            raise\\n                        elif errors == \\'coerce\\':\\n                            result = np.empty(arg.shape, dtype=\\'M8[ns]\\')\\n                            iresult = result.view(\\'i8\\')\\n                            iresult.fill(tslibs.iNaT)\\n                        else:\\n                            result = arg\\n        except ValueError as e:\\n            # Fallback to try to convert datetime objects if timezone-aware\\n            #  datetime objects are found without passing `utc=True`\\n            try:\\n                values, tz = conversion.datetime_to_datetime64(arg)\\n                return DatetimeIndex._simple_new(values, name=name, tz=tz)\\n            except (ValueError, TypeError):\\n                raise e\\n\\n    if result is None:\\n        assert format is None or infer_datetime_format\\n        utc = tz == \\'utc\\'\\n        result, tz_parsed = objects_to_datetime64ns(\\n            arg, dayfirst=dayfirst, yearfirst=yearfirst,\\n            utc=utc, errors=errors, require_iso8601=require_iso8601,\\n            allow_object=True)\\n\\n    if tz_parsed is not None:\\n        if box:\\n            # We can take a shortcut since the datetime64 numpy array\\n            # is in UTC\\n            return DatetimeIndex._simple_new(result, name=name,\\n                                             tz=tz_parsed)\\n        else:\\n            # Convert the datetime64 numpy array to an numpy array\\n            # of datetime objects\\n            result = [Timestamp(ts, tz=tz_parsed).to_pydatetime()\\n                      for ts in result]\\n            return np.array(result, dtype=object)\\n\\n    if box:\\n        # Ensure we return an Index in all cases where box=True\\n        if is_datetime64_dtype(result):\\n            return DatetimeIndex(result, tz=tz, name=name)\\n        elif is_object_dtype(result):\\n            # e.g. an Index of datetime objects\\n            from pandas import Index\\n            return Index(result, name=name)\\n    return result',\n 'def _adjust_to_origin(arg, origin, unit):\\n    \"\"\"\\n    Helper function for to_datetime.\\n    Adjust input argument to the specified origin\\n\\n    Parameters\\n    ----------\\n    arg : list, tuple, ndarray, Series, Index\\n        date to be adjusted\\n    origin : \\'julian\\' or Timestamp\\n        origin offset for the arg\\n    unit : string\\n        passed unit from to_datetime, must be \\'D\\'\\n\\n    Returns\\n    -------\\n    ndarray or scalar of adjusted date(s)\\n    \"\"\"\\n    if origin == \\'julian\\':\\n        original = arg\\n        j0 = Timestamp(0).to_julian_date()\\n        if unit != \\'D\\':\\n            raise ValueError(\"unit must be \\'D\\' for origin=\\'julian\\'\")\\n        try:\\n            arg = arg - j0\\n        except TypeError:\\n            raise ValueError(\"incompatible \\'arg\\' type for given \"\\n                             \"\\'origin\\'=\\'julian\\'\")\\n\\n        # premptively check this for a nice range\\n        j_max = Timestamp.max.to_julian_date() - j0\\n        j_min = Timestamp.min.to_julian_date() - j0\\n        if np.any(arg > j_max) or np.any(arg < j_min):\\n            raise tslibs.OutOfBoundsDatetime(\\n                \"{original} is Out of Bounds for \"\\n                \"origin=\\'julian\\'\".format(original=original))\\n    else:\\n        # arg must be numeric\\n        if not ((is_scalar(arg) and (is_integer(arg) or is_float(arg))) or\\n                is_numeric_dtype(np.asarray(arg))):\\n            raise ValueError(\\n                \"\\'{arg}\\' is not compatible with origin=\\'{origin}\\'; \"\\n                \"it must be numeric with a unit specified \".format(\\n                    arg=arg,\\n                    origin=origin))\\n\\n        # we are going to offset back to unix / epoch time\\n        try:\\n            offset = Timestamp(origin)\\n        except tslibs.OutOfBoundsDatetime:\\n            raise tslibs.OutOfBoundsDatetime(\\n                \"origin {origin} is Out of Bounds\".format(origin=origin))\\n        except ValueError:\\n            raise ValueError(\"origin {origin} cannot be converted \"\\n                             \"to a Timestamp\".format(origin=origin))\\n\\n        if offset.tz is not None:\\n            raise ValueError(\\n                \"origin offset {} must be tz-naive\".format(offset))\\n        offset -= Timestamp(0)\\n\\n        # convert the offset to the unit of the arg\\n        # this should be lossless in terms of precision\\n        offset = offset // tslibs.Timedelta(1, unit=unit)\\n\\n        # scalars & ndarray-like can handle the addition\\n        if is_list_like(arg) and not isinstance(\\n                arg, (ABCSeries, ABCIndexClass, np.ndarray)):\\n            arg = np.asarray(arg)\\n        arg = arg + offset\\n    return arg',\n 'def to_datetime(arg, errors=\\'raise\\', dayfirst=False, yearfirst=False,\\n                utc=None, box=True, format=None, exact=True,\\n                unit=None, infer_datetime_format=False, origin=\\'unix\\',\\n                cache=False):\\n    \"\"\"\\n    Convert argument to datetime.\\n\\n    Parameters\\n    ----------\\n    arg : integer, float, string, datetime, list, tuple, 1-d array, Series\\n\\n        .. versionadded:: 0.18.1\\n\\n           or DataFrame/dict-like\\n\\n    errors : {\\'ignore\\', \\'raise\\', \\'coerce\\'}, default \\'raise\\'\\n\\n        - If \\'raise\\', then invalid parsing will raise an exception\\n        - If \\'coerce\\', then invalid parsing will be set as NaT\\n        - If \\'ignore\\', then invalid parsing will return the input\\n    dayfirst : boolean, default False\\n        Specify a date parse order if `arg` is str or its list-likes.\\n        If True, parses dates with the day first, eg 10/11/12 is parsed as\\n        2012-11-10.\\n        Warning: dayfirst=True is not strict, but will prefer to parse\\n        with day first (this is a known bug, based on dateutil behavior).\\n    yearfirst : boolean, default False\\n        Specify a date parse order if `arg` is str or its list-likes.\\n\\n        - If True parses dates with the year first, eg 10/11/12 is parsed as\\n          2010-11-12.\\n        - If both dayfirst and yearfirst are True, yearfirst is preceded (same\\n          as dateutil).\\n\\n        Warning: yearfirst=True is not strict, but will prefer to parse\\n        with year first (this is a known bug, based on dateutil behavior).\\n\\n        .. versionadded:: 0.16.1\\n\\n    utc : boolean, default None\\n        Return UTC DatetimeIndex if True (converting any tz-aware\\n        datetime.datetime objects as well).\\n    box : boolean, default True\\n\\n        - If True returns a DatetimeIndex or Index-like object\\n        - If False returns ndarray of values.\\n\\n        .. deprecated:: 0.25.0\\n            Use :meth:`.to_numpy` or :meth:`Timestamp.to_datetime64`\\n            instead to get an ndarray of values or numpy.datetime64,\\n            respectively.\\n\\n    format : string, default None\\n        strftime to parse time, eg \"%d/%m/%Y\", note that \"%f\" will parse\\n        all the way up to nanoseconds.\\n        See strftime documentation for more information on choices:\\n        https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\\n    exact : boolean, True by default\\n\\n        - If True, require an exact format match.\\n        - If False, allow the format to match anywhere in the target string.\\n\\n    unit : string, default \\'ns\\'\\n        unit of the arg (D,s,ms,us,ns) denote the unit, which is an\\n        integer or float number. This will be based off the origin.\\n        Example, with unit=\\'ms\\' and origin=\\'unix\\' (the default), this\\n        would calculate the number of milliseconds to the unix epoch start.\\n    infer_datetime_format : boolean, default False\\n        If True and no `format` is given, attempt to infer the format of the\\n        datetime strings, and if it can be inferred, switch to a faster\\n        method of parsing them. In some cases this can increase the parsing\\n        speed by ~5-10x.\\n    origin : scalar, default is \\'unix\\'\\n        Define the reference date. The numeric values would be parsed as number\\n        of units (defined by `unit`) since this reference date.\\n\\n        - If \\'unix\\' (or POSIX) time; origin is set to 1970-01-01.\\n        - If \\'julian\\', unit must be \\'D\\', and origin is set to beginning of\\n          Julian Calendar. Julian day number 0 is assigned to the day starting\\n          at noon on January 1, 4713 BC.\\n        - If Timestamp convertible, origin is set to Timestamp identified by\\n          origin.\\n\\n        .. versionadded:: 0.20.0\\n    cache : boolean, default False\\n        If True, use a cache of unique, converted dates to apply the datetime\\n        conversion. May produce significant speed-up when parsing duplicate\\n        date strings, especially ones with timezone offsets.\\n\\n        .. versionadded:: 0.23.0\\n\\n    Returns\\n    -------\\n    ret : datetime if parsing succeeded.\\n        Return type depends on input:\\n\\n        - list-like: DatetimeIndex\\n        - Series: Series of datetime64 dtype\\n        - scalar: Timestamp\\n\\n        In case when it is not possible to return designated types (e.g. when\\n        any element of input is before Timestamp.min or after Timestamp.max)\\n        return will have datetime.datetime type (or corresponding\\n        array/Series).\\n\\n    See Also\\n    --------\\n    DataFrame.astype : Cast argument to a specified dtype.\\n    to_timedelta : Convert argument to timedelta.\\n\\n    Examples\\n    --------\\n    Assembling a datetime from multiple columns of a DataFrame. The keys can be\\n    common abbreviations like [\\'year\\', \\'month\\', \\'day\\', \\'minute\\', \\'second\\',\\n    \\'ms\\', \\'us\\', \\'ns\\']) or plurals of the same\\n\\n    >>> df = pd.DataFrame({\\'year\\': [2015, 2016],\\n                           \\'month\\': [2, 3],\\n                           \\'day\\': [4, 5]})\\n    >>> pd.to_datetime(df)\\n    0   2015-02-04\\n    1   2016-03-05\\n    dtype: datetime64[ns]\\n\\n    If a date does not meet the `timestamp limitations\\n    <http://pandas.pydata.org/pandas-docs/stable/timeseries.html\\n    #timeseries-timestamp-limits>`_, passing errors=\\'ignore\\'\\n    will return the original input instead of raising any exception.\\n\\n    Passing errors=\\'coerce\\' will force an out-of-bounds date to NaT,\\n    in addition to forcing non-dates (or non-parseable dates) to NaT.\\n\\n    >>> pd.to_datetime(\\'13000101\\', format=\\'%Y%m%d\\', errors=\\'ignore\\')\\n    datetime.datetime(1300, 1, 1, 0, 0)\\n    >>> pd.to_datetime(\\'13000101\\', format=\\'%Y%m%d\\', errors=\\'coerce\\')\\n    NaT\\n\\n    Passing infer_datetime_format=True can often-times speedup a parsing\\n    if its not an ISO8601 format exactly, but in a regular format.\\n\\n    >>> s = pd.Series([\\'3/11/2000\\', \\'3/12/2000\\', \\'3/13/2000\\']*1000)\\n\\n    >>> s.head()\\n    0    3/11/2000\\n    1    3/12/2000\\n    2    3/13/2000\\n    3    3/11/2000\\n    4    3/12/2000\\n    dtype: object\\n\\n    >>> %timeit pd.to_datetime(s,infer_datetime_format=True)\\n    100 loops, best of 3: 10.4 ms per loop\\n\\n    >>> %timeit pd.to_datetime(s,infer_datetime_format=False)\\n    1 loop, best of 3: 471 ms per loop\\n\\n    Using a unix epoch time\\n\\n    >>> pd.to_datetime(1490195805, unit=\\'s\\')\\n    Timestamp(\\'2017-03-22 15:16:45\\')\\n    >>> pd.to_datetime(1490195805433502912, unit=\\'ns\\')\\n    Timestamp(\\'2017-03-22 15:16:45.433502912\\')\\n\\n    .. warning:: For float arg, precision rounding might happen. To prevent\\n        unexpected behavior use a fixed-width exact type.\\n\\n    Using a non-unix epoch origin\\n\\n    >>> pd.to_datetime([1, 2, 3], unit=\\'D\\',\\n                       origin=pd.Timestamp(\\'1960-01-01\\'))\\n    0    1960-01-02\\n    1    1960-01-03\\n    2    1960-01-04\\n    \"\"\"\\n    if arg is None:\\n        return None\\n\\n    if origin != \\'unix\\':\\n        arg = _adjust_to_origin(arg, origin, unit)\\n\\n    tz = \\'utc\\' if utc else None\\n    convert_listlike = partial(_convert_listlike_datetimes, tz=tz, unit=unit,\\n                               dayfirst=dayfirst, yearfirst=yearfirst,\\n                               errors=errors, exact=exact,\\n                               infer_datetime_format=infer_datetime_format)\\n\\n    if isinstance(arg, Timestamp):\\n        result = arg\\n        if tz is not None:\\n            if arg.tz is not None:\\n                result = result.tz_convert(tz)\\n            else:\\n                result = result.tz_localize(tz)\\n    elif isinstance(arg, ABCSeries):\\n        cache_array = _maybe_cache(arg, format, cache, convert_listlike)\\n        if not cache_array.empty:\\n            result = arg.map(cache_array)\\n        else:\\n            values = convert_listlike(arg._values, True, format)\\n            result = arg._constructor(values, index=arg.index, name=arg.name)\\n    elif isinstance(arg, (ABCDataFrame, abc.MutableMapping)):\\n        result = _assemble_from_unit_mappings(arg, errors, box, tz)\\n    elif isinstance(arg, ABCIndexClass):\\n        cache_array = _maybe_cache(arg, format, cache, convert_listlike)\\n        if not cache_array.empty:\\n            result = _convert_and_box_cache(arg, cache_array, box, errors,\\n                                            name=arg.name)\\n        else:\\n            convert_listlike = partial(convert_listlike, name=arg.name)\\n            result = convert_listlike(arg, box, format)\\n    elif is_list_like(arg):\\n        cache_array = _maybe_cache(arg, format, cache, convert_listlike)\\n        if not cache_array.empty:\\n            result = _convert_and_box_cache(arg, cache_array, box, errors)\\n        else:\\n            result = convert_listlike(arg, box, format)\\n    else:\\n        result = convert_listlike(np.array([arg]), box, format)[0]\\n\\n    return result',\n 'def _assemble_from_unit_mappings(arg, errors, box, tz):\\n    \"\"\"\\n    assemble the unit specified fields from the arg (DataFrame)\\n    Return a Series for actual parsing\\n\\n    Parameters\\n    ----------\\n    arg : DataFrame\\n    errors : {\\'ignore\\', \\'raise\\', \\'coerce\\'}, default \\'raise\\'\\n\\n        - If \\'raise\\', then invalid parsing will raise an exception\\n        - If \\'coerce\\', then invalid parsing will be set as NaT\\n        - If \\'ignore\\', then invalid parsing will return the input\\n    box : boolean\\n\\n        - If True, return a DatetimeIndex\\n        - If False, return an array\\n    tz : None or \\'utc\\'\\n\\n    Returns\\n    -------\\n    Series\\n    \"\"\"\\n    from pandas import to_timedelta, to_numeric, DataFrame\\n    arg = DataFrame(arg)\\n    if not arg.columns.is_unique:\\n        raise ValueError(\"cannot assemble with duplicate keys\")\\n\\n    # replace passed unit with _unit_map\\n    def f(value):\\n        if value in _unit_map:\\n            return _unit_map[value]\\n\\n        # m is case significant\\n        if value.lower() in _unit_map:\\n            return _unit_map[value.lower()]\\n\\n        return value\\n\\n    unit = {k: f(k) for k in arg.keys()}\\n    unit_rev = {v: k for k, v in unit.items()}\\n\\n    # we require at least Ymd\\n    required = [\\'year\\', \\'month\\', \\'day\\']\\n    req = sorted(list(set(required) - set(unit_rev.keys())))\\n    if len(req):\\n        raise ValueError(\"to assemble mappings requires at least that \"\\n                         \"[year, month, day] be specified: [{required}] \"\\n                         \"is missing\".format(required=\\',\\'.join(req)))\\n\\n    # keys we don\\'t recognize\\n    excess = sorted(list(set(unit_rev.keys()) - set(_unit_map.values())))\\n    if len(excess):\\n        raise ValueError(\"extra keys have been passed \"\\n                         \"to the datetime assemblage: \"\\n                         \"[{excess}]\".format(excess=\\',\\'.join(excess)))\\n\\n    def coerce(values):\\n        # we allow coercion to if errors allows\\n        values = to_numeric(values, errors=errors)\\n\\n        # prevent overflow in case of int8 or int16\\n        if is_integer_dtype(values):\\n            values = values.astype(\\'int64\\', copy=False)\\n        return values\\n\\n    values = (coerce(arg[unit_rev[\\'year\\']]) * 10000 +\\n              coerce(arg[unit_rev[\\'month\\']]) * 100 +\\n              coerce(arg[unit_rev[\\'day\\']]))\\n    try:\\n        values = to_datetime(values, format=\\'%Y%m%d\\', errors=errors, utc=tz)\\n    except (TypeError, ValueError) as e:\\n        raise ValueError(\"cannot assemble the \"\\n                         \"datetimes: {error}\".format(error=e))\\n\\n    for u in [\\'h\\', \\'m\\', \\'s\\', \\'ms\\', \\'us\\', \\'ns\\']:\\n        value = unit_rev.get(u)\\n        if value is not None and value in arg:\\n            try:\\n                values += to_timedelta(coerce(arg[value]),\\n                                       unit=u,\\n                                       errors=errors)\\n            except (TypeError, ValueError) as e:\\n                raise ValueError(\"cannot assemble the datetimes [{value}]: \"\\n                                 \"{error}\".format(value=value, error=e))\\n    if not box:\\n        return values.values\\n    return values',\n 'def _attempt_YYYYMMDD(arg, errors):\\n    \"\"\"\\n    try to parse the YYYYMMDD/%Y%m%d format, try to deal with NaT-like,\\n    arg is a passed in as an object dtype, but could really be ints/strings\\n    with nan-like/or floats (e.g. with nan)\\n\\n    Parameters\\n    ----------\\n    arg : passed value\\n    errors : \\'raise\\',\\'ignore\\',\\'coerce\\'\\n    \"\"\"\\n\\n    def calc(carg):\\n        # calculate the actual result\\n        carg = carg.astype(object)\\n        parsed = parsing.try_parse_year_month_day(carg / 10000,\\n                                                  carg / 100 % 100,\\n                                                  carg % 100)\\n        return tslib.array_to_datetime(parsed, errors=errors)[0]\\n\\n    def calc_with_mask(carg, mask):\\n        result = np.empty(carg.shape, dtype=\\'M8[ns]\\')\\n        iresult = result.view(\\'i8\\')\\n        iresult[~mask] = tslibs.iNaT\\n\\n        masked_result = calc(carg[mask].astype(np.float64).astype(np.int64))\\n        result[mask] = masked_result.astype(\\'M8[ns]\\')\\n        return result\\n\\n    # try intlike / strings that are ints\\n    try:\\n        return calc(arg.astype(np.int64))\\n    except ValueError:\\n        pass\\n\\n    # a float with actual np.nan\\n    try:\\n        carg = arg.astype(np.float64)\\n        return calc_with_mask(carg, notna(carg))\\n    except ValueError:\\n        pass\\n\\n    # string with NaN-like\\n    try:\\n        mask = ~algorithms.isin(arg, list(tslib.nat_strings))\\n        return calc_with_mask(arg, mask)\\n    except ValueError:\\n        pass\\n\\n    return None',\n 'def to_time(arg, format=None, infer_time_format=False, errors=\\'raise\\'):\\n    \"\"\"\\n    Parse time strings to time objects using fixed strptime formats (\"%H:%M\",\\n    \"%H%M\", \"%I:%M%p\", \"%I%M%p\", \"%H:%M:%S\", \"%H%M%S\", \"%I:%M:%S%p\",\\n    \"%I%M%S%p\")\\n\\n    Use infer_time_format if all the strings are in the same format to speed\\n    up conversion.\\n\\n    Parameters\\n    ----------\\n    arg : string in time format, datetime.time, list, tuple, 1-d array,  Series\\n    format : str, default None\\n        Format used to convert arg into a time object.  If None, fixed formats\\n        are used.\\n    infer_time_format: bool, default False\\n        Infer the time format based on the first non-NaN element.  If all\\n        strings are in the same format, this will speed up conversion.\\n    errors : {\\'ignore\\', \\'raise\\', \\'coerce\\'}, default \\'raise\\'\\n        - If \\'raise\\', then invalid parsing will raise an exception\\n        - If \\'coerce\\', then invalid parsing will be set as None\\n        - If \\'ignore\\', then invalid parsing will return the input\\n\\n    Returns\\n    -------\\n    datetime.time\\n    \"\"\"\\n\\n    def _convert_listlike(arg, format):\\n\\n        if isinstance(arg, (list, tuple)):\\n            arg = np.array(arg, dtype=\\'O\\')\\n\\n        elif getattr(arg, \\'ndim\\', 1) > 1:\\n            raise TypeError(\\'arg must be a string, datetime, list, tuple, \\'\\n                            \\'1-d array, or Series\\')\\n\\n        arg = ensure_object(arg)\\n\\n        if infer_time_format and format is None:\\n            format = _guess_time_format_for_array(arg)\\n\\n        times = []\\n        if format is not None:\\n            for element in arg:\\n                try:\\n                    times.append(datetime.strptime(element, format).time())\\n                except (ValueError, TypeError):\\n                    if errors == \\'raise\\':\\n                        msg = (\"Cannot convert {element} to a time with given \"\\n                               \"format {format}\").format(element=element,\\n                                                         format=format)\\n                        raise ValueError(msg)\\n                    elif errors == \\'ignore\\':\\n                        return arg\\n                    else:\\n                        times.append(None)\\n        else:\\n            formats = _time_formats[:]\\n            format_found = False\\n            for element in arg:\\n                time_object = None\\n                for time_format in formats:\\n                    try:\\n                        time_object = datetime.strptime(element,\\n                                                        time_format).time()\\n                        if not format_found:\\n                            # Put the found format in front\\n                            fmt = formats.pop(formats.index(time_format))\\n                            formats.insert(0, fmt)\\n                            format_found = True\\n                        break\\n                    except (ValueError, TypeError):\\n                        continue\\n\\n                if time_object is not None:\\n                    times.append(time_object)\\n                elif errors == \\'raise\\':\\n                    raise ValueError(\"Cannot convert arg {arg} to \"\\n                                     \"a time\".format(arg=arg))\\n                elif errors == \\'ignore\\':\\n                    return arg\\n                else:\\n                    times.append(None)\\n\\n        return times\\n\\n    if arg is None:\\n        return arg\\n    elif isinstance(arg, time):\\n        return arg\\n    elif isinstance(arg, ABCSeries):\\n        values = _convert_listlike(arg._values, format)\\n        return arg._constructor(values, index=arg.index, name=arg.name)\\n    elif isinstance(arg, ABCIndexClass):\\n        return _convert_listlike(arg, format)\\n    elif is_list_like(arg):\\n        return _convert_listlike(arg, format)\\n\\n    return _convert_listlike(np.array([arg]), format)[0]',\n 'def deprecate(name, alternative, version, alt_name=None,\\n              klass=None, stacklevel=2, msg=None):\\n    \"\"\"\\n    Return a new function that emits a deprecation warning on use.\\n\\n    To use this method for a deprecated function, another function\\n    `alternative` with the same signature must exist. The deprecated\\n    function will emit a deprecation warning, and in the docstring\\n    it will contain the deprecation directive with the provided version\\n    so it can be detected for future removal.\\n\\n    Parameters\\n    ----------\\n    name : str\\n        Name of function to deprecate.\\n    alternative : func\\n        Function to use instead.\\n    version : str\\n        Version of pandas in which the method has been deprecated.\\n    alt_name : str, optional\\n        Name to use in preference of alternative.__name__.\\n    klass : Warning, default FutureWarning\\n    stacklevel : int, default 2\\n    msg : str\\n        The message to display in the warning.\\n        Default is \\'{name} is deprecated. Use {alt_name} instead.\\'\\n    \"\"\"\\n\\n    alt_name = alt_name or alternative.__name__\\n    klass = klass or FutureWarning\\n    warning_msg = msg or \\'{} is deprecated, use {} instead\\'.format(name,\\n                                                                   alt_name)\\n\\n    @wraps(alternative)\\n    def wrapper(*args, **kwargs):\\n        warnings.warn(warning_msg, klass, stacklevel=stacklevel)\\n        return alternative(*args, **kwargs)\\n\\n    # adding deprecated directive to the docstring\\n    msg = msg or \\'Use `{alt_name}` instead.\\'.format(alt_name=alt_name)\\n    doc_error_msg = (\\'deprecate needs a correctly formatted docstring in \\'\\n                     \\'the target function (should have a one liner short \\'\\n                     \\'summary, and opening quotes should be in their own \\'\\n                     \\'line). Found:\\\\n{}\\'.format(alternative.__doc__))\\n\\n    # when python is running in optimized mode (i.e. `-OO`), docstrings are\\n    # removed, so we check that a docstring with correct formatting is used\\n    # but we allow empty docstrings\\n    if alternative.__doc__:\\n        if alternative.__doc__.count(\\'\\\\n\\') < 3:\\n            raise AssertionError(doc_error_msg)\\n        empty1, summary, empty2, doc = alternative.__doc__.split(\\'\\\\n\\', 3)\\n        if empty1 or empty2 and not summary:\\n            raise AssertionError(doc_error_msg)\\n        wrapper.__doc__ = dedent(\"\"\"\\n        {summary}\\n\\n        .. deprecated:: {depr_version}\\n            {depr_msg}\\n\\n        {rest_of_docstring}\"\"\").format(summary=summary.strip(),\\n                                       depr_version=version,\\n                                       depr_msg=msg,\\n                                       rest_of_docstring=dedent(doc))\\n\\n    return wrapper',\n 'def deprecate_kwarg(old_arg_name, new_arg_name, mapping=None, stacklevel=2):\\n    \"\"\"\\n    Decorator to deprecate a keyword argument of a function.\\n\\n    Parameters\\n    ----------\\n    old_arg_name : str\\n        Name of argument in function to deprecate\\n    new_arg_name : str or None\\n        Name of preferred argument in function. Use None to raise warning that\\n        ``old_arg_name`` keyword is deprecated.\\n    mapping : dict or callable\\n        If mapping is present, use it to translate old arguments to\\n        new arguments. A callable must do its own value checking;\\n        values not found in a dict will be forwarded unchanged.\\n\\n    Examples\\n    --------\\n    The following deprecates \\'cols\\', using \\'columns\\' instead\\n\\n    >>> @deprecate_kwarg(old_arg_name=\\'cols\\', new_arg_name=\\'columns\\')\\n    ... def f(columns=\\'\\'):\\n    ...     print(columns)\\n    ...\\n    >>> f(columns=\\'should work ok\\')\\n    should work ok\\n\\n    >>> f(cols=\\'should raise warning\\')\\n    FutureWarning: cols is deprecated, use columns instead\\n      warnings.warn(msg, FutureWarning)\\n    should raise warning\\n\\n    >>> f(cols=\\'should error\\', columns=\"can\\\\\\'t pass do both\")\\n    TypeError: Can only specify \\'cols\\' or \\'columns\\', not both\\n\\n    >>> @deprecate_kwarg(\\'old\\', \\'new\\', {\\'yes\\': True, \\'no\\': False})\\n    ... def f(new=False):\\n    ...     print(\\'yes!\\' if new else \\'no!\\')\\n    ...\\n    >>> f(old=\\'yes\\')\\n    FutureWarning: old=\\'yes\\' is deprecated, use new=True instead\\n      warnings.warn(msg, FutureWarning)\\n    yes!\\n\\n    To raise a warning that a keyword will be removed entirely in the future\\n\\n    >>> @deprecate_kwarg(old_arg_name=\\'cols\\', new_arg_name=None)\\n    ... def f(cols=\\'\\', another_param=\\'\\'):\\n    ...     print(cols)\\n    ...\\n    >>> f(cols=\\'should raise warning\\')\\n    FutureWarning: the \\'cols\\' keyword is deprecated and will be removed in a\\n    future version please takes steps to stop use of \\'cols\\'\\n    should raise warning\\n    >>> f(another_param=\\'should not raise warning\\')\\n    should not raise warning\\n\\n    >>> f(cols=\\'should raise warning\\', another_param=\\'\\')\\n    FutureWarning: the \\'cols\\' keyword is deprecated and will be removed in a\\n    future version please takes steps to stop use of \\'cols\\'\\n    should raise warning\\n    \"\"\"\\n\\n    if mapping is not None and not hasattr(mapping, \\'get\\') and \\\\\\n            not callable(mapping):\\n        raise TypeError(\"mapping from old to new argument values \"\\n                        \"must be dict or callable!\")\\n\\n    def _deprecate_kwarg(func):\\n        @wraps(func)\\n        def wrapper(*args, **kwargs):\\n            old_arg_value = kwargs.pop(old_arg_name, None)\\n\\n            if new_arg_name is None and old_arg_value is not None:\\n                msg = (\\n                    \"the \\'{old_name}\\' keyword is deprecated and will be \"\\n                    \"removed in a future version. \"\\n                    \"Please take steps to stop the use of \\'{old_name}\\'\"\\n                ).format(old_name=old_arg_name)\\n                warnings.warn(msg, FutureWarning, stacklevel=stacklevel)\\n                kwargs[old_arg_name] = old_arg_value\\n                return func(*args, **kwargs)\\n\\n            if old_arg_value is not None:\\n                if mapping is not None:\\n                    if hasattr(mapping, \\'get\\'):\\n                        new_arg_value = mapping.get(old_arg_value,\\n                                                    old_arg_value)\\n                    else:\\n                        new_arg_value = mapping(old_arg_value)\\n                    msg = (\"the {old_name}={old_val!r} keyword is deprecated, \"\\n                           \"use {new_name}={new_val!r} instead\"\\n                           ).format(old_name=old_arg_name,\\n                                    old_val=old_arg_value,\\n                                    new_name=new_arg_name,\\n                                    new_val=new_arg_value)\\n                else:\\n                    new_arg_value = old_arg_value\\n                    msg = (\"the \\'{old_name}\\' keyword is deprecated, \"\\n                           \"use \\'{new_name}\\' instead\"\\n                           ).format(old_name=old_arg_name,\\n                                    new_name=new_arg_name)\\n\\n                warnings.warn(msg, FutureWarning, stacklevel=stacklevel)\\n                if kwargs.get(new_arg_name, None) is not None:\\n                    msg = (\"Can only specify \\'{old_name}\\' or \\'{new_name}\\', \"\\n                           \"not both\").format(old_name=old_arg_name,\\n                                              new_name=new_arg_name)\\n                    raise TypeError(msg)\\n                else:\\n                    kwargs[new_arg_name] = new_arg_value\\n            return func(*args, **kwargs)\\n        return wrapper\\n    return _deprecate_kwarg',\n 'def make_signature(func):\\n    \"\"\"\\n    Returns a tuple containing the paramenter list with defaults\\n    and parameter list.\\n\\n    Examples\\n    --------\\n    >>> def f(a, b, c=2):\\n    >>>     return a * b * c\\n    >>> print(make_signature(f))\\n    ([\\'a\\', \\'b\\', \\'c=2\\'], [\\'a\\', \\'b\\', \\'c\\'])\\n    \"\"\"\\n\\n    spec = inspect.getfullargspec(func)\\n    if spec.defaults is None:\\n        n_wo_defaults = len(spec.args)\\n        defaults = (\\'\\',) * n_wo_defaults\\n    else:\\n        n_wo_defaults = len(spec.args) - len(spec.defaults)\\n        defaults = (\\'\\',) * n_wo_defaults + tuple(spec.defaults)\\n    args = []\\n    for var, default in zip(spec.args, defaults):\\n        args.append(var if default == \\'\\' else var + \\'=\\' + repr(default))\\n    if spec.varargs:\\n        args.append(\\'*\\' + spec.varargs)\\n    if spec.varkw:\\n        args.append(\\'**\\' + spec.varkw)\\n    return args, spec.args',\n 'def period_range(start=None, end=None, periods=None, freq=None, name=None):\\n    \"\"\"\\n    Return a fixed frequency PeriodIndex, with day (calendar) as the default\\n    frequency\\n\\n    Parameters\\n    ----------\\n    start : string or period-like, default None\\n        Left bound for generating periods\\n    end : string or period-like, default None\\n        Right bound for generating periods\\n    periods : integer, default None\\n        Number of periods to generate\\n    freq : string or DateOffset, optional\\n        Frequency alias. By default the freq is taken from `start` or `end`\\n        if those are Period objects. Otherwise, the default is ``\"D\"`` for\\n        daily frequency.\\n\\n    name : string, default None\\n        Name of the resulting PeriodIndex\\n\\n    Returns\\n    -------\\n    prng : PeriodIndex\\n\\n    Notes\\n    -----\\n    Of the three parameters: ``start``, ``end``, and ``periods``, exactly two\\n    must be specified.\\n\\n    To learn more about the frequency strings, please see `this link\\n    <http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases>`__.\\n\\n    Examples\\n    --------\\n\\n    >>> pd.period_range(start=\\'2017-01-01\\', end=\\'2018-01-01\\', freq=\\'M\\')\\n    PeriodIndex([\\'2017-01\\', \\'2017-02\\', \\'2017-03\\', \\'2017-04\\', \\'2017-05\\',\\n                 \\'2017-06\\', \\'2017-06\\', \\'2017-07\\', \\'2017-08\\', \\'2017-09\\',\\n                 \\'2017-10\\', \\'2017-11\\', \\'2017-12\\', \\'2018-01\\'],\\n                dtype=\\'period[M]\\', freq=\\'M\\')\\n\\n    If ``start`` or ``end`` are ``Period`` objects, they will be used as anchor\\n    endpoints for a ``PeriodIndex`` with frequency matching that of the\\n    ``period_range`` constructor.\\n\\n    >>> pd.period_range(start=pd.Period(\\'2017Q1\\', freq=\\'Q\\'),\\n    ...                 end=pd.Period(\\'2017Q2\\', freq=\\'Q\\'), freq=\\'M\\')\\n    PeriodIndex([\\'2017-03\\', \\'2017-04\\', \\'2017-05\\', \\'2017-06\\'],\\n                dtype=\\'period[M]\\', freq=\\'M\\')\\n    \"\"\"\\n    if com.count_not_none(start, end, periods) != 2:\\n        raise ValueError(\\'Of the three parameters: start, end, and periods, \\'\\n                         \\'exactly two must be specified\\')\\n    if freq is None and (not isinstance(start, Period)\\n                         and not isinstance(end, Period)):\\n        freq = \\'D\\'\\n\\n    data, freq = PeriodArray._generate_range(start, end, periods, freq,\\n                                             fields={})\\n    data = PeriodArray(data, freq=freq)\\n    return PeriodIndex(data, name=name)',\n 'def from_range(cls, data, name=None, dtype=None, **kwargs):\\n        \"\"\" Create RangeIndex from a range object. \"\"\"\\n        if not isinstance(data, range):\\n            raise TypeError(\\n                \\'{0}(...) must be called with object coercible to a \\'\\n                \\'range, {1} was passed\\'.format(cls.__name__, repr(data)))\\n\\n        start, stop, step = data.start, data.stop, data.step\\n        return RangeIndex(start, stop, step, dtype=dtype, name=name, **kwargs)',\n 'def _format_attrs(self):\\n        \"\"\"\\n        Return a list of tuples of the (attr, formatted_value)\\n        \"\"\"\\n        attrs = self._get_data_as_items()\\n        if self.name is not None:\\n            attrs.append((\\'name\\', ibase.default_pprint(self.name)))\\n        return attrs',\n 'def min(self, axis=None, skipna=True):\\n        \"\"\"The minimum value of the RangeIndex\"\"\"\\n        nv.validate_minmax_axis(axis)\\n        return self._minmax(\\'min\\')',\n 'def max(self, axis=None, skipna=True):\\n        \"\"\"The maximum value of the RangeIndex\"\"\"\\n        nv.validate_minmax_axis(axis)\\n        return self._minmax(\\'max\\')',\n 'def argsort(self, *args, **kwargs):\\n        \"\"\"\\n        Returns the indices that would sort the index and its\\n        underlying data.\\n\\n        Returns\\n        -------\\n        argsorted : numpy array\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argsort\\n        \"\"\"\\n        nv.validate_argsort(args, kwargs)\\n\\n        if self._step > 0:\\n            return np.arange(len(self))\\n        else:\\n            return np.arange(len(self) - 1, -1, -1)',\n 'def equals(self, other):\\n        \"\"\"\\n        Determines if two Index objects contain the same elements.\\n        \"\"\"\\n        if isinstance(other, RangeIndex):\\n            ls = len(self)\\n            lo = len(other)\\n            return (ls == lo == 0 or\\n                    ls == lo == 1 and\\n                    self._start == other._start or\\n                    ls == lo and\\n                    self._start == other._start and\\n                    self._step == other._step)\\n\\n        return super().equals(other)',\n 'def intersection(self, other, sort=False):\\n        \"\"\"\\n        Form the intersection of two Index objects.\\n\\n        Parameters\\n        ----------\\n        other : Index or array-like\\n        sort : False or None, default False\\n            Sort the resulting index if possible\\n\\n            .. versionadded:: 0.24.0\\n\\n            .. versionchanged:: 0.24.1\\n\\n               Changed the default to ``False`` to match the behaviour\\n               from before 0.24.0.\\n\\n        Returns\\n        -------\\n        intersection : Index\\n        \"\"\"\\n        self._validate_sort_keyword(sort)\\n\\n        if self.equals(other):\\n            return self._get_reconciled_name_object(other)\\n\\n        if not isinstance(other, RangeIndex):\\n            return super().intersection(other, sort=sort)\\n\\n        if not len(self) or not len(other):\\n            return RangeIndex._simple_new(None)\\n\\n        first = self[::-1] if self._step < 0 else self\\n        second = other[::-1] if other._step < 0 else other\\n\\n        # check whether intervals intersect\\n        # deals with in- and decreasing ranges\\n        int_low = max(first._start, second._start)\\n        int_high = min(first._stop, second._stop)\\n        if int_high <= int_low:\\n            return RangeIndex._simple_new(None)\\n\\n        # Method hint: linear Diophantine equation\\n        # solve intersection problem\\n        # performance hint: for identical step sizes, could use\\n        # cheaper alternative\\n        gcd, s, t = first._extended_gcd(first._step, second._step)\\n\\n        # check whether element sets intersect\\n        if (first._start - second._start) % gcd:\\n            return RangeIndex._simple_new(None)\\n\\n        # calculate parameters for the RangeIndex describing the\\n        # intersection disregarding the lower bounds\\n        tmp_start = first._start + (second._start - first._start) * \\\\\\n            first._step // gcd * s\\n        new_step = first._step * second._step // gcd\\n        new_index = RangeIndex._simple_new(tmp_start, int_high, new_step)\\n\\n        # adjust index to limiting interval\\n        new_index._start = new_index._min_fitting_element(int_low)\\n\\n        if (self._step < 0 and other._step < 0) is not (new_index._step < 0):\\n            new_index = new_index[::-1]\\n        if sort is None:\\n            new_index = new_index.sort_values()\\n        return new_index',\n 'def _min_fitting_element(self, lower_limit):\\n        \"\"\"Returns the smallest element greater than or equal to the limit\"\"\"\\n        no_steps = -(-(lower_limit - self._start) // abs(self._step))\\n        return self._start + abs(self._step) * no_steps',\n 'def _max_fitting_element(self, upper_limit):\\n        \"\"\"Returns the largest element smaller than or equal to the limit\"\"\"\\n        no_steps = (upper_limit - self._start) // abs(self._step)\\n        return self._start + abs(self._step) * no_steps',\n 'def _extended_gcd(self, a, b):\\n        \"\"\"\\n        Extended Euclidean algorithms to solve Bezout\\'s identity:\\n           a*x + b*y = gcd(x, y)\\n        Finds one particular solution for x, y: s, t\\n        Returns: gcd, s, t\\n        \"\"\"\\n        s, old_s = 0, 1\\n        t, old_t = 1, 0\\n        r, old_r = b, a\\n        while r:\\n            quotient = old_r // r\\n            old_r, r = r, old_r - quotient * r\\n            old_s, s = s, old_s - quotient * s\\n            old_t, t = t, old_t - quotient * t\\n        return old_r, old_s, old_t',\n 'def union(self, other, sort=None):\\n        \"\"\"\\n        Form the union of two Index objects and sorts if possible\\n\\n        Parameters\\n        ----------\\n        other : Index or array-like\\n\\n        sort : False or None, default None\\n            Whether to sort resulting index. ``sort=None`` returns a\\n            mononotically increasing ``RangeIndex`` if possible or a sorted\\n            ``Int64Index`` if not. ``sort=False`` always returns an\\n            unsorted ``Int64Index``\\n\\n            .. versionadded:: 0.25.0\\n\\n        Returns\\n        -------\\n        union : Index\\n        \"\"\"\\n        self._assert_can_do_setop(other)\\n        if len(other) == 0 or self.equals(other) or len(self) == 0:\\n            return super().union(other, sort=sort)\\n\\n        if isinstance(other, RangeIndex) and sort is None:\\n            start_s, step_s = self._start, self._step\\n            end_s = self._start + self._step * (len(self) - 1)\\n            start_o, step_o = other._start, other._step\\n            end_o = other._start + other._step * (len(other) - 1)\\n            if self._step < 0:\\n                start_s, step_s, end_s = end_s, -step_s, start_s\\n            if other._step < 0:\\n                start_o, step_o, end_o = end_o, -step_o, start_o\\n            if len(self) == 1 and len(other) == 1:\\n                step_s = step_o = abs(self._start - other._start)\\n            elif len(self) == 1:\\n                step_s = step_o\\n            elif len(other) == 1:\\n                step_o = step_s\\n            start_r = min(start_s, start_o)\\n            end_r = max(end_s, end_o)\\n            if step_o == step_s:\\n                if ((start_s - start_o) % step_s == 0 and\\n                        (start_s - end_o) <= step_s and\\n                        (start_o - end_s) <= step_s):\\n                    return RangeIndex(start_r, end_r + step_s, step_s)\\n                if ((step_s % 2 == 0) and\\n                        (abs(start_s - start_o) <= step_s / 2) and\\n                        (abs(end_s - end_o) <= step_s / 2)):\\n                    return RangeIndex(start_r, end_r + step_s / 2, step_s / 2)\\n            elif step_o % step_s == 0:\\n                if ((start_o - start_s) % step_s == 0 and\\n                        (start_o + step_s >= start_s) and\\n                        (end_o - step_s <= end_s)):\\n                    return RangeIndex(start_r, end_r + step_s, step_s)\\n            elif step_s % step_o == 0:\\n                if ((start_s - start_o) % step_o == 0 and\\n                        (start_s + step_o >= start_o) and\\n                        (end_s - step_o <= end_o)):\\n                    return RangeIndex(start_r, end_r + step_o, step_o)\\n\\n        return self._int64index.union(other, sort=sort)',\n 'def _add_numeric_methods_binary(cls):\\n        \"\"\" add in numeric methods, specialized to RangeIndex \"\"\"\\n\\n        def _make_evaluate_binop(op, step=False):\\n            \"\"\"\\n            Parameters\\n            ----------\\n            op : callable that accepts 2 parms\\n                perform the binary op\\n            step : callable, optional, default to False\\n                op to apply to the step parm if not None\\n                if False, use the existing step\\n            \"\"\"\\n\\n            def _evaluate_numeric_binop(self, other):\\n                if isinstance(other, (ABCSeries, ABCDataFrame)):\\n                    return NotImplemented\\n                elif isinstance(other, ABCTimedeltaIndex):\\n                    # Defer to TimedeltaIndex implementation\\n                    return NotImplemented\\n                elif isinstance(other, (timedelta, np.timedelta64)):\\n                    # GH#19333 is_integer evaluated True on timedelta64,\\n                    # so we need to catch these explicitly\\n                    return op(self._int64index, other)\\n                elif is_timedelta64_dtype(other):\\n                    # Must be an np.ndarray; GH#22390\\n                    return op(self._int64index, other)\\n\\n                other = self._validate_for_numeric_binop(other, op)\\n                attrs = self._get_attributes_dict()\\n                attrs = self._maybe_update_attributes(attrs)\\n\\n                left, right = self, other\\n\\n                try:\\n                    # apply if we have an override\\n                    if step:\\n                        with np.errstate(all=\\'ignore\\'):\\n                            rstep = step(left._step, right)\\n\\n                        # we don\\'t have a representable op\\n                        # so return a base index\\n                        if not is_integer(rstep) or not rstep:\\n                            raise ValueError\\n\\n                    else:\\n                        rstep = left._step\\n\\n                    with np.errstate(all=\\'ignore\\'):\\n                        rstart = op(left._start, right)\\n                        rstop = op(left._stop, right)\\n\\n                    result = RangeIndex(rstart,\\n                                        rstop,\\n                                        rstep,\\n                                        **attrs)\\n\\n                    # for compat with numpy / Int64Index\\n                    # even if we can represent as a RangeIndex, return\\n                    # as a Float64Index if we have float-like descriptors\\n                    if not all(is_integer(x) for x in\\n                               [rstart, rstop, rstep]):\\n                        result = result.astype(\\'float64\\')\\n\\n                    return result\\n\\n                except (ValueError, TypeError, ZeroDivisionError):\\n                    # Defer to Int64Index implementation\\n                    return op(self._int64index, other)\\n                    # TODO: Do attrs get handled reliably?\\n\\n            name = \\'__{name}__\\'.format(name=op.__name__)\\n            return compat.set_function_name(_evaluate_numeric_binop, name, cls)\\n\\n        cls.__add__ = _make_evaluate_binop(operator.add)\\n        cls.__radd__ = _make_evaluate_binop(ops.radd)\\n        cls.__sub__ = _make_evaluate_binop(operator.sub)\\n        cls.__rsub__ = _make_evaluate_binop(ops.rsub)\\n        cls.__mul__ = _make_evaluate_binop(operator.mul, step=operator.mul)\\n        cls.__rmul__ = _make_evaluate_binop(ops.rmul, step=ops.rmul)\\n        cls.__truediv__ = _make_evaluate_binop(operator.truediv,\\n                                               step=operator.truediv)\\n        cls.__rtruediv__ = _make_evaluate_binop(ops.rtruediv,\\n                                                step=ops.rtruediv)',\n 'def to_numpy(self, dtype=None, copy=False):\\n        \"\"\"\\n        Convert the PandasArray to a :class:`numpy.ndarray`.\\n\\n        By default, this requires no coercion or copying of data.\\n\\n        Parameters\\n        ----------\\n        dtype : numpy.dtype\\n            The NumPy dtype to pass to :func:`numpy.asarray`.\\n        copy : bool, default False\\n            Whether to copy the underlying data.\\n\\n        Returns\\n        -------\\n        ndarray\\n        \"\"\"\\n        result = np.asarray(self._ndarray, dtype=dtype)\\n        if copy and result is self._ndarray:\\n            result = result.copy()\\n\\n        return result',\n 'def adjoin(space, *lists, **kwargs):\\n    \"\"\"\\n    Glues together two sets of strings using the amount of space requested.\\n    The idea is to prettify.\\n\\n    ----------\\n    space : int\\n        number of spaces for padding\\n    lists : str\\n        list of str which being joined\\n    strlen : callable\\n        function used to calculate the length of each str. Needed for unicode\\n        handling.\\n    justfunc : callable\\n        function used to justify str. Needed for unicode handling.\\n    \"\"\"\\n    strlen = kwargs.pop(\\'strlen\\', len)\\n    justfunc = kwargs.pop(\\'justfunc\\', justify)\\n\\n    out_lines = []\\n    newLists = []\\n    lengths = [max(map(strlen, x)) + space for x in lists[:-1]]\\n    # not the last one\\n    lengths.append(max(map(len, lists[-1])))\\n    maxLen = max(map(len, lists))\\n    for i, lst in enumerate(lists):\\n        nl = justfunc(lst, lengths[i], mode=\\'left\\')\\n        nl.extend([\\' \\' * lengths[i]] * (maxLen - len(lst)))\\n        newLists.append(nl)\\n    toJoin = zip(*newLists)\\n    for lines in toJoin:\\n        out_lines.append(_join_unicode(lines))\\n    return _join_unicode(out_lines, sep=\\'\\\\n\\')',\n 'def justify(texts, max_len, mode=\\'right\\'):\\n    \"\"\"\\n    Perform ljust, center, rjust against string or list-like\\n    \"\"\"\\n    if mode == \\'left\\':\\n        return [x.ljust(max_len) for x in texts]\\n    elif mode == \\'center\\':\\n        return [x.center(max_len) for x in texts]\\n    else:\\n        return [x.rjust(max_len) for x in texts]',\n 'def _pprint_seq(seq, _nest_lvl=0, max_seq_items=None, **kwds):\\n    \"\"\"\\n    internal. pprinter for iterables. you should probably use pprint_thing()\\n    rather then calling this directly.\\n\\n    bounds length of printed sequence, depending on options\\n    \"\"\"\\n    if isinstance(seq, set):\\n        fmt = \"{{{body}}}\"\\n    else:\\n        fmt = \"[{body}]\" if hasattr(seq, \\'__setitem__\\') else \"({body})\"\\n\\n    if max_seq_items is False:\\n        nitems = len(seq)\\n    else:\\n        nitems = max_seq_items or get_option(\"max_seq_items\") or len(seq)\\n\\n    s = iter(seq)\\n    # handle sets, no slicing\\n    r = [pprint_thing(next(s),\\n                      _nest_lvl + 1, max_seq_items=max_seq_items, **kwds)\\n         for i in range(min(nitems, len(seq)))]\\n    body = \", \".join(r)\\n\\n    if nitems < len(seq):\\n        body += \", ...\"\\n    elif isinstance(seq, tuple) and len(seq) == 1:\\n        body += \\',\\'\\n\\n    return fmt.format(body=body)',\n 'def _pprint_dict(seq, _nest_lvl=0, max_seq_items=None, **kwds):\\n    \"\"\"\\n    internal. pprinter for iterables. you should probably use pprint_thing()\\n    rather then calling this directly.\\n    \"\"\"\\n    fmt = \"{{{things}}}\"\\n    pairs = []\\n\\n    pfmt = \"{key}: {val}\"\\n\\n    if max_seq_items is False:\\n        nitems = len(seq)\\n    else:\\n        nitems = max_seq_items or get_option(\"max_seq_items\") or len(seq)\\n\\n    for k, v in list(seq.items())[:nitems]:\\n        pairs.append(\\n            pfmt.format(\\n                key=pprint_thing(k, _nest_lvl + 1,\\n                                 max_seq_items=max_seq_items, **kwds),\\n                val=pprint_thing(v, _nest_lvl + 1,\\n                                 max_seq_items=max_seq_items, **kwds)))\\n\\n    if nitems < len(seq):\\n        return fmt.format(things=\", \".join(pairs) + \", ...\")\\n    else:\\n        return fmt.format(things=\", \".join(pairs))',\n 'def pprint_thing(thing, _nest_lvl=0, escape_chars=None, default_escapes=False,\\n                 quote_strings=False, max_seq_items=None):\\n    \"\"\"\\n    This function is the sanctioned way of converting objects\\n    to a unicode representation.\\n\\n    properly handles nested sequences containing unicode strings\\n    (unicode(object) does not)\\n\\n    Parameters\\n    ----------\\n    thing : anything to be formatted\\n    _nest_lvl : internal use only. pprint_thing() is mutually-recursive\\n        with pprint_sequence, this argument is used to keep track of the\\n        current nesting level, and limit it.\\n    escape_chars : list or dict, optional\\n        Characters to escape. If a dict is passed the values are the\\n        replacements\\n    default_escapes : bool, default False\\n        Whether the input escape characters replaces or adds to the defaults\\n    max_seq_items : False, int, default None\\n        Pass thru to other pretty printers to limit sequence printing\\n\\n    Returns\\n    -------\\n    result - unicode str\\n\\n    \"\"\"\\n\\n    def as_escaped_unicode(thing, escape_chars=escape_chars):\\n        # Unicode is fine, else we try to decode using utf-8 and \\'replace\\'\\n        # if that\\'s not it either, we have no way of knowing and the user\\n        # should deal with it himself.\\n\\n        try:\\n            result = str(thing)  # we should try this first\\n        except UnicodeDecodeError:\\n            # either utf-8 or we replace errors\\n            result = str(thing).decode(\\'utf-8\\', \"replace\")\\n\\n        translate = {\\'\\\\t\\': r\\'\\\\t\\', \\'\\\\n\\': r\\'\\\\n\\', \\'\\\\r\\': r\\'\\\\r\\', }\\n        if isinstance(escape_chars, dict):\\n            if default_escapes:\\n                translate.update(escape_chars)\\n            else:\\n                translate = escape_chars\\n            escape_chars = list(escape_chars.keys())\\n        else:\\n            escape_chars = escape_chars or tuple()\\n        for c in escape_chars:\\n            result = result.replace(c, translate[c])\\n\\n        return str(result)\\n\\n    if hasattr(thing, \\'__next__\\'):\\n        return str(thing)\\n    elif (isinstance(thing, dict) and\\n          _nest_lvl < get_option(\"display.pprint_nest_depth\")):\\n        result = _pprint_dict(thing, _nest_lvl, quote_strings=True,\\n                              max_seq_items=max_seq_items)\\n    elif (is_sequence(thing) and\\n          _nest_lvl < get_option(\"display.pprint_nest_depth\")):\\n        result = _pprint_seq(thing, _nest_lvl, escape_chars=escape_chars,\\n                             quote_strings=quote_strings,\\n                             max_seq_items=max_seq_items)\\n    elif isinstance(thing, str) and quote_strings:\\n        result = \"\\'{thing}\\'\".format(thing=as_escaped_unicode(thing))\\n    else:\\n        result = as_escaped_unicode(thing)\\n\\n    return str(result)',\n 'def format_object_summary(obj, formatter, is_justify=True, name=None,\\n                          indent_for_name=True):\\n    \"\"\"\\n    Return the formatted obj as a unicode string\\n\\n    Parameters\\n    ----------\\n    obj : object\\n        must be iterable and support __getitem__\\n    formatter : callable\\n        string formatter for an element\\n    is_justify : boolean\\n        should justify the display\\n    name : name, optional\\n        defaults to the class name of the obj\\n    indent_for_name : bool, default True\\n        Whether subsequent lines should be be indented to\\n        align with the name.\\n\\n    Returns\\n    -------\\n    summary string\\n\\n    \"\"\"\\n    from pandas.io.formats.console import get_console_size\\n    from pandas.io.formats.format import _get_adjustment\\n\\n    display_width, _ = get_console_size()\\n    if display_width is None:\\n        display_width = get_option(\\'display.width\\') or 80\\n    if name is None:\\n        name = obj.__class__.__name__\\n\\n    if indent_for_name:\\n        name_len = len(name)\\n        space1 = \"\\\\n%s\" % (\\' \\' * (name_len + 1))\\n        space2 = \"\\\\n%s\" % (\\' \\' * (name_len + 2))\\n    else:\\n        space1 = \"\\\\n\"\\n        space2 = \"\\\\n \"  # space for the opening \\'[\\'\\n\\n    n = len(obj)\\n    sep = \\',\\'\\n    max_seq_items = get_option(\\'display.max_seq_items\\') or n\\n\\n    # are we a truncated display\\n    is_truncated = n > max_seq_items\\n\\n    # adj can optionally handle unicode eastern asian width\\n    adj = _get_adjustment()\\n\\n    def _extend_line(s, line, value, display_width, next_line_prefix):\\n\\n        if (adj.len(line.rstrip()) + adj.len(value.rstrip()) >=\\n                display_width):\\n            s += line.rstrip()\\n            line = next_line_prefix\\n        line += value\\n        return s, line\\n\\n    def best_len(values):\\n        if values:\\n            return max(adj.len(x) for x in values)\\n        else:\\n            return 0\\n\\n    close = \\', \\'\\n\\n    if n == 0:\\n        summary = \\'[]{}\\'.format(close)\\n    elif n == 1:\\n        first = formatter(obj[0])\\n        summary = \\'[{}]{}\\'.format(first, close)\\n    elif n == 2:\\n        first = formatter(obj[0])\\n        last = formatter(obj[-1])\\n        summary = \\'[{}, {}]{}\\'.format(first, last, close)\\n    else:\\n\\n        if n > max_seq_items:\\n            n = min(max_seq_items // 2, 10)\\n            head = [formatter(x) for x in obj[:n]]\\n            tail = [formatter(x) for x in obj[-n:]]\\n        else:\\n            head = []\\n            tail = [formatter(x) for x in obj]\\n\\n        # adjust all values to max length if needed\\n        if is_justify:\\n\\n            # however, if we are not truncated and we are only a single\\n            # line, then don\\'t justify\\n            if (is_truncated or\\n                    not (len(\\', \\'.join(head)) < display_width and\\n                         len(\\', \\'.join(tail)) < display_width)):\\n                max_len = max(best_len(head), best_len(tail))\\n                head = [x.rjust(max_len) for x in head]\\n                tail = [x.rjust(max_len) for x in tail]\\n\\n        summary = \"\"\\n        line = space2\\n\\n        for i in range(len(head)):\\n            word = head[i] + sep + \\' \\'\\n            summary, line = _extend_line(summary, line, word,\\n                                         display_width, space2)\\n\\n        if is_truncated:\\n            # remove trailing space of last line\\n            summary += line.rstrip() + space2 + \\'...\\'\\n            line = space2\\n\\n        for i in range(len(tail) - 1):\\n            word = tail[i] + sep + \\' \\'\\n            summary, line = _extend_line(summary, line, word,\\n                                         display_width, space2)\\n\\n        # last value: no sep added + 1 space of width used for trailing \\',\\'\\n        summary, line = _extend_line(summary, line, tail[-1],\\n                                     display_width - 2, space2)\\n        summary += line\\n\\n        # right now close is either \\'\\' or \\', \\'\\n        # Now we want to include the \\']\\', but not the maybe space.\\n        close = \\']\\' + close.rstrip(\\' \\')\\n        summary += close\\n\\n        if len(summary) > (display_width):\\n            summary += space1\\n        else:  # one row\\n            summary += \\' \\'\\n\\n        # remove initial space\\n        summary = \\'[\\' + summary[len(space2):]\\n\\n    return summary',\n 'def format_object_attrs(obj):\\n    \"\"\"\\n    Return a list of tuples of the (attr, formatted_value)\\n    for common attrs, including dtype, name, length\\n\\n    Parameters\\n    ----------\\n    obj : object\\n        must be iterable\\n\\n    Returns\\n    -------\\n    list\\n\\n    \"\"\"\\n    attrs = []\\n    if hasattr(obj, \\'dtype\\'):\\n        attrs.append((\\'dtype\\', \"\\'{}\\'\".format(obj.dtype)))\\n    if getattr(obj, \\'name\\', None) is not None:\\n        attrs.append((\\'name\\', default_pprint(obj.name)))\\n    max_seq_items = get_option(\\'display.max_seq_items\\') or len(obj)\\n    if len(obj) > max_seq_items:\\n        attrs.append((\\'length\\', len(obj)))\\n    return attrs',\n 'def read_gbq(query, project_id=None, index_col=None, col_order=None,\\n             reauth=False, auth_local_webserver=False, dialect=None,\\n             location=None, configuration=None, credentials=None,\\n             use_bqstorage_api=None, private_key=None, verbose=None):\\n    \"\"\"\\n    Load data from Google BigQuery.\\n\\n    This function requires the `pandas-gbq package\\n    <https://pandas-gbq.readthedocs.io>`__.\\n\\n    See the `How to authenticate with Google BigQuery\\n    <https://pandas-gbq.readthedocs.io/en/latest/howto/authentication.html>`__\\n    guide for authentication instructions.\\n\\n    Parameters\\n    ----------\\n    query : str\\n        SQL-Like Query to return data values.\\n    project_id : str, optional\\n        Google BigQuery Account project ID. Optional when available from\\n        the environment.\\n    index_col : str, optional\\n        Name of result column to use for index in results DataFrame.\\n    col_order : list(str), optional\\n        List of BigQuery column names in the desired order for results\\n        DataFrame.\\n    reauth : boolean, default False\\n        Force Google BigQuery to re-authenticate the user. This is useful\\n        if multiple accounts are used.\\n    auth_local_webserver : boolean, default False\\n        Use the `local webserver flow`_ instead of the `console flow`_\\n        when getting user credentials.\\n\\n        .. _local webserver flow:\\n            http://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_local_server\\n        .. _console flow:\\n            http://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_console\\n\\n        *New in version 0.2.0 of pandas-gbq*.\\n    dialect : str, default \\'legacy\\'\\n        Note: The default value is changing to \\'standard\\' in a future verion.\\n\\n        SQL syntax dialect to use. Value can be one of:\\n\\n        ``\\'legacy\\'``\\n            Use BigQuery\\'s legacy SQL dialect. For more information see\\n            `BigQuery Legacy SQL Reference\\n            <https://cloud.google.com/bigquery/docs/reference/legacy-sql>`__.\\n        ``\\'standard\\'``\\n            Use BigQuery\\'s standard SQL, which is\\n            compliant with the SQL 2011 standard. For more information\\n            see `BigQuery Standard SQL Reference\\n            <https://cloud.google.com/bigquery/docs/reference/standard-sql/>`__.\\n\\n        .. versionchanged:: 0.24.0\\n    location : str, optional\\n        Location where the query job should run. See the `BigQuery locations\\n        documentation\\n        <https://cloud.google.com/bigquery/docs/dataset-locations>`__ for a\\n        list of available locations. The location must match that of any\\n        datasets used in the query.\\n\\n        *New in version 0.5.0 of pandas-gbq*.\\n    configuration : dict, optional\\n        Query config parameters for job processing.\\n        For example:\\n\\n            configuration = {\\'query\\': {\\'useQueryCache\\': False}}\\n\\n        For more information see `BigQuery REST API Reference\\n        <https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.query>`__.\\n    credentials : google.auth.credentials.Credentials, optional\\n        Credentials for accessing Google APIs. Use this parameter to override\\n        default credentials, such as to use Compute Engine\\n        :class:`google.auth.compute_engine.Credentials` or Service Account\\n        :class:`google.oauth2.service_account.Credentials` directly.\\n\\n        *New in version 0.8.0 of pandas-gbq*.\\n\\n        .. versionadded:: 0.24.0\\n    use_bqstorage_api : bool, default False\\n        Use the `BigQuery Storage API\\n        <https://cloud.google.com/bigquery/docs/reference/storage/>`__ to\\n        download query results quickly, but at an increased cost. To use this\\n        API, first `enable it in the Cloud Console\\n        <https://console.cloud.google.com/apis/library/bigquerystorage.googleapis.com>`__.\\n        You must also have the `bigquery.readsessions.create\\n        <https://cloud.google.com/bigquery/docs/access-control#roles>`__\\n        permission on the project you are billing queries to.\\n\\n        This feature requires version 0.10.0 or later of the ``pandas-gbq``\\n        package. It also requires the ``google-cloud-bigquery-storage`` and\\n        ``fastavro`` packages.\\n\\n        .. versionadded:: 0.25.0\\n    private_key : str, deprecated\\n        Deprecated in pandas-gbq version 0.8.0. Use the ``credentials``\\n        parameter and\\n        :func:`google.oauth2.service_account.Credentials.from_service_account_info`\\n        or\\n        :func:`google.oauth2.service_account.Credentials.from_service_account_file`\\n        instead.\\n\\n        Service account private key in JSON format. Can be file path\\n        or string contents. This is useful for remote server\\n        authentication (eg. Jupyter/IPython notebook on remote host).\\n    verbose : None, deprecated\\n        Deprecated in pandas-gbq version 0.4.0. Use the `logging module to\\n        adjust verbosity instead\\n        <https://pandas-gbq.readthedocs.io/en/latest/intro.html#logging>`__.\\n\\n    Returns\\n    -------\\n    df: DataFrame\\n        DataFrame representing results of query.\\n\\n    See Also\\n    --------\\n    pandas_gbq.read_gbq : This function in the pandas-gbq library.\\n    DataFrame.to_gbq : Write a DataFrame to Google BigQuery.\\n    \"\"\"\\n    pandas_gbq = _try_import()\\n\\n    kwargs = {}\\n\\n    # START: new kwargs.  Don\\'t populate unless explicitly set.\\n    if use_bqstorage_api is not None:\\n        kwargs[\"use_bqstorage_api\"] = use_bqstorage_api\\n    # END: new kwargs\\n\\n    # START: deprecated kwargs.  Don\\'t populate unless explicitly set.\\n    if verbose is not None:\\n        kwargs[\"verbose\"] = verbose\\n\\n    if private_key is not None:\\n        kwargs[\"private_key\"] = private_key\\n    # END: deprecated kwargs\\n\\n    return pandas_gbq.read_gbq(\\n        query, project_id=project_id, index_col=index_col,\\n        col_order=col_order, reauth=reauth,\\n        auth_local_webserver=auth_local_webserver, dialect=dialect,\\n        location=location, configuration=configuration,\\n        credentials=credentials, **kwargs)',\n 'def scatter_matrix(frame, alpha=0.5, figsize=None, ax=None, grid=False,\\n                   diagonal=\\'hist\\', marker=\\'.\\', density_kwds=None,\\n                   hist_kwds=None, range_padding=0.05, **kwds):\\n    \"\"\"\\n    Draw a matrix of scatter plots.\\n\\n    Parameters\\n    ----------\\n    frame : DataFrame\\n    alpha : float, optional\\n        amount of transparency applied\\n    figsize : (float,float), optional\\n        a tuple (width, height) in inches\\n    ax : Matplotlib axis object, optional\\n    grid : bool, optional\\n        setting this to True will show the grid\\n    diagonal : {\\'hist\\', \\'kde\\'}\\n        pick between \\'kde\\' and \\'hist\\' for\\n        either Kernel Density Estimation or Histogram\\n        plot in the diagonal\\n    marker : str, optional\\n        Matplotlib marker type, default \\'.\\'\\n    hist_kwds : other plotting keyword arguments\\n        To be passed to hist function\\n    density_kwds : other plotting keyword arguments\\n        To be passed to kernel density estimate plot\\n    range_padding : float, optional\\n        relative extension of axis range in x and y\\n        with respect to (x_max - x_min) or (y_max - y_min),\\n        default 0.05\\n    kwds : other plotting keyword arguments\\n        To be passed to scatter function\\n\\n    Examples\\n    --------\\n    >>> df = pd.DataFrame(np.random.randn(1000, 4), columns=[\\'A\\',\\'B\\',\\'C\\',\\'D\\'])\\n    >>> scatter_matrix(df, alpha=0.2)\\n    \"\"\"\\n\\n    df = frame._get_numeric_data()\\n    n = df.columns.size\\n    naxes = n * n\\n    fig, axes = _subplots(naxes=naxes, figsize=figsize, ax=ax,\\n                          squeeze=False)\\n\\n    # no gaps between subplots\\n    fig.subplots_adjust(wspace=0, hspace=0)\\n\\n    mask = notna(df)\\n\\n    marker = _get_marker_compat(marker)\\n\\n    hist_kwds = hist_kwds or {}\\n    density_kwds = density_kwds or {}\\n\\n    # GH 14855\\n    kwds.setdefault(\\'edgecolors\\', \\'none\\')\\n\\n    boundaries_list = []\\n    for a in df.columns:\\n        values = df[a].values[mask[a].values]\\n        rmin_, rmax_ = np.min(values), np.max(values)\\n        rdelta_ext = (rmax_ - rmin_) * range_padding / 2.\\n        boundaries_list.append((rmin_ - rdelta_ext, rmax_ + rdelta_ext))\\n\\n    for i, a in zip(lrange(n), df.columns):\\n        for j, b in zip(lrange(n), df.columns):\\n            ax = axes[i, j]\\n\\n            if i == j:\\n                values = df[a].values[mask[a].values]\\n\\n                # Deal with the diagonal by drawing a histogram there.\\n                if diagonal == \\'hist\\':\\n                    ax.hist(values, **hist_kwds)\\n\\n                elif diagonal in (\\'kde\\', \\'density\\'):\\n                    from scipy.stats import gaussian_kde\\n                    y = values\\n                    gkde = gaussian_kde(y)\\n                    ind = np.linspace(y.min(), y.max(), 1000)\\n                    ax.plot(ind, gkde.evaluate(ind), **density_kwds)\\n\\n                ax.set_xlim(boundaries_list[i])\\n\\n            else:\\n                common = (mask[a] & mask[b]).values\\n\\n                ax.scatter(df[b][common], df[a][common],\\n                           marker=marker, alpha=alpha, **kwds)\\n\\n                ax.set_xlim(boundaries_list[j])\\n                ax.set_ylim(boundaries_list[i])\\n\\n            ax.set_xlabel(b)\\n            ax.set_ylabel(a)\\n\\n            if j != 0:\\n                ax.yaxis.set_visible(False)\\n            if i != n - 1:\\n                ax.xaxis.set_visible(False)\\n\\n    if len(df.columns) > 1:\\n        lim1 = boundaries_list[0]\\n        locs = axes[0][1].yaxis.get_majorticklocs()\\n        locs = locs[(lim1[0] <= locs) & (locs <= lim1[1])]\\n        adj = (locs - lim1[0]) / (lim1[1] - lim1[0])\\n\\n        lim0 = axes[0][0].get_ylim()\\n        adj = adj * (lim0[1] - lim0[0]) + lim0[0]\\n        axes[0][0].yaxis.set_ticks(adj)\\n\\n        if np.all(locs == locs.astype(int)):\\n            # if all ticks are int\\n            locs = locs.astype(int)\\n        axes[0][0].yaxis.set_ticklabels(locs)\\n\\n    _set_ticks_props(axes, xlabelsize=8, xrot=90, ylabelsize=8, yrot=0)\\n\\n    return axes',\n 'def radviz(frame, class_column, ax=None, color=None, colormap=None, **kwds):\\n    \"\"\"\\n    Plot a multidimensional dataset in 2D.\\n\\n    Each Series in the DataFrame is represented as a evenly distributed\\n    slice on a circle. Each data point is rendered in the circle according to\\n    the value on each Series. Highly correlated `Series` in the `DataFrame`\\n    are placed closer on the unit circle.\\n\\n    RadViz allow to project a N-dimensional data set into a 2D space where the\\n    influence of each dimension can be interpreted as a balance between the\\n    influence of all dimensions.\\n\\n    More info available at the `original article\\n    <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.889>`_\\n    describing RadViz.\\n\\n    Parameters\\n    ----------\\n    frame : `DataFrame`\\n        Pandas object holding the data.\\n    class_column : str\\n        Column name containing the name of the data point category.\\n    ax : :class:`matplotlib.axes.Axes`, optional\\n        A plot instance to which to add the information.\\n    color : list[str] or tuple[str], optional\\n        Assign a color to each category. Example: [\\'blue\\', \\'green\\'].\\n    colormap : str or :class:`matplotlib.colors.Colormap`, default None\\n        Colormap to select colors from. If string, load colormap with that\\n        name from matplotlib.\\n    kwds : optional\\n        Options to pass to matplotlib scatter plotting method.\\n\\n    Returns\\n    -------\\n    class:`matplotlib.axes.Axes`\\n\\n    See Also\\n    --------\\n    plotting.andrews_curves : Plot clustering visualization.\\n\\n    Examples\\n    --------\\n    .. plot::\\n        :context: close-figs\\n\\n        >>> df = pd.DataFrame({\\n        ...         \\'SepalLength\\': [6.5, 7.7, 5.1, 5.8, 7.6, 5.0, 5.4, 4.6,\\n        ...                         6.7, 4.6],\\n        ...         \\'SepalWidth\\': [3.0, 3.8, 3.8, 2.7, 3.0, 2.3, 3.0, 3.2,\\n        ...                        3.3, 3.6],\\n        ...         \\'PetalLength\\': [5.5, 6.7, 1.9, 5.1, 6.6, 3.3, 4.5, 1.4,\\n        ...                         5.7, 1.0],\\n        ...         \\'PetalWidth\\': [1.8, 2.2, 0.4, 1.9, 2.1, 1.0, 1.5, 0.2,\\n        ...                        2.1, 0.2],\\n        ...         \\'Category\\': [\\'virginica\\', \\'virginica\\', \\'setosa\\',\\n        ...                      \\'virginica\\', \\'virginica\\', \\'versicolor\\',\\n        ...                      \\'versicolor\\', \\'setosa\\', \\'virginica\\',\\n        ...                      \\'setosa\\']\\n        ...     })\\n        >>> rad_viz = pd.plotting.radviz(df, \\'Category\\')  # doctest: +SKIP\\n    \"\"\"\\n    import matplotlib.pyplot as plt\\n    import matplotlib.patches as patches\\n\\n    def normalize(series):\\n        a = min(series)\\n        b = max(series)\\n        return (series - a) / (b - a)\\n\\n    n = len(frame)\\n    classes = frame[class_column].drop_duplicates()\\n    class_col = frame[class_column]\\n    df = frame.drop(class_column, axis=1).apply(normalize)\\n\\n    if ax is None:\\n        ax = plt.gca(xlim=[-1, 1], ylim=[-1, 1])\\n\\n    to_plot = {}\\n    colors = _get_standard_colors(num_colors=len(classes), colormap=colormap,\\n                                  color_type=\\'random\\', color=color)\\n\\n    for kls in classes:\\n        to_plot[kls] = [[], []]\\n\\n    m = len(frame.columns) - 1\\n    s = np.array([(np.cos(t), np.sin(t))\\n                  for t in [2.0 * np.pi * (i / float(m))\\n                            for i in range(m)]])\\n\\n    for i in range(n):\\n        row = df.iloc[i].values\\n        row_ = np.repeat(np.expand_dims(row, axis=1), 2, axis=1)\\n        y = (s * row_).sum(axis=0) / row.sum()\\n        kls = class_col.iat[i]\\n        to_plot[kls][0].append(y[0])\\n        to_plot[kls][1].append(y[1])\\n\\n    for i, kls in enumerate(classes):\\n        ax.scatter(to_plot[kls][0], to_plot[kls][1], color=colors[i],\\n                   label=pprint_thing(kls), **kwds)\\n    ax.legend()\\n\\n    ax.add_patch(patches.Circle((0.0, 0.0), radius=1.0, facecolor=\\'none\\'))\\n\\n    for xy, name in zip(s, df.columns):\\n\\n        ax.add_patch(patches.Circle(xy, radius=0.025, facecolor=\\'gray\\'))\\n\\n        if xy[0] < 0.0 and xy[1] < 0.0:\\n            ax.text(xy[0] - 0.025, xy[1] - 0.025, name,\\n                    ha=\\'right\\', va=\\'top\\', size=\\'small\\')\\n        elif xy[0] < 0.0 and xy[1] >= 0.0:\\n            ax.text(xy[0] - 0.025, xy[1] + 0.025, name,\\n                    ha=\\'right\\', va=\\'bottom\\', size=\\'small\\')\\n        elif xy[0] >= 0.0 and xy[1] < 0.0:\\n            ax.text(xy[0] + 0.025, xy[1] - 0.025, name,\\n                    ha=\\'left\\', va=\\'top\\', size=\\'small\\')\\n        elif xy[0] >= 0.0 and xy[1] >= 0.0:\\n            ax.text(xy[0] + 0.025, xy[1] + 0.025, name,\\n                    ha=\\'left\\', va=\\'bottom\\', size=\\'small\\')\\n\\n    ax.axis(\\'equal\\')\\n    return ax',\n 'def andrews_curves(frame, class_column, ax=None, samples=200, color=None,\\n                   colormap=None, **kwds):\\n    \"\"\"\\n    Generate a matplotlib plot of Andrews curves, for visualising clusters of\\n    multivariate data.\\n\\n    Andrews curves have the functional form:\\n\\n    f(t) = x_1/sqrt(2) + x_2 sin(t) + x_3 cos(t) +\\n           x_4 sin(2t) + x_5 cos(2t) + ...\\n\\n    Where x coefficients correspond to the values of each dimension and t is\\n    linearly spaced between -pi and +pi. Each row of frame then corresponds to\\n    a single curve.\\n\\n    Parameters\\n    ----------\\n    frame : DataFrame\\n        Data to be plotted, preferably normalized to (0.0, 1.0)\\n    class_column : Name of the column containing class names\\n    ax : matplotlib axes object, default None\\n    samples : Number of points to plot in each curve\\n    color : list or tuple, optional\\n        Colors to use for the different classes\\n    colormap : str or matplotlib colormap object, default None\\n        Colormap to select colors from. If string, load colormap with that name\\n        from matplotlib.\\n    kwds : keywords\\n        Options to pass to matplotlib plotting method\\n\\n    Returns\\n    -------\\n    class:`matplotlip.axis.Axes`\\n    \"\"\"\\n    from math import sqrt, pi\\n    import matplotlib.pyplot as plt\\n\\n    def function(amplitudes):\\n        def f(t):\\n            x1 = amplitudes[0]\\n            result = x1 / sqrt(2.0)\\n\\n            # Take the rest of the coefficients and resize them\\n            # appropriately. Take a copy of amplitudes as otherwise numpy\\n            # deletes the element from amplitudes itself.\\n            coeffs = np.delete(np.copy(amplitudes), 0)\\n            coeffs.resize(int((coeffs.size + 1) / 2), 2)\\n\\n            # Generate the harmonics and arguments for the sin and cos\\n            # functions.\\n            harmonics = np.arange(0, coeffs.shape[0]) + 1\\n            trig_args = np.outer(harmonics, t)\\n\\n            result += np.sum(coeffs[:, 0, np.newaxis] * np.sin(trig_args) +\\n                             coeffs[:, 1, np.newaxis] * np.cos(trig_args),\\n                             axis=0)\\n            return result\\n        return f\\n\\n    n = len(frame)\\n    class_col = frame[class_column]\\n    classes = frame[class_column].drop_duplicates()\\n    df = frame.drop(class_column, axis=1)\\n    t = np.linspace(-pi, pi, samples)\\n    used_legends = set()\\n\\n    color_values = _get_standard_colors(num_colors=len(classes),\\n                                        colormap=colormap, color_type=\\'random\\',\\n                                        color=color)\\n    colors = dict(zip(classes, color_values))\\n    if ax is None:\\n        ax = plt.gca(xlim=(-pi, pi))\\n    for i in range(n):\\n        row = df.iloc[i].values\\n        f = function(row)\\n        y = f(t)\\n        kls = class_col.iat[i]\\n        label = pprint_thing(kls)\\n        if label not in used_legends:\\n            used_legends.add(label)\\n            ax.plot(t, y, color=colors[kls], label=label, **kwds)\\n        else:\\n            ax.plot(t, y, color=colors[kls], **kwds)\\n\\n    ax.legend(loc=\\'upper right\\')\\n    ax.grid()\\n    return ax',\n 'def bootstrap_plot(series, fig=None, size=50, samples=500, **kwds):\\n    \"\"\"\\n    Bootstrap plot on mean, median and mid-range statistics.\\n\\n    The bootstrap plot is used to estimate the uncertainty of a statistic\\n    by relaying on random sampling with replacement [1]_. This function will\\n    generate bootstrapping plots for mean, median and mid-range statistics\\n    for the given number of samples of the given size.\\n\\n    .. [1] \"Bootstrapping (statistics)\" in \\\\\\n    https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29\\n\\n    Parameters\\n    ----------\\n    series : pandas.Series\\n        Pandas Series from where to get the samplings for the bootstrapping.\\n    fig : matplotlib.figure.Figure, default None\\n        If given, it will use the `fig` reference for plotting instead of\\n        creating a new one with default parameters.\\n    size : int, default 50\\n        Number of data points to consider during each sampling. It must be\\n        greater or equal than the length of the `series`.\\n    samples : int, default 500\\n        Number of times the bootstrap procedure is performed.\\n    **kwds :\\n        Options to pass to matplotlib plotting method.\\n\\n    Returns\\n    -------\\n    matplotlib.figure.Figure\\n        Matplotlib figure.\\n\\n    See Also\\n    --------\\n    DataFrame.plot : Basic plotting for DataFrame objects.\\n    Series.plot : Basic plotting for Series objects.\\n\\n    Examples\\n    --------\\n\\n    .. plot::\\n            :context: close-figs\\n\\n            >>> s = pd.Series(np.random.uniform(size=100))\\n            >>> fig = pd.plotting.bootstrap_plot(s)  # doctest: +SKIP\\n    \"\"\"\\n    import random\\n    import matplotlib.pyplot as plt\\n\\n    # random.sample(ndarray, int) fails on python 3.3, sigh\\n    data = list(series.values)\\n    samplings = [random.sample(data, size) for _ in range(samples)]\\n\\n    means = np.array([np.mean(sampling) for sampling in samplings])\\n    medians = np.array([np.median(sampling) for sampling in samplings])\\n    midranges = np.array([(min(sampling) + max(sampling)) * 0.5\\n                          for sampling in samplings])\\n    if fig is None:\\n        fig = plt.figure()\\n    x = lrange(samples)\\n    axes = []\\n    ax1 = fig.add_subplot(2, 3, 1)\\n    ax1.set_xlabel(\"Sample\")\\n    axes.append(ax1)\\n    ax1.plot(x, means, **kwds)\\n    ax2 = fig.add_subplot(2, 3, 2)\\n    ax2.set_xlabel(\"Sample\")\\n    axes.append(ax2)\\n    ax2.plot(x, medians, **kwds)\\n    ax3 = fig.add_subplot(2, 3, 3)\\n    ax3.set_xlabel(\"Sample\")\\n    axes.append(ax3)\\n    ax3.plot(x, midranges, **kwds)\\n    ax4 = fig.add_subplot(2, 3, 4)\\n    ax4.set_xlabel(\"Mean\")\\n    axes.append(ax4)\\n    ax4.hist(means, **kwds)\\n    ax5 = fig.add_subplot(2, 3, 5)\\n    ax5.set_xlabel(\"Median\")\\n    axes.append(ax5)\\n    ax5.hist(medians, **kwds)\\n    ax6 = fig.add_subplot(2, 3, 6)\\n    ax6.set_xlabel(\"Midrange\")\\n    axes.append(ax6)\\n    ax6.hist(midranges, **kwds)\\n    for axis in axes:\\n        plt.setp(axis.get_xticklabels(), fontsize=8)\\n        plt.setp(axis.get_yticklabels(), fontsize=8)\\n    return fig',\n 'def parallel_coordinates(frame, class_column, cols=None, ax=None, color=None,\\n                         use_columns=False, xticks=None, colormap=None,\\n                         axvlines=True, axvlines_kwds=None, sort_labels=False,\\n                         **kwds):\\n    \"\"\"Parallel coordinates plotting.\\n\\n    Parameters\\n    ----------\\n    frame : DataFrame\\n    class_column : str\\n        Column name containing class names\\n    cols : list, optional\\n        A list of column names to use\\n    ax : matplotlib.axis, optional\\n        matplotlib axis object\\n    color : list or tuple, optional\\n        Colors to use for the different classes\\n    use_columns : bool, optional\\n        If true, columns will be used as xticks\\n    xticks : list or tuple, optional\\n        A list of values to use for xticks\\n    colormap : str or matplotlib colormap, default None\\n        Colormap to use for line colors.\\n    axvlines : bool, optional\\n        If true, vertical lines will be added at each xtick\\n    axvlines_kwds : keywords, optional\\n        Options to be passed to axvline method for vertical lines\\n    sort_labels : bool, False\\n        Sort class_column labels, useful when assigning colors\\n\\n        .. versionadded:: 0.20.0\\n\\n    kwds : keywords\\n        Options to pass to matplotlib plotting method\\n\\n    Returns\\n    -------\\n    class:`matplotlib.axis.Axes`\\n\\n    Examples\\n    --------\\n    >>> from matplotlib import pyplot as plt\\n    >>> df = pd.read_csv(\\'https://raw.github.com/pandas-dev/pandas/master\\'\\n                        \\'/pandas/tests/data/iris.csv\\')\\n    >>> pd.plotting.parallel_coordinates(\\n            df, \\'Name\\',\\n            color=(\\'#556270\\', \\'#4ECDC4\\', \\'#C7F464\\'))\\n    >>> plt.show()\\n    \"\"\"\\n    if axvlines_kwds is None:\\n        axvlines_kwds = {\\'linewidth\\': 1, \\'color\\': \\'black\\'}\\n    import matplotlib.pyplot as plt\\n\\n    n = len(frame)\\n    classes = frame[class_column].drop_duplicates()\\n    class_col = frame[class_column]\\n\\n    if cols is None:\\n        df = frame.drop(class_column, axis=1)\\n    else:\\n        df = frame[cols]\\n\\n    used_legends = set()\\n\\n    ncols = len(df.columns)\\n\\n    # determine values to use for xticks\\n    if use_columns is True:\\n        if not np.all(np.isreal(list(df.columns))):\\n            raise ValueError(\\'Columns must be numeric to be used as xticks\\')\\n        x = df.columns\\n    elif xticks is not None:\\n        if not np.all(np.isreal(xticks)):\\n            raise ValueError(\\'xticks specified must be numeric\\')\\n        elif len(xticks) != ncols:\\n            raise ValueError(\\'Length of xticks must match number of columns\\')\\n        x = xticks\\n    else:\\n        x = lrange(ncols)\\n\\n    if ax is None:\\n        ax = plt.gca()\\n\\n    color_values = _get_standard_colors(num_colors=len(classes),\\n                                        colormap=colormap, color_type=\\'random\\',\\n                                        color=color)\\n\\n    if sort_labels:\\n        classes = sorted(classes)\\n        color_values = sorted(color_values)\\n    colors = dict(zip(classes, color_values))\\n\\n    for i in range(n):\\n        y = df.iloc[i].values\\n        kls = class_col.iat[i]\\n        label = pprint_thing(kls)\\n        if label not in used_legends:\\n            used_legends.add(label)\\n            ax.plot(x, y, color=colors[kls], label=label, **kwds)\\n        else:\\n            ax.plot(x, y, color=colors[kls], **kwds)\\n\\n    if axvlines:\\n        for i in x:\\n            ax.axvline(i, **axvlines_kwds)\\n\\n    ax.set_xticks(x)\\n    ax.set_xticklabels(df.columns)\\n    ax.set_xlim(x[0], x[-1])\\n    ax.legend(loc=\\'upper right\\')\\n    ax.grid()\\n    return ax',\n 'def lag_plot(series, lag=1, ax=None, **kwds):\\n    \"\"\"Lag plot for time series.\\n\\n    Parameters\\n    ----------\\n    series : Time series\\n    lag : lag of the scatter plot, default 1\\n    ax : Matplotlib axis object, optional\\n    kwds : Matplotlib scatter method keyword arguments, optional\\n\\n    Returns\\n    -------\\n    class:`matplotlib.axis.Axes`\\n    \"\"\"\\n    import matplotlib.pyplot as plt\\n\\n    # workaround because `c=\\'b\\'` is hardcoded in matplotlibs scatter method\\n    kwds.setdefault(\\'c\\', plt.rcParams[\\'patch.facecolor\\'])\\n\\n    data = series.values\\n    y1 = data[:-lag]\\n    y2 = data[lag:]\\n    if ax is None:\\n        ax = plt.gca()\\n    ax.set_xlabel(\"y(t)\")\\n    ax.set_ylabel(\"y(t + {lag})\".format(lag=lag))\\n    ax.scatter(y1, y2, **kwds)\\n    return ax',\n 'def autocorrelation_plot(series, ax=None, **kwds):\\n    \"\"\"\\n    Autocorrelation plot for time series.\\n\\n    Parameters:\\n    -----------\\n    series: Time series\\n    ax: Matplotlib axis object, optional\\n    kwds : keywords\\n        Options to pass to matplotlib plotting method\\n\\n    Returns:\\n    -----------\\n    class:`matplotlib.axis.Axes`\\n    \"\"\"\\n    import matplotlib.pyplot as plt\\n    n = len(series)\\n    data = np.asarray(series)\\n    if ax is None:\\n        ax = plt.gca(xlim=(1, n), ylim=(-1.0, 1.0))\\n    mean = np.mean(data)\\n    c0 = np.sum((data - mean) ** 2) / float(n)\\n\\n    def r(h):\\n        return ((data[:n - h] - mean) *\\n                (data[h:] - mean)).sum() / float(n) / c0\\n    x = np.arange(n) + 1\\n    y = lmap(r, x)\\n    z95 = 1.959963984540054\\n    z99 = 2.5758293035489004\\n    ax.axhline(y=z99 / np.sqrt(n), linestyle=\\'--\\', color=\\'grey\\')\\n    ax.axhline(y=z95 / np.sqrt(n), color=\\'grey\\')\\n    ax.axhline(y=0.0, color=\\'black\\')\\n    ax.axhline(y=-z95 / np.sqrt(n), color=\\'grey\\')\\n    ax.axhline(y=-z99 / np.sqrt(n), linestyle=\\'--\\', color=\\'grey\\')\\n    ax.set_xlabel(\"Lag\")\\n    ax.set_ylabel(\"Autocorrelation\")\\n    ax.plot(x, y, **kwds)\\n    if \\'label\\' in kwds:\\n        ax.legend()\\n    ax.grid()\\n    return ax',\n 'def _any_pandas_objects(terms):\\n    \"\"\"Check a sequence of terms for instances of PandasObject.\"\"\"\\n    return any(isinstance(term.value, pd.core.generic.PandasObject)\\n               for term in terms)',\n 'def _align(terms):\\n    \"\"\"Align a set of terms\"\"\"\\n    try:\\n        # flatten the parse tree (a nested list, really)\\n        terms = list(com.flatten(terms))\\n    except TypeError:\\n        # can\\'t iterate so it must just be a constant or single variable\\n        if isinstance(terms.value, pd.core.generic.NDFrame):\\n            typ = type(terms.value)\\n            return typ, _zip_axes_from_type(typ, terms.value.axes)\\n        return np.result_type(terms.type), None\\n\\n    # if all resolved variables are numeric scalars\\n    if all(term.is_scalar for term in terms):\\n        return _result_type_many(*(term.value for term in terms)).type, None\\n\\n    # perform the main alignment\\n    typ, axes = _align_core(terms)\\n    return typ, axes',\n 'def _reconstruct_object(typ, obj, axes, dtype):\\n    \"\"\"Reconstruct an object given its type, raw value, and possibly empty\\n    (None) axes.\\n\\n    Parameters\\n    ----------\\n    typ : object\\n        A type\\n    obj : object\\n        The value to use in the type constructor\\n    axes : dict\\n        The axes to use to construct the resulting pandas object\\n\\n    Returns\\n    -------\\n    ret : typ\\n        An object of type ``typ`` with the value `obj` and possible axes\\n        `axes`.\\n    \"\"\"\\n    try:\\n        typ = typ.type\\n    except AttributeError:\\n        pass\\n\\n    res_t = np.result_type(obj.dtype, dtype)\\n\\n    if (not isinstance(typ, partial) and\\n            issubclass(typ, pd.core.generic.PandasObject)):\\n        return typ(obj, dtype=res_t, **axes)\\n\\n    # special case for pathological things like ~True/~False\\n    if hasattr(res_t, \\'type\\') and typ == np.bool_ and res_t != np.bool_:\\n        ret_value = res_t.type(obj)\\n    else:\\n        ret_value = typ(obj).astype(res_t)\\n        # The condition is to distinguish 0-dim array (returned in case of\\n        # scalar) and 1 element array\\n        # e.g. np.array(0) and np.array([0])\\n        if len(obj.shape) == 1 and len(obj) == 1:\\n            if not isinstance(ret_value, np.ndarray):\\n                ret_value = np.array([ret_value]).astype(res_t)\\n\\n    return ret_value',\n 'def tsplot(series, plotf, ax=None, **kwargs):\\n    import warnings\\n    \"\"\"\\n    Plots a Series on the given Matplotlib axes or the current axes\\n\\n    Parameters\\n    ----------\\n    axes : Axes\\n    series : Series\\n\\n    Notes\\n    _____\\n    Supports same kwargs as Axes.plot\\n\\n\\n    .. deprecated:: 0.23.0\\n       Use Series.plot() instead\\n    \"\"\"\\n    warnings.warn(\"\\'tsplot\\' is deprecated and will be removed in a \"\\n                  \"future version. Please use Series.plot() instead.\",\\n                  FutureWarning, stacklevel=2)\\n\\n    # Used inferred freq is possible, need a test case for inferred\\n    if ax is None:\\n        import matplotlib.pyplot as plt\\n        ax = plt.gca()\\n\\n    freq, series = _maybe_resample(series, ax, kwargs)\\n\\n    # Set ax with freq info\\n    _decorate_axes(ax, freq, kwargs)\\n    ax._plot_data.append((series, plotf, kwargs))\\n    lines = plotf(ax, series.index._mpl_repr(), series.values, **kwargs)\\n\\n    # set date formatter, locators and rescale limits\\n    format_dateaxis(ax, ax.freq, series.index)\\n    return lines',\n 'def _decorate_axes(ax, freq, kwargs):\\n    \"\"\"Initialize axes for time-series plotting\"\"\"\\n    if not hasattr(ax, \\'_plot_data\\'):\\n        ax._plot_data = []\\n\\n    ax.freq = freq\\n    xaxis = ax.get_xaxis()\\n    xaxis.freq = freq\\n    if not hasattr(ax, \\'legendlabels\\'):\\n        ax.legendlabels = [kwargs.get(\\'label\\', None)]\\n    else:\\n        ax.legendlabels.append(kwargs.get(\\'label\\', None))\\n    ax.view_interval = None\\n    ax.date_axis_info = None',\n 'def _get_ax_freq(ax):\\n    \"\"\"\\n    Get the freq attribute of the ax object if set.\\n    Also checks shared axes (eg when using secondary yaxis, sharex=True\\n    or twinx)\\n    \"\"\"\\n    ax_freq = getattr(ax, \\'freq\\', None)\\n    if ax_freq is None:\\n        # check for left/right ax in case of secondary yaxis\\n        if hasattr(ax, \\'left_ax\\'):\\n            ax_freq = getattr(ax.left_ax, \\'freq\\', None)\\n        elif hasattr(ax, \\'right_ax\\'):\\n            ax_freq = getattr(ax.right_ax, \\'freq\\', None)\\n    if ax_freq is None:\\n        # check if a shared ax (sharex/twinx) has already freq set\\n        shared_axes = ax.get_shared_x_axes().get_siblings(ax)\\n        if len(shared_axes) > 1:\\n            for shared_ax in shared_axes:\\n                ax_freq = getattr(shared_ax, \\'freq\\', None)\\n                if ax_freq is not None:\\n                    break\\n    return ax_freq',\n 'def format_timedelta_ticks(x, pos, n_decimals):\\n    \"\"\"\\n    Convert seconds to \\'D days HH:MM:SS.F\\'\\n    \"\"\"\\n    s, ns = divmod(x, 1e9)\\n    m, s = divmod(s, 60)\\n    h, m = divmod(m, 60)\\n    d, h = divmod(h, 24)\\n    decimals = int(ns * 10**(n_decimals - 9))\\n    s = r\\'{:02d}:{:02d}:{:02d}\\'.format(int(h), int(m), int(s))\\n    if n_decimals > 0:\\n        s += \\'.{{:0{:0d}d}}\\'.format(n_decimals).format(decimals)\\n    if d != 0:\\n        s = \\'{:d} days \\'.format(int(d)) + s\\n    return s',\n 'def format_dateaxis(subplot, freq, index):\\n    \"\"\"\\n    Pretty-formats the date axis (x-axis).\\n\\n    Major and minor ticks are automatically set for the frequency of the\\n    current underlying series.  As the dynamic mode is activated by\\n    default, changing the limits of the x axis will intelligently change\\n    the positions of the ticks.\\n    \"\"\"\\n\\n    # handle index specific formatting\\n    # Note: DatetimeIndex does not use this\\n    # interface. DatetimeIndex uses matplotlib.date directly\\n    if isinstance(index, ABCPeriodIndex):\\n\\n        majlocator = TimeSeries_DateLocator(freq, dynamic_mode=True,\\n                                            minor_locator=False,\\n                                            plot_obj=subplot)\\n        minlocator = TimeSeries_DateLocator(freq, dynamic_mode=True,\\n                                            minor_locator=True,\\n                                            plot_obj=subplot)\\n        subplot.xaxis.set_major_locator(majlocator)\\n        subplot.xaxis.set_minor_locator(minlocator)\\n\\n        majformatter = TimeSeries_DateFormatter(freq, dynamic_mode=True,\\n                                                minor_locator=False,\\n                                                plot_obj=subplot)\\n        minformatter = TimeSeries_DateFormatter(freq, dynamic_mode=True,\\n                                                minor_locator=True,\\n                                                plot_obj=subplot)\\n        subplot.xaxis.set_major_formatter(majformatter)\\n        subplot.xaxis.set_minor_formatter(minformatter)\\n\\n        # x and y coord info\\n        subplot.format_coord = functools.partial(_format_coord, freq)\\n\\n    elif isinstance(index, ABCTimedeltaIndex):\\n        subplot.xaxis.set_major_formatter(\\n            TimeSeries_TimedeltaFormatter())\\n    else:\\n        raise TypeError(\\'index type not supported\\')\\n\\n    pylab.draw_if_interactive()',\n 'def _is_homogeneous_type(self):\\n        \"\"\"\\n        Whether all the columns in a DataFrame have the same type.\\n\\n        Returns\\n        -------\\n        bool\\n\\n        Examples\\n        --------\\n        >>> DataFrame({\"A\": [1, 2], \"B\": [3, 4]})._is_homogeneous_type\\n        True\\n        >>> DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.0]})._is_homogeneous_type\\n        False\\n\\n        Items with the same type but different sizes are considered\\n        different types.\\n\\n        >>> DataFrame({\\n        ...    \"A\": np.array([1, 2], dtype=np.int32),\\n        ...    \"B\": np.array([1, 2], dtype=np.int64)})._is_homogeneous_type\\n        False\\n        \"\"\"\\n        if self._data.any_extension_types:\\n            return len({block.dtype for block in self._data.blocks}) == 1\\n        else:\\n            return not self._data.is_mixed_type',\n 'def _repr_html_(self):\\n        \"\"\"\\n        Return a html representation for a particular DataFrame.\\n\\n        Mainly for IPython notebook.\\n        \"\"\"\\n        if self._info_repr():\\n            buf = StringIO(\"\")\\n            self.info(buf=buf)\\n            # need to escape the <class>, should be the first line.\\n            val = buf.getvalue().replace(\\'<\\', r\\'&lt;\\', 1)\\n            val = val.replace(\\'>\\', r\\'&gt;\\', 1)\\n            return \\'<pre>\\' + val + \\'</pre>\\'\\n\\n        if get_option(\"display.notebook_repr_html\"):\\n            max_rows = get_option(\"display.max_rows\")\\n            max_cols = get_option(\"display.max_columns\")\\n            show_dimensions = get_option(\"display.show_dimensions\")\\n\\n            return self.to_html(max_rows=max_rows, max_cols=max_cols,\\n                                show_dimensions=show_dimensions, notebook=True)\\n        else:\\n            return None',\n 'def to_string(self, buf=None, columns=None, col_space=None, header=True,\\n                  index=True, na_rep=\\'NaN\\', formatters=None, float_format=None,\\n                  sparsify=None, index_names=True, justify=None,\\n                  max_rows=None, max_cols=None, show_dimensions=False,\\n                  decimal=\\'.\\', line_width=None):\\n        \"\"\"\\n        Render a DataFrame to a console-friendly tabular output.\\n        %(shared_params)s\\n        line_width : int, optional\\n            Width to wrap a line in characters.\\n        %(returns)s\\n        See Also\\n        --------\\n        to_html : Convert DataFrame to HTML.\\n\\n        Examples\\n        --------\\n        >>> d = {\\'col1\\': [1, 2, 3], \\'col2\\': [4, 5, 6]}\\n        >>> df = pd.DataFrame(d)\\n        >>> print(df.to_string())\\n           col1  col2\\n        0     1     4\\n        1     2     5\\n        2     3     6\\n        \"\"\"\\n\\n        formatter = fmt.DataFrameFormatter(self, buf=buf, columns=columns,\\n                                           col_space=col_space, na_rep=na_rep,\\n                                           formatters=formatters,\\n                                           float_format=float_format,\\n                                           sparsify=sparsify, justify=justify,\\n                                           index_names=index_names,\\n                                           header=header, index=index,\\n                                           max_rows=max_rows,\\n                                           max_cols=max_cols,\\n                                           show_dimensions=show_dimensions,\\n                                           decimal=decimal,\\n                                           line_width=line_width)\\n        formatter.to_string()\\n\\n        if buf is None:\\n            result = formatter.buf.getvalue()\\n            return result',\n 'def iteritems(self):\\n        r\"\"\"\\n        Iterator over (column name, Series) pairs.\\n\\n        Iterates over the DataFrame columns, returning a tuple with\\n        the column name and the content as a Series.\\n\\n        Yields\\n        ------\\n        label : object\\n            The column names for the DataFrame being iterated over.\\n        content : Series\\n            The column entries belonging to each label, as a Series.\\n\\n        See Also\\n        --------\\n        DataFrame.iterrows : Iterate over DataFrame rows as\\n            (index, Series) pairs.\\n        DataFrame.itertuples : Iterate over DataFrame rows as namedtuples\\n            of the values.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'species\\': [\\'bear\\', \\'bear\\', \\'marsupial\\'],\\n        ...                   \\'population\\': [1864, 22000, 80000]},\\n        ...                   index=[\\'panda\\', \\'polar\\', \\'koala\\'])\\n        >>> df\\n                species   population\\n        panda \\tbear \\t  1864\\n        polar \\tbear \\t  22000\\n        koala \\tmarsupial 80000\\n        >>> for label, content in df.iteritems():\\n        ...     print(\\'label:\\', label)\\n        ...     print(\\'content:\\', content, sep=\\'\\\\n\\')\\n        ...\\n        label: species\\n        content:\\n        panda         bear\\n        polar         bear\\n        koala    marsupial\\n        Name: species, dtype: object\\n        label: population\\n        content:\\n        panda     1864\\n        polar    22000\\n        koala    80000\\n        Name: population, dtype: int64\\n        \"\"\"\\n        if self.columns.is_unique and hasattr(self, \\'_item_cache\\'):\\n            for k in self.columns:\\n                yield k, self._get_item_cache(k)\\n        else:\\n            for i, k in enumerate(self.columns):\\n                yield k, self._ixs(i, axis=1)',\n 'def iterrows(self):\\n        \"\"\"\\n        Iterate over DataFrame rows as (index, Series) pairs.\\n\\n        Yields\\n        ------\\n        index : label or tuple of label\\n            The index of the row. A tuple for a `MultiIndex`.\\n        data : Series\\n            The data of the row as a Series.\\n\\n        it : generator\\n            A generator that iterates over the rows of the frame.\\n\\n        See Also\\n        --------\\n        itertuples : Iterate over DataFrame rows as namedtuples of the values.\\n        iteritems : Iterate over (column name, Series) pairs.\\n\\n        Notes\\n        -----\\n\\n        1. Because ``iterrows`` returns a Series for each row,\\n           it does **not** preserve dtypes across the rows (dtypes are\\n           preserved across columns for DataFrames). For example,\\n\\n           >>> df = pd.DataFrame([[1, 1.5]], columns=[\\'int\\', \\'float\\'])\\n           >>> row = next(df.iterrows())[1]\\n           >>> row\\n           int      1.0\\n           float    1.5\\n           Name: 0, dtype: float64\\n           >>> print(row[\\'int\\'].dtype)\\n           float64\\n           >>> print(df[\\'int\\'].dtype)\\n           int64\\n\\n           To preserve dtypes while iterating over the rows, it is better\\n           to use :meth:`itertuples` which returns namedtuples of the values\\n           and which is generally faster than ``iterrows``.\\n\\n        2. You should **never modify** something you are iterating over.\\n           This is not guaranteed to work in all cases. Depending on the\\n           data types, the iterator returns a copy and not a view, and writing\\n           to it will have no effect.\\n        \"\"\"\\n        columns = self.columns\\n        klass = self._constructor_sliced\\n        for k, v in zip(self.index, self.values):\\n            s = klass(v, index=columns, name=k)\\n            yield k, s',\n 'def itertuples(self, index=True, name=\"Pandas\"):\\n        \"\"\"\\n        Iterate over DataFrame rows as namedtuples.\\n\\n        Parameters\\n        ----------\\n        index : bool, default True\\n            If True, return the index as the first element of the tuple.\\n        name : str or None, default \"Pandas\"\\n            The name of the returned namedtuples or None to return regular\\n            tuples.\\n\\n        Yields\\n        -------\\n        collections.namedtuple\\n            Yields a namedtuple for each row in the DataFrame with the first\\n            field possibly being the index and following fields being the\\n            column values.\\n\\n        See Also\\n        --------\\n        DataFrame.iterrows : Iterate over DataFrame rows as (index, Series)\\n            pairs.\\n        DataFrame.iteritems : Iterate over (column name, Series) pairs.\\n\\n        Notes\\n        -----\\n        The column names will be renamed to positional names if they are\\n        invalid Python identifiers, repeated, or start with an underscore.\\n        With a large number of columns (>255), regular tuples are returned.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'num_legs\\': [4, 2], \\'num_wings\\': [0, 2]},\\n        ...                   index=[\\'dog\\', \\'hawk\\'])\\n        >>> df\\n              num_legs  num_wings\\n        dog          4          0\\n        hawk         2          2\\n        >>> for row in df.itertuples():\\n        ...     print(row)\\n        ...\\n        Pandas(Index=\\'dog\\', num_legs=4, num_wings=0)\\n        Pandas(Index=\\'hawk\\', num_legs=2, num_wings=2)\\n\\n        By setting the `index` parameter to False we can remove the index\\n        as the first element of the tuple:\\n\\n        >>> for row in df.itertuples(index=False):\\n        ...     print(row)\\n        ...\\n        Pandas(num_legs=4, num_wings=0)\\n        Pandas(num_legs=2, num_wings=2)\\n\\n        With the `name` parameter set we set a custom name for the yielded\\n        namedtuples:\\n\\n        >>> for row in df.itertuples(name=\\'Animal\\'):\\n        ...     print(row)\\n        ...\\n        Animal(Index=\\'dog\\', num_legs=4, num_wings=0)\\n        Animal(Index=\\'hawk\\', num_legs=2, num_wings=2)\\n        \"\"\"\\n        arrays = []\\n        fields = list(self.columns)\\n        if index:\\n            arrays.append(self.index)\\n            fields.insert(0, \"Index\")\\n\\n        # use integer indexing because of possible duplicate column names\\n        arrays.extend(self.iloc[:, k] for k in range(len(self.columns)))\\n\\n        # Python 3 supports at most 255 arguments to constructor\\n        if name is not None and len(self.columns) + index < 256:\\n            itertuple = collections.namedtuple(name, fields, rename=True)\\n            return map(itertuple._make, zip(*arrays))\\n\\n        # fallback to regular tuples\\n        return zip(*arrays)',\n 'def dot(self, other):\\n        \"\"\"\\n        Compute the matrix mutiplication between the DataFrame and other.\\n\\n        This method computes the matrix product between the DataFrame and the\\n        values of an other Series, DataFrame or a numpy array.\\n\\n        It can also be called using ``self @ other`` in Python >= 3.5.\\n\\n        Parameters\\n        ----------\\n        other : Series, DataFrame or array-like\\n            The other object to compute the matrix product with.\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            If other is a Series, return the matrix product between self and\\n            other as a Serie. If other is a DataFrame or a numpy.array, return\\n            the matrix product of self and other in a DataFrame of a np.array.\\n\\n        See Also\\n        --------\\n        Series.dot: Similar method for Series.\\n\\n        Notes\\n        -----\\n        The dimensions of DataFrame and other must be compatible in order to\\n        compute the matrix multiplication.\\n\\n        The dot method for Series computes the inner product, instead of the\\n        matrix product here.\\n\\n        Examples\\n        --------\\n        Here we multiply a DataFrame with a Series.\\n\\n        >>> df = pd.DataFrame([[0, 1, -2, -1], [1, 1, 1, 1]])\\n        >>> s = pd.Series([1, 1, 2, 1])\\n        >>> df.dot(s)\\n        0    -4\\n        1     5\\n        dtype: int64\\n\\n        Here we multiply a DataFrame with another DataFrame.\\n\\n        >>> other = pd.DataFrame([[0, 1], [1, 2], [-1, -1], [2, 0]])\\n        >>> df.dot(other)\\n            0   1\\n        0   1   4\\n        1   2   2\\n\\n        Note that the dot method give the same result as @\\n\\n        >>> df @ other\\n            0   1\\n        0   1   4\\n        1   2   2\\n\\n        The dot method works also if other is an np.array.\\n\\n        >>> arr = np.array([[0, 1], [1, 2], [-1, -1], [2, 0]])\\n        >>> df.dot(arr)\\n            0   1\\n        0   1   4\\n        1   2   2\\n        \"\"\"\\n        if isinstance(other, (Series, DataFrame)):\\n            common = self.columns.union(other.index)\\n            if (len(common) > len(self.columns) or\\n                    len(common) > len(other.index)):\\n                raise ValueError(\\'matrices are not aligned\\')\\n\\n            left = self.reindex(columns=common, copy=False)\\n            right = other.reindex(index=common, copy=False)\\n            lvals = left.values\\n            rvals = right.values\\n        else:\\n            left = self\\n            lvals = self.values\\n            rvals = np.asarray(other)\\n            if lvals.shape[1] != rvals.shape[0]:\\n                raise ValueError(\\'Dot product shape mismatch, \\'\\n                                 \\'{s} vs {r}\\'.format(s=lvals.shape,\\n                                                     r=rvals.shape))\\n\\n        if isinstance(other, DataFrame):\\n            return self._constructor(np.dot(lvals, rvals), index=left.index,\\n                                     columns=other.columns)\\n        elif isinstance(other, Series):\\n            return Series(np.dot(lvals, rvals), index=left.index)\\n        elif isinstance(rvals, (np.ndarray, Index)):\\n            result = np.dot(lvals, rvals)\\n            if result.ndim == 2:\\n                return self._constructor(result, index=left.index)\\n            else:\\n                return Series(result, index=left.index)\\n        else:  # pragma: no cover\\n            raise TypeError(\\'unsupported type: {oth}\\'.format(oth=type(other)))',\n 'def from_dict(cls, data, orient=\\'columns\\', dtype=None, columns=None):\\n        \"\"\"\\n        Construct DataFrame from dict of array-like or dicts.\\n\\n        Creates DataFrame object from dictionary by columns or by index\\n        allowing dtype specification.\\n\\n        Parameters\\n        ----------\\n        data : dict\\n            Of the form {field : array-like} or {field : dict}.\\n        orient : {\\'columns\\', \\'index\\'}, default \\'columns\\'\\n            The \"orientation\" of the data. If the keys of the passed dict\\n            should be the columns of the resulting DataFrame, pass \\'columns\\'\\n            (default). Otherwise if the keys should be rows, pass \\'index\\'.\\n        dtype : dtype, default None\\n            Data type to force, otherwise infer.\\n        columns : list, default None\\n            Column labels to use when ``orient=\\'index\\'``. Raises a ValueError\\n            if used with ``orient=\\'columns\\'``.\\n\\n            .. versionadded:: 0.23.0\\n\\n        Returns\\n        -------\\n        DataFrame\\n\\n        See Also\\n        --------\\n        DataFrame.from_records : DataFrame from ndarray (structured\\n            dtype), list of tuples, dict, or DataFrame.\\n        DataFrame : DataFrame object creation using constructor.\\n\\n        Examples\\n        --------\\n        By default the keys of the dict become the DataFrame columns:\\n\\n        >>> data = {\\'col_1\\': [3, 2, 1, 0], \\'col_2\\': [\\'a\\', \\'b\\', \\'c\\', \\'d\\']}\\n        >>> pd.DataFrame.from_dict(data)\\n           col_1 col_2\\n        0      3     a\\n        1      2     b\\n        2      1     c\\n        3      0     d\\n\\n        Specify ``orient=\\'index\\'`` to create the DataFrame using dictionary\\n        keys as rows:\\n\\n        >>> data = {\\'row_1\\': [3, 2, 1, 0], \\'row_2\\': [\\'a\\', \\'b\\', \\'c\\', \\'d\\']}\\n        >>> pd.DataFrame.from_dict(data, orient=\\'index\\')\\n               0  1  2  3\\n        row_1  3  2  1  0\\n        row_2  a  b  c  d\\n\\n        When using the \\'index\\' orientation, the column names can be\\n        specified manually:\\n\\n        >>> pd.DataFrame.from_dict(data, orient=\\'index\\',\\n        ...                        columns=[\\'A\\', \\'B\\', \\'C\\', \\'D\\'])\\n               A  B  C  D\\n        row_1  3  2  1  0\\n        row_2  a  b  c  d\\n        \"\"\"\\n        index = None\\n        orient = orient.lower()\\n        if orient == \\'index\\':\\n            if len(data) > 0:\\n                # TODO speed up Series case\\n                if isinstance(list(data.values())[0], (Series, dict)):\\n                    data = _from_nested_dict(data)\\n                else:\\n                    data, index = list(data.values()), list(data.keys())\\n        elif orient == \\'columns\\':\\n            if columns is not None:\\n                raise ValueError(\"cannot use columns parameter with \"\\n                                 \"orient=\\'columns\\'\")\\n        else:  # pragma: no cover\\n            raise ValueError(\\'only recognize index or columns for orient\\')\\n\\n        return cls(data, index=index, columns=columns, dtype=dtype)',\n 'def to_numpy(self, dtype=None, copy=False):\\n        \"\"\"\\n        Convert the DataFrame to a NumPy array.\\n\\n        .. versionadded:: 0.24.0\\n\\n        By default, the dtype of the returned array will be the common NumPy\\n        dtype of all types in the DataFrame. For example, if the dtypes are\\n        ``float16`` and ``float32``, the results dtype will be ``float32``.\\n        This may require copying data and coercing values, which may be\\n        expensive.\\n\\n        Parameters\\n        ----------\\n        dtype : str or numpy.dtype, optional\\n            The dtype to pass to :meth:`numpy.asarray`\\n        copy : bool, default False\\n            Whether to ensure that the returned value is a not a view on\\n            another array. Note that ``copy=False`` does not *ensure* that\\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\\n            a copy is made, even if not strictly necessary.\\n\\n        Returns\\n        -------\\n        numpy.ndarray\\n\\n        See Also\\n        --------\\n        Series.to_numpy : Similar method for Series.\\n\\n        Examples\\n        --------\\n        >>> pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]}).to_numpy()\\n        array([[1, 3],\\n               [2, 4]])\\n\\n        With heterogenous data, the lowest common type will have to\\n        be used.\\n\\n        >>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.5]})\\n        >>> df.to_numpy()\\n        array([[1. , 3. ],\\n               [2. , 4.5]])\\n\\n        For a mix of numeric and non-numeric types, the output array will\\n        have object dtype.\\n\\n        >>> df[\\'C\\'] = pd.date_range(\\'2000\\', periods=2)\\n        >>> df.to_numpy()\\n        array([[1, 3.0, Timestamp(\\'2000-01-01 00:00:00\\')],\\n               [2, 4.5, Timestamp(\\'2000-01-02 00:00:00\\')]], dtype=object)\\n        \"\"\"\\n        result = np.array(self.values, dtype=dtype, copy=copy)\\n        return result',\n 'def to_dict(self, orient=\\'dict\\', into=dict):\\n        \"\"\"\\n        Convert the DataFrame to a dictionary.\\n\\n        The type of the key-value pairs can be customized with the parameters\\n        (see below).\\n\\n        Parameters\\n        ----------\\n        orient : str {\\'dict\\', \\'list\\', \\'series\\', \\'split\\', \\'records\\', \\'index\\'}\\n            Determines the type of the values of the dictionary.\\n\\n            - \\'dict\\' (default) : dict like {column -> {index -> value}}\\n            - \\'list\\' : dict like {column -> [values]}\\n            - \\'series\\' : dict like {column -> Series(values)}\\n            - \\'split\\' : dict like\\n              {\\'index\\' -> [index], \\'columns\\' -> [columns], \\'data\\' -> [values]}\\n            - \\'records\\' : list like\\n              [{column -> value}, ... , {column -> value}]\\n            - \\'index\\' : dict like {index -> {column -> value}}\\n\\n            Abbreviations are allowed. `s` indicates `series` and `sp`\\n            indicates `split`.\\n\\n        into : class, default dict\\n            The collections.abc.Mapping subclass used for all Mappings\\n            in the return value.  Can be the actual class or an empty\\n            instance of the mapping type you want.  If you want a\\n            collections.defaultdict, you must pass it initialized.\\n\\n            .. versionadded:: 0.21.0\\n\\n        Returns\\n        -------\\n        dict, list or collections.abc.Mapping\\n            Return a collections.abc.Mapping object representing the DataFrame.\\n            The resulting transformation depends on the `orient` parameter.\\n\\n        See Also\\n        --------\\n        DataFrame.from_dict: Create a DataFrame from a dictionary.\\n        DataFrame.to_json: Convert a DataFrame to JSON format.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'col1\\': [1, 2],\\n        ...                    \\'col2\\': [0.5, 0.75]},\\n        ...                   index=[\\'row1\\', \\'row2\\'])\\n        >>> df\\n              col1  col2\\n        row1     1  0.50\\n        row2     2  0.75\\n        >>> df.to_dict()\\n        {\\'col1\\': {\\'row1\\': 1, \\'row2\\': 2}, \\'col2\\': {\\'row1\\': 0.5, \\'row2\\': 0.75}}\\n\\n        You can specify the return orientation.\\n\\n        >>> df.to_dict(\\'series\\')\\n        {\\'col1\\': row1    1\\n                 row2    2\\n        Name: col1, dtype: int64,\\n        \\'col2\\': row1    0.50\\n                row2    0.75\\n        Name: col2, dtype: float64}\\n\\n        >>> df.to_dict(\\'split\\')\\n        {\\'index\\': [\\'row1\\', \\'row2\\'], \\'columns\\': [\\'col1\\', \\'col2\\'],\\n         \\'data\\': [[1, 0.5], [2, 0.75]]}\\n\\n        >>> df.to_dict(\\'records\\')\\n        [{\\'col1\\': 1, \\'col2\\': 0.5}, {\\'col1\\': 2, \\'col2\\': 0.75}]\\n\\n        >>> df.to_dict(\\'index\\')\\n        {\\'row1\\': {\\'col1\\': 1, \\'col2\\': 0.5}, \\'row2\\': {\\'col1\\': 2, \\'col2\\': 0.75}}\\n\\n        You can also specify the mapping type.\\n\\n        >>> from collections import OrderedDict, defaultdict\\n        >>> df.to_dict(into=OrderedDict)\\n        OrderedDict([(\\'col1\\', OrderedDict([(\\'row1\\', 1), (\\'row2\\', 2)])),\\n                     (\\'col2\\', OrderedDict([(\\'row1\\', 0.5), (\\'row2\\', 0.75)]))])\\n\\n        If you want a `defaultdict`, you need to initialize it:\\n\\n        >>> dd = defaultdict(list)\\n        >>> df.to_dict(\\'records\\', into=dd)\\n        [defaultdict(<class \\'list\\'>, {\\'col1\\': 1, \\'col2\\': 0.5}),\\n         defaultdict(<class \\'list\\'>, {\\'col1\\': 2, \\'col2\\': 0.75})]\\n        \"\"\"\\n        if not self.columns.is_unique:\\n            warnings.warn(\"DataFrame columns are not unique, some \"\\n                          \"columns will be omitted.\", UserWarning,\\n                          stacklevel=2)\\n        # GH16122\\n        into_c = com.standardize_mapping(into)\\n        if orient.lower().startswith(\\'d\\'):\\n            return into_c(\\n                (k, v.to_dict(into)) for k, v in self.items())\\n        elif orient.lower().startswith(\\'l\\'):\\n            return into_c((k, v.tolist()) for k, v in self.items())\\n        elif orient.lower().startswith(\\'sp\\'):\\n            return into_c(((\\'index\\', self.index.tolist()),\\n                           (\\'columns\\', self.columns.tolist()),\\n                           (\\'data\\', [\\n                               list(map(com.maybe_box_datetimelike, t))\\n                               for t in self.itertuples(index=False, name=None)\\n                           ])))\\n        elif orient.lower().startswith(\\'s\\'):\\n            return into_c((k, com.maybe_box_datetimelike(v))\\n                          for k, v in self.items())\\n        elif orient.lower().startswith(\\'r\\'):\\n            columns = self.columns.tolist()\\n            rows = (dict(zip(columns, row))\\n                    for row in self.itertuples(index=False, name=None))\\n            return [\\n                into_c((k, com.maybe_box_datetimelike(v))\\n                       for k, v in row.items())\\n                for row in rows]\\n        elif orient.lower().startswith(\\'i\\'):\\n            if not self.index.is_unique:\\n                raise ValueError(\\n                    \"DataFrame index must be unique for orient=\\'index\\'.\"\\n                )\\n            return into_c((t[0], dict(zip(self.columns, t[1:])))\\n                          for t in self.itertuples(name=None))\\n        else:\\n            raise ValueError(\"orient \\'{o}\\' not understood\".format(o=orient))',\n 'def to_gbq(self, destination_table, project_id=None, chunksize=None,\\n               reauth=False, if_exists=\\'fail\\', auth_local_webserver=False,\\n               table_schema=None, location=None, progress_bar=True,\\n               credentials=None, verbose=None, private_key=None):\\n        \"\"\"\\n        Write a DataFrame to a Google BigQuery table.\\n\\n        This function requires the `pandas-gbq package\\n        <https://pandas-gbq.readthedocs.io>`__.\\n\\n        See the `How to authenticate with Google BigQuery\\n        <https://pandas-gbq.readthedocs.io/en/latest/howto/authentication.html>`__\\n        guide for authentication instructions.\\n\\n        Parameters\\n        ----------\\n        destination_table : str\\n            Name of table to be written, in the form ``dataset.tablename``.\\n        project_id : str, optional\\n            Google BigQuery Account project ID. Optional when available from\\n            the environment.\\n        chunksize : int, optional\\n            Number of rows to be inserted in each chunk from the dataframe.\\n            Set to ``None`` to load the whole dataframe at once.\\n        reauth : bool, default False\\n            Force Google BigQuery to re-authenticate the user. This is useful\\n            if multiple accounts are used.\\n        if_exists : str, default \\'fail\\'\\n            Behavior when the destination table exists. Value can be one of:\\n\\n            ``\\'fail\\'``\\n                If table exists, do nothing.\\n            ``\\'replace\\'``\\n                If table exists, drop it, recreate it, and insert data.\\n            ``\\'append\\'``\\n                If table exists, insert data. Create if does not exist.\\n        auth_local_webserver : bool, default False\\n            Use the `local webserver flow`_ instead of the `console flow`_\\n            when getting user credentials.\\n\\n            .. _local webserver flow:\\n                http://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_local_server\\n            .. _console flow:\\n                http://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_console\\n\\n            *New in version 0.2.0 of pandas-gbq*.\\n        table_schema : list of dicts, optional\\n            List of BigQuery table fields to which according DataFrame\\n            columns conform to, e.g. ``[{\\'name\\': \\'col1\\', \\'type\\':\\n            \\'STRING\\'},...]``. If schema is not provided, it will be\\n            generated according to dtypes of DataFrame columns. See\\n            BigQuery API documentation on available names of a field.\\n\\n            *New in version 0.3.1 of pandas-gbq*.\\n        location : str, optional\\n            Location where the load job should run. See the `BigQuery locations\\n            documentation\\n            <https://cloud.google.com/bigquery/docs/dataset-locations>`__ for a\\n            list of available locations. The location must match that of the\\n            target dataset.\\n\\n            *New in version 0.5.0 of pandas-gbq*.\\n        progress_bar : bool, default True\\n            Use the library `tqdm` to show the progress bar for the upload,\\n            chunk by chunk.\\n\\n            *New in version 0.5.0 of pandas-gbq*.\\n        credentials : google.auth.credentials.Credentials, optional\\n            Credentials for accessing Google APIs. Use this parameter to\\n            override default credentials, such as to use Compute Engine\\n            :class:`google.auth.compute_engine.Credentials` or Service\\n            Account :class:`google.oauth2.service_account.Credentials`\\n            directly.\\n\\n            *New in version 0.8.0 of pandas-gbq*.\\n\\n            .. versionadded:: 0.24.0\\n        verbose : bool, deprecated\\n            Deprecated in pandas-gbq version 0.4.0. Use the `logging module\\n            to adjust verbosity instead\\n            <https://pandas-gbq.readthedocs.io/en/latest/intro.html#logging>`__.\\n        private_key : str, deprecated\\n            Deprecated in pandas-gbq version 0.8.0. Use the ``credentials``\\n            parameter and\\n            :func:`google.oauth2.service_account.Credentials.from_service_account_info`\\n            or\\n            :func:`google.oauth2.service_account.Credentials.from_service_account_file`\\n            instead.\\n\\n            Service account private key in JSON format. Can be file path\\n            or string contents. This is useful for remote server\\n            authentication (eg. Jupyter/IPython notebook on remote host).\\n\\n        See Also\\n        --------\\n        pandas_gbq.to_gbq : This function in the pandas-gbq library.\\n        read_gbq : Read a DataFrame from Google BigQuery.\\n        \"\"\"\\n        from pandas.io import gbq\\n        return gbq.to_gbq(\\n            self, destination_table, project_id=project_id,\\n            chunksize=chunksize, reauth=reauth, if_exists=if_exists,\\n            auth_local_webserver=auth_local_webserver,\\n            table_schema=table_schema, location=location,\\n            progress_bar=progress_bar, credentials=credentials,\\n            verbose=verbose, private_key=private_key)',\n 'def from_records(cls, data, index=None, exclude=None, columns=None,\\n                     coerce_float=False, nrows=None):\\n        \"\"\"\\n        Convert structured or record ndarray to DataFrame.\\n\\n        Parameters\\n        ----------\\n        data : ndarray (structured dtype), list of tuples, dict, or DataFrame\\n        index : string, list of fields, array-like\\n            Field of array to use as the index, alternately a specific set of\\n            input labels to use\\n        exclude : sequence, default None\\n            Columns or fields to exclude\\n        columns : sequence, default None\\n            Column names to use. If the passed data do not have names\\n            associated with them, this argument provides names for the\\n            columns. Otherwise this argument indicates the order of the columns\\n            in the result (any names not found in the data will become all-NA\\n            columns)\\n        coerce_float : boolean, default False\\n            Attempt to convert values of non-string, non-numeric objects (like\\n            decimal.Decimal) to floating point, useful for SQL result sets\\n        nrows : int, default None\\n            Number of rows to read if data is an iterator\\n\\n        Returns\\n        -------\\n        DataFrame\\n        \"\"\"\\n\\n        # Make a copy of the input columns so we can modify it\\n        if columns is not None:\\n            columns = ensure_index(columns)\\n\\n        if is_iterator(data):\\n            if nrows == 0:\\n                return cls()\\n\\n            try:\\n                first_row = next(data)\\n            except StopIteration:\\n                return cls(index=index, columns=columns)\\n\\n            dtype = None\\n            if hasattr(first_row, \\'dtype\\') and first_row.dtype.names:\\n                dtype = first_row.dtype\\n\\n            values = [first_row]\\n\\n            if nrows is None:\\n                values += data\\n            else:\\n                values.extend(itertools.islice(data, nrows - 1))\\n\\n            if dtype is not None:\\n                data = np.array(values, dtype=dtype)\\n            else:\\n                data = values\\n\\n        if isinstance(data, dict):\\n            if columns is None:\\n                columns = arr_columns = ensure_index(sorted(data))\\n                arrays = [data[k] for k in columns]\\n            else:\\n                arrays = []\\n                arr_columns = []\\n                for k, v in data.items():\\n                    if k in columns:\\n                        arr_columns.append(k)\\n                        arrays.append(v)\\n\\n                arrays, arr_columns = reorder_arrays(arrays, arr_columns,\\n                                                     columns)\\n\\n        elif isinstance(data, (np.ndarray, DataFrame)):\\n            arrays, columns = to_arrays(data, columns)\\n            if columns is not None:\\n                columns = ensure_index(columns)\\n            arr_columns = columns\\n        else:\\n            arrays, arr_columns = to_arrays(data, columns,\\n                                            coerce_float=coerce_float)\\n\\n            arr_columns = ensure_index(arr_columns)\\n            if columns is not None:\\n                columns = ensure_index(columns)\\n            else:\\n                columns = arr_columns\\n\\n        if exclude is None:\\n            exclude = set()\\n        else:\\n            exclude = set(exclude)\\n\\n        result_index = None\\n        if index is not None:\\n            if (isinstance(index, str) or\\n                    not hasattr(index, \"__iter__\")):\\n                i = columns.get_loc(index)\\n                exclude.add(index)\\n                if len(arrays) > 0:\\n                    result_index = Index(arrays[i], name=index)\\n                else:\\n                    result_index = Index([], name=index)\\n            else:\\n                try:\\n                    index_data = [arrays[arr_columns.get_loc(field)]\\n                                  for field in index]\\n                    result_index = ensure_index_from_sequences(index_data,\\n                                                               names=index)\\n\\n                    exclude.update(index)\\n                except Exception:\\n                    result_index = index\\n\\n        if any(exclude):\\n            arr_exclude = [x for x in exclude if x in arr_columns]\\n            to_remove = [arr_columns.get_loc(col) for col in arr_exclude]\\n            arrays = [v for i, v in enumerate(arrays) if i not in to_remove]\\n\\n            arr_columns = arr_columns.drop(arr_exclude)\\n            columns = columns.drop(exclude)\\n\\n        mgr = arrays_to_mgr(arrays, arr_columns, result_index, columns)\\n\\n        return cls(mgr)',\n 'def to_records(self, index=True, convert_datetime64=None,\\n                   column_dtypes=None, index_dtypes=None):\\n        \"\"\"\\n        Convert DataFrame to a NumPy record array.\\n\\n        Index will be included as the first field of the record array if\\n        requested.\\n\\n        Parameters\\n        ----------\\n        index : bool, default True\\n            Include index in resulting record array, stored in \\'index\\'\\n            field or using the index label, if set.\\n        convert_datetime64 : bool, default None\\n            .. deprecated:: 0.23.0\\n\\n            Whether to convert the index to datetime.datetime if it is a\\n            DatetimeIndex.\\n        column_dtypes : str, type, dict, default None\\n            .. versionadded:: 0.24.0\\n\\n            If a string or type, the data type to store all columns. If\\n            a dictionary, a mapping of column names and indices (zero-indexed)\\n            to specific data types.\\n        index_dtypes : str, type, dict, default None\\n            .. versionadded:: 0.24.0\\n\\n            If a string or type, the data type to store all index levels. If\\n            a dictionary, a mapping of index level names and indices\\n            (zero-indexed) to specific data types.\\n\\n            This mapping is applied only if `index=True`.\\n\\n        Returns\\n        -------\\n        numpy.recarray\\n            NumPy ndarray with the DataFrame labels as fields and each row\\n            of the DataFrame as entries.\\n\\n        See Also\\n        --------\\n        DataFrame.from_records: Convert structured or record ndarray\\n            to DataFrame.\\n        numpy.recarray: An ndarray that allows field access using\\n            attributes, analogous to typed columns in a\\n            spreadsheet.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'A\\': [1, 2], \\'B\\': [0.5, 0.75]},\\n        ...                   index=[\\'a\\', \\'b\\'])\\n        >>> df\\n           A     B\\n        a  1  0.50\\n        b  2  0.75\\n        >>> df.to_records()\\n        rec.array([(\\'a\\', 1, 0.5 ), (\\'b\\', 2, 0.75)],\\n                  dtype=[(\\'index\\', \\'O\\'), (\\'A\\', \\'<i8\\'), (\\'B\\', \\'<f8\\')])\\n\\n        If the DataFrame index has no label then the recarray field name\\n        is set to \\'index\\'. If the index has a label then this is used as the\\n        field name:\\n\\n        >>> df.index = df.index.rename(\"I\")\\n        >>> df.to_records()\\n        rec.array([(\\'a\\', 1, 0.5 ), (\\'b\\', 2, 0.75)],\\n                  dtype=[(\\'I\\', \\'O\\'), (\\'A\\', \\'<i8\\'), (\\'B\\', \\'<f8\\')])\\n\\n        The index can be excluded from the record array:\\n\\n        >>> df.to_records(index=False)\\n        rec.array([(1, 0.5 ), (2, 0.75)],\\n                  dtype=[(\\'A\\', \\'<i8\\'), (\\'B\\', \\'<f8\\')])\\n\\n        Data types can be specified for the columns:\\n\\n        >>> df.to_records(column_dtypes={\"A\": \"int32\"})\\n        rec.array([(\\'a\\', 1, 0.5 ), (\\'b\\', 2, 0.75)],\\n                  dtype=[(\\'I\\', \\'O\\'), (\\'A\\', \\'<i4\\'), (\\'B\\', \\'<f8\\')])\\n\\n        As well as for the index:\\n\\n        >>> df.to_records(index_dtypes=\"<S2\")\\n        rec.array([(b\\'a\\', 1, 0.5 ), (b\\'b\\', 2, 0.75)],\\n                  dtype=[(\\'I\\', \\'S2\\'), (\\'A\\', \\'<i8\\'), (\\'B\\', \\'<f8\\')])\\n\\n        >>> index_dtypes = \"<S{}\".format(df.index.str.len().max())\\n        >>> df.to_records(index_dtypes=index_dtypes)\\n        rec.array([(b\\'a\\', 1, 0.5 ), (b\\'b\\', 2, 0.75)],\\n                  dtype=[(\\'I\\', \\'S1\\'), (\\'A\\', \\'<i8\\'), (\\'B\\', \\'<f8\\')])\\n        \"\"\"\\n\\n        if convert_datetime64 is not None:\\n            warnings.warn(\"The \\'convert_datetime64\\' parameter is \"\\n                          \"deprecated and will be removed in a future \"\\n                          \"version\",\\n                          FutureWarning, stacklevel=2)\\n\\n        if index:\\n            if is_datetime64_any_dtype(self.index) and convert_datetime64:\\n                ix_vals = [self.index.to_pydatetime()]\\n            else:\\n                if isinstance(self.index, MultiIndex):\\n                    # array of tuples to numpy cols. copy copy copy\\n                    ix_vals = lmap(np.array, zip(*self.index.values))\\n                else:\\n                    ix_vals = [self.index.values]\\n\\n            arrays = ix_vals + [self[c].get_values() for c in self.columns]\\n\\n            count = 0\\n            index_names = list(self.index.names)\\n\\n            if isinstance(self.index, MultiIndex):\\n                for i, n in enumerate(index_names):\\n                    if n is None:\\n                        index_names[i] = \\'level_%d\\' % count\\n                        count += 1\\n            elif index_names[0] is None:\\n                index_names = [\\'index\\']\\n\\n            names = lmap(str, index_names) + lmap(str, self.columns)\\n        else:\\n            arrays = [self[c].get_values() for c in self.columns]\\n            names = lmap(str, self.columns)\\n            index_names = []\\n\\n        index_len = len(index_names)\\n        formats = []\\n\\n        for i, v in enumerate(arrays):\\n            index = i\\n\\n            # When the names and arrays are collected, we\\n            # first collect those in the DataFrame\\'s index,\\n            # followed by those in its columns.\\n            #\\n            # Thus, the total length of the array is:\\n            # len(index_names) + len(DataFrame.columns).\\n            #\\n            # This check allows us to see whether we are\\n            # handling a name / array in the index or column.\\n            if index < index_len:\\n                dtype_mapping = index_dtypes\\n                name = index_names[index]\\n            else:\\n                index -= index_len\\n                dtype_mapping = column_dtypes\\n                name = self.columns[index]\\n\\n            # We have a dictionary, so we get the data type\\n            # associated with the index or column (which can\\n            # be denoted by its name in the DataFrame or its\\n            # position in DataFrame\\'s array of indices or\\n            # columns, whichever is applicable.\\n            if is_dict_like(dtype_mapping):\\n                if name in dtype_mapping:\\n                    dtype_mapping = dtype_mapping[name]\\n                elif index in dtype_mapping:\\n                    dtype_mapping = dtype_mapping[index]\\n                else:\\n                    dtype_mapping = None\\n\\n            # If no mapping can be found, use the array\\'s\\n            # dtype attribute for formatting.\\n            #\\n            # A valid dtype must either be a type or\\n            # string naming a type.\\n            if dtype_mapping is None:\\n                formats.append(v.dtype)\\n            elif isinstance(dtype_mapping, (type, np.dtype, str)):\\n                formats.append(dtype_mapping)\\n            else:\\n                element = \"row\" if i < index_len else \"column\"\\n                msg = (\"Invalid dtype {dtype} specified for \"\\n                       \"{element} {name}\").format(dtype=dtype_mapping,\\n                                                  element=element, name=name)\\n                raise ValueError(msg)\\n\\n        return np.rec.fromarrays(\\n            arrays,\\n            dtype={\\'names\\': names, \\'formats\\': formats}\\n        )',\n 'def from_items(cls, items, columns=None, orient=\\'columns\\'):\\n        \"\"\"\\n        Construct a DataFrame from a list of tuples.\\n\\n        .. deprecated:: 0.23.0\\n          `from_items` is deprecated and will be removed in a future version.\\n          Use :meth:`DataFrame.from_dict(dict(items)) <DataFrame.from_dict>`\\n          instead.\\n          :meth:`DataFrame.from_dict(OrderedDict(items)) <DataFrame.from_dict>`\\n          may be used to preserve the key order.\\n\\n        Convert (key, value) pairs to DataFrame. The keys will be the axis\\n        index (usually the columns, but depends on the specified\\n        orientation). The values should be arrays or Series.\\n\\n        Parameters\\n        ----------\\n        items : sequence of (key, value) pairs\\n            Values should be arrays or Series.\\n        columns : sequence of column labels, optional\\n            Must be passed if orient=\\'index\\'.\\n        orient : {\\'columns\\', \\'index\\'}, default \\'columns\\'\\n            The \"orientation\" of the data. If the keys of the\\n            input correspond to column labels, pass \\'columns\\'\\n            (default). Otherwise if the keys correspond to the index,\\n            pass \\'index\\'.\\n\\n        Returns\\n        -------\\n        DataFrame\\n        \"\"\"\\n\\n        warnings.warn(\"from_items is deprecated. Please use \"\\n                      \"DataFrame.from_dict(dict(items), ...) instead. \"\\n                      \"DataFrame.from_dict(OrderedDict(items)) may be used to \"\\n                      \"preserve the key order.\",\\n                      FutureWarning, stacklevel=2)\\n\\n        keys, values = lzip(*items)\\n\\n        if orient == \\'columns\\':\\n            if columns is not None:\\n                columns = ensure_index(columns)\\n\\n                idict = dict(items)\\n                if len(idict) < len(items):\\n                    if not columns.equals(ensure_index(keys)):\\n                        raise ValueError(\\'With non-unique item names, passed \\'\\n                                         \\'columns must be identical\\')\\n                    arrays = values\\n                else:\\n                    arrays = [idict[k] for k in columns if k in idict]\\n            else:\\n                columns = ensure_index(keys)\\n                arrays = values\\n\\n            # GH 17312\\n            # Provide more informative error msg when scalar values passed\\n            try:\\n                return cls._from_arrays(arrays, columns, None)\\n\\n            except ValueError:\\n                if not is_nested_list_like(values):\\n                    raise ValueError(\\'The value in each (key, value) pair \\'\\n                                     \\'must be an array, Series, or dict\\')\\n\\n        elif orient == \\'index\\':\\n            if columns is None:\\n                raise TypeError(\"Must pass columns with orient=\\'index\\'\")\\n\\n            keys = ensure_index(keys)\\n\\n            # GH 17312\\n            # Provide more informative error msg when scalar values passed\\n            try:\\n                arr = np.array(values, dtype=object).T\\n                data = [lib.maybe_convert_objects(v) for v in arr]\\n                return cls._from_arrays(data, columns, keys)\\n\\n            except TypeError:\\n                if not is_nested_list_like(values):\\n                    raise ValueError(\\'The value in each (key, value) pair \\'\\n                                     \\'must be an array, Series, or dict\\')\\n\\n        else:  # pragma: no cover\\n            raise ValueError(\"\\'orient\\' must be either \\'columns\\' or \\'index\\'\")',\n 'def from_csv(cls, path, header=0, sep=\\',\\', index_col=0, parse_dates=True,\\n                 encoding=None, tupleize_cols=None,\\n                 infer_datetime_format=False):\\n        \"\"\"\\n        Read CSV file.\\n\\n        .. deprecated:: 0.21.0\\n            Use :func:`read_csv` instead.\\n\\n        It is preferable to use the more powerful :func:`read_csv`\\n        for most general purposes, but ``from_csv`` makes for an easy\\n        roundtrip to and from a file (the exact counterpart of\\n        ``to_csv``), especially with a DataFrame of time series data.\\n\\n        This method only differs from the preferred :func:`read_csv`\\n        in some defaults:\\n\\n        - `index_col` is ``0`` instead of ``None`` (take first column as index\\n          by default)\\n        - `parse_dates` is ``True`` instead of ``False`` (try parsing the index\\n          as datetime by default)\\n\\n        So a ``pd.DataFrame.from_csv(path)`` can be replaced by\\n        ``pd.read_csv(path, index_col=0, parse_dates=True)``.\\n\\n        Parameters\\n        ----------\\n        path : string file path or file handle / StringIO\\n        header : int, default 0\\n            Row to use as header (skip prior rows)\\n        sep : string, default \\',\\'\\n            Field delimiter\\n        index_col : int or sequence, default 0\\n            Column to use for index. If a sequence is given, a MultiIndex\\n            is used. Different default from read_table\\n        parse_dates : boolean, default True\\n            Parse dates. Different default from read_table\\n        tupleize_cols : boolean, default False\\n            write multi_index columns as a list of tuples (if True)\\n            or new (expanded format) if False)\\n        infer_datetime_format : boolean, default False\\n            If True and `parse_dates` is True for a column, try to infer the\\n            datetime format based on the first datetime string. If the format\\n            can be inferred, there often will be a large parsing speed-up.\\n\\n        Returns\\n        -------\\n        DataFrame\\n\\n        See Also\\n        --------\\n        read_csv\\n        \"\"\"\\n\\n        warnings.warn(\"from_csv is deprecated. Please use read_csv(...) \"\\n                      \"instead. Note that some of the default arguments are \"\\n                      \"different, so please refer to the documentation \"\\n                      \"for from_csv when changing your function calls\",\\n                      FutureWarning, stacklevel=2)\\n\\n        from pandas.io.parsers import read_csv\\n        return read_csv(path, header=header, sep=sep,\\n                        parse_dates=parse_dates, index_col=index_col,\\n                        encoding=encoding, tupleize_cols=tupleize_cols,\\n                        infer_datetime_format=infer_datetime_format)',\n 'def to_sparse(self, fill_value=None, kind=\\'block\\'):\\n        \"\"\"\\n        Convert to SparseDataFrame.\\n\\n        Implement the sparse version of the DataFrame meaning that any data\\n        matching a specific value it\\'s omitted in the representation.\\n        The sparse DataFrame allows for a more efficient storage.\\n\\n        Parameters\\n        ----------\\n        fill_value : float, default None\\n            The specific value that should be omitted in the representation.\\n        kind : {\\'block\\', \\'integer\\'}, default \\'block\\'\\n            The kind of the SparseIndex tracking where data is not equal to\\n            the fill value:\\n\\n            - \\'block\\' tracks only the locations and sizes of blocks of data.\\n            - \\'integer\\' keeps an array with all the locations of the data.\\n\\n            In most cases \\'block\\' is recommended, since it\\'s more memory\\n            efficient.\\n\\n        Returns\\n        -------\\n        SparseDataFrame\\n            The sparse representation of the DataFrame.\\n\\n        See Also\\n        --------\\n        DataFrame.to_dense :\\n            Converts the DataFrame back to the its dense form.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame([(np.nan, np.nan),\\n        ...                    (1., np.nan),\\n        ...                    (np.nan, 1.)])\\n        >>> df\\n             0    1\\n        0  NaN  NaN\\n        1  1.0  NaN\\n        2  NaN  1.0\\n        >>> type(df)\\n        <class \\'pandas.core.frame.DataFrame\\'>\\n\\n        >>> sdf = df.to_sparse()\\n        >>> sdf\\n             0    1\\n        0  NaN  NaN\\n        1  1.0  NaN\\n        2  NaN  1.0\\n        >>> type(sdf)\\n        <class \\'pandas.core.sparse.frame.SparseDataFrame\\'>\\n        \"\"\"\\n        from pandas.core.sparse.api import SparseDataFrame\\n        return SparseDataFrame(self._series, index=self.index,\\n                               columns=self.columns, default_kind=kind,\\n                               default_fill_value=fill_value)',\n 'def to_stata(self, fname, convert_dates=None, write_index=True,\\n                 encoding=\"latin-1\", byteorder=None, time_stamp=None,\\n                 data_label=None, variable_labels=None, version=114,\\n                 convert_strl=None):\\n        \"\"\"\\n        Export DataFrame object to Stata dta format.\\n\\n        Writes the DataFrame to a Stata dataset file.\\n        \"dta\" files contain a Stata dataset.\\n\\n        Parameters\\n        ----------\\n        fname : str, buffer or path object\\n            String, path object (pathlib.Path or py._path.local.LocalPath) or\\n            object implementing a binary write() function. If using a buffer\\n            then the buffer will not be automatically closed after the file\\n            data has been written.\\n        convert_dates : dict\\n            Dictionary mapping columns containing datetime types to stata\\n            internal format to use when writing the dates. Options are \\'tc\\',\\n            \\'td\\', \\'tm\\', \\'tw\\', \\'th\\', \\'tq\\', \\'ty\\'. Column can be either an integer\\n            or a name. Datetime columns that do not have a conversion type\\n            specified will be converted to \\'tc\\'. Raises NotImplementedError if\\n            a datetime column has timezone information.\\n        write_index : bool\\n            Write the index to Stata dataset.\\n        encoding : str\\n            Default is latin-1. Unicode is not supported.\\n        byteorder : str\\n            Can be \">\", \"<\", \"little\", or \"big\". default is `sys.byteorder`.\\n        time_stamp : datetime\\n            A datetime to use as file creation date.  Default is the current\\n            time.\\n        data_label : str, optional\\n            A label for the data set.  Must be 80 characters or smaller.\\n        variable_labels : dict\\n            Dictionary containing columns as keys and variable labels as\\n            values. Each label must be 80 characters or smaller.\\n\\n            .. versionadded:: 0.19.0\\n\\n        version : {114, 117}, default 114\\n            Version to use in the output dta file.  Version 114 can be used\\n            read by Stata 10 and later.  Version 117 can be read by Stata 13\\n            or later. Version 114 limits string variables to 244 characters or\\n            fewer while 117 allows strings with lengths up to 2,000,000\\n            characters.\\n\\n            .. versionadded:: 0.23.0\\n\\n        convert_strl : list, optional\\n            List of column names to convert to string columns to Stata StrL\\n            format. Only available if version is 117.  Storing strings in the\\n            StrL format can produce smaller dta files if strings have more than\\n            8 characters and values are repeated.\\n\\n            .. versionadded:: 0.23.0\\n\\n        Raises\\n        ------\\n        NotImplementedError\\n            * If datetimes contain timezone information\\n            * Column dtype is not representable in Stata\\n        ValueError\\n            * Columns listed in convert_dates are neither datetime64[ns]\\n              or datetime.datetime\\n            * Column listed in convert_dates is not in DataFrame\\n            * Categorical label contains more than 32,000 characters\\n\\n            .. versionadded:: 0.19.0\\n\\n        See Also\\n        --------\\n        read_stata : Import Stata data files.\\n        io.stata.StataWriter : Low-level writer for Stata data files.\\n        io.stata.StataWriter117 : Low-level writer for version 117 files.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'animal\\': [\\'falcon\\', \\'parrot\\', \\'falcon\\',\\n        ...                               \\'parrot\\'],\\n        ...                    \\'speed\\': [350, 18, 361, 15]})\\n        >>> df.to_stata(\\'animals.dta\\')  # doctest: +SKIP\\n        \"\"\"\\n        kwargs = {}\\n        if version not in (114, 117):\\n            raise ValueError(\\'Only formats 114 and 117 supported.\\')\\n        if version == 114:\\n            if convert_strl is not None:\\n                raise ValueError(\\'strl support is only available when using \\'\\n                                 \\'format 117\\')\\n            from pandas.io.stata import StataWriter as statawriter\\n        else:\\n            from pandas.io.stata import StataWriter117 as statawriter\\n            kwargs[\\'convert_strl\\'] = convert_strl\\n\\n        writer = statawriter(fname, self, convert_dates=convert_dates,\\n                             byteorder=byteorder, time_stamp=time_stamp,\\n                             data_label=data_label, write_index=write_index,\\n                             variable_labels=variable_labels, **kwargs)\\n        writer.write_file()',\n 'def to_feather(self, fname):\\n        \"\"\"\\n        Write out the binary feather-format for DataFrames.\\n\\n        .. versionadded:: 0.20.0\\n\\n        Parameters\\n        ----------\\n        fname : str\\n            string file path\\n        \"\"\"\\n        from pandas.io.feather_format import to_feather\\n        to_feather(self, fname)',\n 'def to_parquet(self, fname, engine=\\'auto\\', compression=\\'snappy\\',\\n                   index=None, partition_cols=None, **kwargs):\\n        \"\"\"\\n        Write a DataFrame to the binary parquet format.\\n\\n        .. versionadded:: 0.21.0\\n\\n        This function writes the dataframe as a `parquet file\\n        <https://parquet.apache.org/>`_. You can choose different parquet\\n        backends, and have the option of compression. See\\n        :ref:`the user guide <io.parquet>` for more details.\\n\\n        Parameters\\n        ----------\\n        fname : str\\n            File path or Root Directory path. Will be used as Root Directory\\n            path while writing a partitioned dataset.\\n\\n            .. versionchanged:: 0.24.0\\n\\n        engine : {\\'auto\\', \\'pyarrow\\', \\'fastparquet\\'}, default \\'auto\\'\\n            Parquet library to use. If \\'auto\\', then the option\\n            ``io.parquet.engine`` is used. The default ``io.parquet.engine``\\n            behavior is to try \\'pyarrow\\', falling back to \\'fastparquet\\' if\\n            \\'pyarrow\\' is unavailable.\\n        compression : {\\'snappy\\', \\'gzip\\', \\'brotli\\', None}, default \\'snappy\\'\\n            Name of the compression to use. Use ``None`` for no compression.\\n        index : bool, default None\\n            If ``True``, include the dataframe\\'s index(es) in the file output.\\n            If ``False``, they will not be written to the file. If ``None``,\\n            the behavior depends on the chosen engine.\\n\\n            .. versionadded:: 0.24.0\\n\\n        partition_cols : list, optional, default None\\n            Column names by which to partition the dataset\\n            Columns are partitioned in the order they are given\\n\\n            .. versionadded:: 0.24.0\\n\\n        **kwargs\\n            Additional arguments passed to the parquet library. See\\n            :ref:`pandas io <io.parquet>` for more details.\\n\\n        See Also\\n        --------\\n        read_parquet : Read a parquet file.\\n        DataFrame.to_csv : Write a csv file.\\n        DataFrame.to_sql : Write to a sql table.\\n        DataFrame.to_hdf : Write to hdf.\\n\\n        Notes\\n        -----\\n        This function requires either the `fastparquet\\n        <https://pypi.org/project/fastparquet>`_ or `pyarrow\\n        <https://arrow.apache.org/docs/python/>`_ library.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame(data={\\'col1\\': [1, 2], \\'col2\\': [3, 4]})\\n        >>> df.to_parquet(\\'df.parquet.gzip\\',\\n        ...               compression=\\'gzip\\')  # doctest: +SKIP\\n        >>> pd.read_parquet(\\'df.parquet.gzip\\')  # doctest: +SKIP\\n           col1  col2\\n        0     1     3\\n        1     2     4\\n        \"\"\"\\n        from pandas.io.parquet import to_parquet\\n        to_parquet(self, fname, engine,\\n                   compression=compression, index=index,\\n                   partition_cols=partition_cols, **kwargs)',\n 'def to_html(self, buf=None, columns=None, col_space=None, header=True,\\n                index=True, na_rep=\\'NaN\\', formatters=None, float_format=None,\\n                sparsify=None, index_names=True, justify=None, max_rows=None,\\n                max_cols=None, show_dimensions=False, decimal=\\'.\\',\\n                bold_rows=True, classes=None, escape=True, notebook=False,\\n                border=None, table_id=None, render_links=False):\\n        \"\"\"\\n        Render a DataFrame as an HTML table.\\n        %(shared_params)s\\n        bold_rows : bool, default True\\n            Make the row labels bold in the output.\\n        classes : str or list or tuple, default None\\n            CSS class(es) to apply to the resulting html table.\\n        escape : bool, default True\\n            Convert the characters <, >, and & to HTML-safe sequences.\\n        notebook : {True, False}, default False\\n            Whether the generated HTML is for IPython Notebook.\\n        border : int\\n            A ``border=border`` attribute is included in the opening\\n            `<table>` tag. Default ``pd.options.html.border``.\\n\\n            .. versionadded:: 0.19.0\\n\\n        table_id : str, optional\\n            A css id is included in the opening `<table>` tag if specified.\\n\\n            .. versionadded:: 0.23.0\\n\\n        render_links : bool, default False\\n            Convert URLs to HTML links.\\n\\n            .. versionadded:: 0.24.0\\n        %(returns)s\\n        See Also\\n        --------\\n        to_string : Convert DataFrame to a string.\\n        \"\"\"\\n\\n        if (justify is not None and\\n                justify not in fmt._VALID_JUSTIFY_PARAMETERS):\\n            raise ValueError(\"Invalid value for justify parameter\")\\n\\n        formatter = fmt.DataFrameFormatter(self, buf=buf, columns=columns,\\n                                           col_space=col_space, na_rep=na_rep,\\n                                           formatters=formatters,\\n                                           float_format=float_format,\\n                                           sparsify=sparsify, justify=justify,\\n                                           index_names=index_names,\\n                                           header=header, index=index,\\n                                           bold_rows=bold_rows, escape=escape,\\n                                           max_rows=max_rows,\\n                                           max_cols=max_cols,\\n                                           show_dimensions=show_dimensions,\\n                                           decimal=decimal, table_id=table_id,\\n                                           render_links=render_links)\\n        # TODO: a generic formatter wld b in DataFrameFormatter\\n        formatter.to_html(classes=classes, notebook=notebook, border=border)\\n\\n        if buf is None:\\n            return formatter.buf.getvalue()',\n 'def info(self, verbose=None, buf=None, max_cols=None, memory_usage=None,\\n             null_counts=None):\\n        \"\"\"\\n        Print a concise summary of a DataFrame.\\n\\n        This method prints information about a DataFrame including\\n        the index dtype and column dtypes, non-null values and memory usage.\\n\\n        Parameters\\n        ----------\\n        verbose : bool, optional\\n            Whether to print the full summary. By default, the setting in\\n            ``pandas.options.display.max_info_columns`` is followed.\\n        buf : writable buffer, defaults to sys.stdout\\n            Where to send the output. By default, the output is printed to\\n            sys.stdout. Pass a writable buffer if you need to further process\\n            the output.\\n        max_cols : int, optional\\n            When to switch from the verbose to the truncated output. If the\\n            DataFrame has more than `max_cols` columns, the truncated output\\n            is used. By default, the setting in\\n            ``pandas.options.display.max_info_columns`` is used.\\n        memory_usage : bool, str, optional\\n            Specifies whether total memory usage of the DataFrame\\n            elements (including the index) should be displayed. By default,\\n            this follows the ``pandas.options.display.memory_usage`` setting.\\n\\n            True always show memory usage. False never shows memory usage.\\n            A value of \\'deep\\' is equivalent to \"True with deep introspection\".\\n            Memory usage is shown in human-readable units (base-2\\n            representation). Without deep introspection a memory estimation is\\n            made based in column dtype and number of rows assuming values\\n            consume the same memory amount for corresponding dtypes. With deep\\n            memory introspection, a real memory usage calculation is performed\\n            at the cost of computational resources.\\n        null_counts : bool, optional\\n            Whether to show the non-null counts. By default, this is shown\\n            only if the frame is smaller than\\n            ``pandas.options.display.max_info_rows`` and\\n            ``pandas.options.display.max_info_columns``. A value of True always\\n            shows the counts, and False never shows the counts.\\n\\n        Returns\\n        -------\\n        None\\n            This method prints a summary of a DataFrame and returns None.\\n\\n        See Also\\n        --------\\n        DataFrame.describe: Generate descriptive statistics of DataFrame\\n            columns.\\n        DataFrame.memory_usage: Memory usage of DataFrame columns.\\n\\n        Examples\\n        --------\\n        >>> int_values = [1, 2, 3, 4, 5]\\n        >>> text_values = [\\'alpha\\', \\'beta\\', \\'gamma\\', \\'delta\\', \\'epsilon\\']\\n        >>> float_values = [0.0, 0.25, 0.5, 0.75, 1.0]\\n        >>> df = pd.DataFrame({\"int_col\": int_values, \"text_col\": text_values,\\n        ...                   \"float_col\": float_values})\\n        >>> df\\n           int_col text_col  float_col\\n        0        1    alpha       0.00\\n        1        2     beta       0.25\\n        2        3    gamma       0.50\\n        3        4    delta       0.75\\n        4        5  epsilon       1.00\\n\\n        Prints information of all columns:\\n\\n        >>> df.info(verbose=True)\\n        <class \\'pandas.core.frame.DataFrame\\'>\\n        RangeIndex: 5 entries, 0 to 4\\n        Data columns (total 3 columns):\\n        int_col      5 non-null int64\\n        text_col     5 non-null object\\n        float_col    5 non-null float64\\n        dtypes: float64(1), int64(1), object(1)\\n        memory usage: 200.0+ bytes\\n\\n        Prints a summary of columns count and its dtypes but not per column\\n        information:\\n\\n        >>> df.info(verbose=False)\\n        <class \\'pandas.core.frame.DataFrame\\'>\\n        RangeIndex: 5 entries, 0 to 4\\n        Columns: 3 entries, int_col to float_col\\n        dtypes: float64(1), int64(1), object(1)\\n        memory usage: 200.0+ bytes\\n\\n        Pipe output of DataFrame.info to buffer instead of sys.stdout, get\\n        buffer content and writes to a text file:\\n\\n        >>> import io\\n        >>> buffer = io.StringIO()\\n        >>> df.info(buf=buffer)\\n        >>> s = buffer.getvalue()\\n        >>> with open(\"df_info.txt\", \"w\",\\n        ...           encoding=\"utf-8\") as f:  # doctest: +SKIP\\n        ...     f.write(s)\\n        260\\n\\n        The `memory_usage` parameter allows deep introspection mode, specially\\n        useful for big DataFrames and fine-tune memory optimization:\\n\\n        >>> random_strings_array = np.random.choice([\\'a\\', \\'b\\', \\'c\\'], 10 ** 6)\\n        >>> df = pd.DataFrame({\\n        ...     \\'column_1\\': np.random.choice([\\'a\\', \\'b\\', \\'c\\'], 10 ** 6),\\n        ...     \\'column_2\\': np.random.choice([\\'a\\', \\'b\\', \\'c\\'], 10 ** 6),\\n        ...     \\'column_3\\': np.random.choice([\\'a\\', \\'b\\', \\'c\\'], 10 ** 6)\\n        ... })\\n        >>> df.info()\\n        <class \\'pandas.core.frame.DataFrame\\'>\\n        RangeIndex: 1000000 entries, 0 to 999999\\n        Data columns (total 3 columns):\\n        column_1    1000000 non-null object\\n        column_2    1000000 non-null object\\n        column_3    1000000 non-null object\\n        dtypes: object(3)\\n        memory usage: 22.9+ MB\\n\\n        >>> df.info(memory_usage=\\'deep\\')\\n        <class \\'pandas.core.frame.DataFrame\\'>\\n        RangeIndex: 1000000 entries, 0 to 999999\\n        Data columns (total 3 columns):\\n        column_1    1000000 non-null object\\n        column_2    1000000 non-null object\\n        column_3    1000000 non-null object\\n        dtypes: object(3)\\n        memory usage: 188.8 MB\\n        \"\"\"\\n\\n        if buf is None:  # pragma: no cover\\n            buf = sys.stdout\\n\\n        lines = []\\n\\n        lines.append(str(type(self)))\\n        lines.append(self.index._summary())\\n\\n        if len(self.columns) == 0:\\n            lines.append(\\'Empty {name}\\'.format(name=type(self).__name__))\\n            fmt.buffer_put_lines(buf, lines)\\n            return\\n\\n        cols = self.columns\\n\\n        # hack\\n        if max_cols is None:\\n            max_cols = get_option(\\'display.max_info_columns\\',\\n                                  len(self.columns) + 1)\\n\\n        max_rows = get_option(\\'display.max_info_rows\\', len(self) + 1)\\n\\n        if null_counts is None:\\n            show_counts = ((len(self.columns) <= max_cols) and\\n                           (len(self) < max_rows))\\n        else:\\n            show_counts = null_counts\\n        exceeds_info_cols = len(self.columns) > max_cols\\n\\n        def _verbose_repr():\\n            lines.append(\\'Data columns (total %d columns):\\' %\\n                         len(self.columns))\\n            space = max(len(pprint_thing(k)) for k in self.columns) + 4\\n            counts = None\\n\\n            tmpl = \"{count}{dtype}\"\\n            if show_counts:\\n                counts = self.count()\\n                if len(cols) != len(counts):  # pragma: no cover\\n                    raise AssertionError(\\n                        \\'Columns must equal counts \\'\\n                        \\'({cols:d} != {counts:d})\\'.format(\\n                            cols=len(cols), counts=len(counts)))\\n                tmpl = \"{count} non-null {dtype}\"\\n\\n            dtypes = self.dtypes\\n            for i, col in enumerate(self.columns):\\n                dtype = dtypes.iloc[i]\\n                col = pprint_thing(col)\\n\\n                count = \"\"\\n                if show_counts:\\n                    count = counts.iloc[i]\\n\\n                lines.append(_put_str(col, space) + tmpl.format(count=count,\\n                                                                dtype=dtype))\\n\\n        def _non_verbose_repr():\\n            lines.append(self.columns._summary(name=\\'Columns\\'))\\n\\n        def _sizeof_fmt(num, size_qualifier):\\n            # returns size in human readable format\\n            for x in [\\'bytes\\', \\'KB\\', \\'MB\\', \\'GB\\', \\'TB\\']:\\n                if num < 1024.0:\\n                    return (\"{num:3.1f}{size_q} \"\\n                            \"{x}\".format(num=num, size_q=size_qualifier, x=x))\\n                num /= 1024.0\\n            return \"{num:3.1f}{size_q} {pb}\".format(num=num,\\n                                                    size_q=size_qualifier,\\n                                                    pb=\\'PB\\')\\n\\n        if verbose:\\n            _verbose_repr()\\n        elif verbose is False:  # specifically set to False, not nesc None\\n            _non_verbose_repr()\\n        else:\\n            if exceeds_info_cols:\\n                _non_verbose_repr()\\n            else:\\n                _verbose_repr()\\n\\n        counts = self.get_dtype_counts()\\n        dtypes = [\\'{k}({kk:d})\\'.format(k=k[0], kk=k[1]) for k\\n                  in sorted(counts.items())]\\n        lines.append(\\'dtypes: {types}\\'.format(types=\\', \\'.join(dtypes)))\\n\\n        if memory_usage is None:\\n            memory_usage = get_option(\\'display.memory_usage\\')\\n        if memory_usage:\\n            # append memory usage of df to display\\n            size_qualifier = \\'\\'\\n            if memory_usage == \\'deep\\':\\n                deep = True\\n            else:\\n                # size_qualifier is just a best effort; not guaranteed to catch\\n                # all cases (e.g., it misses categorical data even with object\\n                # categories)\\n                deep = False\\n                if (\\'object\\' in counts or\\n                        self.index._is_memory_usage_qualified()):\\n                    size_qualifier = \\'+\\'\\n            mem_usage = self.memory_usage(index=True, deep=deep).sum()\\n            lines.append(\"memory usage: {mem}\\\\n\".format(\\n                mem=_sizeof_fmt(mem_usage, size_qualifier)))\\n\\n        fmt.buffer_put_lines(buf, lines)',\n 'def memory_usage(self, index=True, deep=False):\\n        \"\"\"\\n        Return the memory usage of each column in bytes.\\n\\n        The memory usage can optionally include the contribution of\\n        the index and elements of `object` dtype.\\n\\n        This value is displayed in `DataFrame.info` by default. This can be\\n        suppressed by setting ``pandas.options.display.memory_usage`` to False.\\n\\n        Parameters\\n        ----------\\n        index : bool, default True\\n            Specifies whether to include the memory usage of the DataFrame\\'s\\n            index in returned Series. If ``index=True``, the memory usage of\\n            the index is the first item in the output.\\n        deep : bool, default False\\n            If True, introspect the data deeply by interrogating\\n            `object` dtypes for system-level memory consumption, and include\\n            it in the returned values.\\n\\n        Returns\\n        -------\\n        Series\\n            A Series whose index is the original column names and whose values\\n            is the memory usage of each column in bytes.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.nbytes : Total bytes consumed by the elements of an\\n            ndarray.\\n        Series.memory_usage : Bytes consumed by a Series.\\n        Categorical : Memory-efficient array for string values with\\n            many repeated values.\\n        DataFrame.info : Concise summary of a DataFrame.\\n\\n        Examples\\n        --------\\n        >>> dtypes = [\\'int64\\', \\'float64\\', \\'complex128\\', \\'object\\', \\'bool\\']\\n        >>> data = dict([(t, np.ones(shape=5000).astype(t))\\n        ...              for t in dtypes])\\n        >>> df = pd.DataFrame(data)\\n        >>> df.head()\\n           int64  float64  complex128  object  bool\\n        0      1      1.0    1.0+0.0j       1  True\\n        1      1      1.0    1.0+0.0j       1  True\\n        2      1      1.0    1.0+0.0j       1  True\\n        3      1      1.0    1.0+0.0j       1  True\\n        4      1      1.0    1.0+0.0j       1  True\\n\\n        >>> df.memory_usage()\\n        Index            80\\n        int64         40000\\n        float64       40000\\n        complex128    80000\\n        object        40000\\n        bool           5000\\n        dtype: int64\\n\\n        >>> df.memory_usage(index=False)\\n        int64         40000\\n        float64       40000\\n        complex128    80000\\n        object        40000\\n        bool           5000\\n        dtype: int64\\n\\n        The memory footprint of `object` dtype columns is ignored by default:\\n\\n        >>> df.memory_usage(deep=True)\\n        Index             80\\n        int64          40000\\n        float64        40000\\n        complex128     80000\\n        object        160000\\n        bool            5000\\n        dtype: int64\\n\\n        Use a Categorical for efficient storage of an object-dtype column with\\n        many repeated values.\\n\\n        >>> df[\\'object\\'].astype(\\'category\\').memory_usage(deep=True)\\n        5168\\n        \"\"\"\\n        result = Series([c.memory_usage(index=False, deep=deep)\\n                         for col, c in self.iteritems()], index=self.columns)\\n        if index:\\n            result = Series(self.index.memory_usage(deep=deep),\\n                            index=[\\'Index\\']).append(result)\\n        return result',\n 'def transpose(self, *args, **kwargs):\\n        \"\"\"\\n        Transpose index and columns.\\n\\n        Reflect the DataFrame over its main diagonal by writing rows as columns\\n        and vice-versa. The property :attr:`.T` is an accessor to the method\\n        :meth:`transpose`.\\n\\n        Parameters\\n        ----------\\n        copy : bool, default False\\n            If True, the underlying data is copied. Otherwise (default), no\\n            copy is made if possible.\\n        *args, **kwargs\\n            Additional keywords have no effect but might be accepted for\\n            compatibility with numpy.\\n\\n        Returns\\n        -------\\n        DataFrame\\n            The transposed DataFrame.\\n\\n        See Also\\n        --------\\n        numpy.transpose : Permute the dimensions of a given array.\\n\\n        Notes\\n        -----\\n        Transposing a DataFrame with mixed dtypes will result in a homogeneous\\n        DataFrame with the `object` dtype. In such a case, a copy of the data\\n        is always made.\\n\\n        Examples\\n        --------\\n        **Square DataFrame with homogeneous dtype**\\n\\n        >>> d1 = {\\'col1\\': [1, 2], \\'col2\\': [3, 4]}\\n        >>> df1 = pd.DataFrame(data=d1)\\n        >>> df1\\n           col1  col2\\n        0     1     3\\n        1     2     4\\n\\n        >>> df1_transposed = df1.T # or df1.transpose()\\n        >>> df1_transposed\\n              0  1\\n        col1  1  2\\n        col2  3  4\\n\\n        When the dtype is homogeneous in the original DataFrame, we get a\\n        transposed DataFrame with the same dtype:\\n\\n        >>> df1.dtypes\\n        col1    int64\\n        col2    int64\\n        dtype: object\\n        >>> df1_transposed.dtypes\\n        0    int64\\n        1    int64\\n        dtype: object\\n\\n        **Non-square DataFrame with mixed dtypes**\\n\\n        >>> d2 = {\\'name\\': [\\'Alice\\', \\'Bob\\'],\\n        ...       \\'score\\': [9.5, 8],\\n        ...       \\'employed\\': [False, True],\\n        ...       \\'kids\\': [0, 0]}\\n        >>> df2 = pd.DataFrame(data=d2)\\n        >>> df2\\n            name  score  employed  kids\\n        0  Alice    9.5     False     0\\n        1    Bob    8.0      True     0\\n\\n        >>> df2_transposed = df2.T # or df2.transpose()\\n        >>> df2_transposed\\n                      0     1\\n        name      Alice   Bob\\n        score       9.5     8\\n        employed  False  True\\n        kids          0     0\\n\\n        When the DataFrame has mixed dtypes, we get a transposed DataFrame with\\n        the `object` dtype:\\n\\n        >>> df2.dtypes\\n        name         object\\n        score       float64\\n        employed       bool\\n        kids          int64\\n        dtype: object\\n        >>> df2_transposed.dtypes\\n        0    object\\n        1    object\\n        dtype: object\\n        \"\"\"\\n        nv.validate_transpose(args, dict())\\n        return super().transpose(1, 0, **kwargs)',\n 'def get_value(self, index, col, takeable=False):\\n        \"\"\"\\n        Quickly retrieve single value at passed column and index.\\n\\n        .. deprecated:: 0.21.0\\n            Use .at[] or .iat[] accessors instead.\\n\\n        Parameters\\n        ----------\\n        index : row label\\n        col : column label\\n        takeable : interpret the index/col as indexers, default False\\n\\n        Returns\\n        -------\\n        scalar\\n        \"\"\"\\n\\n        warnings.warn(\"get_value is deprecated and will be removed \"\\n                      \"in a future release. Please use \"\\n                      \".at[] or .iat[] accessors instead\", FutureWarning,\\n                      stacklevel=2)\\n        return self._get_value(index, col, takeable=takeable)',\n 'def set_value(self, index, col, value, takeable=False):\\n        \"\"\"\\n        Put single value at passed column and index.\\n\\n        .. deprecated:: 0.21.0\\n            Use .at[] or .iat[] accessors instead.\\n\\n        Parameters\\n        ----------\\n        index : row label\\n        col : column label\\n        value : scalar\\n        takeable : interpret the index/col as indexers, default False\\n\\n        Returns\\n        -------\\n        DataFrame\\n            If label pair is contained, will be reference to calling DataFrame,\\n            otherwise a new object.\\n        \"\"\"\\n        warnings.warn(\"set_value is deprecated and will be removed \"\\n                      \"in a future release. Please use \"\\n                      \".at[] or .iat[] accessors instead\", FutureWarning,\\n                      stacklevel=2)\\n        return self._set_value(index, col, value, takeable=takeable)',\n 'def _ixs(self, i, axis=0):\\n        \"\"\"\\n        Parameters\\n        ----------\\n        i : int, slice, or sequence of integers\\n        axis : int\\n\\n        Notes\\n        -----\\n        If slice passed, the resulting data will be a view.\\n        \"\"\"\\n        # irow\\n        if axis == 0:\\n            if isinstance(i, slice):\\n                return self[i]\\n            else:\\n                label = self.index[i]\\n                if isinstance(label, Index):\\n                    # a location index by definition\\n                    result = self.take(i, axis=axis)\\n                    copy = True\\n                else:\\n                    new_values = self._data.fast_xs(i)\\n                    if is_scalar(new_values):\\n                        return new_values\\n\\n                    # if we are a copy, mark as such\\n                    copy = (isinstance(new_values, np.ndarray) and\\n                            new_values.base is None)\\n                    result = self._constructor_sliced(new_values,\\n                                                      index=self.columns,\\n                                                      name=self.index[i],\\n                                                      dtype=new_values.dtype)\\n                result._set_is_copy(self, copy=copy)\\n                return result\\n\\n        # icol\\n        else:\\n            label = self.columns[i]\\n            if isinstance(i, slice):\\n                # need to return view\\n                lab_slice = slice(label[0], label[-1])\\n                return self.loc[:, lab_slice]\\n            else:\\n                if isinstance(label, Index):\\n                    return self._take(i, axis=1)\\n\\n                index_len = len(self.index)\\n\\n                # if the values returned are not the same length\\n                # as the index (iow a not found value), iget returns\\n                # a 0-len ndarray. This is effectively catching\\n                # a numpy error (as numpy should really raise)\\n                values = self._data.iget(i)\\n\\n                if index_len and not len(values):\\n                    values = np.array([np.nan] * index_len, dtype=object)\\n                result = self._box_col_values(values, label)\\n\\n                # this is a cached value, mark it so\\n                result._set_as_cached(label, self)\\n\\n                return result',\n 'def query(self, expr, inplace=False, **kwargs):\\n        \"\"\"\\n        Query the columns of a DataFrame with a boolean expression.\\n\\n        Parameters\\n        ----------\\n        expr : str\\n            The query string to evaluate.  You can refer to variables\\n            in the environment by prefixing them with an \\'@\\' character like\\n            ``@a + b``.\\n\\n            .. versionadded:: 0.25.0\\n\\n            You can refer to column names that contain spaces by surrounding\\n            them in backticks.\\n\\n            For example, if one of your columns is called ``a a`` and you want\\n            to sum it with ``b``, your query should be ```a a` + b``.\\n\\n        inplace : bool\\n            Whether the query should modify the data in place or return\\n            a modified copy.\\n        **kwargs\\n            See the documentation for :func:`eval` for complete details\\n            on the keyword arguments accepted by :meth:`DataFrame.query`.\\n\\n            .. versionadded:: 0.18.0\\n\\n        Returns\\n        -------\\n        DataFrame\\n            DataFrame resulting from the provided query expression.\\n\\n        See Also\\n        --------\\n        eval : Evaluate a string describing operations on\\n            DataFrame columns.\\n        DataFrame.eval : Evaluate a string describing operations on\\n            DataFrame columns.\\n\\n        Notes\\n        -----\\n        The result of the evaluation of this expression is first passed to\\n        :attr:`DataFrame.loc` and if that fails because of a\\n        multidimensional key (e.g., a DataFrame) then the result will be passed\\n        to :meth:`DataFrame.__getitem__`.\\n\\n        This method uses the top-level :func:`eval` function to\\n        evaluate the passed query.\\n\\n        The :meth:`~pandas.DataFrame.query` method uses a slightly\\n        modified Python syntax by default. For example, the ``&`` and ``|``\\n        (bitwise) operators have the precedence of their boolean cousins,\\n        :keyword:`and` and :keyword:`or`. This *is* syntactically valid Python,\\n        however the semantics are different.\\n\\n        You can change the semantics of the expression by passing the keyword\\n        argument ``parser=\\'python\\'``. This enforces the same semantics as\\n        evaluation in Python space. Likewise, you can pass ``engine=\\'python\\'``\\n        to evaluate an expression using Python itself as a backend. This is not\\n        recommended as it is inefficient compared to using ``numexpr`` as the\\n        engine.\\n\\n        The :attr:`DataFrame.index` and\\n        :attr:`DataFrame.columns` attributes of the\\n        :class:`~pandas.DataFrame` instance are placed in the query namespace\\n        by default, which allows you to treat both the index and columns of the\\n        frame as a column in the frame.\\n        The identifier ``index`` is used for the frame index; you can also\\n        use the name of the index to identify it in a query. Please note that\\n        Python keywords may not be used as identifiers.\\n\\n        For further details and examples see the ``query`` documentation in\\n        :ref:`indexing <indexing.query>`.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'A\\': range(1, 6),\\n        ...                    \\'B\\': range(10, 0, -2),\\n        ...                    \\'C C\\': range(10, 5, -1)})\\n        >>> df\\n           A   B  C C\\n        0  1  10   10\\n        1  2   8    9\\n        2  3   6    8\\n        3  4   4    7\\n        4  5   2    6\\n        >>> df.query(\\'A > B\\')\\n           A  B  C C\\n        4  5  2    6\\n\\n        The previous expression is equivalent to\\n\\n        >>> df[df.A > df.B]\\n           A  B  C C\\n        4  5  2    6\\n\\n        For columns with spaces in their name, you can use backtick quoting.\\n\\n        >>> df.query(\\'B == `C C`\\')\\n           A   B  C C\\n        0  1  10   10\\n\\n        The previous expression is equivalent to\\n\\n        >>> df[df.B == df[\\'C C\\']]\\n           A   B  C C\\n        0  1  10   10\\n        \"\"\"\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        if not isinstance(expr, str):\\n            msg = \"expr must be a string to be evaluated, {0} given\"\\n            raise ValueError(msg.format(type(expr)))\\n        kwargs[\\'level\\'] = kwargs.pop(\\'level\\', 0) + 1\\n        kwargs[\\'target\\'] = None\\n        res = self.eval(expr, **kwargs)\\n\\n        try:\\n            new_data = self.loc[res]\\n        except ValueError:\\n            # when res is multi-dimensional loc raises, but this is sometimes a\\n            # valid query\\n            new_data = self[res]\\n\\n        if inplace:\\n            self._update_inplace(new_data)\\n        else:\\n            return new_data',\n 'def eval(self, expr, inplace=False, **kwargs):\\n        \"\"\"\\n        Evaluate a string describing operations on DataFrame columns.\\n\\n        Operates on columns only, not specific rows or elements.  This allows\\n        `eval` to run arbitrary code, which can make you vulnerable to code\\n        injection if you pass user input to this function.\\n\\n        Parameters\\n        ----------\\n        expr : str\\n            The expression string to evaluate.\\n        inplace : bool, default False\\n            If the expression contains an assignment, whether to perform the\\n            operation inplace and mutate the existing DataFrame. Otherwise,\\n            a new DataFrame is returned.\\n\\n            .. versionadded:: 0.18.0.\\n        kwargs : dict\\n            See the documentation for :func:`eval` for complete details\\n            on the keyword arguments accepted by\\n            :meth:`~pandas.DataFrame.query`.\\n\\n        Returns\\n        -------\\n        ndarray, scalar, or pandas object\\n            The result of the evaluation.\\n\\n        See Also\\n        --------\\n        DataFrame.query : Evaluates a boolean expression to query the columns\\n            of a frame.\\n        DataFrame.assign : Can evaluate an expression or function to create new\\n            values for a column.\\n        eval : Evaluate a Python expression as a string using various\\n            backends.\\n\\n        Notes\\n        -----\\n        For more details see the API documentation for :func:`~eval`.\\n        For detailed examples see :ref:`enhancing performance with eval\\n        <enhancingperf.eval>`.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'A\\': range(1, 6), \\'B\\': range(10, 0, -2)})\\n        >>> df\\n           A   B\\n        0  1  10\\n        1  2   8\\n        2  3   6\\n        3  4   4\\n        4  5   2\\n        >>> df.eval(\\'A + B\\')\\n        0    11\\n        1    10\\n        2     9\\n        3     8\\n        4     7\\n        dtype: int64\\n\\n        Assignment is allowed though by default the original DataFrame is not\\n        modified.\\n\\n        >>> df.eval(\\'C = A + B\\')\\n           A   B   C\\n        0  1  10  11\\n        1  2   8  10\\n        2  3   6   9\\n        3  4   4   8\\n        4  5   2   7\\n        >>> df\\n           A   B\\n        0  1  10\\n        1  2   8\\n        2  3   6\\n        3  4   4\\n        4  5   2\\n\\n        Use ``inplace=True`` to modify the original DataFrame.\\n\\n        >>> df.eval(\\'C = A + B\\', inplace=True)\\n        >>> df\\n           A   B   C\\n        0  1  10  11\\n        1  2   8  10\\n        2  3   6   9\\n        3  4   4   8\\n        4  5   2   7\\n        \"\"\"\\n        from pandas.core.computation.eval import eval as _eval\\n\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        resolvers = kwargs.pop(\\'resolvers\\', None)\\n        kwargs[\\'level\\'] = kwargs.pop(\\'level\\', 0) + 1\\n        if resolvers is None:\\n            index_resolvers = self._get_index_resolvers()\\n            column_resolvers = \\\\\\n                self._get_space_character_free_column_resolvers()\\n            resolvers = column_resolvers, index_resolvers\\n        if \\'target\\' not in kwargs:\\n            kwargs[\\'target\\'] = self\\n        kwargs[\\'resolvers\\'] = kwargs.get(\\'resolvers\\', ()) + tuple(resolvers)\\n        return _eval(expr, inplace=inplace, **kwargs)',\n 'def select_dtypes(self, include=None, exclude=None):\\n        \"\"\"\\n        Return a subset of the DataFrame\\'s columns based on the column dtypes.\\n\\n        Parameters\\n        ----------\\n        include, exclude : scalar or list-like\\n            A selection of dtypes or strings to be included/excluded. At least\\n            one of these parameters must be supplied.\\n\\n        Returns\\n        -------\\n        DataFrame\\n            The subset of the frame including the dtypes in ``include`` and\\n            excluding the dtypes in ``exclude``.\\n\\n        Raises\\n        ------\\n        ValueError\\n            * If both of ``include`` and ``exclude`` are empty\\n            * If ``include`` and ``exclude`` have overlapping elements\\n            * If any kind of string dtype is passed in.\\n\\n        Notes\\n        -----\\n        * To select all *numeric* types, use ``np.number`` or ``\\'number\\'``\\n        * To select strings you must use the ``object`` dtype, but note that\\n          this will return *all* object dtype columns\\n        * See the `numpy dtype hierarchy\\n          <http://docs.scipy.org/doc/numpy/reference/arrays.scalars.html>`__\\n        * To select datetimes, use ``np.datetime64``, ``\\'datetime\\'`` or\\n          ``\\'datetime64\\'``\\n        * To select timedeltas, use ``np.timedelta64``, ``\\'timedelta\\'`` or\\n          ``\\'timedelta64\\'``\\n        * To select Pandas categorical dtypes, use ``\\'category\\'``\\n        * To select Pandas datetimetz dtypes, use ``\\'datetimetz\\'`` (new in\\n          0.20.0) or ``\\'datetime64[ns, tz]\\'``\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'a\\': [1, 2] * 3,\\n        ...                    \\'b\\': [True, False] * 3,\\n        ...                    \\'c\\': [1.0, 2.0] * 3})\\n        >>> df\\n                a      b  c\\n        0       1   True  1.0\\n        1       2  False  2.0\\n        2       1   True  1.0\\n        3       2  False  2.0\\n        4       1   True  1.0\\n        5       2  False  2.0\\n\\n        >>> df.select_dtypes(include=\\'bool\\')\\n           b\\n        0  True\\n        1  False\\n        2  True\\n        3  False\\n        4  True\\n        5  False\\n\\n        >>> df.select_dtypes(include=[\\'float64\\'])\\n           c\\n        0  1.0\\n        1  2.0\\n        2  1.0\\n        3  2.0\\n        4  1.0\\n        5  2.0\\n\\n        >>> df.select_dtypes(exclude=[\\'int\\'])\\n               b    c\\n        0   True  1.0\\n        1  False  2.0\\n        2   True  1.0\\n        3  False  2.0\\n        4   True  1.0\\n        5  False  2.0\\n        \"\"\"\\n        def _get_info_slice(obj, indexer):\\n            \"\"\"Slice the info axis of `obj` with `indexer`.\"\"\"\\n            if not hasattr(obj, \\'_info_axis_number\\'):\\n                msg = \\'object of type {typ!r} has no info axis\\'\\n                raise TypeError(msg.format(typ=type(obj).__name__))\\n            slices = [slice(None)] * obj.ndim\\n            slices[obj._info_axis_number] = indexer\\n            return tuple(slices)\\n\\n        if not is_list_like(include):\\n            include = (include,) if include is not None else ()\\n        if not is_list_like(exclude):\\n            exclude = (exclude,) if exclude is not None else ()\\n\\n        selection = tuple(map(frozenset, (include, exclude)))\\n\\n        if not any(selection):\\n            raise ValueError(\\'at least one of include or exclude must be \\'\\n                             \\'nonempty\\')\\n\\n        # convert the myriad valid dtypes object to a single representation\\n        include, exclude = map(\\n            lambda x: frozenset(map(infer_dtype_from_object, x)), selection)\\n        for dtypes in (include, exclude):\\n            invalidate_string_dtypes(dtypes)\\n\\n        # can\\'t both include AND exclude!\\n        if not include.isdisjoint(exclude):\\n            raise ValueError(\\'include and exclude overlap on {inc_ex}\\'.format(\\n                inc_ex=(include & exclude)))\\n\\n        # empty include/exclude -> defaults to True\\n        # three cases (we\\'ve already raised if both are empty)\\n        # case 1: empty include, nonempty exclude\\n        # we have True, True, ... True for include, same for exclude\\n        # in the loop below we get the excluded\\n        # and when we call \\'&\\' below we get only the excluded\\n        # case 2: nonempty include, empty exclude\\n        # same as case 1, but with include\\n        # case 3: both nonempty\\n        # the \"union\" of the logic of case 1 and case 2:\\n        # we get the included and excluded, and return their logical and\\n        include_these = Series(not bool(include), index=self.columns)\\n        exclude_these = Series(not bool(exclude), index=self.columns)\\n\\n        def is_dtype_instance_mapper(idx, dtype):\\n            return idx, functools.partial(issubclass, dtype.type)\\n\\n        for idx, f in itertools.starmap(is_dtype_instance_mapper,\\n                                        enumerate(self.dtypes)):\\n            if include:  # checks for the case of empty include or exclude\\n                include_these.iloc[idx] = any(map(f, include))\\n            if exclude:\\n                exclude_these.iloc[idx] = not any(map(f, exclude))\\n\\n        dtype_indexer = include_these & exclude_these\\n        return self.loc[_get_info_slice(self, dtype_indexer)]',\n 'def _box_col_values(self, values, items):\\n        \"\"\"\\n        Provide boxed values for a column.\\n        \"\"\"\\n        klass = self._constructor_sliced\\n        return klass(values, index=self.index, name=items, fastpath=True)',\n 'def _ensure_valid_index(self, value):\\n        \"\"\"\\n        Ensure that if we don\\'t have an index, that we can create one from the\\n        passed value.\\n        \"\"\"\\n        # GH5632, make sure that we are a Series convertible\\n        if not len(self.index) and is_list_like(value):\\n            try:\\n                value = Series(value)\\n            except (ValueError, NotImplementedError, TypeError):\\n                raise ValueError(\\'Cannot set a frame with no defined index \\'\\n                                 \\'and a value that cannot be converted to a \\'\\n                                 \\'Series\\')\\n\\n            self._data = self._data.reindex_axis(value.index.copy(), axis=1,\\n                                                 fill_value=np.nan)',\n 'def _set_item(self, key, value):\\n        \"\"\"\\n        Add series to DataFrame in specified column.\\n\\n        If series is a numpy-array (not a Series/TimeSeries), it must be the\\n        same length as the DataFrames index or an error will be thrown.\\n\\n        Series/TimeSeries will be conformed to the DataFrames index to\\n        ensure homogeneity.\\n        \"\"\"\\n\\n        self._ensure_valid_index(value)\\n        value = self._sanitize_column(key, value)\\n        NDFrame._set_item(self, key, value)\\n\\n        # check if we are modifying a copy\\n        # try to set first as we want an invalid\\n        # value exception to occur first\\n        if len(self):\\n            self._check_setitem_copy()',\n 'def insert(self, loc, column, value, allow_duplicates=False):\\n        \"\"\"\\n        Insert column into DataFrame at specified location.\\n\\n        Raises a ValueError if `column` is already contained in the DataFrame,\\n        unless `allow_duplicates` is set to True.\\n\\n        Parameters\\n        ----------\\n        loc : int\\n            Insertion index. Must verify 0 <= loc <= len(columns)\\n        column : string, number, or hashable object\\n            label of the inserted column\\n        value : int, Series, or array-like\\n        allow_duplicates : bool, optional\\n        \"\"\"\\n        self._ensure_valid_index(value)\\n        value = self._sanitize_column(column, value, broadcast=False)\\n        self._data.insert(loc, column, value,\\n                          allow_duplicates=allow_duplicates)',\n 'def assign(self, **kwargs):\\n        r\"\"\"\\n        Assign new columns to a DataFrame.\\n\\n        Returns a new object with all original columns in addition to new ones.\\n        Existing columns that are re-assigned will be overwritten.\\n\\n        Parameters\\n        ----------\\n        **kwargs : dict of {str: callable or Series}\\n            The column names are keywords. If the values are\\n            callable, they are computed on the DataFrame and\\n            assigned to the new columns. The callable must not\\n            change input DataFrame (though pandas doesn\\'t check it).\\n            If the values are not callable, (e.g. a Series, scalar, or array),\\n            they are simply assigned.\\n\\n        Returns\\n        -------\\n        DataFrame\\n            A new DataFrame with the new columns in addition to\\n            all the existing columns.\\n\\n        Notes\\n        -----\\n        Assigning multiple columns within the same ``assign`` is possible.\\n        For Python 3.6 and above, later items in \\'\\\\*\\\\*kwargs\\' may refer to\\n        newly created or modified columns in \\'df\\'; items are computed and\\n        assigned into \\'df\\' in order.  For Python 3.5 and below, the order of\\n        keyword arguments is not specified, you cannot refer to newly created\\n        or modified columns. All items are computed first, and then assigned\\n        in alphabetical order.\\n\\n        .. versionchanged :: 0.23.0\\n\\n           Keyword argument order is maintained for Python 3.6 and later.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'temp_c\\': [17.0, 25.0]},\\n        ...                   index=[\\'Portland\\', \\'Berkeley\\'])\\n        >>> df\\n                  temp_c\\n        Portland    17.0\\n        Berkeley    25.0\\n\\n        Where the value is a callable, evaluated on `df`:\\n\\n        >>> df.assign(temp_f=lambda x: x.temp_c * 9 / 5 + 32)\\n                  temp_c  temp_f\\n        Portland    17.0    62.6\\n        Berkeley    25.0    77.0\\n\\n        Alternatively, the same behavior can be achieved by directly\\n        referencing an existing Series or sequence:\\n\\n        >>> df.assign(temp_f=df[\\'temp_c\\'] * 9 / 5 + 32)\\n                  temp_c  temp_f\\n        Portland    17.0    62.6\\n        Berkeley    25.0    77.0\\n\\n        In Python 3.6+, you can create multiple columns within the same assign\\n        where one of the columns depends on another one defined within the same\\n        assign:\\n\\n        >>> df.assign(temp_f=lambda x: x[\\'temp_c\\'] * 9 / 5 + 32,\\n        ...           temp_k=lambda x: (x[\\'temp_f\\'] +  459.67) * 5 / 9)\\n                  temp_c  temp_f  temp_k\\n        Portland    17.0    62.6  290.15\\n        Berkeley    25.0    77.0  298.15\\n        \"\"\"\\n        data = self.copy()\\n\\n        # >= 3.6 preserve order of kwargs\\n        if PY36:\\n            for k, v in kwargs.items():\\n                data[k] = com.apply_if_callable(v, data)\\n        else:\\n            # <= 3.5: do all calculations first...\\n            results = OrderedDict()\\n            for k, v in kwargs.items():\\n                results[k] = com.apply_if_callable(v, data)\\n\\n            # <= 3.5 and earlier\\n            results = sorted(results.items())\\n            # ... and then assign\\n            for k, v in results:\\n                data[k] = v\\n        return data',\n 'def _sanitize_column(self, key, value, broadcast=True):\\n        \"\"\"\\n        Ensures new columns (which go into the BlockManager as new blocks) are\\n        always copied and converted into an array.\\n\\n        Parameters\\n        ----------\\n        key : object\\n        value : scalar, Series, or array-like\\n        broadcast : bool, default True\\n            If ``key`` matches multiple duplicate column names in the\\n            DataFrame, this parameter indicates whether ``value`` should be\\n            tiled so that the returned array contains a (duplicated) column for\\n            each occurrence of the key. If False, ``value`` will not be tiled.\\n\\n        Returns\\n        -------\\n        numpy.ndarray\\n        \"\"\"\\n\\n        def reindexer(value):\\n            # reindex if necessary\\n\\n            if value.index.equals(self.index) or not len(self.index):\\n                value = value._values.copy()\\n            else:\\n\\n                # GH 4107\\n                try:\\n                    value = value.reindex(self.index)._values\\n                except Exception as e:\\n\\n                    # duplicate axis\\n                    if not value.index.is_unique:\\n                        raise e\\n\\n                    # other\\n                    raise TypeError(\\'incompatible index of inserted column \\'\\n                                    \\'with frame index\\')\\n            return value\\n\\n        if isinstance(value, Series):\\n            value = reindexer(value)\\n\\n        elif isinstance(value, DataFrame):\\n            # align right-hand-side columns if self.columns\\n            # is multi-index and self[key] is a sub-frame\\n            if isinstance(self.columns, MultiIndex) and key in self.columns:\\n                loc = self.columns.get_loc(key)\\n                if isinstance(loc, (slice, Series, np.ndarray, Index)):\\n                    cols = maybe_droplevels(self.columns[loc], key)\\n                    if len(cols) and not cols.equals(value.columns):\\n                        value = value.reindex(cols, axis=1)\\n            # now align rows\\n            value = reindexer(value).T\\n\\n        elif isinstance(value, ExtensionArray):\\n            # Explicitly copy here, instead of in sanitize_index,\\n            # as sanitize_index won\\'t copy an EA, even with copy=True\\n            value = value.copy()\\n            value = sanitize_index(value, self.index, copy=False)\\n\\n        elif isinstance(value, Index) or is_sequence(value):\\n\\n            # turn me into an ndarray\\n            value = sanitize_index(value, self.index, copy=False)\\n            if not isinstance(value, (np.ndarray, Index)):\\n                if isinstance(value, list) and len(value) > 0:\\n                    value = maybe_convert_platform(value)\\n                else:\\n                    value = com.asarray_tuplesafe(value)\\n            elif value.ndim == 2:\\n                value = value.copy().T\\n            elif isinstance(value, Index):\\n                value = value.copy(deep=True)\\n            else:\\n                value = value.copy()\\n\\n            # possibly infer to datetimelike\\n            if is_object_dtype(value.dtype):\\n                value = maybe_infer_to_datetimelike(value)\\n\\n        else:\\n            # cast ignores pandas dtypes. so save the dtype first\\n            infer_dtype, _ = infer_dtype_from_scalar(\\n                value, pandas_dtype=True)\\n\\n            # upcast\\n            value = cast_scalar_to_array(len(self.index), value)\\n            value = maybe_cast_to_datetime(value, infer_dtype)\\n\\n        # return internal types directly\\n        if is_extension_type(value) or is_extension_array_dtype(value):\\n            return value\\n\\n        # broadcast across multiple columns if necessary\\n        if broadcast and key in self.columns and value.ndim == 1:\\n            if (not self.columns.is_unique or\\n                    isinstance(self.columns, MultiIndex)):\\n                existing_piece = self[key]\\n                if isinstance(existing_piece, DataFrame):\\n                    value = np.tile(value, (len(existing_piece.columns), 1))\\n\\n        return np.atleast_2d(np.asarray(value))',\n 'def lookup(self, row_labels, col_labels):\\n        \"\"\"\\n        Label-based \"fancy indexing\" function for DataFrame.\\n\\n        Given equal-length arrays of row and column labels, return an\\n        array of the values corresponding to each (row, col) pair.\\n\\n        Parameters\\n        ----------\\n        row_labels : sequence\\n            The row labels to use for lookup\\n        col_labels : sequence\\n            The column labels to use for lookup\\n\\n        Notes\\n        -----\\n        Akin to::\\n\\n            result = [df.get_value(row, col)\\n                      for row, col in zip(row_labels, col_labels)]\\n\\n        Examples\\n        --------\\n        values : ndarray\\n            The found values\\n        \"\"\"\\n        n = len(row_labels)\\n        if n != len(col_labels):\\n            raise ValueError(\\'Row labels must have same size as column labels\\')\\n\\n        thresh = 1000\\n        if not self._is_mixed_type or n > thresh:\\n            values = self.values\\n            ridx = self.index.get_indexer(row_labels)\\n            cidx = self.columns.get_indexer(col_labels)\\n            if (ridx == -1).any():\\n                raise KeyError(\\'One or more row labels was not found\\')\\n            if (cidx == -1).any():\\n                raise KeyError(\\'One or more column labels was not found\\')\\n            flat_index = ridx * len(self.columns) + cidx\\n            result = values.flat[flat_index]\\n        else:\\n            result = np.empty(n, dtype=\\'O\\')\\n            for i, (r, c) in enumerate(zip(row_labels, col_labels)):\\n                result[i] = self._get_value(r, c)\\n\\n        if is_object_dtype(result):\\n            result = lib.maybe_convert_objects(result)\\n\\n        return result',\n 'def _reindex_multi(self, axes, copy, fill_value):\\n        \"\"\"\\n        We are guaranteed non-Nones in the axes.\\n        \"\"\"\\n\\n        new_index, row_indexer = self.index.reindex(axes[\\'index\\'])\\n        new_columns, col_indexer = self.columns.reindex(axes[\\'columns\\'])\\n\\n        if row_indexer is not None and col_indexer is not None:\\n            indexer = row_indexer, col_indexer\\n            new_values = algorithms.take_2d_multi(self.values, indexer,\\n                                                  fill_value=fill_value)\\n            return self._constructor(new_values, index=new_index,\\n                                     columns=new_columns)\\n        else:\\n            return self._reindex_with_indexers({0: [new_index, row_indexer],\\n                                                1: [new_columns, col_indexer]},\\n                                               copy=copy,\\n                                               fill_value=fill_value)',\n 'def drop(self, labels=None, axis=0, index=None, columns=None,\\n             level=None, inplace=False, errors=\\'raise\\'):\\n        \"\"\"\\n        Drop specified labels from rows or columns.\\n\\n        Remove rows or columns by specifying label names and corresponding\\n        axis, or by specifying directly index or column names. When using a\\n        multi-index, labels on different levels can be removed by specifying\\n        the level.\\n\\n        Parameters\\n        ----------\\n        labels : single label or list-like\\n            Index or column labels to drop.\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n            Whether to drop labels from the index (0 or \\'index\\') or\\n            columns (1 or \\'columns\\').\\n        index : single label or list-like\\n            Alternative to specifying axis (``labels, axis=0``\\n            is equivalent to ``index=labels``).\\n\\n            .. versionadded:: 0.21.0\\n        columns : single label or list-like\\n            Alternative to specifying axis (``labels, axis=1``\\n            is equivalent to ``columns=labels``).\\n\\n            .. versionadded:: 0.21.0\\n        level : int or level name, optional\\n            For MultiIndex, level from which the labels will be removed.\\n        inplace : bool, default False\\n            If True, do operation inplace and return None.\\n        errors : {\\'ignore\\', \\'raise\\'}, default \\'raise\\'\\n            If \\'ignore\\', suppress error and only existing labels are\\n            dropped.\\n\\n        Returns\\n        -------\\n        DataFrame\\n            DataFrame without the removed index or column labels.\\n\\n        Raises\\n        ------\\n        KeyError\\n            If any of the labels is not found in the selected axis.\\n\\n        See Also\\n        --------\\n        DataFrame.loc : Label-location based indexer for selection by label.\\n        DataFrame.dropna : Return DataFrame with labels on given axis omitted\\n            where (all or any) data are missing.\\n        DataFrame.drop_duplicates : Return DataFrame with duplicate rows\\n            removed, optionally only considering certain columns.\\n        Series.drop : Return Series with specified index labels removed.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame(np.arange(12).reshape(3, 4),\\n        ...                   columns=[\\'A\\', \\'B\\', \\'C\\', \\'D\\'])\\n        >>> df\\n           A  B   C   D\\n        0  0  1   2   3\\n        1  4  5   6   7\\n        2  8  9  10  11\\n\\n        Drop columns\\n\\n        >>> df.drop([\\'B\\', \\'C\\'], axis=1)\\n           A   D\\n        0  0   3\\n        1  4   7\\n        2  8  11\\n\\n        >>> df.drop(columns=[\\'B\\', \\'C\\'])\\n           A   D\\n        0  0   3\\n        1  4   7\\n        2  8  11\\n\\n        Drop a row by index\\n\\n        >>> df.drop([0, 1])\\n           A  B   C   D\\n        2  8  9  10  11\\n\\n        Drop columns and/or rows of MultiIndex DataFrame\\n\\n        >>> midx = pd.MultiIndex(levels=[[\\'lama\\', \\'cow\\', \\'falcon\\'],\\n        ...                              [\\'speed\\', \\'weight\\', \\'length\\']],\\n        ...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\\n        ...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\\n        >>> df = pd.DataFrame(index=midx, columns=[\\'big\\', \\'small\\'],\\n        ...                   data=[[45, 30], [200, 100], [1.5, 1], [30, 20],\\n        ...                         [250, 150], [1.5, 0.8], [320, 250],\\n        ...                         [1, 0.8], [0.3, 0.2]])\\n        >>> df\\n                        big     small\\n        lama    speed   45.0    30.0\\n                weight  200.0   100.0\\n                length  1.5     1.0\\n        cow     speed   30.0    20.0\\n                weight  250.0   150.0\\n                length  1.5     0.8\\n        falcon  speed   320.0   250.0\\n                weight  1.0     0.8\\n                length  0.3     0.2\\n\\n        >>> df.drop(index=\\'cow\\', columns=\\'small\\')\\n                        big\\n        lama    speed   45.0\\n                weight  200.0\\n                length  1.5\\n        falcon  speed   320.0\\n                weight  1.0\\n                length  0.3\\n\\n        >>> df.drop(index=\\'length\\', level=1)\\n                        big     small\\n        lama    speed   45.0    30.0\\n                weight  200.0   100.0\\n        cow     speed   30.0    20.0\\n                weight  250.0   150.0\\n        falcon  speed   320.0   250.0\\n                weight  1.0     0.8\\n        \"\"\"\\n        return super().drop(labels=labels, axis=axis, index=index,\\n                            columns=columns, level=level, inplace=inplace,\\n                            errors=errors)',\n 'def rename(self, *args, **kwargs):\\n        \"\"\"\\n        Alter axes labels.\\n\\n        Function / dict values must be unique (1-to-1). Labels not contained in\\n        a dict / Series will be left as-is. Extra labels listed don\\'t throw an\\n        error.\\n\\n        See the :ref:`user guide <basics.rename>` for more.\\n\\n        Parameters\\n        ----------\\n        mapper : dict-like or function\\n            Dict-like or functions transformations to apply to\\n            that axis\\' values. Use either ``mapper`` and ``axis`` to\\n            specify the axis to target with ``mapper``, or ``index`` and\\n            ``columns``.\\n        index : dict-like or function\\n            Alternative to specifying axis (``mapper, axis=0``\\n            is equivalent to ``index=mapper``).\\n        columns : dict-like or function\\n            Alternative to specifying axis (``mapper, axis=1``\\n            is equivalent to ``columns=mapper``).\\n        axis : int or str\\n            Axis to target with ``mapper``. Can be either the axis name\\n            (\\'index\\', \\'columns\\') or number (0, 1). The default is \\'index\\'.\\n        copy : bool, default True\\n            Also copy underlying data.\\n        inplace : bool, default False\\n            Whether to return a new DataFrame. If True then value of copy is\\n            ignored.\\n        level : int or level name, default None\\n            In case of a MultiIndex, only rename labels in the specified\\n            level.\\n        errors : {\\'ignore\\', \\'raise\\'}, default \\'ignore\\'\\n            If \\'raise\\', raise a `KeyError` when a dict-like `mapper`, `index`,\\n            or `columns` contains labels that are not present in the Index\\n            being transformed.\\n            If \\'ignore\\', existing keys will be renamed and extra keys will be\\n            ignored.\\n\\n        Returns\\n        -------\\n        DataFrame\\n            DataFrame with the renamed axis labels.\\n\\n        Raises\\n        ------\\n        KeyError\\n            If any of the labels is not found in the selected axis and\\n            \"errors=\\'raise\\'\".\\n\\n        See Also\\n        --------\\n        DataFrame.rename_axis : Set the name of the axis.\\n\\n        Examples\\n        --------\\n\\n        ``DataFrame.rename`` supports two calling conventions\\n\\n        * ``(index=index_mapper, columns=columns_mapper, ...)``\\n        * ``(mapper, axis={\\'index\\', \\'columns\\'}, ...)``\\n\\n        We *highly* recommend using keyword arguments to clarify your\\n        intent.\\n\\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\\n        >>> df.rename(index=str, columns={\"A\": \"a\", \"B\": \"c\"})\\n           a  c\\n        0  1  4\\n        1  2  5\\n        2  3  6\\n\\n        >>> df.rename(index=str, columns={\"A\": \"a\", \"C\": \"c\"})\\n           a  B\\n        0  1  4\\n        1  2  5\\n        2  3  6\\n\\n        >>> df.rename(index=str, columns={\"A\": \"a\", \"C\": \"c\"}, errors=\"raise\")\\n        Traceback (most recent call last):\\n        KeyError: [\\'C\\'] not found in axis\\n\\n        Using axis-style parameters\\n\\n        >>> df.rename(str.lower, axis=\\'columns\\')\\n           a  b\\n        0  1  4\\n        1  2  5\\n        2  3  6\\n\\n        >>> df.rename({1: 2, 2: 4}, axis=\\'index\\')\\n           A  B\\n        0  1  4\\n        2  2  5\\n        4  3  6\\n        \"\"\"\\n        axes = validate_axis_style_args(self, args, kwargs, \\'mapper\\', \\'rename\\')\\n        kwargs.update(axes)\\n        # Pop these, since the values are in `kwargs` under different names\\n        kwargs.pop(\\'axis\\', None)\\n        kwargs.pop(\\'mapper\\', None)\\n        return super().rename(**kwargs)',\n 'def set_index(self, keys, drop=True, append=False, inplace=False,\\n                  verify_integrity=False):\\n        \"\"\"\\n        Set the DataFrame index using existing columns.\\n\\n        Set the DataFrame index (row labels) using one or more existing\\n        columns or arrays (of the correct length). The index can replace the\\n        existing index or expand on it.\\n\\n        Parameters\\n        ----------\\n        keys : label or array-like or list of labels/arrays\\n            This parameter can be either a single column key, a single array of\\n            the same length as the calling DataFrame, or a list containing an\\n            arbitrary combination of column keys and arrays. Here, \"array\"\\n            encompasses :class:`Series`, :class:`Index`, ``np.ndarray``, and\\n            instances of :class:`~collections.abc.Iterator`.\\n        drop : bool, default True\\n            Delete columns to be used as the new index.\\n        append : bool, default False\\n            Whether to append columns to existing index.\\n        inplace : bool, default False\\n            Modify the DataFrame in place (do not create a new object).\\n        verify_integrity : bool, default False\\n            Check the new index for duplicates. Otherwise defer the check until\\n            necessary. Setting to False will improve the performance of this\\n            method.\\n\\n        Returns\\n        -------\\n        DataFrame\\n            Changed row labels.\\n\\n        See Also\\n        --------\\n        DataFrame.reset_index : Opposite of set_index.\\n        DataFrame.reindex : Change to new indices or expand indices.\\n        DataFrame.reindex_like : Change to same indices as other DataFrame.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'month\\': [1, 4, 7, 10],\\n        ...                    \\'year\\': [2012, 2014, 2013, 2014],\\n        ...                    \\'sale\\': [55, 40, 84, 31]})\\n        >>> df\\n           month  year  sale\\n        0      1  2012    55\\n        1      4  2014    40\\n        2      7  2013    84\\n        3     10  2014    31\\n\\n        Set the index to become the \\'month\\' column:\\n\\n        >>> df.set_index(\\'month\\')\\n               year  sale\\n        month\\n        1      2012    55\\n        4      2014    40\\n        7      2013    84\\n        10     2014    31\\n\\n        Create a MultiIndex using columns \\'year\\' and \\'month\\':\\n\\n        >>> df.set_index([\\'year\\', \\'month\\'])\\n                    sale\\n        year  month\\n        2012  1     55\\n        2014  4     40\\n        2013  7     84\\n        2014  10    31\\n\\n        Create a MultiIndex using an Index and a column:\\n\\n        >>> df.set_index([pd.Index([1, 2, 3, 4]), \\'year\\'])\\n                 month  sale\\n           year\\n        1  2012  1      55\\n        2  2014  4      40\\n        3  2013  7      84\\n        4  2014  10     31\\n\\n        Create a MultiIndex using two Series:\\n\\n        >>> s = pd.Series([1, 2, 3, 4])\\n        >>> df.set_index([s, s**2])\\n              month  year  sale\\n        1 1       1  2012    55\\n        2 4       4  2014    40\\n        3 9       7  2013    84\\n        4 16     10  2014    31\\n        \"\"\"\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        if not isinstance(keys, list):\\n            keys = [keys]\\n\\n        err_msg = (\\'The parameter \"keys\" may be a column key, one-dimensional \\'\\n                   \\'array, or a list containing only valid column keys and \\'\\n                   \\'one-dimensional arrays.\\')\\n\\n        missing = []\\n        for col in keys:\\n            if isinstance(col, (ABCIndexClass, ABCSeries, np.ndarray,\\n                                list, abc.Iterator)):\\n                # arrays are fine as long as they are one-dimensional\\n                # iterators get converted to list below\\n                if getattr(col, \\'ndim\\', 1) != 1:\\n                    raise ValueError(err_msg)\\n            else:\\n                # everything else gets tried as a key; see GH 24969\\n                try:\\n                    found = col in self.columns\\n                except TypeError:\\n                    raise TypeError(err_msg + \\' Received column of \\'\\n                                    \\'type {}\\'.format(type(col)))\\n                else:\\n                    if not found:\\n                        missing.append(col)\\n\\n        if missing:\\n            raise KeyError(\\'None of {} are in the columns\\'.format(missing))\\n\\n        if inplace:\\n            frame = self\\n        else:\\n            frame = self.copy()\\n\\n        arrays = []\\n        names = []\\n        if append:\\n            names = [x for x in self.index.names]\\n            if isinstance(self.index, ABCMultiIndex):\\n                for i in range(self.index.nlevels):\\n                    arrays.append(self.index._get_level_values(i))\\n            else:\\n                arrays.append(self.index)\\n\\n        to_remove = []\\n        for col in keys:\\n            if isinstance(col, ABCMultiIndex):\\n                for n in range(col.nlevels):\\n                    arrays.append(col._get_level_values(n))\\n                names.extend(col.names)\\n            elif isinstance(col, (ABCIndexClass, ABCSeries)):\\n                # if Index then not MultiIndex (treated above)\\n                arrays.append(col)\\n                names.append(col.name)\\n            elif isinstance(col, (list, np.ndarray)):\\n                arrays.append(col)\\n                names.append(None)\\n            elif isinstance(col, abc.Iterator):\\n                arrays.append(list(col))\\n                names.append(None)\\n            # from here, col can only be a column label\\n            else:\\n                arrays.append(frame[col]._values)\\n                names.append(col)\\n                if drop:\\n                    to_remove.append(col)\\n\\n            if len(arrays[-1]) != len(self):\\n                # check newest element against length of calling frame, since\\n                # ensure_index_from_sequences would not raise for append=False.\\n                raise ValueError(\\'Length mismatch: Expected {len_self} rows, \\'\\n                                 \\'received array of length {len_col}\\'.format(\\n                                     len_self=len(self),\\n                                     len_col=len(arrays[-1])\\n                                 ))\\n\\n        index = ensure_index_from_sequences(arrays, names)\\n\\n        if verify_integrity and not index.is_unique:\\n            duplicates = index[index.duplicated()].unique()\\n            raise ValueError(\\'Index has duplicate keys: {dup}\\'.format(\\n                dup=duplicates))\\n\\n        # use set to handle duplicate column names gracefully in case of drop\\n        for c in set(to_remove):\\n            del frame[c]\\n\\n        # clear up memory usage\\n        index._cleanup()\\n\\n        frame.index = index\\n\\n        if not inplace:\\n            return frame',\n 'def reset_index(self, level=None, drop=False, inplace=False, col_level=0,\\n                    col_fill=\\'\\'):\\n        \"\"\"\\n        Reset the index, or a level of it.\\n\\n        Reset the index of the DataFrame, and use the default one instead.\\n        If the DataFrame has a MultiIndex, this method can remove one or more\\n        levels.\\n\\n        Parameters\\n        ----------\\n        level : int, str, tuple, or list, default None\\n            Only remove the given levels from the index. Removes all levels by\\n            default.\\n        drop : bool, default False\\n            Do not try to insert index into dataframe columns. This resets\\n            the index to the default integer index.\\n        inplace : bool, default False\\n            Modify the DataFrame in place (do not create a new object).\\n        col_level : int or str, default 0\\n            If the columns have multiple levels, determines which level the\\n            labels are inserted into. By default it is inserted into the first\\n            level.\\n        col_fill : object, default \\'\\'\\n            If the columns have multiple levels, determines how the other\\n            levels are named. If None then the index name is repeated.\\n\\n        Returns\\n        -------\\n        DataFrame\\n            DataFrame with the new index.\\n\\n        See Also\\n        --------\\n        DataFrame.set_index : Opposite of reset_index.\\n        DataFrame.reindex : Change to new indices or expand indices.\\n        DataFrame.reindex_like : Change to same indices as other DataFrame.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame([(\\'bird\\', 389.0),\\n        ...                    (\\'bird\\', 24.0),\\n        ...                    (\\'mammal\\', 80.5),\\n        ...                    (\\'mammal\\', np.nan)],\\n        ...                   index=[\\'falcon\\', \\'parrot\\', \\'lion\\', \\'monkey\\'],\\n        ...                   columns=(\\'class\\', \\'max_speed\\'))\\n        >>> df\\n                 class  max_speed\\n        falcon    bird      389.0\\n        parrot    bird       24.0\\n        lion    mammal       80.5\\n        monkey  mammal        NaN\\n\\n        When we reset the index, the old index is added as a column, and a\\n        new sequential index is used:\\n\\n        >>> df.reset_index()\\n            index   class  max_speed\\n        0  falcon    bird      389.0\\n        1  parrot    bird       24.0\\n        2    lion  mammal       80.5\\n        3  monkey  mammal        NaN\\n\\n        We can use the `drop` parameter to avoid the old index being added as\\n        a column:\\n\\n        >>> df.reset_index(drop=True)\\n            class  max_speed\\n        0    bird      389.0\\n        1    bird       24.0\\n        2  mammal       80.5\\n        3  mammal        NaN\\n\\n        You can also use `reset_index` with `MultiIndex`.\\n\\n        >>> index = pd.MultiIndex.from_tuples([(\\'bird\\', \\'falcon\\'),\\n        ...                                    (\\'bird\\', \\'parrot\\'),\\n        ...                                    (\\'mammal\\', \\'lion\\'),\\n        ...                                    (\\'mammal\\', \\'monkey\\')],\\n        ...                                   names=[\\'class\\', \\'name\\'])\\n        >>> columns = pd.MultiIndex.from_tuples([(\\'speed\\', \\'max\\'),\\n        ...                                      (\\'species\\', \\'type\\')])\\n        >>> df = pd.DataFrame([(389.0, \\'fly\\'),\\n        ...                    ( 24.0, \\'fly\\'),\\n        ...                    ( 80.5, \\'run\\'),\\n        ...                    (np.nan, \\'jump\\')],\\n        ...                   index=index,\\n        ...                   columns=columns)\\n        >>> df\\n                       speed species\\n                         max    type\\n        class  name\\n        bird   falcon  389.0     fly\\n               parrot   24.0     fly\\n        mammal lion     80.5     run\\n               monkey    NaN    jump\\n\\n        If the index has multiple levels, we can reset a subset of them:\\n\\n        >>> df.reset_index(level=\\'class\\')\\n                 class  speed species\\n                          max    type\\n        name\\n        falcon    bird  389.0     fly\\n        parrot    bird   24.0     fly\\n        lion    mammal   80.5     run\\n        monkey  mammal    NaN    jump\\n\\n        If we are not dropping the index, by default, it is placed in the top\\n        level. We can place it in another level:\\n\\n        >>> df.reset_index(level=\\'class\\', col_level=1)\\n                        speed species\\n                 class    max    type\\n        name\\n        falcon    bird  389.0     fly\\n        parrot    bird   24.0     fly\\n        lion    mammal   80.5     run\\n        monkey  mammal    NaN    jump\\n\\n        When the index is inserted under another level, we can specify under\\n        which one with the parameter `col_fill`:\\n\\n        >>> df.reset_index(level=\\'class\\', col_level=1, col_fill=\\'species\\')\\n                      species  speed species\\n                        class    max    type\\n        name\\n        falcon           bird  389.0     fly\\n        parrot           bird   24.0     fly\\n        lion           mammal   80.5     run\\n        monkey         mammal    NaN    jump\\n\\n        If we specify a nonexistent level for `col_fill`, it is created:\\n\\n        >>> df.reset_index(level=\\'class\\', col_level=1, col_fill=\\'genus\\')\\n                        genus  speed species\\n                        class    max    type\\n        name\\n        falcon           bird  389.0     fly\\n        parrot           bird   24.0     fly\\n        lion           mammal   80.5     run\\n        monkey         mammal    NaN    jump\\n        \"\"\"\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        if inplace:\\n            new_obj = self\\n        else:\\n            new_obj = self.copy()\\n\\n        def _maybe_casted_values(index, labels=None):\\n            values = index._values\\n            if not isinstance(index, (PeriodIndex, DatetimeIndex)):\\n                if values.dtype == np.object_:\\n                    values = lib.maybe_convert_objects(values)\\n\\n            # if we have the labels, extract the values with a mask\\n            if labels is not None:\\n                mask = labels == -1\\n\\n                # we can have situations where the whole mask is -1,\\n                # meaning there is nothing found in labels, so make all nan\\'s\\n                if mask.all():\\n                    values = np.empty(len(mask))\\n                    values.fill(np.nan)\\n                else:\\n                    values = values.take(labels)\\n\\n                    # TODO(https://github.com/pandas-dev/pandas/issues/24206)\\n                    # Push this into maybe_upcast_putmask?\\n                    # We can\\'t pass EAs there right now. Looks a bit\\n                    # complicated.\\n                    # So we unbox the ndarray_values, op, re-box.\\n                    values_type = type(values)\\n                    values_dtype = values.dtype\\n\\n                    if issubclass(values_type, DatetimeLikeArray):\\n                        values = values._data\\n\\n                    if mask.any():\\n                        values, changed = maybe_upcast_putmask(\\n                            values, mask, np.nan)\\n\\n                    if issubclass(values_type, DatetimeLikeArray):\\n                        values = values_type(values, dtype=values_dtype)\\n\\n            return values\\n\\n        new_index = ibase.default_index(len(new_obj))\\n        if level is not None:\\n            if not isinstance(level, (tuple, list)):\\n                level = [level]\\n            level = [self.index._get_level_number(lev) for lev in level]\\n            if len(level) < self.index.nlevels:\\n                new_index = self.index.droplevel(level)\\n\\n        if not drop:\\n            if isinstance(self.index, MultiIndex):\\n                names = [n if n is not None else (\\'level_%d\\' % i)\\n                         for (i, n) in enumerate(self.index.names)]\\n                to_insert = lzip(self.index.levels, self.index.codes)\\n            else:\\n                default = \\'index\\' if \\'index\\' not in self else \\'level_0\\'\\n                names = ([default] if self.index.name is None\\n                         else [self.index.name])\\n                to_insert = ((self.index, None),)\\n\\n            multi_col = isinstance(self.columns, MultiIndex)\\n            for i, (lev, lab) in reversed(list(enumerate(to_insert))):\\n                if not (level is None or i in level):\\n                    continue\\n                name = names[i]\\n                if multi_col:\\n                    col_name = (list(name) if isinstance(name, tuple)\\n                                else [name])\\n                    if col_fill is None:\\n                        if len(col_name) not in (1, self.columns.nlevels):\\n                            raise ValueError(\"col_fill=None is incompatible \"\\n                                             \"with incomplete column name \"\\n                                             \"{}\".format(name))\\n                        col_fill = col_name[0]\\n\\n                    lev_num = self.columns._get_level_number(col_level)\\n                    name_lst = [col_fill] * lev_num + col_name\\n                    missing = self.columns.nlevels - len(name_lst)\\n                    name_lst += [col_fill] * missing\\n                    name = tuple(name_lst)\\n                # to ndarray and maybe infer different dtype\\n                level_values = _maybe_casted_values(lev, lab)\\n                new_obj.insert(0, name, level_values)\\n\\n        new_obj.index = new_index\\n        if not inplace:\\n            return new_obj',\n 'def dropna(self, axis=0, how=\\'any\\', thresh=None, subset=None,\\n               inplace=False):\\n        \"\"\"\\n        Remove missing values.\\n\\n        See the :ref:`User Guide <missing_data>` for more on which values are\\n        considered missing, and how to work with missing data.\\n\\n        Parameters\\n        ----------\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n            Determine if rows or columns which contain missing values are\\n            removed.\\n\\n            * 0, or \\'index\\' : Drop rows which contain missing values.\\n            * 1, or \\'columns\\' : Drop columns which contain missing value.\\n\\n            .. deprecated:: 0.23.0\\n\\n               Pass tuple or list to drop on multiple axes.\\n               Only a single axis is allowed.\\n\\n        how : {\\'any\\', \\'all\\'}, default \\'any\\'\\n            Determine if row or column is removed from DataFrame, when we have\\n            at least one NA or all NA.\\n\\n            * \\'any\\' : If any NA values are present, drop that row or column.\\n            * \\'all\\' : If all values are NA, drop that row or column.\\n\\n        thresh : int, optional\\n            Require that many non-NA values.\\n        subset : array-like, optional\\n            Labels along other axis to consider, e.g. if you are dropping rows\\n            these would be a list of columns to include.\\n        inplace : bool, default False\\n            If True, do operation inplace and return None.\\n\\n        Returns\\n        -------\\n        DataFrame\\n            DataFrame with NA entries dropped from it.\\n\\n        See Also\\n        --------\\n        DataFrame.isna: Indicate missing values.\\n        DataFrame.notna : Indicate existing (non-missing) values.\\n        DataFrame.fillna : Replace missing values.\\n        Series.dropna : Drop missing values.\\n        Index.dropna : Drop missing indices.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\"name\": [\\'Alfred\\', \\'Batman\\', \\'Catwoman\\'],\\n        ...                    \"toy\": [np.nan, \\'Batmobile\\', \\'Bullwhip\\'],\\n        ...                    \"born\": [pd.NaT, pd.Timestamp(\"1940-04-25\"),\\n        ...                             pd.NaT]})\\n        >>> df\\n               name        toy       born\\n        0    Alfred        NaN        NaT\\n        1    Batman  Batmobile 1940-04-25\\n        2  Catwoman   Bullwhip        NaT\\n\\n        Drop the rows where at least one element is missing.\\n\\n        >>> df.dropna()\\n             name        toy       born\\n        1  Batman  Batmobile 1940-04-25\\n\\n        Drop the columns where at least one element is missing.\\n\\n        >>> df.dropna(axis=\\'columns\\')\\n               name\\n        0    Alfred\\n        1    Batman\\n        2  Catwoman\\n\\n        Drop the rows where all elements are missing.\\n\\n        >>> df.dropna(how=\\'all\\')\\n               name        toy       born\\n        0    Alfred        NaN        NaT\\n        1    Batman  Batmobile 1940-04-25\\n        2  Catwoman   Bullwhip        NaT\\n\\n        Keep only the rows with at least 2 non-NA values.\\n\\n        >>> df.dropna(thresh=2)\\n               name        toy       born\\n        1    Batman  Batmobile 1940-04-25\\n        2  Catwoman   Bullwhip        NaT\\n\\n        Define in which columns to look for missing values.\\n\\n        >>> df.dropna(subset=[\\'name\\', \\'born\\'])\\n               name        toy       born\\n        1    Batman  Batmobile 1940-04-25\\n\\n        Keep the DataFrame with valid entries in the same variable.\\n\\n        >>> df.dropna(inplace=True)\\n        >>> df\\n             name        toy       born\\n        1  Batman  Batmobile 1940-04-25\\n        \"\"\"\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        if isinstance(axis, (tuple, list)):\\n            # GH20987\\n            msg = (\"supplying multiple axes to axis is deprecated and \"\\n                   \"will be removed in a future version.\")\\n            warnings.warn(msg, FutureWarning, stacklevel=2)\\n\\n            result = self\\n            for ax in axis:\\n                result = result.dropna(how=how, thresh=thresh, subset=subset,\\n                                       axis=ax)\\n        else:\\n            axis = self._get_axis_number(axis)\\n            agg_axis = 1 - axis\\n\\n            agg_obj = self\\n            if subset is not None:\\n                ax = self._get_axis(agg_axis)\\n                indices = ax.get_indexer_for(subset)\\n                check = indices == -1\\n                if check.any():\\n                    raise KeyError(list(np.compress(check, subset)))\\n                agg_obj = self.take(indices, axis=agg_axis)\\n\\n            count = agg_obj.count(axis=agg_axis)\\n\\n            if thresh is not None:\\n                mask = count >= thresh\\n            elif how == \\'any\\':\\n                mask = count == len(agg_obj._get_axis(agg_axis))\\n            elif how == \\'all\\':\\n                mask = count > 0\\n            else:\\n                if how is not None:\\n                    raise ValueError(\\'invalid how option: {h}\\'.format(h=how))\\n                else:\\n                    raise TypeError(\\'must specify how or thresh\\')\\n\\n            result = self.loc(axis=axis)[mask]\\n\\n        if inplace:\\n            self._update_inplace(result)\\n        else:\\n            return result',\n 'def drop_duplicates(self, subset=None, keep=\\'first\\', inplace=False):\\n        \"\"\"\\n        Return DataFrame with duplicate rows removed, optionally only\\n        considering certain columns. Indexes, including time indexes\\n        are ignored.\\n\\n        Parameters\\n        ----------\\n        subset : column label or sequence of labels, optional\\n            Only consider certain columns for identifying duplicates, by\\n            default use all of the columns\\n        keep : {\\'first\\', \\'last\\', False}, default \\'first\\'\\n            - ``first`` : Drop duplicates except for the first occurrence.\\n            - ``last`` : Drop duplicates except for the last occurrence.\\n            - False : Drop all duplicates.\\n        inplace : boolean, default False\\n            Whether to drop duplicates in place or to return a copy\\n\\n        Returns\\n        -------\\n        DataFrame\\n        \"\"\"\\n        if self.empty:\\n            return self.copy()\\n\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        duplicated = self.duplicated(subset, keep=keep)\\n\\n        if inplace:\\n            inds, = (-duplicated)._ndarray_values.nonzero()\\n            new_data = self._data.take(inds)\\n            self._update_inplace(new_data)\\n        else:\\n            return self[-duplicated]',\n 'def duplicated(self, subset=None, keep=\\'first\\'):\\n        \"\"\"\\n        Return boolean Series denoting duplicate rows, optionally only\\n        considering certain columns.\\n\\n        Parameters\\n        ----------\\n        subset : column label or sequence of labels, optional\\n            Only consider certain columns for identifying duplicates, by\\n            default use all of the columns\\n        keep : {\\'first\\', \\'last\\', False}, default \\'first\\'\\n            - ``first`` : Mark duplicates as ``True`` except for the\\n              first occurrence.\\n            - ``last`` : Mark duplicates as ``True`` except for the\\n              last occurrence.\\n            - False : Mark all duplicates as ``True``.\\n\\n        Returns\\n        -------\\n        Series\\n        \"\"\"\\n        from pandas.core.sorting import get_group_index\\n        from pandas._libs.hashtable import duplicated_int64, _SIZE_HINT_LIMIT\\n\\n        if self.empty:\\n            return Series(dtype=bool)\\n\\n        def f(vals):\\n            labels, shape = algorithms.factorize(\\n                vals, size_hint=min(len(self), _SIZE_HINT_LIMIT))\\n            return labels.astype(\\'i8\\', copy=False), len(shape)\\n\\n        if subset is None:\\n            subset = self.columns\\n        elif (not np.iterable(subset) or\\n              isinstance(subset, str) or\\n              isinstance(subset, tuple) and subset in self.columns):\\n            subset = subset,\\n\\n        # Verify all columns in subset exist in the queried dataframe\\n        # Otherwise, raise a KeyError, same as if you try to __getitem__ with a\\n        # key that doesn\\'t exist.\\n        diff = Index(subset).difference(self.columns)\\n        if not diff.empty:\\n            raise KeyError(diff)\\n\\n        vals = (col.values for name, col in self.iteritems()\\n                if name in subset)\\n        labels, shape = map(list, zip(*map(f, vals)))\\n\\n        ids = get_group_index(labels, shape, sort=False, xnull=False)\\n        return Series(duplicated_int64(ids, keep), index=self.index)',\n 'def nlargest(self, n, columns, keep=\\'first\\'):\\n        \"\"\"\\n        Return the first `n` rows ordered by `columns` in descending order.\\n\\n        Return the first `n` rows with the largest values in `columns`, in\\n        descending order. The columns that are not specified are returned as\\n        well, but not used for ordering.\\n\\n        This method is equivalent to\\n        ``df.sort_values(columns, ascending=False).head(n)``, but more\\n        performant.\\n\\n        Parameters\\n        ----------\\n        n : int\\n            Number of rows to return.\\n        columns : label or list of labels\\n            Column label(s) to order by.\\n        keep : {\\'first\\', \\'last\\', \\'all\\'}, default \\'first\\'\\n            Where there are duplicate values:\\n\\n            - `first` : prioritize the first occurrence(s)\\n            - `last` : prioritize the last occurrence(s)\\n            - ``all`` : do not drop any duplicates, even it means\\n                        selecting more than `n` items.\\n\\n            .. versionadded:: 0.24.0\\n\\n        Returns\\n        -------\\n        DataFrame\\n            The first `n` rows ordered by the given columns in descending\\n            order.\\n\\n        See Also\\n        --------\\n        DataFrame.nsmallest : Return the first `n` rows ordered by `columns` in\\n            ascending order.\\n        DataFrame.sort_values : Sort DataFrame by the values.\\n        DataFrame.head : Return the first `n` rows without re-ordering.\\n\\n        Notes\\n        -----\\n        This function cannot be used with all column types. For example, when\\n        specifying columns with `object` or `category` dtypes, ``TypeError`` is\\n        raised.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'population\\': [59000000, 65000000, 434000,\\n        ...                                   434000, 434000, 337000, 11300,\\n        ...                                   11300, 11300],\\n        ...                    \\'GDP\\': [1937894, 2583560 , 12011, 4520, 12128,\\n        ...                            17036, 182, 38, 311],\\n        ...                    \\'alpha-2\\': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\",\\n        ...                                \"IS\", \"NR\", \"TV\", \"AI\"]},\\n        ...                   index=[\"Italy\", \"France\", \"Malta\",\\n        ...                          \"Maldives\", \"Brunei\", \"Iceland\",\\n        ...                          \"Nauru\", \"Tuvalu\", \"Anguilla\"])\\n        >>> df\\n                  population      GDP alpha-2\\n        Italy       59000000  1937894      IT\\n        France      65000000  2583560      FR\\n        Malta         434000    12011      MT\\n        Maldives      434000     4520      MV\\n        Brunei        434000    12128      BN\\n        Iceland       337000    17036      IS\\n        Nauru          11300      182      NR\\n        Tuvalu         11300       38      TV\\n        Anguilla       11300      311      AI\\n\\n        In the following example, we will use ``nlargest`` to select the three\\n        rows having the largest values in column \"population\".\\n\\n        >>> df.nlargest(3, \\'population\\')\\n                population      GDP alpha-2\\n        France    65000000  2583560      FR\\n        Italy     59000000  1937894      IT\\n        Malta       434000    12011      MT\\n\\n        When using ``keep=\\'last\\'``, ties are resolved in reverse order:\\n\\n        >>> df.nlargest(3, \\'population\\', keep=\\'last\\')\\n                population      GDP alpha-2\\n        France    65000000  2583560      FR\\n        Italy     59000000  1937894      IT\\n        Brunei      434000    12128      BN\\n\\n        When using ``keep=\\'all\\'``, all duplicate items are maintained:\\n\\n        >>> df.nlargest(3, \\'population\\', keep=\\'all\\')\\n                  population      GDP alpha-2\\n        France      65000000  2583560      FR\\n        Italy       59000000  1937894      IT\\n        Malta         434000    12011      MT\\n        Maldives      434000     4520      MV\\n        Brunei        434000    12128      BN\\n\\n        To order by the largest values in column \"population\" and then \"GDP\",\\n        we can specify multiple columns like in the next example.\\n\\n        >>> df.nlargest(3, [\\'population\\', \\'GDP\\'])\\n                population      GDP alpha-2\\n        France    65000000  2583560      FR\\n        Italy     59000000  1937894      IT\\n        Brunei      434000    12128      BN\\n        \"\"\"\\n        return algorithms.SelectNFrame(self,\\n                                       n=n,\\n                                       keep=keep,\\n                                       columns=columns).nlargest()',\n 'def nsmallest(self, n, columns, keep=\\'first\\'):\\n        \"\"\"\\n        Return the first `n` rows ordered by `columns` in ascending order.\\n\\n        Return the first `n` rows with the smallest values in `columns`, in\\n        ascending order. The columns that are not specified are returned as\\n        well, but not used for ordering.\\n\\n        This method is equivalent to\\n        ``df.sort_values(columns, ascending=True).head(n)``, but more\\n        performant.\\n\\n        Parameters\\n        ----------\\n        n : int\\n            Number of items to retrieve.\\n        columns : list or str\\n            Column name or names to order by.\\n        keep : {\\'first\\', \\'last\\', \\'all\\'}, default \\'first\\'\\n            Where there are duplicate values:\\n\\n            - ``first`` : take the first occurrence.\\n            - ``last`` : take the last occurrence.\\n            - ``all`` : do not drop any duplicates, even it means\\n              selecting more than `n` items.\\n\\n            .. versionadded:: 0.24.0\\n\\n        Returns\\n        -------\\n        DataFrame\\n\\n        See Also\\n        --------\\n        DataFrame.nlargest : Return the first `n` rows ordered by `columns` in\\n            descending order.\\n        DataFrame.sort_values : Sort DataFrame by the values.\\n        DataFrame.head : Return the first `n` rows without re-ordering.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'population\\': [59000000, 65000000, 434000,\\n        ...                                   434000, 434000, 337000, 11300,\\n        ...                                   11300, 11300],\\n        ...                    \\'GDP\\': [1937894, 2583560 , 12011, 4520, 12128,\\n        ...                            17036, 182, 38, 311],\\n        ...                    \\'alpha-2\\': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\",\\n        ...                                \"IS\", \"NR\", \"TV\", \"AI\"]},\\n        ...                   index=[\"Italy\", \"France\", \"Malta\",\\n        ...                          \"Maldives\", \"Brunei\", \"Iceland\",\\n        ...                          \"Nauru\", \"Tuvalu\", \"Anguilla\"])\\n        >>> df\\n                  population      GDP alpha-2\\n        Italy       59000000  1937894      IT\\n        France      65000000  2583560      FR\\n        Malta         434000    12011      MT\\n        Maldives      434000     4520      MV\\n        Brunei        434000    12128      BN\\n        Iceland       337000    17036      IS\\n        Nauru          11300      182      NR\\n        Tuvalu         11300       38      TV\\n        Anguilla       11300      311      AI\\n\\n        In the following example, we will use ``nsmallest`` to select the\\n        three rows having the smallest values in column \"a\".\\n\\n        >>> df.nsmallest(3, \\'population\\')\\n                  population  GDP alpha-2\\n        Nauru          11300  182      NR\\n        Tuvalu         11300   38      TV\\n        Anguilla       11300  311      AI\\n\\n        When using ``keep=\\'last\\'``, ties are resolved in reverse order:\\n\\n        >>> df.nsmallest(3, \\'population\\', keep=\\'last\\')\\n                  population  GDP alpha-2\\n        Anguilla       11300  311      AI\\n        Tuvalu         11300   38      TV\\n        Nauru          11300  182      NR\\n\\n        When using ``keep=\\'all\\'``, all duplicate items are maintained:\\n\\n        >>> df.nsmallest(3, \\'population\\', keep=\\'all\\')\\n                  population  GDP alpha-2\\n        Nauru          11300  182      NR\\n        Tuvalu         11300   38      TV\\n        Anguilla       11300  311      AI\\n\\n        To order by the largest values in column \"a\" and then \"c\", we can\\n        specify multiple columns like in the next example.\\n\\n        >>> df.nsmallest(3, [\\'population\\', \\'GDP\\'])\\n                  population  GDP alpha-2\\n        Tuvalu         11300   38      TV\\n        Nauru          11300  182      NR\\n        Anguilla       11300  311      AI\\n        \"\"\"\\n        return algorithms.SelectNFrame(self,\\n                                       n=n,\\n                                       keep=keep,\\n                                       columns=columns).nsmallest()',\n 'def swaplevel(self, i=-2, j=-1, axis=0):\\n        \"\"\"\\n        Swap levels i and j in a MultiIndex on a particular axis.\\n\\n        Parameters\\n        ----------\\n        i, j : int, string (can be mixed)\\n            Level of index to be swapped. Can pass level name as string.\\n\\n        Returns\\n        -------\\n        DataFrame\\n\\n        .. versionchanged:: 0.18.1\\n\\n           The indexes ``i`` and ``j`` are now optional, and default to\\n           the two innermost levels of the index.\\n        \"\"\"\\n        result = self.copy()\\n\\n        axis = self._get_axis_number(axis)\\n        if axis == 0:\\n            result.index = result.index.swaplevel(i, j)\\n        else:\\n            result.columns = result.columns.swaplevel(i, j)\\n        return result',\n 'def reorder_levels(self, order, axis=0):\\n        \"\"\"\\n        Rearrange index levels using input order. May not drop or\\n        duplicate levels.\\n\\n        Parameters\\n        ----------\\n        order : list of int or list of str\\n            List representing new level order. Reference level by number\\n            (position) or by key (label).\\n        axis : int\\n            Where to reorder levels.\\n\\n        Returns\\n        -------\\n        type of caller (new object)\\n        \"\"\"\\n        axis = self._get_axis_number(axis)\\n        if not isinstance(self._get_axis(axis),\\n                          MultiIndex):  # pragma: no cover\\n            raise TypeError(\\'Can only reorder levels on a hierarchical axis.\\')\\n\\n        result = self.copy()\\n\\n        if axis == 0:\\n            result.index = result.index.reorder_levels(order)\\n        else:\\n            result.columns = result.columns.reorder_levels(order)\\n        return result',\n 'def combine(self, other, func, fill_value=None, overwrite=True):\\n        \"\"\"\\n        Perform column-wise combine with another DataFrame.\\n\\n        Combines a DataFrame with `other` DataFrame using `func`\\n        to element-wise combine columns. The row and column indexes of the\\n        resulting DataFrame will be the union of the two.\\n\\n        Parameters\\n        ----------\\n        other : DataFrame\\n            The DataFrame to merge column-wise.\\n        func : function\\n            Function that takes two series as inputs and return a Series or a\\n            scalar. Used to merge the two dataframes column by columns.\\n        fill_value : scalar value, default None\\n            The value to fill NaNs with prior to passing any column to the\\n            merge func.\\n        overwrite : bool, default True\\n            If True, columns in `self` that do not exist in `other` will be\\n            overwritten with NaNs.\\n\\n        Returns\\n        -------\\n        DataFrame\\n            Combination of the provided DataFrames.\\n\\n        See Also\\n        --------\\n        DataFrame.combine_first : Combine two DataFrame objects and default to\\n            non-null values in frame calling the method.\\n\\n        Examples\\n        --------\\n        Combine using a simple function that chooses the smaller column.\\n\\n        >>> df1 = pd.DataFrame({\\'A\\': [0, 0], \\'B\\': [4, 4]})\\n        >>> df2 = pd.DataFrame({\\'A\\': [1, 1], \\'B\\': [3, 3]})\\n        >>> take_smaller = lambda s1, s2: s1 if s1.sum() < s2.sum() else s2\\n        >>> df1.combine(df2, take_smaller)\\n           A  B\\n        0  0  3\\n        1  0  3\\n\\n        Example using a true element-wise combine function.\\n\\n        >>> df1 = pd.DataFrame({\\'A\\': [5, 0], \\'B\\': [2, 4]})\\n        >>> df2 = pd.DataFrame({\\'A\\': [1, 1], \\'B\\': [3, 3]})\\n        >>> df1.combine(df2, np.minimum)\\n           A  B\\n        0  1  2\\n        1  0  3\\n\\n        Using `fill_value` fills Nones prior to passing the column to the\\n        merge function.\\n\\n        >>> df1 = pd.DataFrame({\\'A\\': [0, 0], \\'B\\': [None, 4]})\\n        >>> df2 = pd.DataFrame({\\'A\\': [1, 1], \\'B\\': [3, 3]})\\n        >>> df1.combine(df2, take_smaller, fill_value=-5)\\n           A    B\\n        0  0 -5.0\\n        1  0  4.0\\n\\n        However, if the same element in both dataframes is None, that None\\n        is preserved\\n\\n        >>> df1 = pd.DataFrame({\\'A\\': [0, 0], \\'B\\': [None, 4]})\\n        >>> df2 = pd.DataFrame({\\'A\\': [1, 1], \\'B\\': [None, 3]})\\n        >>> df1.combine(df2, take_smaller, fill_value=-5)\\n            A    B\\n        0  0 -5.0\\n        1  0  3.0\\n\\n        Example that demonstrates the use of `overwrite` and behavior when\\n        the axis differ between the dataframes.\\n\\n        >>> df1 = pd.DataFrame({\\'A\\': [0, 0], \\'B\\': [4, 4]})\\n        >>> df2 = pd.DataFrame({\\'B\\': [3, 3], \\'C\\': [-10, 1], }, index=[1, 2])\\n        >>> df1.combine(df2, take_smaller)\\n             A    B     C\\n        0  NaN  NaN   NaN\\n        1  NaN  3.0 -10.0\\n        2  NaN  3.0   1.0\\n\\n        >>> df1.combine(df2, take_smaller, overwrite=False)\\n             A    B     C\\n        0  0.0  NaN   NaN\\n        1  0.0  3.0 -10.0\\n        2  NaN  3.0   1.0\\n\\n        Demonstrating the preference of the passed in dataframe.\\n\\n        >>> df2 = pd.DataFrame({\\'B\\': [3, 3], \\'C\\': [1, 1], }, index=[1, 2])\\n        >>> df2.combine(df1, take_smaller)\\n           A    B   C\\n        0  0.0  NaN NaN\\n        1  0.0  3.0 NaN\\n        2  NaN  3.0 NaN\\n\\n        >>> df2.combine(df1, take_smaller, overwrite=False)\\n             A    B   C\\n        0  0.0  NaN NaN\\n        1  0.0  3.0 1.0\\n        2  NaN  3.0 1.0\\n        \"\"\"\\n        other_idxlen = len(other.index)  # save for compare\\n\\n        this, other = self.align(other, copy=False)\\n        new_index = this.index\\n\\n        if other.empty and len(new_index) == len(self.index):\\n            return self.copy()\\n\\n        if self.empty and len(other) == other_idxlen:\\n            return other.copy()\\n\\n        # sorts if possible\\n        new_columns = this.columns.union(other.columns)\\n        do_fill = fill_value is not None\\n        result = {}\\n        for col in new_columns:\\n            series = this[col]\\n            otherSeries = other[col]\\n\\n            this_dtype = series.dtype\\n            other_dtype = otherSeries.dtype\\n\\n            this_mask = isna(series)\\n            other_mask = isna(otherSeries)\\n\\n            # don\\'t overwrite columns unecessarily\\n            # DO propagate if this column is not in the intersection\\n            if not overwrite and other_mask.all():\\n                result[col] = this[col].copy()\\n                continue\\n\\n            if do_fill:\\n                series = series.copy()\\n                otherSeries = otherSeries.copy()\\n                series[this_mask] = fill_value\\n                otherSeries[other_mask] = fill_value\\n\\n            if col not in self.columns:\\n                # If self DataFrame does not have col in other DataFrame,\\n                # try to promote series, which is all NaN, as other_dtype.\\n                new_dtype = other_dtype\\n                try:\\n                    series = series.astype(new_dtype, copy=False)\\n                except ValueError:\\n                    # e.g. new_dtype is integer types\\n                    pass\\n            else:\\n                # if we have different dtypes, possibly promote\\n                new_dtype = find_common_type([this_dtype, other_dtype])\\n                if not is_dtype_equal(this_dtype, new_dtype):\\n                    series = series.astype(new_dtype)\\n                if not is_dtype_equal(other_dtype, new_dtype):\\n                    otherSeries = otherSeries.astype(new_dtype)\\n\\n            arr = func(series, otherSeries)\\n            arr = maybe_downcast_to_dtype(arr, this_dtype)\\n\\n            result[col] = arr\\n\\n        # convert_objects just in case\\n        return self._constructor(result, index=new_index,\\n                                 columns=new_columns)',\n 'def combine_first(self, other):\\n        \"\"\"\\n        Update null elements with value in the same location in `other`.\\n\\n        Combine two DataFrame objects by filling null values in one DataFrame\\n        with non-null values from other DataFrame. The row and column indexes\\n        of the resulting DataFrame will be the union of the two.\\n\\n        Parameters\\n        ----------\\n        other : DataFrame\\n            Provided DataFrame to use to fill null values.\\n\\n        Returns\\n        -------\\n        DataFrame\\n\\n        See Also\\n        --------\\n        DataFrame.combine : Perform series-wise operation on two DataFrames\\n            using a given function.\\n\\n        Examples\\n        --------\\n\\n        >>> df1 = pd.DataFrame({\\'A\\': [None, 0], \\'B\\': [None, 4]})\\n        >>> df2 = pd.DataFrame({\\'A\\': [1, 1], \\'B\\': [3, 3]})\\n        >>> df1.combine_first(df2)\\n             A    B\\n        0  1.0  3.0\\n        1  0.0  4.0\\n\\n        Null values still persist if the location of that null value\\n        does not exist in `other`\\n\\n        >>> df1 = pd.DataFrame({\\'A\\': [None, 0], \\'B\\': [4, None]})\\n        >>> df2 = pd.DataFrame({\\'B\\': [3, 3], \\'C\\': [1, 1]}, index=[1, 2])\\n        >>> df1.combine_first(df2)\\n             A    B    C\\n        0  NaN  4.0  NaN\\n        1  0.0  3.0  1.0\\n        2  NaN  3.0  1.0\\n        \"\"\"\\n        import pandas.core.computation.expressions as expressions\\n\\n        def extract_values(arr):\\n            # Does two things:\\n            # 1. maybe gets the values from the Series / Index\\n            # 2. convert datelike to i8\\n            if isinstance(arr, (ABCIndexClass, ABCSeries)):\\n                arr = arr._values\\n\\n            if needs_i8_conversion(arr):\\n                if is_extension_array_dtype(arr.dtype):\\n                    arr = arr.asi8\\n                else:\\n                    arr = arr.view(\\'i8\\')\\n            return arr\\n\\n        def combiner(x, y):\\n            mask = isna(x)\\n            if isinstance(mask, (ABCIndexClass, ABCSeries)):\\n                mask = mask._values\\n\\n            x_values = extract_values(x)\\n            y_values = extract_values(y)\\n\\n            # If the column y in other DataFrame is not in first DataFrame,\\n            # just return y_values.\\n            if y.name not in self.columns:\\n                return y_values\\n\\n            return expressions.where(mask, y_values, x_values)\\n\\n        return self.combine(other, combiner, overwrite=False)',\n 'def update(self, other, join=\\'left\\', overwrite=True, filter_func=None,\\n               errors=\\'ignore\\'):\\n        \"\"\"\\n        Modify in place using non-NA values from another DataFrame.\\n\\n        Aligns on indices. There is no return value.\\n\\n        Parameters\\n        ----------\\n        other : DataFrame, or object coercible into a DataFrame\\n            Should have at least one matching index/column label\\n            with the original DataFrame. If a Series is passed,\\n            its name attribute must be set, and that will be\\n            used as the column name to align with the original DataFrame.\\n        join : {\\'left\\'}, default \\'left\\'\\n            Only left join is implemented, keeping the index and columns of the\\n            original object.\\n        overwrite : bool, default True\\n            How to handle non-NA values for overlapping keys:\\n\\n            * True: overwrite original DataFrame\\'s values\\n              with values from `other`.\\n            * False: only update values that are NA in\\n              the original DataFrame.\\n\\n        filter_func : callable(1d-array) -> bool 1d-array, optional\\n            Can choose to replace values other than NA. Return True for values\\n            that should be updated.\\n        errors : {\\'raise\\', \\'ignore\\'}, default \\'ignore\\'\\n            If \\'raise\\', will raise a ValueError if the DataFrame and `other`\\n            both contain non-NA data in the same place.\\n\\n            .. versionchanged :: 0.24.0\\n               Changed from `raise_conflict=False|True`\\n               to `errors=\\'ignore\\'|\\'raise\\'`.\\n\\n        Returns\\n        -------\\n        None : method directly changes calling object\\n\\n        Raises\\n        ------\\n        ValueError\\n            * When `errors=\\'raise\\'` and there\\'s overlapping non-NA data.\\n            * When `errors` is not either `\\'ignore\\'` or `\\'raise\\'`\\n        NotImplementedError\\n            * If `join != \\'left\\'`\\n\\n        See Also\\n        --------\\n        dict.update : Similar method for dictionaries.\\n        DataFrame.merge : For column(s)-on-columns(s) operations.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'A\\': [1, 2, 3],\\n        ...                    \\'B\\': [400, 500, 600]})\\n        >>> new_df = pd.DataFrame({\\'B\\': [4, 5, 6],\\n        ...                        \\'C\\': [7, 8, 9]})\\n        >>> df.update(new_df)\\n        >>> df\\n           A  B\\n        0  1  4\\n        1  2  5\\n        2  3  6\\n\\n        The DataFrame\\'s length does not increase as a result of the update,\\n        only values at matching index/column labels are updated.\\n\\n        >>> df = pd.DataFrame({\\'A\\': [\\'a\\', \\'b\\', \\'c\\'],\\n        ...                    \\'B\\': [\\'x\\', \\'y\\', \\'z\\']})\\n        >>> new_df = pd.DataFrame({\\'B\\': [\\'d\\', \\'e\\', \\'f\\', \\'g\\', \\'h\\', \\'i\\']})\\n        >>> df.update(new_df)\\n        >>> df\\n           A  B\\n        0  a  d\\n        1  b  e\\n        2  c  f\\n\\n        For Series, it\\'s name attribute must be set.\\n\\n        >>> df = pd.DataFrame({\\'A\\': [\\'a\\', \\'b\\', \\'c\\'],\\n        ...                    \\'B\\': [\\'x\\', \\'y\\', \\'z\\']})\\n        >>> new_column = pd.Series([\\'d\\', \\'e\\'], name=\\'B\\', index=[0, 2])\\n        >>> df.update(new_column)\\n        >>> df\\n           A  B\\n        0  a  d\\n        1  b  y\\n        2  c  e\\n        >>> df = pd.DataFrame({\\'A\\': [\\'a\\', \\'b\\', \\'c\\'],\\n        ...                    \\'B\\': [\\'x\\', \\'y\\', \\'z\\']})\\n        >>> new_df = pd.DataFrame({\\'B\\': [\\'d\\', \\'e\\']}, index=[1, 2])\\n        >>> df.update(new_df)\\n        >>> df\\n           A  B\\n        0  a  x\\n        1  b  d\\n        2  c  e\\n\\n        If `other` contains NaNs the corresponding values are not updated\\n        in the original dataframe.\\n\\n        >>> df = pd.DataFrame({\\'A\\': [1, 2, 3],\\n        ...                    \\'B\\': [400, 500, 600]})\\n        >>> new_df = pd.DataFrame({\\'B\\': [4, np.nan, 6]})\\n        >>> df.update(new_df)\\n        >>> df\\n           A      B\\n        0  1    4.0\\n        1  2  500.0\\n        2  3    6.0\\n        \"\"\"\\n        import pandas.core.computation.expressions as expressions\\n        # TODO: Support other joins\\n        if join != \\'left\\':  # pragma: no cover\\n            raise NotImplementedError(\"Only left join is supported\")\\n        if errors not in [\\'ignore\\', \\'raise\\']:\\n            raise ValueError(\"The parameter errors must be either \"\\n                             \"\\'ignore\\' or \\'raise\\'\")\\n\\n        if not isinstance(other, DataFrame):\\n            other = DataFrame(other)\\n\\n        other = other.reindex_like(self)\\n\\n        for col in self.columns:\\n            this = self[col]._values\\n            that = other[col]._values\\n            if filter_func is not None:\\n                with np.errstate(all=\\'ignore\\'):\\n                    mask = ~filter_func(this) | isna(that)\\n            else:\\n                if errors == \\'raise\\':\\n                    mask_this = notna(that)\\n                    mask_that = notna(this)\\n                    if any(mask_this & mask_that):\\n                        raise ValueError(\"Data overlaps.\")\\n\\n                if overwrite:\\n                    mask = isna(that)\\n                else:\\n                    mask = notna(this)\\n\\n            # don\\'t overwrite columns unecessarily\\n            if mask.all():\\n                continue\\n\\n            self[col] = expressions.where(mask, this, that)',\n 'def stack(self, level=-1, dropna=True):\\n        \"\"\"\\n        Stack the prescribed level(s) from columns to index.\\n\\n        Return a reshaped DataFrame or Series having a multi-level\\n        index with one or more new inner-most levels compared to the current\\n        DataFrame. The new inner-most levels are created by pivoting the\\n        columns of the current dataframe:\\n\\n          - if the columns have a single level, the output is a Series;\\n          - if the columns have multiple levels, the new index\\n            level(s) is (are) taken from the prescribed level(s) and\\n            the output is a DataFrame.\\n\\n        The new index levels are sorted.\\n\\n        Parameters\\n        ----------\\n        level : int, str, list, default -1\\n            Level(s) to stack from the column axis onto the index\\n            axis, defined as one index or label, or a list of indices\\n            or labels.\\n        dropna : bool, default True\\n            Whether to drop rows in the resulting Frame/Series with\\n            missing values. Stacking a column level onto the index\\n            axis can create combinations of index and column values\\n            that are missing from the original dataframe. See Examples\\n            section.\\n\\n        Returns\\n        -------\\n        DataFrame or Series\\n            Stacked dataframe or series.\\n\\n        See Also\\n        --------\\n        DataFrame.unstack : Unstack prescribed level(s) from index axis\\n             onto column axis.\\n        DataFrame.pivot : Reshape dataframe from long format to wide\\n             format.\\n        DataFrame.pivot_table : Create a spreadsheet-style pivot table\\n             as a DataFrame.\\n\\n        Notes\\n        -----\\n        The function is named by analogy with a collection of books\\n        being reorganized from being side by side on a horizontal\\n        position (the columns of the dataframe) to being stacked\\n        vertically on top of each other (in the index of the\\n        dataframe).\\n\\n        Examples\\n        --------\\n        **Single level columns**\\n\\n        >>> df_single_level_cols = pd.DataFrame([[0, 1], [2, 3]],\\n        ...                                     index=[\\'cat\\', \\'dog\\'],\\n        ...                                     columns=[\\'weight\\', \\'height\\'])\\n\\n        Stacking a dataframe with a single level column axis returns a Series:\\n\\n        >>> df_single_level_cols\\n             weight height\\n        cat       0      1\\n        dog       2      3\\n        >>> df_single_level_cols.stack()\\n        cat  weight    0\\n             height    1\\n        dog  weight    2\\n             height    3\\n        dtype: int64\\n\\n        **Multi level columns: simple case**\\n\\n        >>> multicol1 = pd.MultiIndex.from_tuples([(\\'weight\\', \\'kg\\'),\\n        ...                                        (\\'weight\\', \\'pounds\\')])\\n        >>> df_multi_level_cols1 = pd.DataFrame([[1, 2], [2, 4]],\\n        ...                                     index=[\\'cat\\', \\'dog\\'],\\n        ...                                     columns=multicol1)\\n\\n        Stacking a dataframe with a multi-level column axis:\\n\\n        >>> df_multi_level_cols1\\n             weight\\n                 kg    pounds\\n        cat       1        2\\n        dog       2        4\\n        >>> df_multi_level_cols1.stack()\\n                    weight\\n        cat kg           1\\n            pounds       2\\n        dog kg           2\\n            pounds       4\\n\\n        **Missing values**\\n\\n        >>> multicol2 = pd.MultiIndex.from_tuples([(\\'weight\\', \\'kg\\'),\\n        ...                                        (\\'height\\', \\'m\\')])\\n        >>> df_multi_level_cols2 = pd.DataFrame([[1.0, 2.0], [3.0, 4.0]],\\n        ...                                     index=[\\'cat\\', \\'dog\\'],\\n        ...                                     columns=multicol2)\\n\\n        It is common to have missing values when stacking a dataframe\\n        with multi-level columns, as the stacked dataframe typically\\n        has more values than the original dataframe. Missing values\\n        are filled with NaNs:\\n\\n        >>> df_multi_level_cols2\\n            weight height\\n                kg      m\\n        cat    1.0    2.0\\n        dog    3.0    4.0\\n        >>> df_multi_level_cols2.stack()\\n                height  weight\\n        cat kg     NaN     1.0\\n            m      2.0     NaN\\n        dog kg     NaN     3.0\\n            m      4.0     NaN\\n\\n        **Prescribing the level(s) to be stacked**\\n\\n        The first parameter controls which level or levels are stacked:\\n\\n        >>> df_multi_level_cols2.stack(0)\\n                     kg    m\\n        cat height  NaN  2.0\\n            weight  1.0  NaN\\n        dog height  NaN  4.0\\n            weight  3.0  NaN\\n        >>> df_multi_level_cols2.stack([0, 1])\\n        cat  height  m     2.0\\n             weight  kg    1.0\\n        dog  height  m     4.0\\n             weight  kg    3.0\\n        dtype: float64\\n\\n        **Dropping missing values**\\n\\n        >>> df_multi_level_cols3 = pd.DataFrame([[None, 1.0], [2.0, 3.0]],\\n        ...                                     index=[\\'cat\\', \\'dog\\'],\\n        ...                                     columns=multicol2)\\n\\n        Note that rows where all values are missing are dropped by\\n        default but this behaviour can be controlled via the dropna\\n        keyword parameter:\\n\\n        >>> df_multi_level_cols3\\n            weight height\\n                kg      m\\n        cat    NaN    1.0\\n        dog    2.0    3.0\\n        >>> df_multi_level_cols3.stack(dropna=False)\\n                height  weight\\n        cat kg     NaN     NaN\\n            m      1.0     NaN\\n        dog kg     NaN     2.0\\n            m      3.0     NaN\\n        >>> df_multi_level_cols3.stack(dropna=True)\\n                height  weight\\n        cat m      1.0     NaN\\n        dog kg     NaN     2.0\\n            m      3.0     NaN\\n        \"\"\"\\n        from pandas.core.reshape.reshape import stack, stack_multiple\\n\\n        if isinstance(level, (tuple, list)):\\n            return stack_multiple(self, level, dropna=dropna)\\n        else:\\n            return stack(self, level, dropna=dropna)',\n 'def unstack(self, level=-1, fill_value=None):\\n        \"\"\"\\n        Pivot a level of the (necessarily hierarchical) index labels, returning\\n        a DataFrame having a new level of column labels whose inner-most level\\n        consists of the pivoted index labels.\\n\\n        If the index is not a MultiIndex, the output will be a Series\\n        (the analogue of stack when the columns are not a MultiIndex).\\n\\n        The level involved will automatically get sorted.\\n\\n        Parameters\\n        ----------\\n        level : int, string, or list of these, default -1 (last level)\\n            Level(s) of index to unstack, can pass level name\\n        fill_value : replace NaN with this value if the unstack produces\\n            missing values\\n\\n            .. versionadded:: 0.18.0\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n\\n        See Also\\n        --------\\n        DataFrame.pivot : Pivot a table based on column values.\\n        DataFrame.stack : Pivot a level of the column labels (inverse operation\\n            from `unstack`).\\n\\n        Examples\\n        --------\\n        >>> index = pd.MultiIndex.from_tuples([(\\'one\\', \\'a\\'), (\\'one\\', \\'b\\'),\\n        ...                                    (\\'two\\', \\'a\\'), (\\'two\\', \\'b\\')])\\n        >>> s = pd.Series(np.arange(1.0, 5.0), index=index)\\n        >>> s\\n        one  a   1.0\\n             b   2.0\\n        two  a   3.0\\n             b   4.0\\n        dtype: float64\\n\\n        >>> s.unstack(level=-1)\\n             a   b\\n        one  1.0  2.0\\n        two  3.0  4.0\\n\\n        >>> s.unstack(level=0)\\n           one  two\\n        a  1.0   3.0\\n        b  2.0   4.0\\n\\n        >>> df = s.unstack(level=0)\\n        >>> df.unstack()\\n        one  a  1.0\\n             b  2.0\\n        two  a  3.0\\n             b  4.0\\n        dtype: float64\\n        \"\"\"\\n        from pandas.core.reshape.reshape import unstack\\n        return unstack(self, level, fill_value)',\n 'def diff(self, periods=1, axis=0):\\n        \"\"\"\\n        First discrete difference of element.\\n\\n        Calculates the difference of a DataFrame element compared with another\\n        element in the DataFrame (default is the element in the same column\\n        of the previous row).\\n\\n        Parameters\\n        ----------\\n        periods : int, default 1\\n            Periods to shift for calculating difference, accepts negative\\n            values.\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n            Take difference over rows (0) or columns (1).\\n\\n            .. versionadded:: 0.16.1.\\n\\n        Returns\\n        -------\\n        DataFrame\\n\\n        See Also\\n        --------\\n        Series.diff: First discrete difference for a Series.\\n        DataFrame.pct_change: Percent change over given number of periods.\\n        DataFrame.shift: Shift index by desired number of periods with an\\n            optional time freq.\\n\\n        Examples\\n        --------\\n        Difference with previous row\\n\\n        >>> df = pd.DataFrame({\\'a\\': [1, 2, 3, 4, 5, 6],\\n        ...                    \\'b\\': [1, 1, 2, 3, 5, 8],\\n        ...                    \\'c\\': [1, 4, 9, 16, 25, 36]})\\n        >>> df\\n           a  b   c\\n        0  1  1   1\\n        1  2  1   4\\n        2  3  2   9\\n        3  4  3  16\\n        4  5  5  25\\n        5  6  8  36\\n\\n        >>> df.diff()\\n             a    b     c\\n        0  NaN  NaN   NaN\\n        1  1.0  0.0   3.0\\n        2  1.0  1.0   5.0\\n        3  1.0  1.0   7.0\\n        4  1.0  2.0   9.0\\n        5  1.0  3.0  11.0\\n\\n        Difference with previous column\\n\\n        >>> df.diff(axis=1)\\n            a    b     c\\n        0 NaN  0.0   0.0\\n        1 NaN -1.0   3.0\\n        2 NaN -1.0   7.0\\n        3 NaN -1.0  13.0\\n        4 NaN  0.0  20.0\\n        5 NaN  2.0  28.0\\n\\n        Difference with 3rd previous row\\n\\n        >>> df.diff(periods=3)\\n             a    b     c\\n        0  NaN  NaN   NaN\\n        1  NaN  NaN   NaN\\n        2  NaN  NaN   NaN\\n        3  3.0  2.0  15.0\\n        4  3.0  4.0  21.0\\n        5  3.0  6.0  27.0\\n\\n        Difference with following row\\n\\n        >>> df.diff(periods=-1)\\n             a    b     c\\n        0 -1.0  0.0  -3.0\\n        1 -1.0 -1.0  -5.0\\n        2 -1.0 -1.0  -7.0\\n        3 -1.0 -2.0  -9.0\\n        4 -1.0 -3.0 -11.0\\n        5  NaN  NaN   NaN\\n        \"\"\"\\n        bm_axis = self._get_block_manager_axis(axis)\\n        new_data = self._data.diff(n=periods, axis=bm_axis)\\n        return self._constructor(new_data)',\n 'def _gotitem(self,\\n                 key: Union[str, List[str]],\\n                 ndim: int,\\n                 subset: Optional[Union[Series, ABCDataFrame]] = None,\\n                 ) -> Union[Series, ABCDataFrame]:\\n        \"\"\"\\n        Sub-classes to define. Return a sliced object.\\n\\n        Parameters\\n        ----------\\n        key : string / list of selections\\n        ndim : 1,2\\n            requested ndim of result\\n        subset : object, default None\\n            subset to act on\\n        \"\"\"\\n        if subset is None:\\n            subset = self\\n        elif subset.ndim == 1:  # is Series\\n            return subset\\n\\n        # TODO: _shallow_copy(subset)?\\n        return subset[key]',\n 'def apply(self, func, axis=0, broadcast=None, raw=False, reduce=None,\\n              result_type=None, args=(), **kwds):\\n        \"\"\"\\n        Apply a function along an axis of the DataFrame.\\n\\n        Objects passed to the function are Series objects whose index is\\n        either the DataFrame\\'s index (``axis=0``) or the DataFrame\\'s columns\\n        (``axis=1``). By default (``result_type=None``), the final return type\\n        is inferred from the return type of the applied function. Otherwise,\\n        it depends on the `result_type` argument.\\n\\n        Parameters\\n        ----------\\n        func : function\\n            Function to apply to each column or row.\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n            Axis along which the function is applied:\\n\\n            * 0 or \\'index\\': apply function to each column.\\n            * 1 or \\'columns\\': apply function to each row.\\n        broadcast : bool, optional\\n            Only relevant for aggregation functions:\\n\\n            * ``False`` or ``None`` : returns a Series whose length is the\\n              length of the index or the number of columns (based on the\\n              `axis` parameter)\\n            * ``True`` : results will be broadcast to the original shape\\n              of the frame, the original index and columns will be retained.\\n\\n            .. deprecated:: 0.23.0\\n               This argument will be removed in a future version, replaced\\n               by result_type=\\'broadcast\\'.\\n\\n        raw : bool, default False\\n            * ``False`` : passes each row or column as a Series to the\\n              function.\\n            * ``True`` : the passed function will receive ndarray objects\\n              instead.\\n              If you are just applying a NumPy reduction function this will\\n              achieve much better performance.\\n        reduce : bool or None, default None\\n            Try to apply reduction procedures. If the DataFrame is empty,\\n            `apply` will use `reduce` to determine whether the result\\n            should be a Series or a DataFrame. If ``reduce=None`` (the\\n            default), `apply`\\'s return value will be guessed by calling\\n            `func` on an empty Series\\n            (note: while guessing, exceptions raised by `func` will be\\n            ignored).\\n            If ``reduce=True`` a Series will always be returned, and if\\n            ``reduce=False`` a DataFrame will always be returned.\\n\\n            .. deprecated:: 0.23.0\\n               This argument will be removed in a future version, replaced\\n               by ``result_type=\\'reduce\\'``.\\n\\n        result_type : {\\'expand\\', \\'reduce\\', \\'broadcast\\', None}, default None\\n            These only act when ``axis=1`` (columns):\\n\\n            * \\'expand\\' : list-like results will be turned into columns.\\n            * \\'reduce\\' : returns a Series if possible rather than expanding\\n              list-like results. This is the opposite of \\'expand\\'.\\n            * \\'broadcast\\' : results will be broadcast to the original shape\\n              of the DataFrame, the original index and columns will be\\n              retained.\\n\\n            The default behaviour (None) depends on the return value of the\\n            applied function: list-like results will be returned as a Series\\n            of those. However if the apply function returns a Series these\\n            are expanded to columns.\\n\\n            .. versionadded:: 0.23.0\\n\\n        args : tuple\\n            Positional arguments to pass to `func` in addition to the\\n            array/series.\\n        **kwds\\n            Additional keyword arguments to pass as keywords arguments to\\n            `func`.\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            Result of applying ``func`` along the given axis of the\\n            DataFrame.\\n\\n        See Also\\n        --------\\n        DataFrame.applymap: For elementwise operations.\\n        DataFrame.aggregate: Only perform aggregating type operations.\\n        DataFrame.transform: Only perform transforming type operations.\\n\\n        Notes\\n        -----\\n        In the current implementation apply calls `func` twice on the\\n        first column/row to decide whether it can take a fast or slow\\n        code path. This can lead to unexpected behavior if `func` has\\n        side-effects, as they will take effect twice for the first\\n        column/row.\\n\\n        Examples\\n        --------\\n\\n        >>> df = pd.DataFrame([[4, 9]] * 3, columns=[\\'A\\', \\'B\\'])\\n        >>> df\\n           A  B\\n        0  4  9\\n        1  4  9\\n        2  4  9\\n\\n        Using a numpy universal function (in this case the same as\\n        ``np.sqrt(df)``):\\n\\n        >>> df.apply(np.sqrt)\\n             A    B\\n        0  2.0  3.0\\n        1  2.0  3.0\\n        2  2.0  3.0\\n\\n        Using a reducing function on either axis\\n\\n        >>> df.apply(np.sum, axis=0)\\n        A    12\\n        B    27\\n        dtype: int64\\n\\n        >>> df.apply(np.sum, axis=1)\\n        0    13\\n        1    13\\n        2    13\\n        dtype: int64\\n\\n        Retuning a list-like will result in a Series\\n\\n        >>> df.apply(lambda x: [1, 2], axis=1)\\n        0    [1, 2]\\n        1    [1, 2]\\n        2    [1, 2]\\n        dtype: object\\n\\n        Passing result_type=\\'expand\\' will expand list-like results\\n        to columns of a Dataframe\\n\\n        >>> df.apply(lambda x: [1, 2], axis=1, result_type=\\'expand\\')\\n           0  1\\n        0  1  2\\n        1  1  2\\n        2  1  2\\n\\n        Returning a Series inside the function is similar to passing\\n        ``result_type=\\'expand\\'``. The resulting column names\\n        will be the Series index.\\n\\n        >>> df.apply(lambda x: pd.Series([1, 2], index=[\\'foo\\', \\'bar\\']), axis=1)\\n           foo  bar\\n        0    1    2\\n        1    1    2\\n        2    1    2\\n\\n        Passing ``result_type=\\'broadcast\\'`` will ensure the same shape\\n        result, whether list-like or scalar is returned by the function,\\n        and broadcast it along the axis. The resulting column names will\\n        be the originals.\\n\\n        >>> df.apply(lambda x: [1, 2], axis=1, result_type=\\'broadcast\\')\\n           A  B\\n        0  1  2\\n        1  1  2\\n        2  1  2\\n        \"\"\"\\n        from pandas.core.apply import frame_apply\\n        op = frame_apply(self,\\n                         func=func,\\n                         axis=axis,\\n                         broadcast=broadcast,\\n                         raw=raw,\\n                         reduce=reduce,\\n                         result_type=result_type,\\n                         args=args,\\n                         kwds=kwds)\\n        return op.get_result()',\n 'def applymap(self, func):\\n        \"\"\"\\n        Apply a function to a Dataframe elementwise.\\n\\n        This method applies a function that accepts and returns a scalar\\n        to every element of a DataFrame.\\n\\n        Parameters\\n        ----------\\n        func : callable\\n            Python function, returns a single value from a single value.\\n\\n        Returns\\n        -------\\n        DataFrame\\n            Transformed DataFrame.\\n\\n        See Also\\n        --------\\n        DataFrame.apply : Apply a function along input axis of DataFrame.\\n\\n        Notes\\n        -----\\n        In the current implementation applymap calls `func` twice on the\\n        first column/row to decide whether it can take a fast or slow\\n        code path. This can lead to unexpected behavior if `func` has\\n        side-effects, as they will take effect twice for the first\\n        column/row.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame([[1, 2.12], [3.356, 4.567]])\\n        >>> df\\n               0      1\\n        0  1.000  2.120\\n        1  3.356  4.567\\n\\n        >>> df.applymap(lambda x: len(str(x)))\\n           0  1\\n        0  3  4\\n        1  5  5\\n\\n        Note that a vectorized version of `func` often exists, which will\\n        be much faster. You could square each number elementwise.\\n\\n        >>> df.applymap(lambda x: x**2)\\n                   0          1\\n        0   1.000000   4.494400\\n        1  11.262736  20.857489\\n\\n        But it\\'s better to avoid applymap in that case.\\n\\n        >>> df ** 2\\n                   0          1\\n        0   1.000000   4.494400\\n        1  11.262736  20.857489\\n        \"\"\"\\n\\n        # if we have a dtype == \\'M8[ns]\\', provide boxed values\\n        def infer(x):\\n            if x.empty:\\n                return lib.map_infer(x, func)\\n            return lib.map_infer(x.astype(object).values, func)\\n\\n        return self.apply(infer)',\n 'def append(self, other, ignore_index=False,\\n               verify_integrity=False, sort=None):\\n        \"\"\"\\n        Append rows of `other` to the end of caller, returning a new object.\\n\\n        Columns in `other` that are not in the caller are added as new columns.\\n\\n        Parameters\\n        ----------\\n        other : DataFrame or Series/dict-like object, or list of these\\n            The data to append.\\n        ignore_index : boolean, default False\\n            If True, do not use the index labels.\\n        verify_integrity : boolean, default False\\n            If True, raise ValueError on creating index with duplicates.\\n        sort : boolean, default None\\n            Sort columns if the columns of `self` and `other` are not aligned.\\n            The default sorting is deprecated and will change to not-sorting\\n            in a future version of pandas. Explicitly pass ``sort=True`` to\\n            silence the warning and sort. Explicitly pass ``sort=False`` to\\n            silence the warning and not sort.\\n\\n            .. versionadded:: 0.23.0\\n\\n        Returns\\n        -------\\n        DataFrame\\n\\n        See Also\\n        --------\\n        concat : General function to concatenate DataFrame, Series\\n            or Panel objects.\\n\\n        Notes\\n        -----\\n        If a list of dict/series is passed and the keys are all contained in\\n        the DataFrame\\'s index, the order of the columns in the resulting\\n        DataFrame will be unchanged.\\n\\n        Iteratively appending rows to a DataFrame can be more computationally\\n        intensive than a single concatenate. A better solution is to append\\n        those rows to a list and then concatenate the list with the original\\n        DataFrame all at once.\\n\\n        Examples\\n        --------\\n\\n        >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=list(\\'AB\\'))\\n        >>> df\\n           A  B\\n        0  1  2\\n        1  3  4\\n        >>> df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list(\\'AB\\'))\\n        >>> df.append(df2)\\n           A  B\\n        0  1  2\\n        1  3  4\\n        0  5  6\\n        1  7  8\\n\\n        With `ignore_index` set to True:\\n\\n        >>> df.append(df2, ignore_index=True)\\n           A  B\\n        0  1  2\\n        1  3  4\\n        2  5  6\\n        3  7  8\\n\\n        The following, while not recommended methods for generating DataFrames,\\n        show two ways to generate a DataFrame from multiple data sources.\\n\\n        Less efficient:\\n\\n        >>> df = pd.DataFrame(columns=[\\'A\\'])\\n        >>> for i in range(5):\\n        ...     df = df.append({\\'A\\': i}, ignore_index=True)\\n        >>> df\\n           A\\n        0  0\\n        1  1\\n        2  2\\n        3  3\\n        4  4\\n\\n        More efficient:\\n\\n        >>> pd.concat([pd.DataFrame([i], columns=[\\'A\\']) for i in range(5)],\\n        ...           ignore_index=True)\\n           A\\n        0  0\\n        1  1\\n        2  2\\n        3  3\\n        4  4\\n        \"\"\"\\n        if isinstance(other, (Series, dict)):\\n            if isinstance(other, dict):\\n                other = Series(other)\\n            if other.name is None and not ignore_index:\\n                raise TypeError(\\'Can only append a Series if ignore_index=True\\'\\n                                \\' or if the Series has a name\\')\\n\\n            if other.name is None:\\n                index = None\\n            else:\\n                # other must have the same index name as self, otherwise\\n                # index name will be reset\\n                index = Index([other.name], name=self.index.name)\\n\\n            idx_diff = other.index.difference(self.columns)\\n            try:\\n                combined_columns = self.columns.append(idx_diff)\\n            except TypeError:\\n                combined_columns = self.columns.astype(object).append(idx_diff)\\n            other = other.reindex(combined_columns, copy=False)\\n            other = DataFrame(other.values.reshape((1, len(other))),\\n                              index=index,\\n                              columns=combined_columns)\\n            other = other._convert(datetime=True, timedelta=True)\\n            if not self.columns.equals(combined_columns):\\n                self = self.reindex(columns=combined_columns)\\n        elif isinstance(other, list) and not isinstance(other[0], DataFrame):\\n            other = DataFrame(other)\\n            if (self.columns.get_indexer(other.columns) >= 0).all():\\n                other = other.reindex(columns=self.columns)\\n\\n        from pandas.core.reshape.concat import concat\\n        if isinstance(other, (list, tuple)):\\n            to_concat = [self] + other\\n        else:\\n            to_concat = [self, other]\\n        return concat(to_concat, ignore_index=ignore_index,\\n                      verify_integrity=verify_integrity,\\n                      sort=sort)',\n 'def join(self, other, on=None, how=\\'left\\', lsuffix=\\'\\', rsuffix=\\'\\',\\n             sort=False):\\n        \"\"\"\\n        Join columns of another DataFrame.\\n\\n        Join columns with `other` DataFrame either on index or on a key\\n        column. Efficiently join multiple DataFrame objects by index at once by\\n        passing a list.\\n\\n        Parameters\\n        ----------\\n        other : DataFrame, Series, or list of DataFrame\\n            Index should be similar to one of the columns in this one. If a\\n            Series is passed, its name attribute must be set, and that will be\\n            used as the column name in the resulting joined DataFrame.\\n        on : str, list of str, or array-like, optional\\n            Column or index level name(s) in the caller to join on the index\\n            in `other`, otherwise joins index-on-index. If multiple\\n            values given, the `other` DataFrame must have a MultiIndex. Can\\n            pass an array as the join key if it is not already contained in\\n            the calling DataFrame. Like an Excel VLOOKUP operation.\\n        how : {\\'left\\', \\'right\\', \\'outer\\', \\'inner\\'}, default \\'left\\'\\n            How to handle the operation of the two objects.\\n\\n            * left: use calling frame\\'s index (or column if on is specified)\\n            * right: use `other`\\'s index.\\n            * outer: form union of calling frame\\'s index (or column if on is\\n              specified) with `other`\\'s index, and sort it.\\n              lexicographically.\\n            * inner: form intersection of calling frame\\'s index (or column if\\n              on is specified) with `other`\\'s index, preserving the order\\n              of the calling\\'s one.\\n        lsuffix : str, default \\'\\'\\n            Suffix to use from left frame\\'s overlapping columns.\\n        rsuffix : str, default \\'\\'\\n            Suffix to use from right frame\\'s overlapping columns.\\n        sort : bool, default False\\n            Order result DataFrame lexicographically by the join key. If False,\\n            the order of the join key depends on the join type (how keyword).\\n\\n        Returns\\n        -------\\n        DataFrame\\n            A dataframe containing columns from both the caller and `other`.\\n\\n        See Also\\n        --------\\n        DataFrame.merge : For column(s)-on-columns(s) operations.\\n\\n        Notes\\n        -----\\n        Parameters `on`, `lsuffix`, and `rsuffix` are not supported when\\n        passing a list of `DataFrame` objects.\\n\\n        Support for specifying index levels as the `on` parameter was added\\n        in version 0.23.0.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'key\\': [\\'K0\\', \\'K1\\', \\'K2\\', \\'K3\\', \\'K4\\', \\'K5\\'],\\n        ...                    \\'A\\': [\\'A0\\', \\'A1\\', \\'A2\\', \\'A3\\', \\'A4\\', \\'A5\\']})\\n\\n        >>> df\\n          key   A\\n        0  K0  A0\\n        1  K1  A1\\n        2  K2  A2\\n        3  K3  A3\\n        4  K4  A4\\n        5  K5  A5\\n\\n        >>> other = pd.DataFrame({\\'key\\': [\\'K0\\', \\'K1\\', \\'K2\\'],\\n        ...                       \\'B\\': [\\'B0\\', \\'B1\\', \\'B2\\']})\\n\\n        >>> other\\n          key   B\\n        0  K0  B0\\n        1  K1  B1\\n        2  K2  B2\\n\\n        Join DataFrames using their indexes.\\n\\n        >>> df.join(other, lsuffix=\\'_caller\\', rsuffix=\\'_other\\')\\n          key_caller   A key_other    B\\n        0         K0  A0        K0   B0\\n        1         K1  A1        K1   B1\\n        2         K2  A2        K2   B2\\n        3         K3  A3       NaN  NaN\\n        4         K4  A4       NaN  NaN\\n        5         K5  A5       NaN  NaN\\n\\n        If we want to join using the key columns, we need to set key to be\\n        the index in both `df` and `other`. The joined DataFrame will have\\n        key as its index.\\n\\n        >>> df.set_index(\\'key\\').join(other.set_index(\\'key\\'))\\n              A    B\\n        key\\n        K0   A0   B0\\n        K1   A1   B1\\n        K2   A2   B2\\n        K3   A3  NaN\\n        K4   A4  NaN\\n        K5   A5  NaN\\n\\n        Another option to join using the key columns is to use the `on`\\n        parameter. DataFrame.join always uses `other`\\'s index but we can use\\n        any column in `df`. This method preserves the original DataFrame\\'s\\n        index in the result.\\n\\n        >>> df.join(other.set_index(\\'key\\'), on=\\'key\\')\\n          key   A    B\\n        0  K0  A0   B0\\n        1  K1  A1   B1\\n        2  K2  A2   B2\\n        3  K3  A3  NaN\\n        4  K4  A4  NaN\\n        5  K5  A5  NaN\\n        \"\"\"\\n        # For SparseDataFrame\\'s benefit\\n        return self._join_compat(other, on=on, how=how, lsuffix=lsuffix,\\n                                 rsuffix=rsuffix, sort=sort)',\n 'def round(self, decimals=0, *args, **kwargs):\\n        \"\"\"\\n        Round a DataFrame to a variable number of decimal places.\\n\\n        Parameters\\n        ----------\\n        decimals : int, dict, Series\\n            Number of decimal places to round each column to. If an int is\\n            given, round each column to the same number of places.\\n            Otherwise dict and Series round to variable numbers of places.\\n            Column names should be in the keys if `decimals` is a\\n            dict-like, or in the index if `decimals` is a Series. Any\\n            columns not included in `decimals` will be left as is. Elements\\n            of `decimals` which are not columns of the input will be\\n            ignored.\\n        *args\\n            Additional keywords have no effect but might be accepted for\\n            compatibility with numpy.\\n        **kwargs\\n            Additional keywords have no effect but might be accepted for\\n            compatibility with numpy.\\n\\n        Returns\\n        -------\\n        DataFrame\\n            A DataFrame with the affected columns rounded to the specified\\n            number of decimal places.\\n\\n        See Also\\n        --------\\n        numpy.around : Round a numpy array to the given number of decimals.\\n        Series.round : Round a Series to the given number of decimals.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame([(.21, .32), (.01, .67), (.66, .03), (.21, .18)],\\n        ...                   columns=[\\'dogs\\', \\'cats\\'])\\n        >>> df\\n            dogs  cats\\n        0  0.21  0.32\\n        1  0.01  0.67\\n        2  0.66  0.03\\n        3  0.21  0.18\\n\\n        By providing an integer each column is rounded to the same number\\n        of decimal places\\n\\n        >>> df.round(1)\\n            dogs  cats\\n        0   0.2   0.3\\n        1   0.0   0.7\\n        2   0.7   0.0\\n        3   0.2   0.2\\n\\n        With a dict, the number of places for specific columns can be\\n        specfified with the column names as key and the number of decimal\\n        places as value\\n\\n        >>> df.round({\\'dogs\\': 1, \\'cats\\': 0})\\n            dogs  cats\\n        0   0.2   0.0\\n        1   0.0   1.0\\n        2   0.7   0.0\\n        3   0.2   0.0\\n\\n        Using a Series, the number of places for specific columns can be\\n        specfified with the column names as index and the number of\\n        decimal places as value\\n\\n        >>> decimals = pd.Series([0, 1], index=[\\'cats\\', \\'dogs\\'])\\n        >>> df.round(decimals)\\n            dogs  cats\\n        0   0.2   0.0\\n        1   0.0   1.0\\n        2   0.7   0.0\\n        3   0.2   0.0\\n        \"\"\"\\n        from pandas.core.reshape.concat import concat\\n\\n        def _dict_round(df, decimals):\\n            for col, vals in df.iteritems():\\n                try:\\n                    yield _series_round(vals, decimals[col])\\n                except KeyError:\\n                    yield vals\\n\\n        def _series_round(s, decimals):\\n            if is_integer_dtype(s) or is_float_dtype(s):\\n                return s.round(decimals)\\n            return s\\n\\n        nv.validate_round(args, kwargs)\\n\\n        if isinstance(decimals, (dict, Series)):\\n            if isinstance(decimals, Series):\\n                if not decimals.index.is_unique:\\n                    raise ValueError(\"Index of decimals must be unique\")\\n            new_cols = [col for col in _dict_round(self, decimals)]\\n        elif is_integer(decimals):\\n            # Dispatch to Series.round\\n            new_cols = [_series_round(v, decimals)\\n                        for _, v in self.iteritems()]\\n        else:\\n            raise TypeError(\"decimals must be an integer, a dict-like or a \"\\n                            \"Series\")\\n\\n        if len(new_cols) > 0:\\n            return self._constructor(concat(new_cols, axis=1),\\n                                     index=self.index,\\n                                     columns=self.columns)\\n        else:\\n            return self',\n 'def corr(self, method=\\'pearson\\', min_periods=1):\\n        \"\"\"\\n        Compute pairwise correlation of columns, excluding NA/null values.\\n\\n        Parameters\\n        ----------\\n        method : {\\'pearson\\', \\'kendall\\', \\'spearman\\'} or callable\\n            * pearson : standard correlation coefficient\\n            * kendall : Kendall Tau correlation coefficient\\n            * spearman : Spearman rank correlation\\n            * callable: callable with input two 1d ndarrays\\n                and returning a float. Note that the returned matrix from corr\\n                will have 1 along the diagonals and will be symmetric\\n                regardless of the callable\\'s behavior\\n                .. versionadded:: 0.24.0\\n\\n        min_periods : int, optional\\n            Minimum number of observations required per pair of columns\\n            to have a valid result. Currently only available for Pearson\\n            and Spearman correlation.\\n\\n        Returns\\n        -------\\n        DataFrame\\n            Correlation matrix.\\n\\n        See Also\\n        --------\\n        DataFrame.corrwith\\n        Series.corr\\n\\n        Examples\\n        --------\\n        >>> def histogram_intersection(a, b):\\n        ...     v = np.minimum(a, b).sum().round(decimals=1)\\n        ...     return v\\n        >>> df = pd.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)],\\n        ...                   columns=[\\'dogs\\', \\'cats\\'])\\n        >>> df.corr(method=histogram_intersection)\\n              dogs  cats\\n        dogs   1.0   0.3\\n        cats   0.3   1.0\\n        \"\"\"\\n        numeric_df = self._get_numeric_data()\\n        cols = numeric_df.columns\\n        idx = cols.copy()\\n        mat = numeric_df.values\\n\\n        if method == \\'pearson\\':\\n            correl = libalgos.nancorr(ensure_float64(mat), minp=min_periods)\\n        elif method == \\'spearman\\':\\n            correl = libalgos.nancorr_spearman(ensure_float64(mat),\\n                                               minp=min_periods)\\n        elif method == \\'kendall\\' or callable(method):\\n            if min_periods is None:\\n                min_periods = 1\\n            mat = ensure_float64(mat).T\\n            corrf = nanops.get_corr_func(method)\\n            K = len(cols)\\n            correl = np.empty((K, K), dtype=float)\\n            mask = np.isfinite(mat)\\n            for i, ac in enumerate(mat):\\n                for j, bc in enumerate(mat):\\n                    if i > j:\\n                        continue\\n\\n                    valid = mask[i] & mask[j]\\n                    if valid.sum() < min_periods:\\n                        c = np.nan\\n                    elif i == j:\\n                        c = 1.\\n                    elif not valid.all():\\n                        c = corrf(ac[valid], bc[valid])\\n                    else:\\n                        c = corrf(ac, bc)\\n                    correl[i, j] = c\\n                    correl[j, i] = c\\n        else:\\n            raise ValueError(\"method must be either \\'pearson\\', \"\\n                             \"\\'spearman\\', \\'kendall\\', or a callable, \"\\n                             \"\\'{method}\\' was supplied\".format(method=method))\\n\\n        return self._constructor(correl, index=idx, columns=cols)',\n 'def cov(self, min_periods=None):\\n        \"\"\"\\n        Compute pairwise covariance of columns, excluding NA/null values.\\n\\n        Compute the pairwise covariance among the series of a DataFrame.\\n        The returned data frame is the `covariance matrix\\n        <https://en.wikipedia.org/wiki/Covariance_matrix>`__ of the columns\\n        of the DataFrame.\\n\\n        Both NA and null values are automatically excluded from the\\n        calculation. (See the note below about bias from missing values.)\\n        A threshold can be set for the minimum number of\\n        observations for each value created. Comparisons with observations\\n        below this threshold will be returned as ``NaN``.\\n\\n        This method is generally used for the analysis of time series data to\\n        understand the relationship between different measures\\n        across time.\\n\\n        Parameters\\n        ----------\\n        min_periods : int, optional\\n            Minimum number of observations required per pair of columns\\n            to have a valid result.\\n\\n        Returns\\n        -------\\n        DataFrame\\n            The covariance matrix of the series of the DataFrame.\\n\\n        See Also\\n        --------\\n        Series.cov : Compute covariance with another Series.\\n        core.window.EWM.cov: Exponential weighted sample covariance.\\n        core.window.Expanding.cov : Expanding sample covariance.\\n        core.window.Rolling.cov : Rolling sample covariance.\\n\\n        Notes\\n        -----\\n        Returns the covariance matrix of the DataFrame\\'s time series.\\n        The covariance is normalized by N-1.\\n\\n        For DataFrames that have Series that are missing data (assuming that\\n        data is `missing at random\\n        <https://en.wikipedia.org/wiki/Missing_data#Missing_at_random>`__)\\n        the returned covariance matrix will be an unbiased estimate\\n        of the variance and covariance between the member Series.\\n\\n        However, for many applications this estimate may not be acceptable\\n        because the estimate covariance matrix is not guaranteed to be positive\\n        semi-definite. This could lead to estimate correlations having\\n        absolute values which are greater than one, and/or a non-invertible\\n        covariance matrix. See `Estimation of covariance matrices\\n        <http://en.wikipedia.org/w/index.php?title=Estimation_of_covariance_\\n        matrices>`__ for more details.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame([(1, 2), (0, 3), (2, 0), (1, 1)],\\n        ...                   columns=[\\'dogs\\', \\'cats\\'])\\n        >>> df.cov()\\n                  dogs      cats\\n        dogs  0.666667 -1.000000\\n        cats -1.000000  1.666667\\n\\n        >>> np.random.seed(42)\\n        >>> df = pd.DataFrame(np.random.randn(1000, 5),\\n        ...                   columns=[\\'a\\', \\'b\\', \\'c\\', \\'d\\', \\'e\\'])\\n        >>> df.cov()\\n                  a         b         c         d         e\\n        a  0.998438 -0.020161  0.059277 -0.008943  0.014144\\n        b -0.020161  1.059352 -0.008543 -0.024738  0.009826\\n        c  0.059277 -0.008543  1.010670 -0.001486 -0.000271\\n        d -0.008943 -0.024738 -0.001486  0.921297 -0.013692\\n        e  0.014144  0.009826 -0.000271 -0.013692  0.977795\\n\\n        **Minimum number of periods**\\n\\n        This method also supports an optional ``min_periods`` keyword\\n        that specifies the required minimum number of non-NA observations for\\n        each column pair in order to have a valid result:\\n\\n        >>> np.random.seed(42)\\n        >>> df = pd.DataFrame(np.random.randn(20, 3),\\n        ...                   columns=[\\'a\\', \\'b\\', \\'c\\'])\\n        >>> df.loc[df.index[:5], \\'a\\'] = np.nan\\n        >>> df.loc[df.index[5:10], \\'b\\'] = np.nan\\n        >>> df.cov(min_periods=12)\\n                  a         b         c\\n        a  0.316741       NaN -0.150812\\n        b       NaN  1.248003  0.191417\\n        c -0.150812  0.191417  0.895202\\n        \"\"\"\\n        numeric_df = self._get_numeric_data()\\n        cols = numeric_df.columns\\n        idx = cols.copy()\\n        mat = numeric_df.values\\n\\n        if notna(mat).all():\\n            if min_periods is not None and min_periods > len(mat):\\n                baseCov = np.empty((mat.shape[1], mat.shape[1]))\\n                baseCov.fill(np.nan)\\n            else:\\n                baseCov = np.cov(mat.T)\\n            baseCov = baseCov.reshape((len(cols), len(cols)))\\n        else:\\n            baseCov = libalgos.nancorr(ensure_float64(mat), cov=True,\\n                                       minp=min_periods)\\n\\n        return self._constructor(baseCov, index=idx, columns=cols)',\n 'def corrwith(self, other, axis=0, drop=False, method=\\'pearson\\'):\\n        \"\"\"\\n        Compute pairwise correlation between rows or columns of DataFrame\\n        with rows or columns of Series or DataFrame.  DataFrames are first\\n        aligned along both axes before computing the correlations.\\n\\n        Parameters\\n        ----------\\n        other : DataFrame, Series\\n            Object with which to compute correlations.\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n            0 or \\'index\\' to compute column-wise, 1 or \\'columns\\' for row-wise.\\n        drop : bool, default False\\n            Drop missing indices from result.\\n        method : {\\'pearson\\', \\'kendall\\', \\'spearman\\'} or callable\\n            * pearson : standard correlation coefficient\\n            * kendall : Kendall Tau correlation coefficient\\n            * spearman : Spearman rank correlation\\n            * callable: callable with input two 1d ndarrays\\n                and returning a float\\n\\n            .. versionadded:: 0.24.0\\n\\n        Returns\\n        -------\\n        Series\\n            Pairwise correlations.\\n\\n        See Also\\n        -------\\n        DataFrame.corr\\n        \"\"\"\\n        axis = self._get_axis_number(axis)\\n        this = self._get_numeric_data()\\n\\n        if isinstance(other, Series):\\n            return this.apply(lambda x: other.corr(x, method=method),\\n                              axis=axis)\\n\\n        other = other._get_numeric_data()\\n        left, right = this.align(other, join=\\'inner\\', copy=False)\\n\\n        if axis == 1:\\n            left = left.T\\n            right = right.T\\n\\n        if method == \\'pearson\\':\\n            # mask missing values\\n            left = left + right * 0\\n            right = right + left * 0\\n\\n            # demeaned data\\n            ldem = left - left.mean()\\n            rdem = right - right.mean()\\n\\n            num = (ldem * rdem).sum()\\n            dom = (left.count() - 1) * left.std() * right.std()\\n\\n            correl = num / dom\\n\\n        elif method in [\\'kendall\\', \\'spearman\\'] or callable(method):\\n            def c(x):\\n                return nanops.nancorr(x[0], x[1], method=method)\\n\\n            correl = Series(map(c,\\n                                zip(left.values.T, right.values.T)),\\n                            index=left.columns)\\n\\n        else:\\n            raise ValueError(\"Invalid method {method} was passed, \"\\n                             \"valid methods are: \\'pearson\\', \\'kendall\\', \"\\n                             \"\\'spearman\\', or callable\".\\n                             format(method=method))\\n\\n        if not drop:\\n            # Find non-matching labels along the given axis\\n            # and append missing correlations (GH 22375)\\n            raxis = 1 if axis == 0 else 0\\n            result_index = (this._get_axis(raxis).\\n                            union(other._get_axis(raxis)))\\n            idx_diff = result_index.difference(correl.index)\\n\\n            if len(idx_diff) > 0:\\n                correl = correl.append(Series([np.nan] * len(idx_diff),\\n                                              index=idx_diff))\\n\\n        return correl',\n 'def count(self, axis=0, level=None, numeric_only=False):\\n        \"\"\"\\n        Count non-NA cells for each column or row.\\n\\n        The values `None`, `NaN`, `NaT`, and optionally `numpy.inf` (depending\\n        on `pandas.options.mode.use_inf_as_na`) are considered NA.\\n\\n        Parameters\\n        ----------\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n            If 0 or \\'index\\' counts are generated for each column.\\n            If 1 or \\'columns\\' counts are generated for each **row**.\\n        level : int or str, optional\\n            If the axis is a `MultiIndex` (hierarchical), count along a\\n            particular `level`, collapsing into a `DataFrame`.\\n            A `str` specifies the level name.\\n        numeric_only : bool, default False\\n            Include only `float`, `int` or `boolean` data.\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            For each column/row the number of non-NA/null entries.\\n            If `level` is specified returns a `DataFrame`.\\n\\n        See Also\\n        --------\\n        Series.count: Number of non-NA elements in a Series.\\n        DataFrame.shape: Number of DataFrame rows and columns (including NA\\n            elements).\\n        DataFrame.isna: Boolean same-sized DataFrame showing places of NA\\n            elements.\\n\\n        Examples\\n        --------\\n        Constructing DataFrame from a dictionary:\\n\\n        >>> df = pd.DataFrame({\"Person\":\\n        ...                    [\"John\", \"Myla\", \"Lewis\", \"John\", \"Myla\"],\\n        ...                    \"Age\": [24., np.nan, 21., 33, 26],\\n        ...                    \"Single\": [False, True, True, True, False]})\\n        >>> df\\n           Person   Age  Single\\n        0    John  24.0   False\\n        1    Myla   NaN    True\\n        2   Lewis  21.0    True\\n        3    John  33.0    True\\n        4    Myla  26.0   False\\n\\n        Notice the uncounted NA values:\\n\\n        >>> df.count()\\n        Person    5\\n        Age       4\\n        Single    5\\n        dtype: int64\\n\\n        Counts for each **row**:\\n\\n        >>> df.count(axis=\\'columns\\')\\n        0    3\\n        1    2\\n        2    3\\n        3    3\\n        4    3\\n        dtype: int64\\n\\n        Counts for one level of a `MultiIndex`:\\n\\n        >>> df.set_index([\"Person\", \"Single\"]).count(level=\"Person\")\\n                Age\\n        Person\\n        John      2\\n        Lewis     1\\n        Myla      1\\n        \"\"\"\\n        axis = self._get_axis_number(axis)\\n        if level is not None:\\n            return self._count_level(level, axis=axis,\\n                                     numeric_only=numeric_only)\\n\\n        if numeric_only:\\n            frame = self._get_numeric_data()\\n        else:\\n            frame = self\\n\\n        # GH #423\\n        if len(frame._get_axis(axis)) == 0:\\n            result = Series(0, index=frame._get_agg_axis(axis))\\n        else:\\n            if frame._is_mixed_type or frame._data.any_extension_types:\\n                # the or any_extension_types is really only hit for single-\\n                # column frames with an extension array\\n                result = notna(frame).sum(axis=axis)\\n            else:\\n                # GH13407\\n                series_counts = notna(frame).sum(axis=axis)\\n                counts = series_counts.values\\n                result = Series(counts, index=frame._get_agg_axis(axis))\\n\\n        return result.astype(\\'int64\\')',\n 'def nunique(self, axis=0, dropna=True):\\n        \"\"\"\\n        Count distinct observations over requested axis.\\n\\n        Return Series with number of distinct observations. Can ignore NaN\\n        values.\\n\\n        .. versionadded:: 0.20.0\\n\\n        Parameters\\n        ----------\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n            The axis to use. 0 or \\'index\\' for row-wise, 1 or \\'columns\\' for\\n            column-wise.\\n        dropna : bool, default True\\n            Don\\'t include NaN in the counts.\\n\\n        Returns\\n        -------\\n        Series\\n\\n        See Also\\n        --------\\n        Series.nunique: Method nunique for Series.\\n        DataFrame.count: Count non-NA cells for each column or row.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'A\\': [1, 2, 3], \\'B\\': [1, 1, 1]})\\n        >>> df.nunique()\\n        A    3\\n        B    1\\n        dtype: int64\\n\\n        >>> df.nunique(axis=1)\\n        0    1\\n        1    2\\n        2    2\\n        dtype: int64\\n        \"\"\"\\n        return self.apply(Series.nunique, axis=axis, dropna=dropna)',\n 'def idxmin(self, axis=0, skipna=True):\\n        \"\"\"\\n        Return index of first occurrence of minimum over requested axis.\\n        NA/null values are excluded.\\n\\n        Parameters\\n        ----------\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n            0 or \\'index\\' for row-wise, 1 or \\'columns\\' for column-wise\\n        skipna : boolean, default True\\n            Exclude NA/null values. If an entire row/column is NA, the result\\n            will be NA.\\n\\n        Returns\\n        -------\\n        Series\\n            Indexes of minima along the specified axis.\\n\\n        Raises\\n        ------\\n        ValueError\\n            * If the row/column is empty\\n\\n        See Also\\n        --------\\n        Series.idxmin\\n\\n        Notes\\n        -----\\n        This method is the DataFrame version of ``ndarray.argmin``.\\n        \"\"\"\\n        axis = self._get_axis_number(axis)\\n        indices = nanops.nanargmin(self.values, axis=axis, skipna=skipna)\\n        index = self._get_axis(axis)\\n        result = [index[i] if i >= 0 else np.nan for i in indices]\\n        return Series(result, index=self._get_agg_axis(axis))',\n 'def _get_agg_axis(self, axis_num):\\n        \"\"\"\\n        Let\\'s be explicit about this.\\n        \"\"\"\\n        if axis_num == 0:\\n            return self.columns\\n        elif axis_num == 1:\\n            return self.index\\n        else:\\n            raise ValueError(\\'Axis must be 0 or 1 (got %r)\\' % axis_num)',\n 'def mode(self, axis=0, numeric_only=False, dropna=True):\\n        \"\"\"\\n        Get the mode(s) of each element along the selected axis.\\n\\n        The mode of a set of values is the value that appears most often.\\n        It can be multiple values.\\n\\n        Parameters\\n        ----------\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n            The axis to iterate over while searching for the mode:\\n\\n            * 0 or \\'index\\' : get mode of each column\\n            * 1 or \\'columns\\' : get mode of each row\\n        numeric_only : bool, default False\\n            If True, only apply to numeric columns.\\n        dropna : bool, default True\\n            Don\\'t consider counts of NaN/NaT.\\n\\n            .. versionadded:: 0.24.0\\n\\n        Returns\\n        -------\\n        DataFrame\\n            The modes of each column or row.\\n\\n        See Also\\n        --------\\n        Series.mode : Return the highest frequency value in a Series.\\n        Series.value_counts : Return the counts of values in a Series.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame([(\\'bird\\', 2, 2),\\n        ...                    (\\'mammal\\', 4, np.nan),\\n        ...                    (\\'arthropod\\', 8, 0),\\n        ...                    (\\'bird\\', 2, np.nan)],\\n        ...                   index=(\\'falcon\\', \\'horse\\', \\'spider\\', \\'ostrich\\'),\\n        ...                   columns=(\\'species\\', \\'legs\\', \\'wings\\'))\\n        >>> df\\n                   species  legs  wings\\n        falcon        bird     2    2.0\\n        horse       mammal     4    NaN\\n        spider   arthropod     8    0.0\\n        ostrich       bird     2    NaN\\n\\n        By default, missing values are not considered, and the mode of wings\\n        are both 0 and 2. The second row of species and legs contains ``NaN``,\\n        because they have only one mode, but the DataFrame has two rows.\\n\\n        >>> df.mode()\\n          species  legs  wings\\n        0    bird   2.0    0.0\\n        1     NaN   NaN    2.0\\n\\n        Setting ``dropna=False`` ``NaN`` values are considered and they can be\\n        the mode (like for wings).\\n\\n        >>> df.mode(dropna=False)\\n          species  legs  wings\\n        0    bird     2    NaN\\n\\n        Setting ``numeric_only=True``, only the mode of numeric columns is\\n        computed, and columns of other types are ignored.\\n\\n        >>> df.mode(numeric_only=True)\\n           legs  wings\\n        0   2.0    0.0\\n        1   NaN    2.0\\n\\n        To compute the mode over columns and not rows, use the axis parameter:\\n\\n        >>> df.mode(axis=\\'columns\\', numeric_only=True)\\n                   0    1\\n        falcon   2.0  NaN\\n        horse    4.0  NaN\\n        spider   0.0  8.0\\n        ostrich  2.0  NaN\\n        \"\"\"\\n        data = self if not numeric_only else self._get_numeric_data()\\n\\n        def f(s):\\n            return s.mode(dropna=dropna)\\n\\n        return data.apply(f, axis=axis)',\n 'def quantile(self, q=0.5, axis=0, numeric_only=True,\\n                 interpolation=\\'linear\\'):\\n        \"\"\"\\n        Return values at the given quantile over requested axis.\\n\\n        Parameters\\n        ----------\\n        q : float or array-like, default 0.5 (50% quantile)\\n            Value between 0 <= q <= 1, the quantile(s) to compute.\\n        axis : {0, 1, \\'index\\', \\'columns\\'} (default 0)\\n            Equals 0 or \\'index\\' for row-wise, 1 or \\'columns\\' for column-wise.\\n        numeric_only : bool, default True\\n            If False, the quantile of datetime and timedelta data will be\\n            computed as well.\\n        interpolation : {\\'linear\\', \\'lower\\', \\'higher\\', \\'midpoint\\', \\'nearest\\'}\\n            This optional parameter specifies the interpolation method to use,\\n            when the desired quantile lies between two data points `i` and `j`:\\n\\n            * linear: `i + (j - i) * fraction`, where `fraction` is the\\n              fractional part of the index surrounded by `i` and `j`.\\n            * lower: `i`.\\n            * higher: `j`.\\n            * nearest: `i` or `j` whichever is nearest.\\n            * midpoint: (`i` + `j`) / 2.\\n\\n            .. versionadded:: 0.18.0\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n\\n            If ``q`` is an array, a DataFrame will be returned where the\\n              index is ``q``, the columns are the columns of self, and the\\n              values are the quantiles.\\n            If ``q`` is a float, a Series will be returned where the\\n              index is the columns of self and the values are the quantiles.\\n\\n        See Also\\n        --------\\n        core.window.Rolling.quantile: Rolling quantile.\\n        numpy.percentile: Numpy function to compute the percentile.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame(np.array([[1, 1], [2, 10], [3, 100], [4, 100]]),\\n        ...                   columns=[\\'a\\', \\'b\\'])\\n        >>> df.quantile(.1)\\n        a    1.3\\n        b    3.7\\n        Name: 0.1, dtype: float64\\n        >>> df.quantile([.1, .5])\\n               a     b\\n        0.1  1.3   3.7\\n        0.5  2.5  55.0\\n\\n        Specifying `numeric_only=False` will also compute the quantile of\\n        datetime and timedelta data.\\n\\n        >>> df = pd.DataFrame({\\'A\\': [1, 2],\\n        ...                    \\'B\\': [pd.Timestamp(\\'2010\\'),\\n        ...                          pd.Timestamp(\\'2011\\')],\\n        ...                    \\'C\\': [pd.Timedelta(\\'1 days\\'),\\n        ...                          pd.Timedelta(\\'2 days\\')]})\\n        >>> df.quantile(0.5, numeric_only=False)\\n        A                    1.5\\n        B    2010-07-02 12:00:00\\n        C        1 days 12:00:00\\n        Name: 0.5, dtype: object\\n        \"\"\"\\n        self._check_percentile(q)\\n\\n        data = self._get_numeric_data() if numeric_only else self\\n        axis = self._get_axis_number(axis)\\n        is_transposed = axis == 1\\n\\n        if is_transposed:\\n            data = data.T\\n\\n        result = data._data.quantile(qs=q,\\n                                     axis=1,\\n                                     interpolation=interpolation,\\n                                     transposed=is_transposed)\\n\\n        if result.ndim == 2:\\n            result = self._constructor(result)\\n        else:\\n            result = self._constructor_sliced(result, name=q)\\n\\n        if is_transposed:\\n            result = result.T\\n\\n        return result',\n 'def to_timestamp(self, freq=None, how=\\'start\\', axis=0, copy=True):\\n        \"\"\"\\n        Cast to DatetimeIndex of timestamps, at *beginning* of period.\\n\\n        Parameters\\n        ----------\\n        freq : str, default frequency of PeriodIndex\\n            Desired frequency.\\n        how : {\\'s\\', \\'e\\', \\'start\\', \\'end\\'}\\n            Convention for converting period to timestamp; start of period\\n            vs. end.\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n            The axis to convert (the index by default).\\n        copy : bool, default True\\n            If False then underlying input data is not copied.\\n\\n        Returns\\n        -------\\n        DataFrame with DatetimeIndex\\n        \"\"\"\\n        new_data = self._data\\n        if copy:\\n            new_data = new_data.copy()\\n\\n        axis = self._get_axis_number(axis)\\n        if axis == 0:\\n            new_data.set_axis(1, self.index.to_timestamp(freq=freq, how=how))\\n        elif axis == 1:\\n            new_data.set_axis(0, self.columns.to_timestamp(freq=freq, how=how))\\n        else:  # pragma: no cover\\n            raise AssertionError(\\'Axis must be 0 or 1. Got {ax!s}\\'.format(\\n                ax=axis))\\n\\n        return self._constructor(new_data)',\n 'def isin(self, values):\\n        \"\"\"\\n        Whether each element in the DataFrame is contained in values.\\n\\n        Parameters\\n        ----------\\n        values : iterable, Series, DataFrame or dict\\n            The result will only be true at a location if all the\\n            labels match. If `values` is a Series, that\\'s the index. If\\n            `values` is a dict, the keys must be the column names,\\n            which must match. If `values` is a DataFrame,\\n            then both the index and column labels must match.\\n\\n        Returns\\n        -------\\n        DataFrame\\n            DataFrame of booleans showing whether each element in the DataFrame\\n            is contained in values.\\n\\n        See Also\\n        --------\\n        DataFrame.eq: Equality test for DataFrame.\\n        Series.isin: Equivalent method on Series.\\n        Series.str.contains: Test if pattern or regex is contained within a\\n            string of a Series or Index.\\n\\n        Examples\\n        --------\\n\\n        >>> df = pd.DataFrame({\\'num_legs\\': [2, 4], \\'num_wings\\': [2, 0]},\\n        ...                   index=[\\'falcon\\', \\'dog\\'])\\n        >>> df\\n                num_legs  num_wings\\n        falcon         2          2\\n        dog            4          0\\n\\n        When ``values`` is a list check whether every value in the DataFrame\\n        is present in the list (which animals have 0 or 2 legs or wings)\\n\\n        >>> df.isin([0, 2])\\n                num_legs  num_wings\\n        falcon      True       True\\n        dog        False       True\\n\\n        When ``values`` is a dict, we can pass values to check for each\\n        column separately:\\n\\n        >>> df.isin({\\'num_wings\\': [0, 3]})\\n                num_legs  num_wings\\n        falcon     False      False\\n        dog        False       True\\n\\n        When ``values`` is a Series or DataFrame the index and column must\\n        match. Note that \\'falcon\\' does not match based on the number of legs\\n        in df2.\\n\\n        >>> other = pd.DataFrame({\\'num_legs\\': [8, 2], \\'num_wings\\': [0, 2]},\\n        ...                      index=[\\'spider\\', \\'falcon\\'])\\n        >>> df.isin(other)\\n                num_legs  num_wings\\n        falcon      True       True\\n        dog        False      False\\n        \"\"\"\\n        if isinstance(values, dict):\\n            from pandas.core.reshape.concat import concat\\n            values = collections.defaultdict(list, values)\\n            return concat((self.iloc[:, [i]].isin(values[col])\\n                           for i, col in enumerate(self.columns)), axis=1)\\n        elif isinstance(values, Series):\\n            if not values.index.is_unique:\\n                raise ValueError(\"cannot compute isin with \"\\n                                 \"a duplicate axis.\")\\n            return self.eq(values.reindex_like(self), axis=\\'index\\')\\n        elif isinstance(values, DataFrame):\\n            if not (values.columns.is_unique and values.index.is_unique):\\n                raise ValueError(\"cannot compute isin with \"\\n                                 \"a duplicate axis.\")\\n            return self.eq(values.reindex_like(self))\\n        else:\\n            if not is_list_like(values):\\n                raise TypeError(\"only list-like or dict-like objects are \"\\n                                \"allowed to be passed to DataFrame.isin(), \"\\n                                \"you passed a \"\\n                                \"{0!r}\".format(type(values).__name__))\\n            return DataFrame(\\n                algorithms.isin(self.values.ravel(),\\n                                values).reshape(self.shape), self.index,\\n                self.columns)',\n 'def integer_array(values, dtype=None, copy=False):\\n    \"\"\"\\n    Infer and return an integer array of the values.\\n\\n    Parameters\\n    ----------\\n    values : 1D list-like\\n    dtype : dtype, optional\\n        dtype to coerce\\n    copy : boolean, default False\\n\\n    Returns\\n    -------\\n    IntegerArray\\n\\n    Raises\\n    ------\\n    TypeError if incompatible types\\n    \"\"\"\\n    values, mask = coerce_to_array(values, dtype=dtype, copy=copy)\\n    return IntegerArray(values, mask)',\n 'def safe_cast(values, dtype, copy):\\n    \"\"\"\\n    Safely cast the values to the dtype if they\\n    are equivalent, meaning floats must be equivalent to the\\n    ints.\\n\\n    \"\"\"\\n\\n    try:\\n        return values.astype(dtype, casting=\\'safe\\', copy=copy)\\n    except TypeError:\\n\\n        casted = values.astype(dtype, copy=copy)\\n        if (casted == values).all():\\n            return casted\\n\\n        raise TypeError(\"cannot safely cast non-equivalent {} to {}\".format(\\n            values.dtype, np.dtype(dtype)))',\n 'def coerce_to_array(values, dtype, mask=None, copy=False):\\n    \"\"\"\\n    Coerce the input values array to numpy arrays with a mask\\n\\n    Parameters\\n    ----------\\n    values : 1D list-like\\n    dtype : integer dtype\\n    mask : boolean 1D array, optional\\n    copy : boolean, default False\\n        if True, copy the input\\n\\n    Returns\\n    -------\\n    tuple of (values, mask)\\n    \"\"\"\\n    # if values is integer numpy array, preserve it\\'s dtype\\n    if dtype is None and hasattr(values, \\'dtype\\'):\\n        if is_integer_dtype(values.dtype):\\n            dtype = values.dtype\\n\\n    if dtype is not None:\\n        if (isinstance(dtype, str) and\\n                (dtype.startswith(\"Int\") or dtype.startswith(\"UInt\"))):\\n            # Avoid DeprecationWarning from NumPy about np.dtype(\"Int64\")\\n            # https://github.com/numpy/numpy/pull/7476\\n            dtype = dtype.lower()\\n\\n        if not issubclass(type(dtype), _IntegerDtype):\\n            try:\\n                dtype = _dtypes[str(np.dtype(dtype))]\\n            except KeyError:\\n                raise ValueError(\"invalid dtype specified {}\".format(dtype))\\n\\n    if isinstance(values, IntegerArray):\\n        values, mask = values._data, values._mask\\n        if dtype is not None:\\n            values = values.astype(dtype.numpy_dtype, copy=False)\\n\\n        if copy:\\n            values = values.copy()\\n            mask = mask.copy()\\n        return values, mask\\n\\n    values = np.array(values, copy=copy)\\n    if is_object_dtype(values):\\n        inferred_type = lib.infer_dtype(values, skipna=True)\\n        if inferred_type == \\'empty\\':\\n            values = np.empty(len(values))\\n            values.fill(np.nan)\\n        elif inferred_type not in [\\'floating\\', \\'integer\\',\\n                                   \\'mixed-integer\\', \\'mixed-integer-float\\']:\\n            raise TypeError(\"{} cannot be converted to an IntegerDtype\".format(\\n                values.dtype))\\n\\n    elif not (is_integer_dtype(values) or is_float_dtype(values)):\\n        raise TypeError(\"{} cannot be converted to an IntegerDtype\".format(\\n            values.dtype))\\n\\n    if mask is None:\\n        mask = isna(values)\\n    else:\\n        assert len(mask) == len(values)\\n\\n    if not values.ndim == 1:\\n        raise TypeError(\"values must be a 1D list-like\")\\n    if not mask.ndim == 1:\\n        raise TypeError(\"mask must be a 1D list-like\")\\n\\n    # infer dtype if needed\\n    if dtype is None:\\n        dtype = np.dtype(\\'int64\\')\\n    else:\\n        dtype = dtype.type\\n\\n    # if we are float, let\\'s make sure that we can\\n    # safely cast\\n\\n    # we copy as need to coerce here\\n    if mask.any():\\n        values = values.copy()\\n        values[mask] = 1\\n        values = safe_cast(values, dtype, copy=False)\\n    else:\\n        values = safe_cast(values, dtype, copy=False)\\n\\n    return values, mask',\n 'def construct_from_string(cls, string):\\n        \"\"\"\\n        Construction from a string, raise a TypeError if not\\n        possible\\n        \"\"\"\\n        if string == cls.name:\\n            return cls()\\n        raise TypeError(\"Cannot construct a \\'{}\\' from \"\\n                        \"\\'{}\\'\".format(cls, string))',\n 'def _coerce_to_ndarray(self):\\n        \"\"\"\\n        coerce to an ndarary of object dtype\\n        \"\"\"\\n\\n        # TODO(jreback) make this better\\n        data = self._data.astype(object)\\n        data[self._mask] = self._na_value\\n        return data',\n 'def astype(self, dtype, copy=True):\\n        \"\"\"\\n        Cast to a NumPy array or IntegerArray with \\'dtype\\'.\\n\\n        Parameters\\n        ----------\\n        dtype : str or dtype\\n            Typecode or data-type to which the array is cast.\\n        copy : bool, default True\\n            Whether to copy the data, even if not necessary. If False,\\n            a copy is made only if the old dtype does not match the\\n            new dtype.\\n\\n        Returns\\n        -------\\n        array : ndarray or IntegerArray\\n            NumPy ndarray or IntergerArray with \\'dtype\\' for its dtype.\\n\\n        Raises\\n        ------\\n        TypeError\\n            if incompatible type with an IntegerDtype, equivalent of same_kind\\n            casting\\n        \"\"\"\\n\\n        # if we are astyping to an existing IntegerDtype we can fastpath\\n        if isinstance(dtype, _IntegerDtype):\\n            result = self._data.astype(dtype.numpy_dtype, copy=False)\\n            return type(self)(result, mask=self._mask, copy=False)\\n\\n        # coerce\\n        data = self._coerce_to_ndarray()\\n        return astype_nansafe(data, dtype, copy=None)',\n 'def value_counts(self, dropna=True):\\n        \"\"\"\\n        Returns a Series containing counts of each category.\\n\\n        Every category will have an entry, even those with a count of 0.\\n\\n        Parameters\\n        ----------\\n        dropna : boolean, default True\\n            Don\\'t include counts of NaN.\\n\\n        Returns\\n        -------\\n        counts : Series\\n\\n        See Also\\n        --------\\n        Series.value_counts\\n\\n        \"\"\"\\n\\n        from pandas import Index, Series\\n\\n        # compute counts on the data with no nans\\n        data = self._data[~self._mask]\\n        value_counts = Index(data).value_counts()\\n        array = value_counts.values\\n\\n        # TODO(extension)\\n        # if we have allow Index to hold an ExtensionArray\\n        # this is easier\\n        index = value_counts.index.astype(object)\\n\\n        # if we want nans, count the mask\\n        if not dropna:\\n\\n            # TODO(extension)\\n            # appending to an Index *always* infers\\n            # w/o passing the dtype\\n            array = np.append(array, [self._mask.sum()])\\n            index = Index(np.concatenate(\\n                [index.values,\\n                 np.array([np.nan], dtype=object)]), dtype=object)\\n\\n        return Series(array, index=index)',\n 'def _values_for_argsort(self) -> np.ndarray:\\n        \"\"\"Return values for sorting.\\n\\n        Returns\\n        -------\\n        ndarray\\n            The transformed values should maintain the ordering between values\\n            within the array.\\n\\n        See Also\\n        --------\\n        ExtensionArray.argsort\\n        \"\"\"\\n        data = self._data.copy()\\n        data[self._mask] = data.min() - 1\\n        return data',\n 'def _maybe_mask_result(self, result, mask, other, op_name):\\n        \"\"\"\\n        Parameters\\n        ----------\\n        result : array-like\\n        mask : array-like bool\\n        other : scalar or array-like\\n        op_name : str\\n        \"\"\"\\n\\n        # may need to fill infs\\n        # and mask wraparound\\n        if is_float_dtype(result):\\n            mask |= (result == np.inf) | (result == -np.inf)\\n\\n        # if we have a float operand we are by-definition\\n        # a float result\\n        # or our op is a divide\\n        if ((is_float_dtype(other) or is_float(other)) or\\n                (op_name in [\\'rtruediv\\', \\'truediv\\', \\'rdiv\\', \\'div\\'])):\\n            result[mask] = np.nan\\n            return result\\n\\n        return type(self)(result, mask, copy=False)',\n 'def length_of_indexer(indexer, target=None):\\n    \"\"\"\\n    return the length of a single non-tuple indexer which could be a slice\\n    \"\"\"\\n    if target is not None and isinstance(indexer, slice):\\n        target_len = len(target)\\n        start = indexer.start\\n        stop = indexer.stop\\n        step = indexer.step\\n        if start is None:\\n            start = 0\\n        elif start < 0:\\n            start += target_len\\n        if stop is None or stop > target_len:\\n            stop = target_len\\n        elif stop < 0:\\n            stop += target_len\\n        if step is None:\\n            step = 1\\n        elif step < 0:\\n            step = -step\\n        return (stop - start + step - 1) // step\\n    elif isinstance(indexer, (ABCSeries, Index, np.ndarray, list)):\\n        return len(indexer)\\n    elif not is_list_like_indexer(indexer):\\n        return 1\\n    raise AssertionError(\"cannot find the length of the indexer\")',\n 'def convert_to_index_sliceable(obj, key):\\n    \"\"\"\\n    if we are index sliceable, then return my slicer, otherwise return None\\n    \"\"\"\\n    idx = obj.index\\n    if isinstance(key, slice):\\n        return idx._convert_slice_indexer(key, kind=\\'getitem\\')\\n\\n    elif isinstance(key, str):\\n\\n        # we are an actual column\\n        if obj._data.items.contains(key):\\n            return None\\n\\n        # We might have a datetimelike string that we can translate to a\\n        # slice here via partial string indexing\\n        if idx.is_all_dates:\\n            try:\\n                return idx._get_string_slice(key)\\n            except (KeyError, ValueError, NotImplementedError):\\n                return None\\n\\n    return None',\n 'def check_setitem_lengths(indexer, value, values):\\n    \"\"\"\\n    Validate that value and indexer are the same length.\\n\\n    An special-case is allowed for when the indexer is a boolean array\\n    and the number of true values equals the length of ``value``. In\\n    this case, no exception is raised.\\n\\n    Parameters\\n    ----------\\n    indexer : sequence\\n        The key for the setitem\\n    value : array-like\\n        The value for the setitem\\n    values : array-like\\n        The values being set into\\n\\n    Returns\\n    -------\\n    None\\n\\n    Raises\\n    ------\\n    ValueError\\n        When the indexer is an ndarray or list and the lengths don\\'t\\n        match.\\n    \"\"\"\\n    # boolean with truth values == len of the value is ok too\\n    if isinstance(indexer, (np.ndarray, list)):\\n        if is_list_like(value) and len(indexer) != len(value):\\n            if not (isinstance(indexer, np.ndarray) and\\n                    indexer.dtype == np.bool_ and\\n                    len(indexer[indexer]) == len(value)):\\n                raise ValueError(\"cannot set using a list-like indexer \"\\n                                 \"with a different length than the value\")\\n    # slice\\n    elif isinstance(indexer, slice):\\n\\n        if is_list_like(value) and len(values):\\n            if len(value) != length_of_indexer(indexer, values):\\n                raise ValueError(\"cannot set using a slice indexer with a \"\\n                                 \"different length than the value\")',\n 'def convert_missing_indexer(indexer):\\n    \"\"\"\\n    reverse convert a missing indexer, which is a dict\\n    return the scalar indexer and a boolean indicating if we converted\\n    \"\"\"\\n\\n    if isinstance(indexer, dict):\\n\\n        # a missing key (but not a tuple indexer)\\n        indexer = indexer[\\'key\\']\\n\\n        if isinstance(indexer, bool):\\n            raise KeyError(\"cannot use a single bool to index into setitem\")\\n        return indexer, True\\n\\n    return indexer, False',\n 'def convert_from_missing_indexer_tuple(indexer, axes):\\n    \"\"\"\\n    create a filtered indexer that doesn\\'t have any missing indexers\\n    \"\"\"\\n\\n    def get_indexer(_i, _idx):\\n        return (axes[_i].get_loc(_idx[\\'key\\']) if isinstance(_idx, dict) else\\n                _idx)\\n\\n    return tuple(get_indexer(_i, _idx) for _i, _idx in enumerate(indexer))',\n 'def maybe_convert_indices(indices, n):\\n    \"\"\"\\n    Attempt to convert indices into valid, positive indices.\\n\\n    If we have negative indices, translate to positive here.\\n    If we have indices that are out-of-bounds, raise an IndexError.\\n\\n    Parameters\\n    ----------\\n    indices : array-like\\n        The array of indices that we are to convert.\\n    n : int\\n        The number of elements in the array that we are indexing.\\n\\n    Returns\\n    -------\\n    valid_indices : array-like\\n        An array-like of positive indices that correspond to the ones\\n        that were passed in initially to this function.\\n\\n    Raises\\n    ------\\n    IndexError : one of the converted indices either exceeded the number\\n        of elements (specified by `n`) OR was still negative.\\n    \"\"\"\\n\\n    if isinstance(indices, list):\\n        indices = np.array(indices)\\n        if len(indices) == 0:\\n            # If list is empty, np.array will return float and cause indexing\\n            # errors.\\n            return np.empty(0, dtype=np.intp)\\n\\n    mask = indices < 0\\n    if mask.any():\\n        indices = indices.copy()\\n        indices[mask] += n\\n\\n    mask = (indices >= n) | (indices < 0)\\n    if mask.any():\\n        raise IndexError(\"indices are out-of-bounds\")\\n    return indices',\n 'def validate_indices(indices, n):\\n    \"\"\"\\n    Perform bounds-checking for an indexer.\\n\\n    -1 is allowed for indicating missing values.\\n\\n    Parameters\\n    ----------\\n    indices : ndarray\\n    n : int\\n        length of the array being indexed\\n\\n    Raises\\n    ------\\n    ValueError\\n\\n    Examples\\n    --------\\n    >>> validate_indices([1, 2], 3)\\n    # OK\\n    >>> validate_indices([1, -2], 3)\\n    ValueError\\n    >>> validate_indices([1, 2, 3], 3)\\n    IndexError\\n    >>> validate_indices([-1, -1], 0)\\n    # OK\\n    >>> validate_indices([0, 1], 0)\\n    IndexError\\n    \"\"\"\\n    if len(indices):\\n        min_idx = indices.min()\\n        if min_idx < -1:\\n            msg = (\"\\'indices\\' contains values less than allowed ({} < {})\"\\n                   .format(min_idx, -1))\\n            raise ValueError(msg)\\n\\n        max_idx = indices.max()\\n        if max_idx >= n:\\n            raise IndexError(\"indices are out-of-bounds\")',\n 'def maybe_convert_ix(*args):\\n    \"\"\"\\n    We likely want to take the cross-product\\n    \"\"\"\\n\\n    ixify = True\\n    for arg in args:\\n        if not isinstance(arg, (np.ndarray, list, ABCSeries, Index)):\\n            ixify = False\\n\\n    if ixify:\\n        return np.ix_(*args)\\n    else:\\n        return args',\n 'def _non_reducing_slice(slice_):\\n    \"\"\"\\n    Ensurse that a slice doesn\\'t reduce to a Series or Scalar.\\n\\n    Any user-paseed `subset` should have this called on it\\n    to make sure we\\'re always working with DataFrames.\\n    \"\"\"\\n    # default to column slice, like DataFrame\\n    # [\\'A\\', \\'B\\'] -> IndexSlices[:, [\\'A\\', \\'B\\']]\\n    kinds = (ABCSeries, np.ndarray, Index, list, str)\\n    if isinstance(slice_, kinds):\\n        slice_ = IndexSlice[:, slice_]\\n\\n    def pred(part):\\n        # true when slice does *not* reduce, False when part is a tuple,\\n        # i.e. MultiIndex slice\\n        return ((isinstance(part, slice) or is_list_like(part))\\n                and not isinstance(part, tuple))\\n\\n    if not is_list_like(slice_):\\n        if not isinstance(slice_, slice):\\n            # a 1-d slice, like df.loc[1]\\n            slice_ = [[slice_]]\\n        else:\\n            # slice(a, b, c)\\n            slice_ = [slice_]  # to tuplize later\\n    else:\\n        slice_ = [part if pred(part) else [part] for part in slice_]\\n    return tuple(slice_)',\n 'def _maybe_numeric_slice(df, slice_, include_bool=False):\\n    \"\"\"\\n    want nice defaults for background_gradient that don\\'t break\\n    with non-numeric data. But if slice_ is passed go with that.\\n    \"\"\"\\n    if slice_ is None:\\n        dtypes = [np.number]\\n        if include_bool:\\n            dtypes.append(bool)\\n        slice_ = IndexSlice[:, df.select_dtypes(include=dtypes).columns]\\n    return slice_',\n 'def _has_valid_tuple(self, key):\\n        \"\"\" check the key for valid keys across my indexer \"\"\"\\n        for i, k in enumerate(key):\\n            if i >= self.obj.ndim:\\n                raise IndexingError(\\'Too many indexers\\')\\n            try:\\n                self._validate_key(k, i)\\n            except ValueError:\\n                raise ValueError(\"Location based indexing can only have \"\\n                                 \"[{types}] types\"\\n                                 .format(types=self._valid_types))',\n 'def _has_valid_positional_setitem_indexer(self, indexer):\\n        \"\"\" validate that an positional indexer cannot enlarge its target\\n        will raise if needed, does not modify the indexer externally\\n        \"\"\"\\n        if isinstance(indexer, dict):\\n            raise IndexError(\"{0} cannot enlarge its target object\"\\n                             .format(self.name))\\n        else:\\n            if not isinstance(indexer, tuple):\\n                indexer = self._tuplify(indexer)\\n            for ax, i in zip(self.obj.axes, indexer):\\n                if isinstance(i, slice):\\n                    # should check the stop slice?\\n                    pass\\n                elif is_list_like_indexer(i):\\n                    # should check the elements?\\n                    pass\\n                elif is_integer(i):\\n                    if i >= len(ax):\\n                        raise IndexError(\"{name} cannot enlarge its target \"\\n                                         \"object\".format(name=self.name))\\n                elif isinstance(i, dict):\\n                    raise IndexError(\"{name} cannot enlarge its target object\"\\n                                     .format(name=self.name))\\n\\n        return True',\n 'def _align_series(self, indexer, ser, multiindex_indexer=False):\\n        \"\"\"\\n        Parameters\\n        ----------\\n        indexer : tuple, slice, scalar\\n            The indexer used to get the locations that will be set to\\n            `ser`\\n\\n        ser : pd.Series\\n            The values to assign to the locations specified by `indexer`\\n\\n        multiindex_indexer : boolean, optional\\n            Defaults to False. Should be set to True if `indexer` was from\\n            a `pd.MultiIndex`, to avoid unnecessary broadcasting.\\n\\n\\n        Returns:\\n        --------\\n        `np.array` of `ser` broadcast to the appropriate shape for assignment\\n        to the locations selected by `indexer`\\n\\n        \"\"\"\\n        if isinstance(indexer, (slice, np.ndarray, list, Index)):\\n            indexer = tuple([indexer])\\n\\n        if isinstance(indexer, tuple):\\n\\n            # flatten np.ndarray indexers\\n            def ravel(i):\\n                return i.ravel() if isinstance(i, np.ndarray) else i\\n            indexer = tuple(map(ravel, indexer))\\n\\n            aligners = [not com.is_null_slice(idx) for idx in indexer]\\n            sum_aligners = sum(aligners)\\n            single_aligner = sum_aligners == 1\\n            is_frame = self.obj.ndim == 2\\n            is_panel = self.obj.ndim >= 3\\n            obj = self.obj\\n\\n            # are we a single alignable value on a non-primary\\n            # dim (e.g. panel: 1,2, or frame: 0) ?\\n            # hence need to align to a single axis dimension\\n            # rather that find all valid dims\\n\\n            # frame\\n            if is_frame:\\n                single_aligner = single_aligner and aligners[0]\\n\\n            # panel\\n            elif is_panel:\\n                single_aligner = (single_aligner and\\n                                  (aligners[1] or aligners[2]))\\n\\n            # we have a frame, with multiple indexers on both axes; and a\\n            # series, so need to broadcast (see GH5206)\\n            if (sum_aligners == self.ndim and\\n                    all(is_sequence(_) for _ in indexer)):\\n                ser = ser.reindex(obj.axes[0][indexer[0]], copy=True)._values\\n\\n                # single indexer\\n                if len(indexer) > 1 and not multiindex_indexer:\\n                    len_indexer = len(indexer[1])\\n                    ser = np.tile(ser, len_indexer).reshape(len_indexer, -1).T\\n\\n                return ser\\n\\n            for i, idx in enumerate(indexer):\\n                ax = obj.axes[i]\\n\\n                # multiple aligners (or null slices)\\n                if is_sequence(idx) or isinstance(idx, slice):\\n                    if single_aligner and com.is_null_slice(idx):\\n                        continue\\n                    new_ix = ax[idx]\\n                    if not is_list_like_indexer(new_ix):\\n                        new_ix = Index([new_ix])\\n                    else:\\n                        new_ix = Index(new_ix)\\n                    if ser.index.equals(new_ix) or not len(new_ix):\\n                        return ser._values.copy()\\n\\n                    return ser.reindex(new_ix)._values\\n\\n                # 2 dims\\n                elif single_aligner and is_frame:\\n\\n                    # reindex along index\\n                    ax = self.obj.axes[1]\\n                    if ser.index.equals(ax) or not len(ax):\\n                        return ser._values.copy()\\n                    return ser.reindex(ax)._values\\n\\n                # >2 dims\\n                elif single_aligner:\\n\\n                    broadcast = []\\n                    for n, labels in enumerate(self.obj._get_plane_axes(i)):\\n\\n                        # reindex along the matching dimensions\\n                        if len(labels & ser.index):\\n                            ser = ser.reindex(labels)\\n                        else:\\n                            broadcast.append((n, len(labels)))\\n\\n                    # broadcast along other dims\\n                    ser = ser._values.copy()\\n                    for (axis, l) in broadcast:\\n                        shape = [-1] * (len(broadcast) + 1)\\n                        shape[axis] = l\\n                        ser = np.tile(ser, l).reshape(shape)\\n\\n                    if self.obj.ndim == 3:\\n                        ser = ser.T\\n\\n                    return ser\\n\\n        elif is_scalar(indexer):\\n            ax = self.obj._get_axis(1)\\n\\n            if ser.index.equals(ax):\\n                return ser._values.copy()\\n\\n            return ser.reindex(ax)._values\\n\\n        raise ValueError(\\'Incompatible indexer with Series\\')',\n 'def _multi_take_opportunity(self, tup):\\n        \"\"\"\\n        Check whether there is the possibility to use ``_multi_take``.\\n        Currently the limit is that all axes being indexed must be indexed with\\n        list-likes.\\n\\n        Parameters\\n        ----------\\n        tup : tuple\\n            Tuple of indexers, one per axis\\n\\n        Returns\\n        -------\\n        boolean: Whether the current indexing can be passed through _multi_take\\n        \"\"\"\\n        if not all(is_list_like_indexer(x) for x in tup):\\n            return False\\n\\n        # just too complicated\\n        if any(com.is_bool_indexer(x) for x in tup):\\n            return False\\n\\n        return True',\n 'def _multi_take(self, tup):\\n        \"\"\"\\n        Create the indexers for the passed tuple of keys, and execute the take\\n        operation. This allows the take operation to be executed all at once -\\n        rather than once for each dimension - improving efficiency.\\n\\n        Parameters\\n        ----------\\n        tup : tuple\\n            Tuple of indexers, one per axis\\n\\n        Returns\\n        -------\\n        values: same type as the object being indexed\\n        \"\"\"\\n        # GH 836\\n        o = self.obj\\n        d = {axis: self._get_listlike_indexer(key, axis)\\n             for (key, axis) in zip(tup, o._AXIS_ORDERS)}\\n        return o._reindex_with_indexers(d, copy=True, allow_dups=True)',\n 'def _get_listlike_indexer(self, key, axis, raise_missing=False):\\n        \"\"\"\\n        Transform a list-like of keys into a new index and an indexer.\\n\\n        Parameters\\n        ----------\\n        key : list-like\\n            Target labels\\n        axis: int\\n            Dimension on which the indexing is being made\\n        raise_missing: bool\\n            Whether to raise a KeyError if some labels are not found. Will be\\n            removed in the future, and then this method will always behave as\\n            if raise_missing=True.\\n\\n        Raises\\n        ------\\n        KeyError\\n            If at least one key was requested but none was found, and\\n            raise_missing=True.\\n\\n        Returns\\n        -------\\n        keyarr: Index\\n            New index (coinciding with \\'key\\' if the axis is unique)\\n        values : array-like\\n            An indexer for the return object; -1 denotes keys not found\\n        \"\"\"\\n        o = self.obj\\n        ax = o._get_axis(axis)\\n\\n        # Have the index compute an indexer or return None\\n        # if it cannot handle:\\n        indexer, keyarr = ax._convert_listlike_indexer(key,\\n                                                       kind=self.name)\\n        # We only act on all found values:\\n        if indexer is not None and (indexer != -1).all():\\n            self._validate_read_indexer(key, indexer, axis,\\n                                        raise_missing=raise_missing)\\n            return ax[indexer], indexer\\n\\n        if ax.is_unique:\\n            # If we are trying to get actual keys from empty Series, we\\n            # patiently wait for a KeyError later on - otherwise, convert\\n            if len(ax) or not len(key):\\n                key = self._convert_for_reindex(key, axis)\\n            indexer = ax.get_indexer_for(key)\\n            keyarr = ax.reindex(keyarr)[0]\\n        else:\\n            keyarr, indexer, new_indexer = ax._reindex_non_unique(keyarr)\\n\\n        self._validate_read_indexer(keyarr, indexer,\\n                                    o._get_axis_number(axis),\\n                                    raise_missing=raise_missing)\\n        return keyarr, indexer',\n 'def _getitem_iterable(self, key, axis=None):\\n        \"\"\"\\n        Index current object with an an iterable key (which can be a boolean\\n        indexer, or a collection of keys).\\n\\n        Parameters\\n        ----------\\n        key : iterable\\n            Target labels, or boolean indexer\\n        axis: int, default None\\n            Dimension on which the indexing is being made\\n\\n        Raises\\n        ------\\n        KeyError\\n            If no key was found. Will change in the future to raise if not all\\n            keys were found.\\n        IndexingError\\n            If the boolean indexer is unalignable with the object being\\n            indexed.\\n\\n        Returns\\n        -------\\n        scalar, DataFrame, or Series: indexed value(s),\\n        \"\"\"\\n\\n        if axis is None:\\n            axis = self.axis or 0\\n\\n        self._validate_key(key, axis)\\n\\n        labels = self.obj._get_axis(axis)\\n\\n        if com.is_bool_indexer(key):\\n            # A boolean indexer\\n            key = check_bool_indexer(labels, key)\\n            inds, = key.nonzero()\\n            return self.obj._take(inds, axis=axis)\\n        else:\\n            # A collection of keys\\n            keyarr, indexer = self._get_listlike_indexer(key, axis,\\n                                                         raise_missing=False)\\n            return self.obj._reindex_with_indexers({axis: [keyarr, indexer]},\\n                                                   copy=True, allow_dups=True)',\n 'def _validate_read_indexer(self, key, indexer, axis, raise_missing=False):\\n        \"\"\"\\n        Check that indexer can be used to return a result (e.g. at least one\\n        element was found, unless the list of keys was actually empty).\\n\\n        Parameters\\n        ----------\\n        key : list-like\\n            Target labels (only used to show correct error message)\\n        indexer: array-like of booleans\\n            Indices corresponding to the key (with -1 indicating not found)\\n        axis: int\\n            Dimension on which the indexing is being made\\n        raise_missing: bool\\n            Whether to raise a KeyError if some labels are not found. Will be\\n            removed in the future, and then this method will always behave as\\n            if raise_missing=True.\\n\\n        Raises\\n        ------\\n        KeyError\\n            If at least one key was requested but none was found, and\\n            raise_missing=True.\\n        \"\"\"\\n\\n        ax = self.obj._get_axis(axis)\\n\\n        if len(key) == 0:\\n            return\\n\\n        # Count missing values:\\n        missing = (indexer < 0).sum()\\n\\n        if missing:\\n            if missing == len(indexer):\\n                raise KeyError(\\n                    \"None of [{key}] are in the [{axis}]\".format(\\n                        key=key, axis=self.obj._get_axis_name(axis)))\\n\\n            # We (temporarily) allow for some missing keys with .loc, except in\\n            # some cases (e.g. setting) in which \"raise_missing\" will be False\\n            if not(self.name == \\'loc\\' and not raise_missing):\\n                not_found = list(set(key) - set(ax))\\n                raise KeyError(\"{} not in index\".format(not_found))\\n\\n            # we skip the warning on Categorical/Interval\\n            # as this check is actually done (check for\\n            # non-missing values), but a bit later in the\\n            # code, so we want to avoid warning & then\\n            # just raising\\n\\n            _missing_key_warning = textwrap.dedent(\"\"\"\\n            Passing list-likes to .loc or [] with any missing label will raise\\n            KeyError in the future, you can use .reindex() as an alternative.\\n\\n            See the documentation here:\\n            https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\"\"\")  # noqa\\n\\n            if not (ax.is_categorical() or ax.is_interval()):\\n                warnings.warn(_missing_key_warning,\\n                              FutureWarning, stacklevel=6)',\n 'def _convert_to_indexer(self, obj, axis=None, is_setter=False,\\n                            raise_missing=False):\\n        \"\"\"\\n        Convert indexing key into something we can use to do actual fancy\\n        indexing on an ndarray\\n\\n        Examples\\n        ix[:5] -> slice(0, 5)\\n        ix[[1,2,3]] -> [1,2,3]\\n        ix[[\\'foo\\', \\'bar\\', \\'baz\\']] -> [i, j, k] (indices of foo, bar, baz)\\n\\n        Going by Zen of Python?\\n        \\'In the face of ambiguity, refuse the temptation to guess.\\'\\n        raise AmbiguousIndexError with integer labels?\\n        - No, prefer label-based indexing\\n        \"\"\"\\n        if axis is None:\\n            axis = self.axis or 0\\n\\n        labels = self.obj._get_axis(axis)\\n\\n        if isinstance(obj, slice):\\n            return self._convert_slice_indexer(obj, axis)\\n\\n        # try to find out correct indexer, if not type correct raise\\n        try:\\n            obj = self._convert_scalar_indexer(obj, axis)\\n        except TypeError:\\n\\n            # but we will allow setting\\n            if is_setter:\\n                pass\\n\\n        # see if we are positional in nature\\n        is_int_index = labels.is_integer()\\n        is_int_positional = is_integer(obj) and not is_int_index\\n\\n        # if we are a label return me\\n        try:\\n            return labels.get_loc(obj)\\n        except LookupError:\\n            if isinstance(obj, tuple) and isinstance(labels, MultiIndex):\\n                if is_setter and len(obj) == labels.nlevels:\\n                    return {\\'key\\': obj}\\n                raise\\n        except TypeError:\\n            pass\\n        except (ValueError):\\n            if not is_int_positional:\\n                raise\\n\\n        # a positional\\n        if is_int_positional:\\n\\n            # if we are setting and its not a valid location\\n            # its an insert which fails by definition\\n            if is_setter:\\n\\n                # always valid\\n                if self.name == \\'loc\\':\\n                    return {\\'key\\': obj}\\n\\n                # a positional\\n                if (obj >= self.obj.shape[axis] and\\n                        not isinstance(labels, MultiIndex)):\\n                    raise ValueError(\"cannot set by positional indexing with \"\\n                                     \"enlargement\")\\n\\n            return obj\\n\\n        if is_nested_tuple(obj, labels):\\n            return labels.get_locs(obj)\\n\\n        elif is_list_like_indexer(obj):\\n\\n            if com.is_bool_indexer(obj):\\n                obj = check_bool_indexer(labels, obj)\\n                inds, = obj.nonzero()\\n                return inds\\n            else:\\n                # When setting, missing keys are not allowed, even with .loc:\\n                kwargs = {\\'raise_missing\\': True if is_setter else\\n                          raise_missing}\\n                return self._get_listlike_indexer(obj, axis, **kwargs)[1]\\n        else:\\n            try:\\n                return labels.get_loc(obj)\\n            except LookupError:\\n                # allow a not found key only if we are a setter\\n                if not is_list_like_indexer(obj) and is_setter:\\n                    return {\\'key\\': obj}\\n                raise',\n 'def _convert_for_reindex(self, key, axis=None):\\n        \"\"\"\\n        Transform a list of keys into a new array ready to be used as axis of\\n        the object we return (e.g. including NaNs).\\n\\n        Parameters\\n        ----------\\n        key : list-like\\n            Target labels\\n        axis: int\\n            Where the indexing is being made\\n\\n        Returns\\n        -------\\n        list-like of labels\\n        \"\"\"\\n\\n        if axis is None:\\n            axis = self.axis or 0\\n        labels = self.obj._get_axis(axis)\\n\\n        if com.is_bool_indexer(key):\\n            key = check_bool_indexer(labels, key)\\n            return labels[key]\\n\\n        if isinstance(key, Index):\\n            keyarr = labels._convert_index_indexer(key)\\n        else:\\n            # asarray can be unsafe, NumPy strings are weird\\n            keyarr = com.asarray_tuplesafe(key)\\n\\n        if is_integer_dtype(keyarr):\\n            # Cast the indexer to uint64 if possible so\\n            # that the values returned from indexing are\\n            # also uint64.\\n            keyarr = labels._convert_arr_indexer(keyarr)\\n\\n            if not labels.is_integer():\\n                keyarr = ensure_platform_int(keyarr)\\n                return labels.take(keyarr)\\n\\n        return keyarr',\n 'def _get_slice_axis(self, slice_obj, axis=None):\\n        \"\"\" this is pretty simple as we just have to deal with labels \"\"\"\\n        if axis is None:\\n            axis = self.axis or 0\\n\\n        obj = self.obj\\n        if not need_slice(slice_obj):\\n            return obj.copy(deep=False)\\n\\n        labels = obj._get_axis(axis)\\n        indexer = labels.slice_indexer(slice_obj.start, slice_obj.stop,\\n                                       slice_obj.step, kind=self.name)\\n\\n        if isinstance(indexer, slice):\\n            return self._slice(indexer, axis=axis, kind=\\'iloc\\')\\n        else:\\n            return self.obj._take(indexer, axis=axis)',\n 'def _get_partial_string_timestamp_match_key(self, key, labels):\\n        \"\"\"Translate any partial string timestamp matches in key, returning the\\n        new key (GH 10331)\"\"\"\\n        if isinstance(labels, MultiIndex):\\n            if (isinstance(key, str) and labels.levels[0].is_all_dates):\\n                # Convert key \\'2016-01-01\\' to\\n                # (\\'2016-01-01\\'[, slice(None, None, None)]+)\\n                key = tuple([key] + [slice(None)] * (len(labels.levels) - 1))\\n\\n            if isinstance(key, tuple):\\n                # Convert (..., \\'2016-01-01\\', ...) in tuple to\\n                # (..., slice(\\'2016-01-01\\', \\'2016-01-01\\', None), ...)\\n                new_key = []\\n                for i, component in enumerate(key):\\n                    if (isinstance(component, str) and\\n                            labels.levels[i].is_all_dates):\\n                        new_key.append(slice(component, component, None))\\n                    else:\\n                        new_key.append(component)\\n                key = tuple(new_key)\\n\\n        return key',\n 'def _validate_integer(self, key, axis):\\n        \"\"\"\\n        Check that \\'key\\' is a valid position in the desired axis.\\n\\n        Parameters\\n        ----------\\n        key : int\\n            Requested position\\n        axis : int\\n            Desired axis\\n\\n        Returns\\n        -------\\n        None\\n\\n        Raises\\n        ------\\n        IndexError\\n            If \\'key\\' is not a valid position in axis \\'axis\\'\\n        \"\"\"\\n\\n        len_axis = len(self.obj._get_axis(axis))\\n        if key >= len_axis or key < -len_axis:\\n            raise IndexError(\"single positional indexer is out-of-bounds\")',\n 'def _get_list_axis(self, key, axis=None):\\n        \"\"\"\\n        Return Series values by list or array of integers\\n\\n        Parameters\\n        ----------\\n        key : list-like positional indexer\\n        axis : int (can only be zero)\\n\\n        Returns\\n        -------\\n        Series object\\n        \"\"\"\\n        if axis is None:\\n            axis = self.axis or 0\\n        try:\\n            return self.obj._take(key, axis=axis)\\n        except IndexError:\\n            # re-raise with different error message\\n            raise IndexError(\"positional indexers are out-of-bounds\")',\n 'def _convert_to_indexer(self, obj, axis=None, is_setter=False):\\n        \"\"\" much simpler as we only have to deal with our valid types \"\"\"\\n        if axis is None:\\n            axis = self.axis or 0\\n\\n        # make need to convert a float key\\n        if isinstance(obj, slice):\\n            return self._convert_slice_indexer(obj, axis)\\n\\n        elif is_float(obj):\\n            return self._convert_scalar_indexer(obj, axis)\\n\\n        try:\\n            self._validate_key(obj, axis)\\n            return obj\\n        except ValueError:\\n            raise ValueError(\"Can only index by location with \"\\n                             \"a [{types}]\".format(types=self._valid_types))',\n 'def _convert_key(self, key, is_setter=False):\\n        \"\"\" require they keys to be the same type as the index (so we don\\'t\\n        fallback)\\n        \"\"\"\\n\\n        # allow arbitrary setting\\n        if is_setter:\\n            return list(key)\\n\\n        for ax, i in zip(self.obj.axes, key):\\n            if ax.is_integer():\\n                if not is_integer(i):\\n                    raise ValueError(\"At based indexing on an integer index \"\\n                                     \"can only have integer indexers\")\\n            else:\\n                if is_integer(i) and not ax.holds_integer():\\n                    raise ValueError(\"At based indexing on an non-integer \"\\n                                     \"index can only have non-integer \"\\n                                     \"indexers\")\\n        return key',\n 'def _convert_key(self, key, is_setter=False):\\n        \"\"\" require integer args (and convert to label arguments) \"\"\"\\n        for a, i in zip(self.obj.axes, key):\\n            if not is_integer(i):\\n                raise ValueError(\"iAt based indexing can only have integer \"\\n                                 \"indexers\")\\n        return key',\n 'def to_manager(sdf, columns, index):\\n    \"\"\" create and return the block manager from a dataframe of series,\\n    columns, index\\n    \"\"\"\\n\\n    # from BlockManager perspective\\n    axes = [ensure_index(columns), ensure_index(index)]\\n\\n    return create_block_manager_from_arrays(\\n        [sdf[c] for c in columns], columns, axes)',\n 'def stack_sparse_frame(frame):\\n    \"\"\"\\n    Only makes sense when fill_value is NaN\\n    \"\"\"\\n    lengths = [s.sp_index.npoints for _, s in frame.items()]\\n    nobs = sum(lengths)\\n\\n    # this is pretty fast\\n    minor_codes = np.repeat(np.arange(len(frame.columns)), lengths)\\n\\n    inds_to_concat = []\\n    vals_to_concat = []\\n    # TODO: Figure out whether this can be reached.\\n    # I think this currently can\\'t be reached because you can\\'t build a\\n    # SparseDataFrame with a non-np.NaN fill value (fails earlier).\\n    for _, series in frame.items():\\n        if not np.isnan(series.fill_value):\\n            raise TypeError(\\'This routine assumes NaN fill value\\')\\n\\n        int_index = series.sp_index.to_int_index()\\n        inds_to_concat.append(int_index.indices)\\n        vals_to_concat.append(series.sp_values)\\n\\n    major_codes = np.concatenate(inds_to_concat)\\n    stacked_values = np.concatenate(vals_to_concat)\\n    index = MultiIndex(levels=[frame.index, frame.columns],\\n                       codes=[major_codes, minor_codes],\\n                       verify_integrity=False)\\n\\n    lp = DataFrame(stacked_values.reshape((nobs, 1)), index=index,\\n                   columns=[\\'foo\\'])\\n    return lp.sort_index(level=0)',\n 'def homogenize(series_dict):\\n    \"\"\"\\n    Conform a set of SparseSeries (with NaN fill_value) to a common SparseIndex\\n    corresponding to the locations where they all have data\\n\\n    Parameters\\n    ----------\\n    series_dict : dict or DataFrame\\n\\n    Notes\\n    -----\\n    Using the dumbest algorithm I could think of. Should put some more thought\\n    into this\\n\\n    Returns\\n    -------\\n    homogenized : dict of SparseSeries\\n    \"\"\"\\n    index = None\\n\\n    need_reindex = False\\n\\n    for _, series in series_dict.items():\\n        if not np.isnan(series.fill_value):\\n            raise TypeError(\\'this method is only valid with NaN fill values\\')\\n\\n        if index is None:\\n            index = series.sp_index\\n        elif not series.sp_index.equals(index):\\n            need_reindex = True\\n            index = index.intersect(series.sp_index)\\n\\n    if need_reindex:\\n        output = {}\\n        for name, series in series_dict.items():\\n            if not series.sp_index.equals(index):\\n                series = series.sparse_reindex(index)\\n\\n            output[name] = series\\n    else:\\n        output = series_dict\\n\\n    return output',\n 'def _init_matrix(self, data, index, columns, dtype=None):\\n        \"\"\"\\n        Init self from ndarray or list of lists.\\n        \"\"\"\\n        data = prep_ndarray(data, copy=False)\\n        index, columns = self._prep_index(data, index, columns)\\n        data = {idx: data[:, i] for i, idx in enumerate(columns)}\\n        return self._init_dict(data, index, columns, dtype)',\n 'def _init_spmatrix(self, data, index, columns, dtype=None,\\n                       fill_value=None):\\n        \"\"\"\\n        Init self from scipy.sparse matrix.\\n        \"\"\"\\n        index, columns = self._prep_index(data, index, columns)\\n        data = data.tocoo()\\n        N = len(index)\\n\\n        # Construct a dict of SparseSeries\\n        sdict = {}\\n        values = Series(data.data, index=data.row, copy=False)\\n        for col, rowvals in values.groupby(data.col):\\n            # get_blocks expects int32 row indices in sorted order\\n            rowvals = rowvals.sort_index()\\n            rows = rowvals.index.values.astype(np.int32)\\n            blocs, blens = get_blocks(rows)\\n\\n            sdict[columns[col]] = SparseSeries(\\n                rowvals.values, index=index,\\n                fill_value=fill_value,\\n                sparse_index=BlockIndex(N, blocs, blens))\\n\\n        # Add any columns that were empty and thus not grouped on above\\n        sdict.update({column: SparseSeries(index=index,\\n                                           fill_value=fill_value,\\n                                           sparse_index=BlockIndex(N, [], []))\\n                      for column in columns\\n                      if column not in sdict})\\n\\n        return self._init_dict(sdict, index, columns, dtype)',\n 'def to_coo(self):\\n        \"\"\"\\n        Return the contents of the frame as a sparse SciPy COO matrix.\\n\\n        .. versionadded:: 0.20.0\\n\\n        Returns\\n        -------\\n        coo_matrix : scipy.sparse.spmatrix\\n            If the caller is heterogeneous and contains booleans or objects,\\n            the result will be of dtype=object. See Notes.\\n\\n        Notes\\n        -----\\n        The dtype will be the lowest-common-denominator type (implicit\\n        upcasting); that is to say if the dtypes (even of numeric types)\\n        are mixed, the one that accommodates all will be chosen.\\n\\n        e.g. If the dtypes are float16 and float32, dtype will be upcast to\\n        float32. By numpy.find_common_type convention, mixing int64 and\\n        and uint64 will result in a float64 dtype.\\n        \"\"\"\\n        try:\\n            from scipy.sparse import coo_matrix\\n        except ImportError:\\n            raise ImportError(\\'Scipy is not installed\\')\\n\\n        dtype = find_common_type(self.dtypes)\\n        if isinstance(dtype, SparseDtype):\\n            dtype = dtype.subtype\\n\\n        cols, rows, datas = [], [], []\\n        for col, name in enumerate(self):\\n            s = self[name]\\n            row = s.sp_index.to_int_index().indices\\n            cols.append(np.repeat(col, len(row)))\\n            rows.append(row)\\n            datas.append(s.sp_values.astype(dtype, copy=False))\\n\\n        cols = np.concatenate(cols)\\n        rows = np.concatenate(rows)\\n        datas = np.concatenate(datas)\\n        return coo_matrix((datas, (rows, cols)), shape=self.shape)',\n 'def _unpickle_sparse_frame_compat(self, state):\\n        \"\"\"\\n        Original pickle format\\n        \"\"\"\\n        series, cols, idx, fv, kind = state\\n\\n        if not isinstance(cols, Index):  # pragma: no cover\\n            from pandas.io.pickle import _unpickle_array\\n            columns = _unpickle_array(cols)\\n        else:\\n            columns = cols\\n\\n        if not isinstance(idx, Index):  # pragma: no cover\\n            from pandas.io.pickle import _unpickle_array\\n            index = _unpickle_array(idx)\\n        else:\\n            index = idx\\n\\n        series_dict = DataFrame()\\n        for col, (sp_index, sp_values) in series.items():\\n            series_dict[col] = SparseSeries(sp_values, sparse_index=sp_index,\\n                                            fill_value=fv)\\n\\n        self._data = to_manager(series_dict, columns, index)\\n        self._default_fill_value = fv\\n        self._default_kind = kind',\n 'def to_dense(self):\\n        \"\"\"\\n        Convert to dense DataFrame\\n\\n        Returns\\n        -------\\n        df : DataFrame\\n        \"\"\"\\n        data = {k: v.to_dense() for k, v in self.items()}\\n        return DataFrame(data, index=self.index, columns=self.columns)',\n 'def _apply_columns(self, func):\\n        \"\"\"\\n        Get new SparseDataFrame applying func to each columns\\n        \"\"\"\\n\\n        new_data = {col: func(series)\\n                    for col, series in self.items()}\\n\\n        return self._constructor(\\n            data=new_data, index=self.index, columns=self.columns,\\n            default_fill_value=self.default_fill_value).__finalize__(self)',\n 'def copy(self, deep=True):\\n        \"\"\"\\n        Make a copy of this SparseDataFrame\\n        \"\"\"\\n        result = super().copy(deep=deep)\\n        result._default_fill_value = self._default_fill_value\\n        result._default_kind = self._default_kind\\n        return result',\n 'def density(self):\\n        \"\"\"\\n        Ratio of non-sparse points to total (dense) data points\\n        represented in the frame\\n        \"\"\"\\n        tot_nonsparse = sum(ser.sp_index.npoints\\n                            for _, ser in self.items())\\n        tot = len(self.index) * len(self.columns)\\n        return tot_nonsparse / float(tot)',\n 'def _sanitize_column(self, key, value, **kwargs):\\n        \"\"\"\\n        Creates a new SparseArray from the input value.\\n\\n        Parameters\\n        ----------\\n        key : object\\n        value : scalar, Series, or array-like\\n        kwargs : dict\\n\\n        Returns\\n        -------\\n        sanitized_column : SparseArray\\n\\n        \"\"\"\\n        def sp_maker(x, index=None):\\n            return SparseArray(x, index=index,\\n                               fill_value=self._default_fill_value,\\n                               kind=self._default_kind)\\n        if isinstance(value, SparseSeries):\\n            clean = value.reindex(self.index).as_sparse_array(\\n                fill_value=self._default_fill_value, kind=self._default_kind)\\n\\n        elif isinstance(value, SparseArray):\\n            if len(value) != len(self.index):\\n                raise ValueError(\\'Length of values does not match \\'\\n                                 \\'length of index\\')\\n            clean = value\\n\\n        elif hasattr(value, \\'__iter__\\'):\\n            if isinstance(value, Series):\\n                clean = value.reindex(self.index)\\n                if not isinstance(value, SparseSeries):\\n                    clean = sp_maker(clean)\\n            else:\\n                if len(value) != len(self.index):\\n                    raise ValueError(\\'Length of values does not match \\'\\n                                     \\'length of index\\')\\n                clean = sp_maker(value)\\n\\n        # Scalar\\n        else:\\n            clean = sp_maker(value, self.index)\\n\\n        # always return a SparseArray!\\n        return clean',\n 'def xs(self, key, axis=0, copy=False):\\n        \"\"\"\\n        Returns a row (cross-section) from the SparseDataFrame as a Series\\n        object.\\n\\n        Parameters\\n        ----------\\n        key : some index contained in the index\\n\\n        Returns\\n        -------\\n        xs : Series\\n        \"\"\"\\n        if axis == 1:\\n            data = self[key]\\n            return data\\n\\n        i = self.index.get_loc(key)\\n        data = self.take([i]).get_values()[0]\\n        return Series(data, index=self.columns)',\n 'def transpose(self, *args, **kwargs):\\n        \"\"\"\\n        Returns a DataFrame with the rows/columns switched.\\n        \"\"\"\\n        nv.validate_transpose(args, kwargs)\\n        return self._constructor(\\n            self.values.T, index=self.columns, columns=self.index,\\n            default_fill_value=self._default_fill_value,\\n            default_kind=self._default_kind).__finalize__(self)',\n 'def cumsum(self, axis=0, *args, **kwargs):\\n        \"\"\"\\n        Return SparseDataFrame of cumulative sums over requested axis.\\n\\n        Parameters\\n        ----------\\n        axis : {0, 1}\\n            0 for row-wise, 1 for column-wise\\n\\n        Returns\\n        -------\\n        y : SparseDataFrame\\n        \"\"\"\\n        nv.validate_cumsum(args, kwargs)\\n\\n        if axis is None:\\n            axis = self._stat_axis_number\\n\\n        return self.apply(lambda x: x.cumsum(), axis=axis)',\n 'def apply(self, func, axis=0, broadcast=None, reduce=None,\\n              result_type=None):\\n        \"\"\"\\n        Analogous to DataFrame.apply, for SparseDataFrame\\n\\n        Parameters\\n        ----------\\n        func : function\\n            Function to apply to each column\\n        axis : {0, 1, \\'index\\', \\'columns\\'}\\n        broadcast : bool, default False\\n            For aggregation functions, return object of same size with values\\n            propagated\\n\\n            .. deprecated:: 0.23.0\\n               This argument will be removed in a future version, replaced\\n               by result_type=\\'broadcast\\'.\\n\\n        reduce : boolean or None, default None\\n            Try to apply reduction procedures. If the DataFrame is empty,\\n            apply will use reduce to determine whether the result should be a\\n            Series or a DataFrame. If reduce is None (the default), apply\\'s\\n            return value will be guessed by calling func an empty Series (note:\\n            while guessing, exceptions raised by func will be ignored). If\\n            reduce is True a Series will always be returned, and if False a\\n            DataFrame will always be returned.\\n\\n            .. deprecated:: 0.23.0\\n               This argument will be removed in a future version, replaced\\n               by result_type=\\'reduce\\'.\\n\\n        result_type : {\\'expand\\', \\'reduce\\', \\'broadcast, None}\\n            These only act when axis=1 {columns}:\\n\\n            * \\'expand\\' : list-like results will be turned into columns.\\n            * \\'reduce\\' : return a Series if possible rather than expanding\\n              list-like results. This is the opposite to \\'expand\\'.\\n            * \\'broadcast\\' : results will be broadcast to the original shape\\n              of the frame, the original index & columns will be retained.\\n\\n            The default behaviour (None) depends on the return value of the\\n            applied function: list-like results will be returned as a Series\\n            of those. However if the apply function returns a Series these\\n            are expanded to columns.\\n\\n            .. versionadded:: 0.23.0\\n\\n        Returns\\n        -------\\n        applied : Series or SparseDataFrame\\n        \"\"\"\\n        if not len(self.columns):\\n            return self\\n        axis = self._get_axis_number(axis)\\n\\n        if isinstance(func, np.ufunc):\\n            new_series = {}\\n            for k, v in self.items():\\n                applied = func(v)\\n                applied.fill_value = func(v.fill_value)\\n                new_series[k] = applied\\n            return self._constructor(\\n                new_series, index=self.index, columns=self.columns,\\n                default_fill_value=self._default_fill_value,\\n                default_kind=self._default_kind).__finalize__(self)\\n\\n        from pandas.core.apply import frame_apply\\n        op = frame_apply(self,\\n                         func=func,\\n                         axis=axis,\\n                         reduce=reduce,\\n                         broadcast=broadcast,\\n                         result_type=result_type)\\n        return op.get_result()',\n 'def conda_package_to_pip(package):\\n    \"\"\"\\n    Convert a conda package to its pip equivalent.\\n\\n    In most cases they are the same, those are the exceptions:\\n    - Packages that should be excluded (in `EXCLUDE`)\\n    - Packages that should be renamed (in `RENAME`)\\n    - A package requiring a specific version, in conda is defined with a single\\n      equal (e.g. ``pandas=1.0``) and in pip with two (e.g. ``pandas==1.0``)\\n    \"\"\"\\n    if package in EXCLUDE:\\n        return\\n\\n    package = re.sub(\\'(?<=[^<>])=\\', \\'==\\', package).strip()\\n    for compare in (\\'<=\\', \\'>=\\', \\'==\\'):\\n        if compare not in package:\\n            continue\\n\\n        pkg, version = package.split(compare)\\n\\n        if pkg in RENAME:\\n            return \\'\\'.join((RENAME[pkg], compare, version))\\n\\n        break\\n\\n    return package',\n 'def main(conda_fname, pip_fname, compare=False):\\n    \"\"\"\\n    Generate the pip dependencies file from the conda file, or compare that\\n    they are synchronized (``compare=True``).\\n\\n    Parameters\\n    ----------\\n    conda_fname : str\\n        Path to the conda file with dependencies (e.g. `environment.yml`).\\n    pip_fname : str\\n        Path to the pip file with dependencies (e.g. `requirements-dev.txt`).\\n    compare : bool, default False\\n        Whether to generate the pip file (``False``) or to compare if the\\n        pip file has been generated with this script and the last version\\n        of the conda file (``True``).\\n\\n    Returns\\n    -------\\n    bool\\n        True if the comparison fails, False otherwise\\n    \"\"\"\\n    with open(conda_fname) as conda_fd:\\n        deps = yaml.safe_load(conda_fd)[\\'dependencies\\']\\n\\n    pip_deps = []\\n    for dep in deps:\\n        if isinstance(dep, str):\\n            conda_dep = conda_package_to_pip(dep)\\n            if conda_dep:\\n                pip_deps.append(conda_dep)\\n        elif isinstance(dep, dict) and len(dep) == 1 and \\'pip\\' in dep:\\n            pip_deps += dep[\\'pip\\']\\n        else:\\n            raise ValueError(\\'Unexpected dependency {}\\'.format(dep))\\n\\n    pip_content = \\'\\\\n\\'.join(pip_deps)\\n\\n    if compare:\\n        with open(pip_fname) as pip_fd:\\n            return pip_content != pip_fd.read()\\n    else:\\n        with open(pip_fname, \\'w\\') as pip_fd:\\n            pip_fd.write(pip_content)\\n        return False',\n 'def maybe_convert_platform(values):\\n    \"\"\" try to do platform conversion, allow ndarray or list here \"\"\"\\n\\n    if isinstance(values, (list, tuple)):\\n        values = construct_1d_object_array_from_listlike(list(values))\\n    if getattr(values, \\'dtype\\', None) == np.object_:\\n        if hasattr(values, \\'_values\\'):\\n            values = values._values\\n        values = lib.maybe_convert_objects(values)\\n\\n    return values',\n 'def is_nested_object(obj):\\n    \"\"\"\\n    return a boolean if we have a nested object, e.g. a Series with 1 or\\n    more Series elements\\n\\n    This may not be necessarily be performant.\\n\\n    \"\"\"\\n\\n    if isinstance(obj, ABCSeries) and is_object_dtype(obj):\\n\\n        if any(isinstance(v, ABCSeries) for v in obj.values):\\n            return True\\n\\n    return False',\n 'def maybe_downcast_to_dtype(result, dtype):\\n    \"\"\" try to cast to the specified dtype (e.g. convert back to bool/int\\n    or could be an astype of float64->float32\\n    \"\"\"\\n\\n    if is_scalar(result):\\n        return result\\n\\n    def trans(x):\\n        return x\\n\\n    if isinstance(dtype, str):\\n        if dtype == \\'infer\\':\\n            inferred_type = lib.infer_dtype(ensure_object(result.ravel()),\\n                                            skipna=False)\\n            if inferred_type == \\'boolean\\':\\n                dtype = \\'bool\\'\\n            elif inferred_type == \\'integer\\':\\n                dtype = \\'int64\\'\\n            elif inferred_type == \\'datetime64\\':\\n                dtype = \\'datetime64[ns]\\'\\n            elif inferred_type == \\'timedelta64\\':\\n                dtype = \\'timedelta64[ns]\\'\\n\\n            # try to upcast here\\n            elif inferred_type == \\'floating\\':\\n                dtype = \\'int64\\'\\n                if issubclass(result.dtype.type, np.number):\\n\\n                    def trans(x):  # noqa\\n                        return x.round()\\n            else:\\n                dtype = \\'object\\'\\n\\n    if isinstance(dtype, str):\\n        dtype = np.dtype(dtype)\\n\\n    try:\\n\\n        # don\\'t allow upcasts here (except if empty)\\n        if dtype.kind == result.dtype.kind:\\n            if (result.dtype.itemsize <= dtype.itemsize and\\n                    np.prod(result.shape)):\\n                return result\\n\\n        if is_bool_dtype(dtype) or is_integer_dtype(dtype):\\n\\n            # if we don\\'t have any elements, just astype it\\n            if not np.prod(result.shape):\\n                return trans(result).astype(dtype)\\n\\n            # do a test on the first element, if it fails then we are done\\n            r = result.ravel()\\n            arr = np.array([r[0]])\\n\\n            # if we have any nulls, then we are done\\n            if (isna(arr).any() or\\n                    not np.allclose(arr, trans(arr).astype(dtype), rtol=0)):\\n                return result\\n\\n            # a comparable, e.g. a Decimal may slip in here\\n            elif not isinstance(r[0], (np.integer, np.floating, np.bool, int,\\n                                       float, bool)):\\n                return result\\n\\n            if (issubclass(result.dtype.type, (np.object_, np.number)) and\\n                    notna(result).all()):\\n                new_result = trans(result).astype(dtype)\\n                try:\\n                    if np.allclose(new_result, result, rtol=0):\\n                        return new_result\\n                except Exception:\\n\\n                    # comparison of an object dtype with a number type could\\n                    # hit here\\n                    if (new_result == result).all():\\n                        return new_result\\n        elif (issubclass(dtype.type, np.floating) and\\n                not is_bool_dtype(result.dtype)):\\n            return result.astype(dtype)\\n\\n        # a datetimelike\\n        # GH12821, iNaT is casted to float\\n        elif dtype.kind in [\\'M\\', \\'m\\'] and result.dtype.kind in [\\'i\\', \\'f\\']:\\n            try:\\n                result = result.astype(dtype)\\n            except Exception:\\n                if dtype.tz:\\n                    # convert to datetime and change timezone\\n                    from pandas import to_datetime\\n                    result = to_datetime(result).tz_localize(\\'utc\\')\\n                    result = result.tz_convert(dtype.tz)\\n\\n        elif dtype.type == Period:\\n            # TODO(DatetimeArray): merge with previous elif\\n            from pandas.core.arrays import PeriodArray\\n\\n            return PeriodArray(result, freq=dtype.freq)\\n\\n    except Exception:\\n        pass\\n\\n    return result',\n 'def maybe_upcast_putmask(result, mask, other):\\n    \"\"\"\\n    A safe version of putmask that potentially upcasts the result.\\n    The result is replaced with the first N elements of other,\\n    where N is the number of True values in mask.\\n    If the length of other is shorter than N, other will be repeated.\\n\\n    Parameters\\n    ----------\\n    result : ndarray\\n        The destination array. This will be mutated in-place if no upcasting is\\n        necessary.\\n    mask : boolean ndarray\\n    other : ndarray or scalar\\n        The source array or value\\n\\n    Returns\\n    -------\\n    result : ndarray\\n    changed : boolean\\n        Set to true if the result array was upcasted\\n\\n    Examples\\n    --------\\n    >>> result, _ = maybe_upcast_putmask(np.arange(1,6),\\n    np.array([False, True, False, True, True]), np.arange(21,23))\\n    >>> result\\n    array([1, 21, 3, 22, 21])\\n    \"\"\"\\n\\n    if not isinstance(result, np.ndarray):\\n        raise ValueError(\"The result input must be a ndarray.\")\\n\\n    if mask.any():\\n        # Two conversions for date-like dtypes that can\\'t be done automatically\\n        # in np.place:\\n        #   NaN -> NaT\\n        #   integer or integer array -> date-like array\\n        if is_datetimelike(result.dtype):\\n            if is_scalar(other):\\n                if isna(other):\\n                    other = result.dtype.type(\\'nat\\')\\n                elif is_integer(other):\\n                    other = np.array(other, dtype=result.dtype)\\n            elif is_integer_dtype(other):\\n                other = np.array(other, dtype=result.dtype)\\n\\n        def changeit():\\n\\n            # try to directly set by expanding our array to full\\n            # length of the boolean\\n            try:\\n                om = other[mask]\\n                om_at = om.astype(result.dtype)\\n                if (om == om_at).all():\\n                    new_result = result.values.copy()\\n                    new_result[mask] = om_at\\n                    result[:] = new_result\\n                    return result, False\\n            except Exception:\\n                pass\\n\\n            # we are forced to change the dtype of the result as the input\\n            # isn\\'t compatible\\n            r, _ = maybe_upcast(result, fill_value=other, copy=True)\\n            np.place(r, mask, other)\\n\\n            return r, True\\n\\n        # we want to decide whether place will work\\n        # if we have nans in the False portion of our mask then we need to\\n        # upcast (possibly), otherwise we DON\\'t want to upcast (e.g. if we\\n        # have values, say integers, in the success portion then it\\'s ok to not\\n        # upcast)\\n        new_dtype, _ = maybe_promote(result.dtype, other)\\n        if new_dtype != result.dtype:\\n\\n            # we have a scalar or len 0 ndarray\\n            # and its nan and we are changing some values\\n            if (is_scalar(other) or\\n                    (isinstance(other, np.ndarray) and other.ndim < 1)):\\n                if isna(other):\\n                    return changeit()\\n\\n            # we have an ndarray and the masking has nans in it\\n            else:\\n\\n                if isna(other).any():\\n                    return changeit()\\n\\n        try:\\n            np.place(result, mask, other)\\n        except Exception:\\n            return changeit()\\n\\n    return result, False',\n 'def infer_dtype_from(val, pandas_dtype=False):\\n    \"\"\"\\n    interpret the dtype from a scalar or array. This is a convenience\\n    routines to infer dtype from a scalar or an array\\n\\n    Parameters\\n    ----------\\n    pandas_dtype : bool, default False\\n        whether to infer dtype including pandas extension types.\\n        If False, scalar/array belongs to pandas extension types is inferred as\\n        object\\n    \"\"\"\\n    if is_scalar(val):\\n        return infer_dtype_from_scalar(val, pandas_dtype=pandas_dtype)\\n    return infer_dtype_from_array(val, pandas_dtype=pandas_dtype)',\n 'def infer_dtype_from_scalar(val, pandas_dtype=False):\\n    \"\"\"\\n    interpret the dtype from a scalar\\n\\n    Parameters\\n    ----------\\n    pandas_dtype : bool, default False\\n        whether to infer dtype including pandas extension types.\\n        If False, scalar belongs to pandas extension types is inferred as\\n        object\\n    \"\"\"\\n\\n    dtype = np.object_\\n\\n    # a 1-element ndarray\\n    if isinstance(val, np.ndarray):\\n        msg = \"invalid ndarray passed to infer_dtype_from_scalar\"\\n        if val.ndim != 0:\\n            raise ValueError(msg)\\n\\n        dtype = val.dtype\\n        val = val.item()\\n\\n    elif isinstance(val, str):\\n\\n        # If we create an empty array using a string to infer\\n        # the dtype, NumPy will only allocate one character per entry\\n        # so this is kind of bad. Alternately we could use np.repeat\\n        # instead of np.empty (but then you still don\\'t want things\\n        # coming out as np.str_!\\n\\n        dtype = np.object_\\n\\n    elif isinstance(val, (np.datetime64, datetime)):\\n        val = tslibs.Timestamp(val)\\n        if val is tslibs.NaT or val.tz is None:\\n            dtype = np.dtype(\\'M8[ns]\\')\\n        else:\\n            if pandas_dtype:\\n                dtype = DatetimeTZDtype(unit=\\'ns\\', tz=val.tz)\\n            else:\\n                # return datetimetz as object\\n                return np.object_, val\\n        val = val.value\\n\\n    elif isinstance(val, (np.timedelta64, timedelta)):\\n        val = tslibs.Timedelta(val).value\\n        dtype = np.dtype(\\'m8[ns]\\')\\n\\n    elif is_bool(val):\\n        dtype = np.bool_\\n\\n    elif is_integer(val):\\n        if isinstance(val, np.integer):\\n            dtype = type(val)\\n        else:\\n            dtype = np.int64\\n\\n    elif is_float(val):\\n        if isinstance(val, np.floating):\\n            dtype = type(val)\\n        else:\\n            dtype = np.float64\\n\\n    elif is_complex(val):\\n        dtype = np.complex_\\n\\n    elif pandas_dtype:\\n        if lib.is_period(val):\\n            dtype = PeriodDtype(freq=val.freq)\\n            val = val.ordinal\\n\\n    return dtype, val',\n 'def infer_dtype_from_array(arr, pandas_dtype=False):\\n    \"\"\"\\n    infer the dtype from a scalar or array\\n\\n    Parameters\\n    ----------\\n    arr : scalar or array\\n    pandas_dtype : bool, default False\\n        whether to infer dtype including pandas extension types.\\n        If False, array belongs to pandas extension types\\n        is inferred as object\\n\\n    Returns\\n    -------\\n    tuple (numpy-compat/pandas-compat dtype, array)\\n\\n    Notes\\n    -----\\n    if pandas_dtype=False. these infer to numpy dtypes\\n    exactly with the exception that mixed / object dtypes\\n    are not coerced by stringifying or conversion\\n\\n    if pandas_dtype=True. datetime64tz-aware/categorical\\n    types will retain there character.\\n\\n    Examples\\n    --------\\n    >>> np.asarray([1, \\'1\\'])\\n    array([\\'1\\', \\'1\\'], dtype=\\'<U21\\')\\n\\n    >>> infer_dtype_from_array([1, \\'1\\'])\\n    (numpy.object_, [1, \\'1\\'])\\n\\n    \"\"\"\\n\\n    if isinstance(arr, np.ndarray):\\n        return arr.dtype, arr\\n\\n    if not is_list_like(arr):\\n        arr = [arr]\\n\\n    if pandas_dtype and is_extension_type(arr):\\n        return arr.dtype, arr\\n\\n    elif isinstance(arr, ABCSeries):\\n        return arr.dtype, np.asarray(arr)\\n\\n    # don\\'t force numpy coerce with nan\\'s\\n    inferred = lib.infer_dtype(arr, skipna=False)\\n    if inferred in [\\'string\\', \\'bytes\\', \\'unicode\\',\\n                    \\'mixed\\', \\'mixed-integer\\']:\\n        return (np.object_, arr)\\n\\n    arr = np.asarray(arr)\\n    return arr.dtype, arr',\n 'def maybe_infer_dtype_type(element):\\n    \"\"\"Try to infer an object\\'s dtype, for use in arithmetic ops\\n\\n    Uses `element.dtype` if that\\'s available.\\n    Objects implementing the iterator protocol are cast to a NumPy array,\\n    and from there the array\\'s type is used.\\n\\n    Parameters\\n    ----------\\n    element : object\\n        Possibly has a `.dtype` attribute, and possibly the iterator\\n        protocol.\\n\\n    Returns\\n    -------\\n    tipo : type\\n\\n    Examples\\n    --------\\n    >>> from collections import namedtuple\\n    >>> Foo = namedtuple(\"Foo\", \"dtype\")\\n    >>> maybe_infer_dtype_type(Foo(np.dtype(\"i8\")))\\n    numpy.int64\\n    \"\"\"\\n    tipo = None\\n    if hasattr(element, \\'dtype\\'):\\n        tipo = element.dtype\\n    elif is_list_like(element):\\n        element = np.asarray(element)\\n        tipo = element.dtype\\n    return tipo',\n 'def maybe_upcast(values, fill_value=np.nan, dtype=None, copy=False):\\n    \"\"\" provide explicit type promotion and coercion\\n\\n    Parameters\\n    ----------\\n    values : the ndarray that we want to maybe upcast\\n    fill_value : what we want to fill with\\n    dtype : if None, then use the dtype of the values, else coerce to this type\\n    copy : if True always make a copy even if no upcast is required\\n    \"\"\"\\n\\n    if is_extension_type(values):\\n        if copy:\\n            values = values.copy()\\n    else:\\n        if dtype is None:\\n            dtype = values.dtype\\n        new_dtype, fill_value = maybe_promote(dtype, fill_value)\\n        if new_dtype != values.dtype:\\n            values = values.astype(new_dtype)\\n        elif copy:\\n            values = values.copy()\\n\\n    return values, fill_value',\n 'def invalidate_string_dtypes(dtype_set):\\n    \"\"\"Change string like dtypes to object for\\n    ``DataFrame.select_dtypes()``.\\n    \"\"\"\\n    non_string_dtypes = dtype_set - {np.dtype(\\'S\\').type, np.dtype(\\'<U\\').type}\\n    if non_string_dtypes != dtype_set:\\n        raise TypeError(\"string dtypes are not allowed, use \\'object\\' instead\")',\n 'def coerce_indexer_dtype(indexer, categories):\\n    \"\"\" coerce the indexer input array to the smallest dtype possible \"\"\"\\n    length = len(categories)\\n    if length < _int8_max:\\n        return ensure_int8(indexer)\\n    elif length < _int16_max:\\n        return ensure_int16(indexer)\\n    elif length < _int32_max:\\n        return ensure_int32(indexer)\\n    return ensure_int64(indexer)',\n 'def coerce_to_dtypes(result, dtypes):\\n    \"\"\"\\n    given a dtypes and a result set, coerce the result elements to the\\n    dtypes\\n    \"\"\"\\n    if len(result) != len(dtypes):\\n        raise AssertionError(\"_coerce_to_dtypes requires equal len arrays\")\\n\\n    def conv(r, dtype):\\n        try:\\n            if isna(r):\\n                pass\\n            elif dtype == _NS_DTYPE:\\n                r = tslibs.Timestamp(r)\\n            elif dtype == _TD_DTYPE:\\n                r = tslibs.Timedelta(r)\\n            elif dtype == np.bool_:\\n                # messy. non 0/1 integers do not get converted.\\n                if is_integer(r) and r not in [0, 1]:\\n                    return int(r)\\n                r = bool(r)\\n            elif dtype.kind == \\'f\\':\\n                r = float(r)\\n            elif dtype.kind == \\'i\\':\\n                r = int(r)\\n        except Exception:\\n            pass\\n\\n        return r\\n\\n    return [conv(r, dtype) for r, dtype in zip(result, dtypes)]',\n 'def astype_nansafe(arr, dtype, copy=True, skipna=False):\\n    \"\"\"\\n    Cast the elements of an array to a given dtype a nan-safe manner.\\n\\n    Parameters\\n    ----------\\n    arr : ndarray\\n    dtype : np.dtype\\n    copy : bool, default True\\n        If False, a view will be attempted but may fail, if\\n        e.g. the item sizes don\\'t align.\\n    skipna: bool, default False\\n        Whether or not we should skip NaN when casting as a string-type.\\n\\n    Raises\\n    ------\\n    ValueError\\n        The dtype was a datetime64/timedelta64 dtype, but it had no unit.\\n    \"\"\"\\n\\n    # dispatch on extension dtype if needed\\n    if is_extension_array_dtype(dtype):\\n        return dtype.construct_array_type()._from_sequence(\\n            arr, dtype=dtype, copy=copy)\\n\\n    if not isinstance(dtype, np.dtype):\\n        dtype = pandas_dtype(dtype)\\n\\n    if issubclass(dtype.type, str):\\n        return lib.astype_str(arr.ravel(),\\n                              skipna=skipna).reshape(arr.shape)\\n\\n    elif is_datetime64_dtype(arr):\\n        if is_object_dtype(dtype):\\n            return tslib.ints_to_pydatetime(arr.view(np.int64))\\n        elif dtype == np.int64:\\n            return arr.view(dtype)\\n\\n        # allow frequency conversions\\n        if dtype.kind == \\'M\\':\\n            return arr.astype(dtype)\\n\\n        raise TypeError(\"cannot astype a datetimelike from [{from_dtype}] \"\\n                        \"to [{to_dtype}]\".format(from_dtype=arr.dtype,\\n                                                 to_dtype=dtype))\\n\\n    elif is_timedelta64_dtype(arr):\\n        if is_object_dtype(dtype):\\n            return tslibs.ints_to_pytimedelta(arr.view(np.int64))\\n        elif dtype == np.int64:\\n            return arr.view(dtype)\\n\\n        if dtype not in [_INT64_DTYPE, _TD_DTYPE]:\\n\\n            # allow frequency conversions\\n            # we return a float here!\\n            if dtype.kind == \\'m\\':\\n                mask = isna(arr)\\n                result = arr.astype(dtype).astype(np.float64)\\n                result[mask] = np.nan\\n                return result\\n        elif dtype == _TD_DTYPE:\\n            return arr.astype(_TD_DTYPE, copy=copy)\\n\\n        raise TypeError(\"cannot astype a timedelta from [{from_dtype}] \"\\n                        \"to [{to_dtype}]\".format(from_dtype=arr.dtype,\\n                                                 to_dtype=dtype))\\n\\n    elif (np.issubdtype(arr.dtype, np.floating) and\\n          np.issubdtype(dtype, np.integer)):\\n\\n        if not np.isfinite(arr).all():\\n            raise ValueError(\\'Cannot convert non-finite values (NA or inf) to \\'\\n                             \\'integer\\')\\n\\n    elif is_object_dtype(arr):\\n\\n        # work around NumPy brokenness, #1987\\n        if np.issubdtype(dtype.type, np.integer):\\n            return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape)\\n\\n        # if we have a datetime/timedelta array of objects\\n        # then coerce to a proper dtype and recall astype_nansafe\\n\\n        elif is_datetime64_dtype(dtype):\\n            from pandas import to_datetime\\n            return astype_nansafe(to_datetime(arr).values, dtype, copy=copy)\\n        elif is_timedelta64_dtype(dtype):\\n            from pandas import to_timedelta\\n            return astype_nansafe(to_timedelta(arr).values, dtype, copy=copy)\\n\\n    if dtype.name in (\"datetime64\", \"timedelta64\"):\\n        msg = (\"The \\'{dtype}\\' dtype has no unit. \"\\n               \"Please pass in \\'{dtype}[ns]\\' instead.\")\\n        raise ValueError(msg.format(dtype=dtype.name))\\n\\n    if copy or is_object_dtype(arr) or is_object_dtype(dtype):\\n        # Explicit copy, or required since NumPy can\\'t view from / to object.\\n        return arr.astype(dtype, copy=True)\\n\\n    return arr.view(dtype)',\n 'def maybe_convert_objects(values, convert_dates=True, convert_numeric=True,\\n                          convert_timedeltas=True, copy=True):\\n    \"\"\" if we have an object dtype, try to coerce dates and/or numbers \"\"\"\\n\\n    # if we have passed in a list or scalar\\n    if isinstance(values, (list, tuple)):\\n        values = np.array(values, dtype=np.object_)\\n    if not hasattr(values, \\'dtype\\'):\\n        values = np.array([values], dtype=np.object_)\\n\\n    # convert dates\\n    if convert_dates and values.dtype == np.object_:\\n\\n        # we take an aggressive stance and convert to datetime64[ns]\\n        if convert_dates == \\'coerce\\':\\n            new_values = maybe_cast_to_datetime(\\n                values, \\'M8[ns]\\', errors=\\'coerce\\')\\n\\n            # if we are all nans then leave me alone\\n            if not isna(new_values).all():\\n                values = new_values\\n\\n        else:\\n            values = lib.maybe_convert_objects(values,\\n                                               convert_datetime=convert_dates)\\n\\n    # convert timedeltas\\n    if convert_timedeltas and values.dtype == np.object_:\\n\\n        if convert_timedeltas == \\'coerce\\':\\n            from pandas.core.tools.timedeltas import to_timedelta\\n            new_values = to_timedelta(values, errors=\\'coerce\\')\\n\\n            # if we are all nans then leave me alone\\n            if not isna(new_values).all():\\n                values = new_values\\n\\n        else:\\n            values = lib.maybe_convert_objects(\\n                values, convert_timedelta=convert_timedeltas)\\n\\n    # convert to numeric\\n    if values.dtype == np.object_:\\n        if convert_numeric:\\n            try:\\n                new_values = lib.maybe_convert_numeric(values, set(),\\n                                                       coerce_numeric=True)\\n\\n                # if we are all nans then leave me alone\\n                if not isna(new_values).all():\\n                    values = new_values\\n\\n            except Exception:\\n                pass\\n        else:\\n            # soft-conversion\\n            values = lib.maybe_convert_objects(values)\\n\\n    values = values.copy() if copy else values\\n\\n    return values',\n 'def soft_convert_objects(values, datetime=True, numeric=True, timedelta=True,\\n                         coerce=False, copy=True):\\n    \"\"\" if we have an object dtype, try to coerce dates and/or numbers \"\"\"\\n\\n    conversion_count = sum((datetime, numeric, timedelta))\\n    if conversion_count == 0:\\n        raise ValueError(\\'At least one of datetime, numeric or timedelta must \\'\\n                         \\'be True.\\')\\n    elif conversion_count > 1 and coerce:\\n        raise ValueError(\"Only one of \\'datetime\\', \\'numeric\\' or \"\\n                         \"\\'timedelta\\' can be True when when coerce=True.\")\\n\\n    if isinstance(values, (list, tuple)):\\n        # List or scalar\\n        values = np.array(values, dtype=np.object_)\\n    elif not hasattr(values, \\'dtype\\'):\\n        values = np.array([values], dtype=np.object_)\\n    elif not is_object_dtype(values.dtype):\\n        # If not object, do not attempt conversion\\n        values = values.copy() if copy else values\\n        return values\\n\\n    # If 1 flag is coerce, ensure 2 others are False\\n    if coerce:\\n        # Immediate return if coerce\\n        if datetime:\\n            from pandas import to_datetime\\n            return to_datetime(values, errors=\\'coerce\\').to_numpy()\\n        elif timedelta:\\n            from pandas import to_timedelta\\n            return to_timedelta(values, errors=\\'coerce\\').to_numpy()\\n        elif numeric:\\n            from pandas import to_numeric\\n            return to_numeric(values, errors=\\'coerce\\')\\n\\n    # Soft conversions\\n    if datetime:\\n        # GH 20380, when datetime is beyond year 2262, hence outside\\n        # bound of nanosecond-resolution 64-bit integers.\\n        try:\\n            values = lib.maybe_convert_objects(values,\\n                                               convert_datetime=datetime)\\n        except OutOfBoundsDatetime:\\n            pass\\n\\n    if timedelta and is_object_dtype(values.dtype):\\n        # Object check to ensure only run if previous did not convert\\n        values = lib.maybe_convert_objects(values, convert_timedelta=timedelta)\\n\\n    if numeric and is_object_dtype(values.dtype):\\n        try:\\n            converted = lib.maybe_convert_numeric(values, set(),\\n                                                  coerce_numeric=True)\\n            # If all NaNs, then do not-alter\\n            values = converted if not isna(converted).all() else values\\n            values = values.copy() if copy else values\\n        except Exception:\\n            pass\\n\\n    return values',\n 'def maybe_infer_to_datetimelike(value, convert_dates=False):\\n    \"\"\"\\n    we might have a array (or single object) that is datetime like,\\n    and no dtype is passed don\\'t change the value unless we find a\\n    datetime/timedelta set\\n\\n    this is pretty strict in that a datetime/timedelta is REQUIRED\\n    in addition to possible nulls/string likes\\n\\n    Parameters\\n    ----------\\n    value : np.array / Series / Index / list-like\\n    convert_dates : boolean, default False\\n       if True try really hard to convert dates (such as datetime.date), other\\n       leave inferred dtype \\'date\\' alone\\n\\n    \"\"\"\\n\\n    # TODO: why not timedelta?\\n    if isinstance(value, (ABCDatetimeIndex, ABCPeriodIndex,\\n                          ABCDatetimeArray, ABCPeriodArray)):\\n        return value\\n    elif isinstance(value, ABCSeries):\\n        if isinstance(value._values, ABCDatetimeIndex):\\n            return value._values\\n\\n    v = value\\n\\n    if not is_list_like(v):\\n        v = [v]\\n    v = np.array(v, copy=False)\\n\\n    # we only care about object dtypes\\n    if not is_object_dtype(v):\\n        return value\\n\\n    shape = v.shape\\n    if not v.ndim == 1:\\n        v = v.ravel()\\n\\n    if not len(v):\\n        return value\\n\\n    def try_datetime(v):\\n        # safe coerce to datetime64\\n        try:\\n            # GH19671\\n            v = tslib.array_to_datetime(v,\\n                                        require_iso8601=True,\\n                                        errors=\\'raise\\')[0]\\n        except ValueError:\\n\\n            # we might have a sequence of the same-datetimes with tz\\'s\\n            # if so coerce to a DatetimeIndex; if they are not the same,\\n            # then these stay as object dtype, xref GH19671\\n            try:\\n                from pandas._libs.tslibs import conversion\\n                from pandas import DatetimeIndex\\n\\n                values, tz = conversion.datetime_to_datetime64(v)\\n                return DatetimeIndex(values).tz_localize(\\n                    \\'UTC\\').tz_convert(tz=tz)\\n            except (ValueError, TypeError):\\n                pass\\n\\n        except Exception:\\n            pass\\n\\n        return v.reshape(shape)\\n\\n    def try_timedelta(v):\\n        # safe coerce to timedelta64\\n\\n        # will try first with a string & object conversion\\n        from pandas import to_timedelta\\n        try:\\n            return to_timedelta(v)._ndarray_values.reshape(shape)\\n        except Exception:\\n            return v.reshape(shape)\\n\\n    inferred_type = lib.infer_datetimelike_array(ensure_object(v))\\n\\n    if inferred_type == \\'date\\' and convert_dates:\\n        value = try_datetime(v)\\n    elif inferred_type == \\'datetime\\':\\n        value = try_datetime(v)\\n    elif inferred_type == \\'timedelta\\':\\n        value = try_timedelta(v)\\n    elif inferred_type == \\'nat\\':\\n\\n        # if all NaT, return as datetime\\n        if isna(v).all():\\n            value = try_datetime(v)\\n        else:\\n\\n            # We have at least a NaT and a string\\n            # try timedelta first to avoid spurious datetime conversions\\n            # e.g. \\'00:00:01\\' is a timedelta but technically is also a datetime\\n            value = try_timedelta(v)\\n            if lib.infer_dtype(value, skipna=False) in [\\'mixed\\']:\\n                # cannot skip missing values, as NaT implies that the string\\n                # is actually a datetime\\n                value = try_datetime(v)\\n\\n    return value',\n 'def maybe_cast_to_datetime(value, dtype, errors=\\'raise\\'):\\n    \"\"\" try to cast the array/value to a datetimelike dtype, converting float\\n    nan to iNaT\\n    \"\"\"\\n    from pandas.core.tools.timedeltas import to_timedelta\\n    from pandas.core.tools.datetimes import to_datetime\\n\\n    if dtype is not None:\\n        if isinstance(dtype, str):\\n            dtype = np.dtype(dtype)\\n\\n        is_datetime64 = is_datetime64_dtype(dtype)\\n        is_datetime64tz = is_datetime64tz_dtype(dtype)\\n        is_timedelta64 = is_timedelta64_dtype(dtype)\\n\\n        if is_datetime64 or is_datetime64tz or is_timedelta64:\\n\\n            # Force the dtype if needed.\\n            msg = (\"The \\'{dtype}\\' dtype has no unit. \"\\n                   \"Please pass in \\'{dtype}[ns]\\' instead.\")\\n\\n            if is_datetime64 and not is_dtype_equal(dtype, _NS_DTYPE):\\n                if dtype.name in (\\'datetime64\\', \\'datetime64[ns]\\'):\\n                    if dtype.name == \\'datetime64\\':\\n                        raise ValueError(msg.format(dtype=dtype.name))\\n                    dtype = _NS_DTYPE\\n                else:\\n                    raise TypeError(\"cannot convert datetimelike to \"\\n                                    \"dtype [{dtype}]\".format(dtype=dtype))\\n            elif is_datetime64tz:\\n\\n                # our NaT doesn\\'t support tz\\'s\\n                # this will coerce to DatetimeIndex with\\n                # a matching dtype below\\n                if is_scalar(value) and isna(value):\\n                    value = [value]\\n\\n            elif is_timedelta64 and not is_dtype_equal(dtype, _TD_DTYPE):\\n                if dtype.name in (\\'timedelta64\\', \\'timedelta64[ns]\\'):\\n                    if dtype.name == \\'timedelta64\\':\\n                        raise ValueError(msg.format(dtype=dtype.name))\\n                    dtype = _TD_DTYPE\\n                else:\\n                    raise TypeError(\"cannot convert timedeltalike to \"\\n                                    \"dtype [{dtype}]\".format(dtype=dtype))\\n\\n            if is_scalar(value):\\n                if value == iNaT or isna(value):\\n                    value = iNaT\\n            else:\\n                value = np.array(value, copy=False)\\n\\n                # have a scalar array-like (e.g. NaT)\\n                if value.ndim == 0:\\n                    value = iNaT\\n\\n                # we have an array of datetime or timedeltas & nulls\\n                elif np.prod(value.shape) or not is_dtype_equal(value.dtype,\\n                                                                dtype):\\n                    try:\\n                        if is_datetime64:\\n                            value = to_datetime(value, errors=errors)\\n                            # GH 25843: Remove tz information since the dtype\\n                            # didn\\'t specify one\\n                            if value.tz is not None:\\n                                value = value.tz_localize(None)\\n                            value = value._values\\n                        elif is_datetime64tz:\\n                            # The string check can be removed once issue #13712\\n                            # is solved. String data that is passed with a\\n                            # datetime64tz is assumed to be naive which should\\n                            # be localized to the timezone.\\n                            is_dt_string = is_string_dtype(value)\\n                            value = to_datetime(value, errors=errors).array\\n                            if is_dt_string:\\n                                # Strings here are naive, so directly localize\\n                                value = value.tz_localize(dtype.tz)\\n                            else:\\n                                # Numeric values are UTC at this point,\\n                                # so localize and convert\\n                                value = (value.tz_localize(\\'UTC\\')\\n                                         .tz_convert(dtype.tz))\\n                        elif is_timedelta64:\\n                            value = to_timedelta(value, errors=errors)._values\\n                    except (AttributeError, ValueError, TypeError):\\n                        pass\\n\\n        # coerce datetimelike to object\\n        elif is_datetime64_dtype(value) and not is_datetime64_dtype(dtype):\\n            if is_object_dtype(dtype):\\n                if value.dtype != _NS_DTYPE:\\n                    value = value.astype(_NS_DTYPE)\\n                ints = np.asarray(value).view(\\'i8\\')\\n                return tslib.ints_to_pydatetime(ints)\\n\\n            # we have a non-castable dtype that was passed\\n            raise TypeError(\\'Cannot cast datetime64 to {dtype}\\'\\n                            .format(dtype=dtype))\\n\\n    else:\\n\\n        is_array = isinstance(value, np.ndarray)\\n\\n        # catch a datetime/timedelta that is not of ns variety\\n        # and no coercion specified\\n        if is_array and value.dtype.kind in [\\'M\\', \\'m\\']:\\n            dtype = value.dtype\\n\\n            if dtype.kind == \\'M\\' and dtype != _NS_DTYPE:\\n                value = value.astype(_NS_DTYPE)\\n\\n            elif dtype.kind == \\'m\\' and dtype != _TD_DTYPE:\\n                value = to_timedelta(value)\\n\\n        # only do this if we have an array and the dtype of the array is not\\n        # setup already we are not an integer/object, so don\\'t bother with this\\n        # conversion\\n        elif not (is_array and not (issubclass(value.dtype.type, np.integer) or\\n                                    value.dtype == np.object_)):\\n            value = maybe_infer_to_datetimelike(value)\\n\\n    return value',\n 'def find_common_type(types):\\n    \"\"\"\\n    Find a common data type among the given dtypes.\\n\\n    Parameters\\n    ----------\\n    types : list of dtypes\\n\\n    Returns\\n    -------\\n    pandas extension or numpy dtype\\n\\n    See Also\\n    --------\\n    numpy.find_common_type\\n\\n    \"\"\"\\n\\n    if len(types) == 0:\\n        raise ValueError(\\'no types given\\')\\n\\n    first = types[0]\\n\\n    # workaround for find_common_type([np.dtype(\\'datetime64[ns]\\')] * 2)\\n    # => object\\n    if all(is_dtype_equal(first, t) for t in types[1:]):\\n        return first\\n\\n    if any(isinstance(t, (PandasExtensionDtype, ExtensionDtype))\\n           for t in types):\\n        return np.object\\n\\n    # take lowest unit\\n    if all(is_datetime64_dtype(t) for t in types):\\n        return np.dtype(\\'datetime64[ns]\\')\\n    if all(is_timedelta64_dtype(t) for t in types):\\n        return np.dtype(\\'timedelta64[ns]\\')\\n\\n    # don\\'t mix bool / int or float or complex\\n    # this is different from numpy, which casts bool with float/int as int\\n    has_bools = any(is_bool_dtype(t) for t in types)\\n    if has_bools:\\n        for t in types:\\n            if is_integer_dtype(t) or is_float_dtype(t) or is_complex_dtype(t):\\n                return np.object\\n\\n    return np.find_common_type(types, [])',\n 'def cast_scalar_to_array(shape, value, dtype=None):\\n    \"\"\"\\n    create np.ndarray of specified shape and dtype, filled with values\\n\\n    Parameters\\n    ----------\\n    shape : tuple\\n    value : scalar value\\n    dtype : np.dtype, optional\\n        dtype to coerce\\n\\n    Returns\\n    -------\\n    ndarray of shape, filled with value, of specified / inferred dtype\\n\\n    \"\"\"\\n\\n    if dtype is None:\\n        dtype, fill_value = infer_dtype_from_scalar(value)\\n    else:\\n        fill_value = value\\n\\n    values = np.empty(shape, dtype=dtype)\\n    values.fill(fill_value)\\n\\n    return values',\n 'def construct_1d_arraylike_from_scalar(value, length, dtype):\\n    \"\"\"\\n    create a np.ndarray / pandas type of specified shape and dtype\\n    filled with values\\n\\n    Parameters\\n    ----------\\n    value : scalar value\\n    length : int\\n    dtype : pandas_dtype / np.dtype\\n\\n    Returns\\n    -------\\n    np.ndarray / pandas type of length, filled with value\\n\\n    \"\"\"\\n    if is_datetime64tz_dtype(dtype):\\n        from pandas import DatetimeIndex\\n        subarr = DatetimeIndex([value] * length, dtype=dtype)\\n    elif is_categorical_dtype(dtype):\\n        from pandas import Categorical\\n        subarr = Categorical([value] * length, dtype=dtype)\\n    else:\\n        if not isinstance(dtype, (np.dtype, type(np.dtype))):\\n            dtype = dtype.dtype\\n\\n        if length and is_integer_dtype(dtype) and isna(value):\\n            # coerce if we have nan for an integer dtype\\n            dtype = np.dtype(\\'float64\\')\\n        elif isinstance(dtype, np.dtype) and dtype.kind in (\"U\", \"S\"):\\n            # we need to coerce to object dtype to avoid\\n            # to allow numpy to take our string as a scalar value\\n            dtype = object\\n            if not isna(value):\\n                value = to_str(value)\\n\\n        subarr = np.empty(length, dtype=dtype)\\n        subarr.fill(value)\\n\\n    return subarr',\n 'def construct_1d_object_array_from_listlike(values):\\n    \"\"\"\\n    Transform any list-like object in a 1-dimensional numpy array of object\\n    dtype.\\n\\n    Parameters\\n    ----------\\n    values : any iterable which has a len()\\n\\n    Raises\\n    ------\\n    TypeError\\n        * If `values` does not have a len()\\n\\n    Returns\\n    -------\\n    1-dimensional numpy array of dtype object\\n    \"\"\"\\n    # numpy will try to interpret nested lists as further dimensions, hence\\n    # making a 1D array that contains list-likes is a bit tricky:\\n    result = np.empty(len(values), dtype=\\'object\\')\\n    result[:] = values\\n    return result',\n 'def construct_1d_ndarray_preserving_na(values, dtype=None, copy=False):\\n    \"\"\"\\n    Construct a new ndarray, coercing `values` to `dtype`, preserving NA.\\n\\n    Parameters\\n    ----------\\n    values : Sequence\\n    dtype : numpy.dtype, optional\\n    copy : bool, default False\\n        Note that copies may still be made with ``copy=False`` if casting\\n        is required.\\n\\n    Returns\\n    -------\\n    arr : ndarray[dtype]\\n\\n    Examples\\n    --------\\n    >>> np.array([1.0, 2.0, None], dtype=\\'str\\')\\n    array([\\'1.0\\', \\'2.0\\', \\'None\\'], dtype=\\'<U4\\')\\n\\n    >>> construct_1d_ndarray_preserving_na([1.0, 2.0, None], dtype=\\'str\\')\\n\\n\\n    \"\"\"\\n    subarr = np.array(values, dtype=dtype, copy=copy)\\n\\n    if dtype is not None and dtype.kind in (\"U\", \"S\"):\\n        # GH-21083\\n        # We can\\'t just return np.array(subarr, dtype=\\'str\\') since\\n        # NumPy will convert the non-string objects into strings\\n        # Including NA values. Se we have to go\\n        # string -> object -> update NA, which requires an\\n        # additional pass over the data.\\n        na_values = isna(values)\\n        subarr2 = subarr.astype(object)\\n        subarr2[na_values] = np.asarray(values, dtype=object)[na_values]\\n        subarr = subarr2\\n\\n    return subarr',\n 'def maybe_cast_to_integer_array(arr, dtype, copy=False):\\n    \"\"\"\\n    Takes any dtype and returns the casted version, raising for when data is\\n    incompatible with integer/unsigned integer dtypes.\\n\\n    .. versionadded:: 0.24.0\\n\\n    Parameters\\n    ----------\\n    arr : array-like\\n        The array to cast.\\n    dtype : str, np.dtype\\n        The integer dtype to cast the array to.\\n    copy: boolean, default False\\n        Whether to make a copy of the array before returning.\\n\\n    Returns\\n    -------\\n    int_arr : ndarray\\n        An array of integer or unsigned integer dtype\\n\\n    Raises\\n    ------\\n    OverflowError : the dtype is incompatible with the data\\n    ValueError : loss of precision has occurred during casting\\n\\n    Examples\\n    --------\\n    If you try to coerce negative values to unsigned integers, it raises:\\n\\n    >>> Series([-1], dtype=\"uint64\")\\n    Traceback (most recent call last):\\n        ...\\n    OverflowError: Trying to coerce negative values to unsigned integers\\n\\n    Also, if you try to coerce float values to integers, it raises:\\n\\n    >>> Series([1, 2, 3.5], dtype=\"int64\")\\n    Traceback (most recent call last):\\n        ...\\n    ValueError: Trying to coerce float values to integers\\n    \"\"\"\\n\\n    try:\\n        if not hasattr(arr, \"astype\"):\\n            casted = np.array(arr, dtype=dtype, copy=copy)\\n        else:\\n            casted = arr.astype(dtype, copy=copy)\\n    except OverflowError:\\n        raise OverflowError(\"The elements provided in the data cannot all be \"\\n                            \"casted to the dtype {dtype}\".format(dtype=dtype))\\n\\n    if np.array_equal(arr, casted):\\n        return casted\\n\\n    # We do this casting to allow for proper\\n    # data and dtype checking.\\n    #\\n    # We didn\\'t do this earlier because NumPy\\n    # doesn\\'t handle `uint64` correctly.\\n    arr = np.asarray(arr)\\n\\n    if is_unsigned_integer_dtype(dtype) and (arr < 0).any():\\n        raise OverflowError(\"Trying to coerce negative values \"\\n                            \"to unsigned integers\")\\n\\n    if is_integer_dtype(dtype) and (is_float_dtype(arr) or\\n                                    is_object_dtype(arr)):\\n        raise ValueError(\"Trying to coerce float values to integers\")',\n 'def scatter_plot(data, x, y, by=None, ax=None, figsize=None, grid=False,\\n                 **kwargs):\\n    \"\"\"\\n    Make a scatter plot from two DataFrame columns\\n\\n    Parameters\\n    ----------\\n    data : DataFrame\\n    x : Column name for the x-axis values\\n    y : Column name for the y-axis values\\n    ax : Matplotlib axis object\\n    figsize : A tuple (width, height) in inches\\n    grid : Setting this to True will show the grid\\n    kwargs : other plotting keyword arguments\\n        To be passed to scatter function\\n\\n    Returns\\n    -------\\n    matplotlib.Figure\\n    \"\"\"\\n    import matplotlib.pyplot as plt\\n\\n    kwargs.setdefault(\\'edgecolors\\', \\'none\\')\\n\\n    def plot_group(group, ax):\\n        xvals = group[x].values\\n        yvals = group[y].values\\n        ax.scatter(xvals, yvals, **kwargs)\\n        ax.grid(grid)\\n\\n    if by is not None:\\n        fig = _grouped_plot(plot_group, data, by=by, figsize=figsize, ax=ax)\\n    else:\\n        if ax is None:\\n            fig = plt.figure()\\n            ax = fig.add_subplot(111)\\n        else:\\n            fig = ax.get_figure()\\n        plot_group(data, ax)\\n        ax.set_ylabel(pprint_thing(y))\\n        ax.set_xlabel(pprint_thing(x))\\n\\n        ax.grid(grid)\\n\\n    return fig',\n 'def hist_frame(data, column=None, by=None, grid=True, xlabelsize=None,\\n               xrot=None, ylabelsize=None, yrot=None, ax=None, sharex=False,\\n               sharey=False, figsize=None, layout=None, bins=10, **kwds):\\n    \"\"\"\\n    Make a histogram of the DataFrame\\'s.\\n\\n    A `histogram`_ is a representation of the distribution of data.\\n    This function calls :meth:`matplotlib.pyplot.hist`, on each series in\\n    the DataFrame, resulting in one histogram per column.\\n\\n    .. _histogram: https://en.wikipedia.org/wiki/Histogram\\n\\n    Parameters\\n    ----------\\n    data : DataFrame\\n        The pandas object holding the data.\\n    column : string or sequence\\n        If passed, will be used to limit data to a subset of columns.\\n    by : object, optional\\n        If passed, then used to form histograms for separate groups.\\n    grid : bool, default True\\n        Whether to show axis grid lines.\\n    xlabelsize : int, default None\\n        If specified changes the x-axis label size.\\n    xrot : float, default None\\n        Rotation of x axis labels. For example, a value of 90 displays the\\n        x labels rotated 90 degrees clockwise.\\n    ylabelsize : int, default None\\n        If specified changes the y-axis label size.\\n    yrot : float, default None\\n        Rotation of y axis labels. For example, a value of 90 displays the\\n        y labels rotated 90 degrees clockwise.\\n    ax : Matplotlib axes object, default None\\n        The axes to plot the histogram on.\\n    sharex : bool, default True if ax is None else False\\n        In case subplots=True, share x axis and set some x axis labels to\\n        invisible; defaults to True if ax is None otherwise False if an ax\\n        is passed in.\\n        Note that passing in both an ax and sharex=True will alter all x axis\\n        labels for all subplots in a figure.\\n    sharey : bool, default False\\n        In case subplots=True, share y axis and set some y axis labels to\\n        invisible.\\n    figsize : tuple\\n        The size in inches of the figure to create. Uses the value in\\n        `matplotlib.rcParams` by default.\\n    layout : tuple, optional\\n        Tuple of (rows, columns) for the layout of the histograms.\\n    bins : integer or sequence, default 10\\n        Number of histogram bins to be used. If an integer is given, bins + 1\\n        bin edges are calculated and returned. If bins is a sequence, gives\\n        bin edges, including left edge of first bin and right edge of last\\n        bin. In this case, bins is returned unmodified.\\n    **kwds\\n        All other plotting keyword arguments to be passed to\\n        :meth:`matplotlib.pyplot.hist`.\\n\\n    Returns\\n    -------\\n    matplotlib.AxesSubplot or numpy.ndarray of them\\n\\n    See Also\\n    --------\\n    matplotlib.pyplot.hist : Plot a histogram using matplotlib.\\n\\n    Examples\\n    --------\\n\\n    .. plot::\\n        :context: close-figs\\n\\n        This example draws a histogram based on the length and width of\\n        some animals, displayed in three bins\\n\\n        >>> df = pd.DataFrame({\\n        ...     \\'length\\': [1.5, 0.5, 1.2, 0.9, 3],\\n        ...     \\'width\\': [0.7, 0.2, 0.15, 0.2, 1.1]\\n        ...     }, index= [\\'pig\\', \\'rabbit\\', \\'duck\\', \\'chicken\\', \\'horse\\'])\\n        >>> hist = df.hist(bins=3)\\n    \"\"\"\\n    _raise_if_no_mpl()\\n    _converter._WARN = False\\n    if by is not None:\\n        axes = grouped_hist(data, column=column, by=by, ax=ax, grid=grid,\\n                            figsize=figsize, sharex=sharex, sharey=sharey,\\n                            layout=layout, bins=bins, xlabelsize=xlabelsize,\\n                            xrot=xrot, ylabelsize=ylabelsize,\\n                            yrot=yrot, **kwds)\\n        return axes\\n\\n    if column is not None:\\n        if not isinstance(column, (list, np.ndarray, ABCIndexClass)):\\n            column = [column]\\n        data = data[column]\\n    data = data._get_numeric_data()\\n    naxes = len(data.columns)\\n\\n    fig, axes = _subplots(naxes=naxes, ax=ax, squeeze=False,\\n                          sharex=sharex, sharey=sharey, figsize=figsize,\\n                          layout=layout)\\n    _axes = _flatten(axes)\\n\\n    for i, col in enumerate(com.try_sort(data.columns)):\\n        ax = _axes[i]\\n        ax.hist(data[col].dropna().values, bins=bins, **kwds)\\n        ax.set_title(col)\\n        ax.grid(grid)\\n\\n    _set_ticks_props(axes, xlabelsize=xlabelsize, xrot=xrot,\\n                     ylabelsize=ylabelsize, yrot=yrot)\\n    fig.subplots_adjust(wspace=0.3, hspace=0.3)\\n\\n    return axes',\n 'def hist_series(self, by=None, ax=None, grid=True, xlabelsize=None,\\n                xrot=None, ylabelsize=None, yrot=None, figsize=None,\\n                bins=10, **kwds):\\n    \"\"\"\\n    Draw histogram of the input series using matplotlib.\\n\\n    Parameters\\n    ----------\\n    by : object, optional\\n        If passed, then used to form histograms for separate groups\\n    ax : matplotlib axis object\\n        If not passed, uses gca()\\n    grid : bool, default True\\n        Whether to show axis grid lines\\n    xlabelsize : int, default None\\n        If specified changes the x-axis label size\\n    xrot : float, default None\\n        rotation of x axis labels\\n    ylabelsize : int, default None\\n        If specified changes the y-axis label size\\n    yrot : float, default None\\n        rotation of y axis labels\\n    figsize : tuple, default None\\n        figure size in inches by default\\n    bins : integer or sequence, default 10\\n        Number of histogram bins to be used. If an integer is given, bins + 1\\n        bin edges are calculated and returned. If bins is a sequence, gives\\n        bin edges, including left edge of first bin and right edge of last\\n        bin. In this case, bins is returned unmodified.\\n    bins : integer, default 10\\n        Number of histogram bins to be used\\n    `**kwds` : keywords\\n        To be passed to the actual plotting function\\n\\n    See Also\\n    --------\\n    matplotlib.axes.Axes.hist : Plot a histogram using matplotlib.\\n    \"\"\"\\n    import matplotlib.pyplot as plt\\n\\n    if by is None:\\n        if kwds.get(\\'layout\\', None) is not None:\\n            raise ValueError(\"The \\'layout\\' keyword is not supported when \"\\n                             \"\\'by\\' is None\")\\n        # hack until the plotting interface is a bit more unified\\n        fig = kwds.pop(\\'figure\\', plt.gcf() if plt.get_fignums() else\\n                       plt.figure(figsize=figsize))\\n        if (figsize is not None and tuple(figsize) !=\\n                tuple(fig.get_size_inches())):\\n            fig.set_size_inches(*figsize, forward=True)\\n        if ax is None:\\n            ax = fig.gca()\\n        elif ax.get_figure() != fig:\\n            raise AssertionError(\\'passed axis not bound to passed figure\\')\\n        values = self.dropna().values\\n\\n        ax.hist(values, bins=bins, **kwds)\\n        ax.grid(grid)\\n        axes = np.array([ax])\\n\\n        _set_ticks_props(axes, xlabelsize=xlabelsize, xrot=xrot,\\n                         ylabelsize=ylabelsize, yrot=yrot)\\n\\n    else:\\n        if \\'figure\\' in kwds:\\n            raise ValueError(\"Cannot pass \\'figure\\' when using the \"\\n                             \"\\'by\\' argument, since a new \\'Figure\\' instance \"\\n                             \"will be created\")\\n        axes = grouped_hist(self, by=by, ax=ax, grid=grid, figsize=figsize,\\n                            bins=bins, xlabelsize=xlabelsize, xrot=xrot,\\n                            ylabelsize=ylabelsize, yrot=yrot, **kwds)\\n\\n    if hasattr(axes, \\'ndim\\'):\\n        if axes.ndim == 1 and len(axes) == 1:\\n            return axes[0]\\n    return axes',\n 'def grouped_hist(data, column=None, by=None, ax=None, bins=50, figsize=None,\\n                 layout=None, sharex=False, sharey=False, rot=90, grid=True,\\n                 xlabelsize=None, xrot=None, ylabelsize=None, yrot=None,\\n                 **kwargs):\\n    \"\"\"\\n    Grouped histogram\\n\\n    Parameters\\n    ----------\\n    data : Series/DataFrame\\n    column : object, optional\\n    by : object, optional\\n    ax : axes, optional\\n    bins : int, default 50\\n    figsize : tuple, optional\\n    layout : optional\\n    sharex : bool, default False\\n    sharey : bool, default False\\n    rot : int, default 90\\n    grid : bool, default True\\n    kwargs : dict, keyword arguments passed to matplotlib.Axes.hist\\n\\n    Returns\\n    -------\\n    collection of Matplotlib Axes\\n    \"\"\"\\n    _raise_if_no_mpl()\\n    _converter._WARN = False\\n\\n    def plot_group(group, ax):\\n        ax.hist(group.dropna().values, bins=bins, **kwargs)\\n\\n    xrot = xrot or rot\\n\\n    fig, axes = _grouped_plot(plot_group, data, column=column,\\n                              by=by, sharex=sharex, sharey=sharey, ax=ax,\\n                              figsize=figsize, layout=layout, rot=rot)\\n\\n    _set_ticks_props(axes, xlabelsize=xlabelsize, xrot=xrot,\\n                     ylabelsize=ylabelsize, yrot=yrot)\\n\\n    fig.subplots_adjust(bottom=0.15, top=0.9, left=0.1, right=0.9,\\n                        hspace=0.5, wspace=0.3)\\n    return axes',\n 'def boxplot_frame_groupby(grouped, subplots=True, column=None, fontsize=None,\\n                          rot=0, grid=True, ax=None, figsize=None,\\n                          layout=None, sharex=False, sharey=True, **kwds):\\n    \"\"\"\\n    Make box plots from DataFrameGroupBy data.\\n\\n    Parameters\\n    ----------\\n    grouped : Grouped DataFrame\\n    subplots : bool\\n        * ``False`` - no subplots will be used\\n        * ``True`` - create a subplot for each group\\n    column : column name or list of names, or vector\\n        Can be any valid input to groupby\\n    fontsize : int or string\\n    rot : label rotation angle\\n    grid : Setting this to True will show the grid\\n    ax : Matplotlib axis object, default None\\n    figsize : A tuple (width, height) in inches\\n    layout : tuple (optional)\\n        (rows, columns) for the layout of the plot\\n    sharex : bool, default False\\n        Whether x-axes will be shared among subplots\\n\\n        .. versionadded:: 0.23.1\\n    sharey : bool, default True\\n        Whether y-axes will be shared among subplots\\n\\n        .. versionadded:: 0.23.1\\n    `**kwds` : Keyword Arguments\\n        All other plotting keyword arguments to be passed to\\n        matplotlib\\'s boxplot function\\n\\n    Returns\\n    -------\\n    dict of key/value = group key/DataFrame.boxplot return value\\n    or DataFrame.boxplot return value in case subplots=figures=False\\n\\n    Examples\\n    --------\\n    >>> import itertools\\n    >>> tuples = [t for t in itertools.product(range(1000), range(4))]\\n    >>> index = pd.MultiIndex.from_tuples(tuples, names=[\\'lvl0\\', \\'lvl1\\'])\\n    >>> data = np.random.randn(len(index),4)\\n    >>> df = pd.DataFrame(data, columns=list(\\'ABCD\\'), index=index)\\n    >>>\\n    >>> grouped = df.groupby(level=\\'lvl1\\')\\n    >>> boxplot_frame_groupby(grouped)\\n    >>>\\n    >>> grouped = df.unstack(level=\\'lvl1\\').groupby(level=0, axis=1)\\n    >>> boxplot_frame_groupby(grouped, subplots=False)\\n    \"\"\"\\n    _raise_if_no_mpl()\\n    _converter._WARN = False\\n    if subplots is True:\\n        naxes = len(grouped)\\n        fig, axes = _subplots(naxes=naxes, squeeze=False,\\n                              ax=ax, sharex=sharex, sharey=sharey,\\n                              figsize=figsize, layout=layout)\\n        axes = _flatten(axes)\\n\\n        from pandas.core.series import Series\\n        ret = Series()\\n        for (key, group), ax in zip(grouped, axes):\\n            d = group.boxplot(ax=ax, column=column, fontsize=fontsize,\\n                              rot=rot, grid=grid, **kwds)\\n            ax.set_title(pprint_thing(key))\\n            ret.loc[key] = d\\n        fig.subplots_adjust(bottom=0.15, top=0.9, left=0.1,\\n                            right=0.9, wspace=0.2)\\n    else:\\n        from pandas.core.reshape.concat import concat\\n        keys, frames = zip(*grouped)\\n        if grouped.axis == 0:\\n            df = concat(frames, keys=keys, axis=1)\\n        else:\\n            if len(frames) > 1:\\n                df = frames[0].join(frames[1::])\\n            else:\\n                df = frames[0]\\n        ret = df.boxplot(column=column, fontsize=fontsize, rot=rot,\\n                         grid=grid, ax=ax, figsize=figsize,\\n                         layout=layout, **kwds)\\n    return ret',\n 'def _has_plotted_object(self, ax):\\n        \"\"\"check whether ax has data\"\"\"\\n        return (len(ax.lines) != 0 or\\n                len(ax.artists) != 0 or\\n                len(ax.containers) != 0)',\n 'def result(self):\\n        \"\"\"\\n        Return result axes\\n        \"\"\"\\n        if self.subplots:\\n            if self.layout is not None and not is_list_like(self.ax):\\n                return self.axes.reshape(*self.layout)\\n            else:\\n                return self.axes\\n        else:\\n            sec_true = isinstance(self.secondary_y, bool) and self.secondary_y\\n            all_sec = (is_list_like(self.secondary_y) and\\n                       len(self.secondary_y) == self.nseries)\\n            if (sec_true or all_sec):\\n                # if all data is plotted on secondary, return right axes\\n                return self._get_ax_layer(self.axes[0], primary=False)\\n            else:\\n                return self.axes[0]',\n 'def _post_plot_logic_common(self, ax, data):\\n        \"\"\"Common post process for each axes\"\"\"\\n\\n        def get_label(i):\\n            try:\\n                return pprint_thing(data.index[i])\\n            except Exception:\\n                return \\'\\'\\n\\n        if self.orientation == \\'vertical\\' or self.orientation is None:\\n            if self._need_to_set_index:\\n                xticklabels = [get_label(x) for x in ax.get_xticks()]\\n                ax.set_xticklabels(xticklabels)\\n            self._apply_axis_properties(ax.xaxis, rot=self.rot,\\n                                        fontsize=self.fontsize)\\n            self._apply_axis_properties(ax.yaxis, fontsize=self.fontsize)\\n\\n            if hasattr(ax, \\'right_ax\\'):\\n                self._apply_axis_properties(ax.right_ax.yaxis,\\n                                            fontsize=self.fontsize)\\n\\n        elif self.orientation == \\'horizontal\\':\\n            if self._need_to_set_index:\\n                yticklabels = [get_label(y) for y in ax.get_yticks()]\\n                ax.set_yticklabels(yticklabels)\\n            self._apply_axis_properties(ax.yaxis, rot=self.rot,\\n                                        fontsize=self.fontsize)\\n            self._apply_axis_properties(ax.xaxis, fontsize=self.fontsize)\\n\\n            if hasattr(ax, \\'right_ax\\'):\\n                self._apply_axis_properties(ax.right_ax.yaxis,\\n                                            fontsize=self.fontsize)\\n        else:  # pragma no cover\\n            raise ValueError',\n 'def _adorn_subplots(self):\\n        \"\"\"Common post process unrelated to data\"\"\"\\n        if len(self.axes) > 0:\\n            all_axes = self._get_subplots()\\n            nrows, ncols = self._get_axes_layout()\\n            _handle_shared_axes(axarr=all_axes, nplots=len(all_axes),\\n                                naxes=nrows * ncols, nrows=nrows,\\n                                ncols=ncols, sharex=self.sharex,\\n                                sharey=self.sharey)\\n\\n        for ax in self.axes:\\n            if self.yticks is not None:\\n                ax.set_yticks(self.yticks)\\n\\n            if self.xticks is not None:\\n                ax.set_xticks(self.xticks)\\n\\n            if self.ylim is not None:\\n                ax.set_ylim(self.ylim)\\n\\n            if self.xlim is not None:\\n                ax.set_xlim(self.xlim)\\n\\n            ax.grid(self.grid)\\n\\n        if self.title:\\n            if self.subplots:\\n                if is_list_like(self.title):\\n                    if len(self.title) != self.nseries:\\n                        msg = (\\'The length of `title` must equal the number \\'\\n                               \\'of columns if using `title` of type `list` \\'\\n                               \\'and `subplots=True`.\\\\n\\'\\n                               \\'length of title = {}\\\\n\\'\\n                               \\'number of columns = {}\\').format(\\n                            len(self.title), self.nseries)\\n                        raise ValueError(msg)\\n\\n                    for (ax, title) in zip(self.axes, self.title):\\n                        ax.set_title(title)\\n                else:\\n                    self.fig.suptitle(self.title)\\n            else:\\n                if is_list_like(self.title):\\n                    msg = (\\'Using `title` of type `list` is not supported \\'\\n                           \\'unless `subplots=True` is passed\\')\\n                    raise ValueError(msg)\\n                self.axes[0].set_title(self.title)',\n 'def _apply_axis_properties(self, axis, rot=None, fontsize=None):\\n        \"\"\" Tick creation within matplotlib is reasonably expensive and is\\n            internally deferred until accessed as Ticks are created/destroyed\\n            multiple times per draw. It\\'s therefore beneficial for us to avoid\\n            accessing unless we will act on the Tick.\\n        \"\"\"\\n\\n        if rot is not None or fontsize is not None:\\n            # rot=0 is a valid setting, hence the explicit None check\\n            labels = axis.get_majorticklabels() + axis.get_minorticklabels()\\n            for label in labels:\\n                if rot is not None:\\n                    label.set_rotation(rot)\\n                if fontsize is not None:\\n                    label.set_fontsize(fontsize)',\n 'def _get_ax_layer(cls, ax, primary=True):\\n        \"\"\"get left (primary) or right (secondary) axes\"\"\"\\n        if primary:\\n            return getattr(ax, \\'left_ax\\', ax)\\n        else:\\n            return getattr(ax, \\'right_ax\\', ax)',\n 'def _apply_style_colors(self, colors, kwds, col_num, label):\\n        \"\"\"\\n        Manage style and color based on column number and its label.\\n        Returns tuple of appropriate style and kwds which \"color\" may be added.\\n        \"\"\"\\n        style = None\\n        if self.style is not None:\\n            if isinstance(self.style, list):\\n                try:\\n                    style = self.style[col_num]\\n                except IndexError:\\n                    pass\\n            elif isinstance(self.style, dict):\\n                style = self.style.get(label, style)\\n            else:\\n                style = self.style\\n\\n        has_color = \\'color\\' in kwds or self.colormap is not None\\n        nocolor_style = style is None or re.match(\\'[a-z]+\\', style) is None\\n        if (has_color or self.subplots) and nocolor_style:\\n            kwds[\\'color\\'] = colors[col_num % len(colors)]\\n        return style, kwds',\n 'def _parse_errorbars(self, label, err):\\n        \"\"\"\\n        Look for error keyword arguments and return the actual errorbar data\\n        or return the error DataFrame/dict\\n\\n        Error bars can be specified in several ways:\\n            Series: the user provides a pandas.Series object of the same\\n                    length as the data\\n            ndarray: provides a np.ndarray of the same length as the data\\n            DataFrame/dict: error values are paired with keys matching the\\n                    key in the plotted DataFrame\\n            str: the name of the column within the plotted DataFrame\\n        \"\"\"\\n\\n        if err is None:\\n            return None\\n\\n        def match_labels(data, e):\\n            e = e.reindex(data.index)\\n            return e\\n\\n        # key-matched DataFrame\\n        if isinstance(err, ABCDataFrame):\\n\\n            err = match_labels(self.data, err)\\n        # key-matched dict\\n        elif isinstance(err, dict):\\n            pass\\n\\n        # Series of error values\\n        elif isinstance(err, ABCSeries):\\n            # broadcast error series across data\\n            err = match_labels(self.data, err)\\n            err = np.atleast_2d(err)\\n            err = np.tile(err, (self.nseries, 1))\\n\\n        # errors are a column in the dataframe\\n        elif isinstance(err, str):\\n            evalues = self.data[err].values\\n            self.data = self.data[self.data.columns.drop(err)]\\n            err = np.atleast_2d(evalues)\\n            err = np.tile(err, (self.nseries, 1))\\n\\n        elif is_list_like(err):\\n            if is_iterator(err):\\n                err = np.atleast_2d(list(err))\\n            else:\\n                # raw error values\\n                err = np.atleast_2d(err)\\n\\n            err_shape = err.shape\\n\\n            # asymmetrical error bars\\n            if err.ndim == 3:\\n                if (err_shape[0] != self.nseries) or \\\\\\n                        (err_shape[1] != 2) or \\\\\\n                        (err_shape[2] != len(self.data)):\\n                    msg = \"Asymmetrical error bars should be provided \" + \\\\\\n                        \"with the shape (%u, 2, %u)\" % \\\\\\n                        (self.nseries, len(self.data))\\n                    raise ValueError(msg)\\n\\n            # broadcast errors to each data series\\n            if len(err) == 1:\\n                err = np.tile(err, (self.nseries, 1))\\n\\n        elif is_number(err):\\n            err = np.tile([err], (self.nseries, len(self.data)))\\n\\n        else:\\n            msg = \"No valid {label} detected\".format(label=label)\\n            raise ValueError(msg)\\n\\n        return err',\n 'def _make_plot_keywords(self, kwds, y):\\n        \"\"\"merge BoxPlot/KdePlot properties to passed kwds\"\"\"\\n        # y is required for KdePlot\\n        kwds[\\'bottom\\'] = self.bottom\\n        kwds[\\'bins\\'] = self.bins\\n        return kwds',\n 'def line(self, x=None, y=None, **kwds):\\n        \"\"\"\\n        Plot DataFrame columns as lines.\\n\\n        This function is useful to plot lines using DataFrame\\'s values\\n        as coordinates.\\n\\n        Parameters\\n        ----------\\n        x : int or str, optional\\n            Columns to use for the horizontal axis.\\n            Either the location or the label of the columns to be used.\\n            By default, it will use the DataFrame indices.\\n        y : int, str, or list of them, optional\\n            The values to be plotted.\\n            Either the location or the label of the columns to be used.\\n            By default, it will use the remaining DataFrame numeric columns.\\n        **kwds\\n            Keyword arguments to pass on to :meth:`DataFrame.plot`.\\n\\n        Returns\\n        -------\\n        :class:`matplotlib.axes.Axes` or :class:`numpy.ndarray`\\n            Return an ndarray when ``subplots=True``.\\n\\n        See Also\\n        --------\\n        matplotlib.pyplot.plot : Plot y versus x as lines and/or markers.\\n\\n        Examples\\n        --------\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            The following example shows the populations for some animals\\n            over the years.\\n\\n            >>> df = pd.DataFrame({\\n            ...    \\'pig\\': [20, 18, 489, 675, 1776],\\n            ...    \\'horse\\': [4, 25, 281, 600, 1900]\\n            ...    }, index=[1990, 1997, 2003, 2009, 2014])\\n            >>> lines = df.plot.line()\\n\\n        .. plot::\\n           :context: close-figs\\n\\n           An example with subplots, so an array of axes is returned.\\n\\n           >>> axes = df.plot.line(subplots=True)\\n           >>> type(axes)\\n           <class \\'numpy.ndarray\\'>\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            The following example shows the relationship between both\\n            populations.\\n\\n            >>> lines = df.plot.line(x=\\'pig\\', y=\\'horse\\')\\n        \"\"\"\\n        return self(kind=\\'line\\', x=x, y=y, **kwds)',\n 'def bar(self, x=None, y=None, **kwds):\\n        \"\"\"\\n        Vertical bar plot.\\n\\n        A bar plot is a plot that presents categorical data with\\n        rectangular bars with lengths proportional to the values that they\\n        represent. A bar plot shows comparisons among discrete categories. One\\n        axis of the plot shows the specific categories being compared, and the\\n        other axis represents a measured value.\\n\\n        Parameters\\n        ----------\\n        x : label or position, optional\\n            Allows plotting of one column versus another. If not specified,\\n            the index of the DataFrame is used.\\n        y : label or position, optional\\n            Allows plotting of one column versus another. If not specified,\\n            all numerical columns are used.\\n        **kwds\\n            Additional keyword arguments are documented in\\n            :meth:`DataFrame.plot`.\\n\\n        Returns\\n        -------\\n        matplotlib.axes.Axes or np.ndarray of them\\n            An ndarray is returned with one :class:`matplotlib.axes.Axes`\\n            per column when ``subplots=True``.\\n\\n        See Also\\n        --------\\n        DataFrame.plot.barh : Horizontal bar plot.\\n        DataFrame.plot : Make plots of a DataFrame.\\n        matplotlib.pyplot.bar : Make a bar plot with matplotlib.\\n\\n        Examples\\n        --------\\n        Basic plot.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> df = pd.DataFrame({\\'lab\\':[\\'A\\', \\'B\\', \\'C\\'], \\'val\\':[10, 30, 20]})\\n            >>> ax = df.plot.bar(x=\\'lab\\', y=\\'val\\', rot=0)\\n\\n        Plot a whole dataframe to a bar plot. Each column is assigned a\\n        distinct color, and each row is nested in a group along the\\n        horizontal axis.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\\n            >>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\\n            >>> index = [\\'snail\\', \\'pig\\', \\'elephant\\',\\n            ...          \\'rabbit\\', \\'giraffe\\', \\'coyote\\', \\'horse\\']\\n            >>> df = pd.DataFrame({\\'speed\\': speed,\\n            ...                    \\'lifespan\\': lifespan}, index=index)\\n            >>> ax = df.plot.bar(rot=0)\\n\\n        Instead of nesting, the figure can be split by column with\\n        ``subplots=True``. In this case, a :class:`numpy.ndarray` of\\n        :class:`matplotlib.axes.Axes` are returned.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> axes = df.plot.bar(rot=0, subplots=True)\\n            >>> axes[1].legend(loc=2)  # doctest: +SKIP\\n\\n        Plot a single column.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> ax = df.plot.bar(y=\\'speed\\', rot=0)\\n\\n        Plot only selected categories for the DataFrame.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> ax = df.plot.bar(x=\\'lifespan\\', rot=0)\\n        \"\"\"\\n        return self(kind=\\'bar\\', x=x, y=y, **kwds)',\n 'def barh(self, x=None, y=None, **kwds):\\n        \"\"\"\\n        Make a horizontal bar plot.\\n\\n        A horizontal bar plot is a plot that presents quantitative data with\\n        rectangular bars with lengths proportional to the values that they\\n        represent. A bar plot shows comparisons among discrete categories. One\\n        axis of the plot shows the specific categories being compared, and the\\n        other axis represents a measured value.\\n\\n        Parameters\\n        ----------\\n        x : label or position, default DataFrame.index\\n            Column to be used for categories.\\n        y : label or position, default All numeric columns in dataframe\\n            Columns to be plotted from the DataFrame.\\n        **kwds\\n            Keyword arguments to pass on to :meth:`DataFrame.plot`.\\n\\n        Returns\\n        -------\\n        :class:`matplotlib.axes.Axes` or numpy.ndarray of them\\n\\n        See Also\\n        --------\\n        DataFrame.plot.bar: Vertical bar plot.\\n        DataFrame.plot : Make plots of DataFrame using matplotlib.\\n        matplotlib.axes.Axes.bar : Plot a vertical bar plot using matplotlib.\\n\\n        Examples\\n        --------\\n        Basic example\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> df = pd.DataFrame({\\'lab\\':[\\'A\\', \\'B\\', \\'C\\'], \\'val\\':[10, 30, 20]})\\n            >>> ax = df.plot.barh(x=\\'lab\\', y=\\'val\\')\\n\\n        Plot a whole DataFrame to a horizontal bar plot\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\\n            >>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\\n            >>> index = [\\'snail\\', \\'pig\\', \\'elephant\\',\\n            ...          \\'rabbit\\', \\'giraffe\\', \\'coyote\\', \\'horse\\']\\n            >>> df = pd.DataFrame({\\'speed\\': speed,\\n            ...                    \\'lifespan\\': lifespan}, index=index)\\n            >>> ax = df.plot.barh()\\n\\n        Plot a column of the DataFrame to a horizontal bar plot\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\\n            >>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\\n            >>> index = [\\'snail\\', \\'pig\\', \\'elephant\\',\\n            ...          \\'rabbit\\', \\'giraffe\\', \\'coyote\\', \\'horse\\']\\n            >>> df = pd.DataFrame({\\'speed\\': speed,\\n            ...                    \\'lifespan\\': lifespan}, index=index)\\n            >>> ax = df.plot.barh(y=\\'speed\\')\\n\\n        Plot DataFrame versus the desired column\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\\n            >>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\\n            >>> index = [\\'snail\\', \\'pig\\', \\'elephant\\',\\n            ...          \\'rabbit\\', \\'giraffe\\', \\'coyote\\', \\'horse\\']\\n            >>> df = pd.DataFrame({\\'speed\\': speed,\\n            ...                    \\'lifespan\\': lifespan}, index=index)\\n            >>> ax = df.plot.barh(x=\\'lifespan\\')\\n        \"\"\"\\n        return self(kind=\\'barh\\', x=x, y=y, **kwds)',\n 'def hist(self, by=None, bins=10, **kwds):\\n        \"\"\"\\n        Draw one histogram of the DataFrame\\'s columns.\\n\\n        A histogram is a representation of the distribution of data.\\n        This function groups the values of all given Series in the DataFrame\\n        into bins and draws all bins in one :class:`matplotlib.axes.Axes`.\\n        This is useful when the DataFrame\\'s Series are in a similar scale.\\n\\n        Parameters\\n        ----------\\n        by : str or sequence, optional\\n            Column in the DataFrame to group by.\\n        bins : int, default 10\\n            Number of histogram bins to be used.\\n        **kwds\\n            Additional keyword arguments are documented in\\n            :meth:`DataFrame.plot`.\\n\\n        Returns\\n        -------\\n        class:`matplotlib.AxesSubplot`\\n            Return a histogram plot.\\n\\n        See Also\\n        --------\\n        DataFrame.hist : Draw histograms per DataFrame\\'s Series.\\n        Series.hist : Draw a histogram with Series\\' data.\\n\\n        Examples\\n        --------\\n        When we draw a dice 6000 times, we expect to get each value around 1000\\n        times. But when we draw two dices and sum the result, the distribution\\n        is going to be quite different. A histogram illustrates those\\n        distributions.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> df = pd.DataFrame(\\n            ...     np.random.randint(1, 7, 6000),\\n            ...     columns = [\\'one\\'])\\n            >>> df[\\'two\\'] = df[\\'one\\'] + np.random.randint(1, 7, 6000)\\n            >>> ax = df.plot.hist(bins=12, alpha=0.5)\\n        \"\"\"\\n        return self(kind=\\'hist\\', by=by, bins=bins, **kwds)',\n 'def area(self, x=None, y=None, **kwds):\\n        \"\"\"\\n        Draw a stacked area plot.\\n\\n        An area plot displays quantitative data visually.\\n        This function wraps the matplotlib area function.\\n\\n        Parameters\\n        ----------\\n        x : label or position, optional\\n            Coordinates for the X axis. By default uses the index.\\n        y : label or position, optional\\n            Column to plot. By default uses all columns.\\n        stacked : bool, default True\\n            Area plots are stacked by default. Set to False to create a\\n            unstacked plot.\\n        **kwds : optional\\n            Additional keyword arguments are documented in\\n            :meth:`DataFrame.plot`.\\n\\n        Returns\\n        -------\\n        matplotlib.axes.Axes or numpy.ndarray\\n            Area plot, or array of area plots if subplots is True.\\n\\n        See Also\\n        --------\\n        DataFrame.plot : Make plots of DataFrame using matplotlib / pylab.\\n\\n        Examples\\n        --------\\n        Draw an area plot based on basic business metrics:\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> df = pd.DataFrame({\\n            ...     \\'sales\\': [3, 2, 3, 9, 10, 6],\\n            ...     \\'signups\\': [5, 5, 6, 12, 14, 13],\\n            ...     \\'visits\\': [20, 42, 28, 62, 81, 50],\\n            ... }, index=pd.date_range(start=\\'2018/01/01\\', end=\\'2018/07/01\\',\\n            ...                        freq=\\'M\\'))\\n            >>> ax = df.plot.area()\\n\\n        Area plots are stacked by default. To produce an unstacked plot,\\n        pass ``stacked=False``:\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> ax = df.plot.area(stacked=False)\\n\\n        Draw an area plot for a single column:\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> ax = df.plot.area(y=\\'sales\\')\\n\\n        Draw with a different `x`:\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> df = pd.DataFrame({\\n            ...     \\'sales\\': [3, 2, 3],\\n            ...     \\'visits\\': [20, 42, 28],\\n            ...     \\'day\\': [1, 2, 3],\\n            ... })\\n            >>> ax = df.plot.area(x=\\'day\\')\\n        \"\"\"\\n        return self(kind=\\'area\\', x=x, y=y, **kwds)',\n 'def scatter(self, x, y, s=None, c=None, **kwds):\\n        \"\"\"\\n        Create a scatter plot with varying marker point size and color.\\n\\n        The coordinates of each point are defined by two dataframe columns and\\n        filled circles are used to represent each point. This kind of plot is\\n        useful to see complex correlations between two variables. Points could\\n        be for instance natural 2D coordinates like longitude and latitude in\\n        a map or, in general, any pair of metrics that can be plotted against\\n        each other.\\n\\n        Parameters\\n        ----------\\n        x : int or str\\n            The column name or column position to be used as horizontal\\n            coordinates for each point.\\n        y : int or str\\n            The column name or column position to be used as vertical\\n            coordinates for each point.\\n        s : scalar or array_like, optional\\n            The size of each point. Possible values are:\\n\\n            - A single scalar so all points have the same size.\\n\\n            - A sequence of scalars, which will be used for each point\\'s size\\n              recursively. For instance, when passing [2,14] all points size\\n              will be either 2 or 14, alternatively.\\n\\n        c : str, int or array_like, optional\\n            The color of each point. Possible values are:\\n\\n            - A single color string referred to by name, RGB or RGBA code,\\n              for instance \\'red\\' or \\'#a98d19\\'.\\n\\n            - A sequence of color strings referred to by name, RGB or RGBA\\n              code, which will be used for each point\\'s color recursively. For\\n              instance [\\'green\\',\\'yellow\\'] all points will be filled in green or\\n              yellow, alternatively.\\n\\n            - A column name or position whose values will be used to color the\\n              marker points according to a colormap.\\n\\n        **kwds\\n            Keyword arguments to pass on to :meth:`DataFrame.plot`.\\n\\n        Returns\\n        -------\\n        :class:`matplotlib.axes.Axes` or numpy.ndarray of them\\n\\n        See Also\\n        --------\\n        matplotlib.pyplot.scatter : Scatter plot using multiple input data\\n            formats.\\n\\n        Examples\\n        --------\\n        Let\\'s see how to draw a scatter plot using coordinates from the values\\n        in a DataFrame\\'s columns.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> df = pd.DataFrame([[5.1, 3.5, 0], [4.9, 3.0, 0], [7.0, 3.2, 1],\\n            ...                    [6.4, 3.2, 1], [5.9, 3.0, 2]],\\n            ...                   columns=[\\'length\\', \\'width\\', \\'species\\'])\\n            >>> ax1 = df.plot.scatter(x=\\'length\\',\\n            ...                       y=\\'width\\',\\n            ...                       c=\\'DarkBlue\\')\\n\\n        And now with the color determined by a column as well.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> ax2 = df.plot.scatter(x=\\'length\\',\\n            ...                       y=\\'width\\',\\n            ...                       c=\\'species\\',\\n            ...                       colormap=\\'viridis\\')\\n        \"\"\"\\n        return self(kind=\\'scatter\\', x=x, y=y, c=c, s=s, **kwds)',\n 'def hexbin(self, x, y, C=None, reduce_C_function=None, gridsize=None,\\n               **kwds):\\n        \"\"\"\\n        Generate a hexagonal binning plot.\\n\\n        Generate a hexagonal binning plot of `x` versus `y`. If `C` is `None`\\n        (the default), this is a histogram of the number of occurrences\\n        of the observations at ``(x[i], y[i])``.\\n\\n        If `C` is specified, specifies values at given coordinates\\n        ``(x[i], y[i])``. These values are accumulated for each hexagonal\\n        bin and then reduced according to `reduce_C_function`,\\n        having as default the NumPy\\'s mean function (:meth:`numpy.mean`).\\n        (If `C` is specified, it must also be a 1-D sequence\\n        of the same length as `x` and `y`, or a column label.)\\n\\n        Parameters\\n        ----------\\n        x : int or str\\n            The column label or position for x points.\\n        y : int or str\\n            The column label or position for y points.\\n        C : int or str, optional\\n            The column label or position for the value of `(x, y)` point.\\n        reduce_C_function : callable, default `np.mean`\\n            Function of one argument that reduces all the values in a bin to\\n            a single number (e.g. `np.mean`, `np.max`, `np.sum`, `np.std`).\\n        gridsize : int or tuple of (int, int), default 100\\n            The number of hexagons in the x-direction.\\n            The corresponding number of hexagons in the y-direction is\\n            chosen in a way that the hexagons are approximately regular.\\n            Alternatively, gridsize can be a tuple with two elements\\n            specifying the number of hexagons in the x-direction and the\\n            y-direction.\\n        **kwds\\n            Additional keyword arguments are documented in\\n            :meth:`DataFrame.plot`.\\n\\n        Returns\\n        -------\\n        matplotlib.AxesSubplot\\n            The matplotlib ``Axes`` on which the hexbin is plotted.\\n\\n        See Also\\n        --------\\n        DataFrame.plot : Make plots of a DataFrame.\\n        matplotlib.pyplot.hexbin : Hexagonal binning plot using matplotlib,\\n            the matplotlib function that is used under the hood.\\n\\n        Examples\\n        --------\\n        The following examples are generated with random data from\\n        a normal distribution.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> n = 10000\\n            >>> df = pd.DataFrame({\\'x\\': np.random.randn(n),\\n            ...                    \\'y\\': np.random.randn(n)})\\n            >>> ax = df.plot.hexbin(x=\\'x\\', y=\\'y\\', gridsize=20)\\n\\n        The next example uses `C` and `np.sum` as `reduce_C_function`.\\n        Note that `\\'observations\\'` values ranges from 1 to 5 but the result\\n        plot shows values up to more than 25. This is because of the\\n        `reduce_C_function`.\\n\\n        .. plot::\\n            :context: close-figs\\n\\n            >>> n = 500\\n            >>> df = pd.DataFrame({\\n            ...     \\'coord_x\\': np.random.uniform(-3, 3, size=n),\\n            ...     \\'coord_y\\': np.random.uniform(30, 50, size=n),\\n            ...     \\'observations\\': np.random.randint(1,5, size=n)\\n            ...     })\\n            >>> ax = df.plot.hexbin(x=\\'coord_x\\',\\n            ...                     y=\\'coord_y\\',\\n            ...                     C=\\'observations\\',\\n            ...                     reduce_C_function=np.sum,\\n            ...                     gridsize=10,\\n            ...                     cmap=\"viridis\")\\n        \"\"\"\\n        if reduce_C_function is not None:\\n            kwds[\\'reduce_C_function\\'] = reduce_C_function\\n        if gridsize is not None:\\n            kwds[\\'gridsize\\'] = gridsize\\n        return self(kind=\\'hexbin\\', x=x, y=y, C=C, **kwds)',\n 'def _get_objs_combined_axis(objs, intersect=False, axis=0, sort=True):\\n    \"\"\"\\n    Extract combined index: return intersection or union (depending on the\\n    value of \"intersect\") of indexes on given axis, or None if all objects\\n    lack indexes (e.g. they are numpy arrays).\\n\\n    Parameters\\n    ----------\\n    objs : list of objects\\n        Each object will only be considered if it has a _get_axis\\n        attribute.\\n    intersect : bool, default False\\n        If True, calculate the intersection between indexes. Otherwise,\\n        calculate the union.\\n    axis : {0 or \\'index\\', 1 or \\'outer\\'}, default 0\\n        The axis to extract indexes from.\\n    sort : bool, default True\\n        Whether the result index should come out sorted or not.\\n\\n    Returns\\n    -------\\n    Index\\n    \"\"\"\\n    obs_idxes = [obj._get_axis(axis) for obj in objs\\n                 if hasattr(obj, \\'_get_axis\\')]\\n    if obs_idxes:\\n        return _get_combined_index(obs_idxes, intersect=intersect, sort=sort)',\n 'def _get_distinct_objs(objs):\\n    \"\"\"\\n    Return a list with distinct elements of \"objs\" (different ids).\\n    Preserves order.\\n    \"\"\"\\n    ids = set()\\n    res = []\\n    for obj in objs:\\n        if not id(obj) in ids:\\n            ids.add(id(obj))\\n            res.append(obj)\\n    return res',\n 'def _get_combined_index(indexes, intersect=False, sort=False):\\n    \"\"\"\\n    Return the union or intersection of indexes.\\n\\n    Parameters\\n    ----------\\n    indexes : list of Index or list objects\\n        When intersect=True, do not accept list of lists.\\n    intersect : bool, default False\\n        If True, calculate the intersection between indexes. Otherwise,\\n        calculate the union.\\n    sort : bool, default False\\n        Whether the result index should come out sorted or not.\\n\\n    Returns\\n    -------\\n    Index\\n    \"\"\"\\n\\n    # TODO: handle index names!\\n    indexes = _get_distinct_objs(indexes)\\n    if len(indexes) == 0:\\n        index = Index([])\\n    elif len(indexes) == 1:\\n        index = indexes[0]\\n    elif intersect:\\n        index = indexes[0]\\n        for other in indexes[1:]:\\n            index = index.intersection(other)\\n    else:\\n        index = _union_indexes(indexes, sort=sort)\\n        index = ensure_index(index)\\n\\n    if sort:\\n        try:\\n            index = index.sort_values()\\n        except TypeError:\\n            pass\\n    return index',\n 'def _union_indexes(indexes, sort=True):\\n    \"\"\"\\n    Return the union of indexes.\\n\\n    The behavior of sort and names is not consistent.\\n\\n    Parameters\\n    ----------\\n    indexes : list of Index or list objects\\n    sort : bool, default True\\n        Whether the result index should come out sorted or not.\\n\\n    Returns\\n    -------\\n    Index\\n    \"\"\"\\n    if len(indexes) == 0:\\n        raise AssertionError(\\'Must have at least 1 Index to union\\')\\n    if len(indexes) == 1:\\n        result = indexes[0]\\n        if isinstance(result, list):\\n            result = Index(sorted(result))\\n        return result\\n\\n    indexes, kind = _sanitize_and_check(indexes)\\n\\n    def _unique_indices(inds):\\n        \"\"\"\\n        Convert indexes to lists and concatenate them, removing duplicates.\\n\\n        The final dtype is inferred.\\n\\n        Parameters\\n        ----------\\n        inds : list of Index or list objects\\n\\n        Returns\\n        -------\\n        Index\\n        \"\"\"\\n        def conv(i):\\n            if isinstance(i, Index):\\n                i = i.tolist()\\n            return i\\n\\n        return Index(\\n            lib.fast_unique_multiple_list([conv(i) for i in inds], sort=sort))\\n\\n    if kind == \\'special\\':\\n        result = indexes[0]\\n\\n        if hasattr(result, \\'union_many\\'):\\n            return result.union_many(indexes[1:])\\n        else:\\n            for other in indexes[1:]:\\n                result = result.union(other)\\n            return result\\n    elif kind == \\'array\\':\\n        index = indexes[0]\\n        for other in indexes[1:]:\\n            if not index.equals(other):\\n\\n                if sort is None:\\n                    # TODO: remove once pd.concat sort default changes\\n                    warnings.warn(_sort_msg, FutureWarning, stacklevel=8)\\n                    sort = True\\n\\n                return _unique_indices(indexes)\\n\\n        name = _get_consensus_names(indexes)[0]\\n        if name != index.name:\\n            index = index._shallow_copy(name=name)\\n        return index\\n    else:  # kind=\\'list\\'\\n        return _unique_indices(indexes)',\n 'def _sanitize_and_check(indexes):\\n    \"\"\"\\n    Verify the type of indexes and convert lists to Index.\\n\\n    Cases:\\n\\n    - [list, list, ...]: Return ([list, list, ...], \\'list\\')\\n    - [list, Index, ...]: Return _sanitize_and_check([Index, Index, ...])\\n        Lists are sorted and converted to Index.\\n    - [Index, Index, ...]: Return ([Index, Index, ...], TYPE)\\n        TYPE = \\'special\\' if at least one special type, \\'array\\' otherwise.\\n\\n    Parameters\\n    ----------\\n    indexes : list of Index or list objects\\n\\n    Returns\\n    -------\\n    sanitized_indexes : list of Index or list objects\\n    type : {\\'list\\', \\'array\\', \\'special\\'}\\n    \"\"\"\\n    kinds = list({type(index) for index in indexes})\\n\\n    if list in kinds:\\n        if len(kinds) > 1:\\n            indexes = [Index(com.try_sort(x))\\n                       if not isinstance(x, Index) else\\n                       x for x in indexes]\\n            kinds.remove(list)\\n        else:\\n            return indexes, \\'list\\'\\n\\n    if len(kinds) > 1 or Index not in kinds:\\n        return indexes, \\'special\\'\\n    else:\\n        return indexes, \\'array\\'',\n 'def _get_consensus_names(indexes):\\n    \"\"\"\\n    Give a consensus \\'names\\' to indexes.\\n\\n    If there\\'s exactly one non-empty \\'names\\', return this,\\n    otherwise, return empty.\\n\\n    Parameters\\n    ----------\\n    indexes : list of Index objects\\n\\n    Returns\\n    -------\\n    list\\n        A list representing the consensus \\'names\\' found.\\n    \"\"\"\\n\\n    # find the non-none names, need to tupleify to make\\n    # the set hashable, then reverse on return\\n    consensus_names = {tuple(i.names) for i in indexes\\n                       if com._any_not_none(*i.names)}\\n    if len(consensus_names) == 1:\\n        return list(list(consensus_names)[0])\\n    return [None] * indexes[0].nlevels',\n 'def _all_indexes_same(indexes):\\n    \"\"\"\\n    Determine if all indexes contain the same elements.\\n\\n    Parameters\\n    ----------\\n    indexes : list of Index objects\\n\\n    Returns\\n    -------\\n    bool\\n        True if all indexes contain the same elements, False otherwise.\\n    \"\"\"\\n    first = indexes[0]\\n    for index in indexes[1:]:\\n        if not first.equals(index):\\n            return False\\n    return True',\n 'def _convert_params(sql, params):\\n    \"\"\"Convert SQL and params args to DBAPI2.0 compliant format.\"\"\"\\n    args = [sql]\\n    if params is not None:\\n        if hasattr(params, \\'keys\\'):  # test if params is a mapping\\n            args += [params]\\n        else:\\n            args += [list(params)]\\n    return args',\n 'def _process_parse_dates_argument(parse_dates):\\n    \"\"\"Process parse_dates argument for read_sql functions\"\"\"\\n    # handle non-list entries for parse_dates gracefully\\n    if parse_dates is True or parse_dates is None or parse_dates is False:\\n        parse_dates = []\\n\\n    elif not hasattr(parse_dates, \\'__iter__\\'):\\n        parse_dates = [parse_dates]\\n    return parse_dates',\n 'def _parse_date_columns(data_frame, parse_dates):\\n    \"\"\"\\n    Force non-datetime columns to be read as such.\\n    Supports both string formatted and integer timestamp columns.\\n    \"\"\"\\n    parse_dates = _process_parse_dates_argument(parse_dates)\\n\\n    # we want to coerce datetime64_tz dtypes for now to UTC\\n    # we could in theory do a \\'nice\\' conversion from a FixedOffset tz\\n    # GH11216\\n    for col_name, df_col in data_frame.iteritems():\\n        if is_datetime64tz_dtype(df_col) or col_name in parse_dates:\\n            try:\\n                fmt = parse_dates[col_name]\\n            except TypeError:\\n                fmt = None\\n            data_frame[col_name] = _handle_date_column(df_col, format=fmt)\\n\\n    return data_frame',\n 'def _wrap_result(data, columns, index_col=None, coerce_float=True,\\n                 parse_dates=None):\\n    \"\"\"Wrap result set of query in a DataFrame.\"\"\"\\n\\n    frame = DataFrame.from_records(data, columns=columns,\\n                                   coerce_float=coerce_float)\\n\\n    frame = _parse_date_columns(frame, parse_dates)\\n\\n    if index_col is not None:\\n        frame.set_index(index_col, inplace=True)\\n\\n    return frame',\n 'def execute(sql, con, cur=None, params=None):\\n    \"\"\"\\n    Execute the given SQL query using the provided connection object.\\n\\n    Parameters\\n    ----------\\n    sql : string\\n        SQL query to be executed.\\n    con : SQLAlchemy connectable(engine/connection) or sqlite3 connection\\n        Using SQLAlchemy makes it possible to use any DB supported by the\\n        library.\\n        If a DBAPI2 object, only sqlite3 is supported.\\n    cur : deprecated, cursor is obtained from connection, default: None\\n    params : list or tuple, optional, default: None\\n        List of parameters to pass to execute method.\\n\\n    Returns\\n    -------\\n    Results Iterable\\n    \"\"\"\\n    if cur is None:\\n        pandas_sql = pandasSQL_builder(con)\\n    else:\\n        pandas_sql = pandasSQL_builder(cur, is_cursor=True)\\n    args = _convert_params(sql, params)\\n    return pandas_sql.execute(*args)',\n 'def read_sql_table(table_name, con, schema=None, index_col=None,\\n                   coerce_float=True, parse_dates=None, columns=None,\\n                   chunksize=None):\\n    \"\"\"\\n    Read SQL database table into a DataFrame.\\n\\n    Given a table name and a SQLAlchemy connectable, returns a DataFrame.\\n    This function does not support DBAPI connections.\\n\\n    Parameters\\n    ----------\\n    table_name : str\\n        Name of SQL table in database.\\n    con : SQLAlchemy connectable or str\\n        A database URI could be provided as as str.\\n        SQLite DBAPI connection mode not supported.\\n    schema : str, default None\\n        Name of SQL schema in database to query (if database flavor\\n        supports this). Uses default schema if None (default).\\n    index_col : str or list of str, optional, default: None\\n        Column(s) to set as index(MultiIndex).\\n    coerce_float : bool, default True\\n        Attempts to convert values of non-string, non-numeric objects (like\\n        decimal.Decimal) to floating point. Can result in loss of Precision.\\n    parse_dates : list or dict, default None\\n        - List of column names to parse as dates.\\n        - Dict of ``{column_name: format string}`` where format string is\\n          strftime compatible in case of parsing string times or is one of\\n          (D, s, ns, ms, us) in case of parsing integer timestamps.\\n        - Dict of ``{column_name: arg dict}``, where the arg dict corresponds\\n          to the keyword arguments of :func:`pandas.to_datetime`\\n          Especially useful with databases without native Datetime support,\\n          such as SQLite.\\n    columns : list, default None\\n        List of column names to select from SQL table.\\n    chunksize : int, default None\\n        If specified, returns an iterator where `chunksize` is the number of\\n        rows to include in each chunk.\\n\\n    Returns\\n    -------\\n    DataFrame\\n        A SQL table is returned as two-dimensional data structure with labeled\\n        axes.\\n\\n    See Also\\n    --------\\n    read_sql_query : Read SQL query into a DataFrame.\\n    read_sql : Read SQL query or database table into a DataFrame.\\n\\n    Notes\\n    -----\\n    Any datetime values with time zone information will be converted to UTC.\\n\\n    Examples\\n    --------\\n    >>> pd.read_sql_table(\\'table_name\\', \\'postgres:///db_name\\')  # doctest:+SKIP\\n    \"\"\"\\n\\n    con = _engine_builder(con)\\n    if not _is_sqlalchemy_connectable(con):\\n        raise NotImplementedError(\"read_sql_table only supported for \"\\n                                  \"SQLAlchemy connectable.\")\\n    import sqlalchemy\\n    from sqlalchemy.schema import MetaData\\n    meta = MetaData(con, schema=schema)\\n    try:\\n        meta.reflect(only=[table_name], views=True)\\n    except sqlalchemy.exc.InvalidRequestError:\\n        raise ValueError(\"Table {name} not found\".format(name=table_name))\\n\\n    pandas_sql = SQLDatabase(con, meta=meta)\\n    table = pandas_sql.read_table(\\n        table_name, index_col=index_col, coerce_float=coerce_float,\\n        parse_dates=parse_dates, columns=columns, chunksize=chunksize)\\n\\n    if table is not None:\\n        return table\\n    else:\\n        raise ValueError(\"Table {name} not found\".format(name=table_name), con)',\n 'def read_sql_query(sql, con, index_col=None, coerce_float=True, params=None,\\n                   parse_dates=None, chunksize=None):\\n    \"\"\"Read SQL query into a DataFrame.\\n\\n    Returns a DataFrame corresponding to the result set of the query\\n    string. Optionally provide an `index_col` parameter to use one of the\\n    columns as the index, otherwise default integer index will be used.\\n\\n    Parameters\\n    ----------\\n    sql : string SQL query or SQLAlchemy Selectable (select or text object)\\n        SQL query to be executed.\\n    con : SQLAlchemy connectable(engine/connection), database string URI,\\n        or sqlite3 DBAPI2 connection\\n        Using SQLAlchemy makes it possible to use any DB supported by that\\n        library.\\n        If a DBAPI2 object, only sqlite3 is supported.\\n    index_col : string or list of strings, optional, default: None\\n        Column(s) to set as index(MultiIndex).\\n    coerce_float : boolean, default True\\n        Attempts to convert values of non-string, non-numeric objects (like\\n        decimal.Decimal) to floating point. Useful for SQL result sets.\\n    params : list, tuple or dict, optional, default: None\\n        List of parameters to pass to execute method.  The syntax used\\n        to pass parameters is database driver dependent. Check your\\n        database driver documentation for which of the five syntax styles,\\n        described in PEP 249\\'s paramstyle, is supported.\\n        Eg. for psycopg2, uses %(name)s so use params={\\'name\\' : \\'value\\'}\\n    parse_dates : list or dict, default: None\\n        - List of column names to parse as dates.\\n        - Dict of ``{column_name: format string}`` where format string is\\n          strftime compatible in case of parsing string times, or is one of\\n          (D, s, ns, ms, us) in case of parsing integer timestamps.\\n        - Dict of ``{column_name: arg dict}``, where the arg dict corresponds\\n          to the keyword arguments of :func:`pandas.to_datetime`\\n          Especially useful with databases without native Datetime support,\\n          such as SQLite.\\n    chunksize : int, default None\\n        If specified, return an iterator where `chunksize` is the number of\\n        rows to include in each chunk.\\n\\n    Returns\\n    -------\\n    DataFrame\\n\\n    See Also\\n    --------\\n    read_sql_table : Read SQL database table into a DataFrame.\\n    read_sql\\n\\n    Notes\\n    -----\\n    Any datetime values with time zone information parsed via the `parse_dates`\\n    parameter will be converted to UTC.\\n    \"\"\"\\n    pandas_sql = pandasSQL_builder(con)\\n    return pandas_sql.read_query(\\n        sql, index_col=index_col, params=params, coerce_float=coerce_float,\\n        parse_dates=parse_dates, chunksize=chunksize)',\n 'def read_sql(sql, con, index_col=None, coerce_float=True, params=None,\\n             parse_dates=None, columns=None, chunksize=None):\\n    \"\"\"\\n    Read SQL query or database table into a DataFrame.\\n\\n    This function is a convenience wrapper around ``read_sql_table`` and\\n    ``read_sql_query`` (for backward compatibility). It will delegate\\n    to the specific function depending on the provided input. A SQL query\\n    will be routed to ``read_sql_query``, while a database table name will\\n    be routed to ``read_sql_table``. Note that the delegated function might\\n    have more specific notes about their functionality not listed here.\\n\\n    Parameters\\n    ----------\\n    sql : string or SQLAlchemy Selectable (select or text object)\\n        SQL query to be executed or a table name.\\n    con : SQLAlchemy connectable (engine/connection) or database string URI\\n        or DBAPI2 connection (fallback mode)\\n\\n        Using SQLAlchemy makes it possible to use any DB supported by that\\n        library. If a DBAPI2 object, only sqlite3 is supported.\\n    index_col : string or list of strings, optional, default: None\\n        Column(s) to set as index(MultiIndex).\\n    coerce_float : boolean, default True\\n        Attempts to convert values of non-string, non-numeric objects (like\\n        decimal.Decimal) to floating point, useful for SQL result sets.\\n    params : list, tuple or dict, optional, default: None\\n        List of parameters to pass to execute method.  The syntax used\\n        to pass parameters is database driver dependent. Check your\\n        database driver documentation for which of the five syntax styles,\\n        described in PEP 249\\'s paramstyle, is supported.\\n        Eg. for psycopg2, uses %(name)s so use params={\\'name\\' : \\'value\\'}\\n    parse_dates : list or dict, default: None\\n        - List of column names to parse as dates.\\n        - Dict of ``{column_name: format string}`` where format string is\\n          strftime compatible in case of parsing string times, or is one of\\n          (D, s, ns, ms, us) in case of parsing integer timestamps.\\n        - Dict of ``{column_name: arg dict}``, where the arg dict corresponds\\n          to the keyword arguments of :func:`pandas.to_datetime`\\n          Especially useful with databases without native Datetime support,\\n          such as SQLite.\\n    columns : list, default: None\\n        List of column names to select from SQL table (only used when reading\\n        a table).\\n    chunksize : int, default None\\n        If specified, return an iterator where `chunksize` is the\\n        number of rows to include in each chunk.\\n\\n    Returns\\n    -------\\n    DataFrame\\n\\n    See Also\\n    --------\\n    read_sql_table : Read SQL database table into a DataFrame.\\n    read_sql_query : Read SQL query into a DataFrame.\\n    \"\"\"\\n    pandas_sql = pandasSQL_builder(con)\\n\\n    if isinstance(pandas_sql, SQLiteDatabase):\\n        return pandas_sql.read_query(\\n            sql, index_col=index_col, params=params,\\n            coerce_float=coerce_float, parse_dates=parse_dates,\\n            chunksize=chunksize)\\n\\n    try:\\n        _is_table_name = pandas_sql.has_table(sql)\\n    except Exception:\\n        # using generic exception to catch errors from sql drivers (GH24988)\\n        _is_table_name = False\\n\\n    if _is_table_name:\\n        pandas_sql.meta.reflect(only=[sql])\\n        return pandas_sql.read_table(\\n            sql, index_col=index_col, coerce_float=coerce_float,\\n            parse_dates=parse_dates, columns=columns, chunksize=chunksize)\\n    else:\\n        return pandas_sql.read_query(\\n            sql, index_col=index_col, params=params,\\n            coerce_float=coerce_float, parse_dates=parse_dates,\\n            chunksize=chunksize)',\n 'def to_sql(frame, name, con, schema=None, if_exists=\\'fail\\', index=True,\\n           index_label=None, chunksize=None, dtype=None, method=None):\\n    \"\"\"\\n    Write records stored in a DataFrame to a SQL database.\\n\\n    Parameters\\n    ----------\\n    frame : DataFrame, Series\\n    name : string\\n        Name of SQL table.\\n    con : SQLAlchemy connectable(engine/connection) or database string URI\\n        or sqlite3 DBAPI2 connection\\n        Using SQLAlchemy makes it possible to use any DB supported by that\\n        library.\\n        If a DBAPI2 object, only sqlite3 is supported.\\n    schema : string, default None\\n        Name of SQL schema in database to write to (if database flavor\\n        supports this). If None, use default schema (default).\\n    if_exists : {\\'fail\\', \\'replace\\', \\'append\\'}, default \\'fail\\'\\n        - fail: If table exists, do nothing.\\n        - replace: If table exists, drop it, recreate it, and insert data.\\n        - append: If table exists, insert data. Create if does not exist.\\n    index : boolean, default True\\n        Write DataFrame index as a column.\\n    index_label : string or sequence, default None\\n        Column label for index column(s). If None is given (default) and\\n        `index` is True, then the index names are used.\\n        A sequence should be given if the DataFrame uses MultiIndex.\\n    chunksize : int, default None\\n        If not None, then rows will be written in batches of this size at a\\n        time.  If None, all rows will be written at once.\\n    dtype : single SQLtype or dict of column name to SQL type, default None\\n        Optional specifying the datatype for columns. The SQL type should\\n        be a SQLAlchemy type, or a string for sqlite3 fallback connection.\\n        If all columns are of the same type, one single value can be used.\\n    method : {None, \\'multi\\', callable}, default None\\n        Controls the SQL insertion clause used:\\n\\n        - None : Uses standard SQL ``INSERT`` clause (one per row).\\n        - \\'multi\\': Pass multiple values in a single ``INSERT`` clause.\\n        - callable with signature ``(pd_table, conn, keys, data_iter)``.\\n\\n        Details and a sample callable implementation can be found in the\\n        section :ref:`insert method <io.sql.method>`.\\n\\n        .. versionadded:: 0.24.0\\n    \"\"\"\\n    if if_exists not in (\\'fail\\', \\'replace\\', \\'append\\'):\\n        raise ValueError(\"\\'{0}\\' is not valid for if_exists\".format(if_exists))\\n\\n    pandas_sql = pandasSQL_builder(con, schema=schema)\\n\\n    if isinstance(frame, Series):\\n        frame = frame.to_frame()\\n    elif not isinstance(frame, DataFrame):\\n        raise NotImplementedError(\"\\'frame\\' argument should be either a \"\\n                                  \"Series or a DataFrame\")\\n\\n    pandas_sql.to_sql(frame, name, if_exists=if_exists, index=index,\\n                      index_label=index_label, schema=schema,\\n                      chunksize=chunksize, dtype=dtype, method=method)',\n 'def has_table(table_name, con, schema=None):\\n    \"\"\"\\n    Check if DataBase has named table.\\n\\n    Parameters\\n    ----------\\n    table_name: string\\n        Name of SQL table.\\n    con: SQLAlchemy connectable(engine/connection) or sqlite3 DBAPI2 connection\\n        Using SQLAlchemy makes it possible to use any DB supported by that\\n        library.\\n        If a DBAPI2 object, only sqlite3 is supported.\\n    schema : string, default None\\n        Name of SQL schema in database to write to (if database flavor supports\\n        this). If None, use default schema (default).\\n\\n    Returns\\n    -------\\n    boolean\\n    \"\"\"\\n    pandas_sql = pandasSQL_builder(con, schema=schema)\\n    return pandas_sql.has_table(table_name)',\n 'def _engine_builder(con):\\n    \"\"\"\\n    Returns a SQLAlchemy engine from a URI (if con is a string)\\n    else it just return con without modifying it.\\n    \"\"\"\\n    global _SQLALCHEMY_INSTALLED\\n    if isinstance(con, str):\\n        try:\\n            import sqlalchemy\\n        except ImportError:\\n            _SQLALCHEMY_INSTALLED = False\\n        else:\\n            con = sqlalchemy.create_engine(con)\\n            return con\\n\\n    return con',\n 'def pandasSQL_builder(con, schema=None, meta=None,\\n                      is_cursor=False):\\n    \"\"\"\\n    Convenience function to return the correct PandasSQL subclass based on the\\n    provided parameters.\\n    \"\"\"\\n    # When support for DBAPI connections is removed,\\n    # is_cursor should not be necessary.\\n    con = _engine_builder(con)\\n    if _is_sqlalchemy_connectable(con):\\n        return SQLDatabase(con, schema=schema, meta=meta)\\n    elif isinstance(con, str):\\n        raise ImportError(\"Using URI string without sqlalchemy installed.\")\\n    else:\\n        return SQLiteDatabase(con, is_cursor=is_cursor)',\n 'def get_schema(frame, name, keys=None, con=None, dtype=None):\\n    \"\"\"\\n    Get the SQL db table schema for the given frame.\\n\\n    Parameters\\n    ----------\\n    frame : DataFrame\\n    name : string\\n        name of SQL table\\n    keys : string or sequence, default: None\\n        columns to use a primary key\\n    con: an open SQL database connection object or a SQLAlchemy connectable\\n        Using SQLAlchemy makes it possible to use any DB supported by that\\n        library, default: None\\n        If a DBAPI2 object, only sqlite3 is supported.\\n    dtype : dict of column name to SQL type, default None\\n        Optional specifying the datatype for columns. The SQL type should\\n        be a SQLAlchemy type, or a string for sqlite3 fallback connection.\\n\\n    \"\"\"\\n\\n    pandas_sql = pandasSQL_builder(con=con)\\n    return pandas_sql._create_sql_schema(frame, name, keys=keys, dtype=dtype)',\n 'def _execute_insert(self, conn, keys, data_iter):\\n        \"\"\"Execute SQL statement inserting data\\n\\n        Parameters\\n        ----------\\n        conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection\\n        keys : list of str\\n           Column names\\n        data_iter : generator of list\\n           Each item contains a list of values to be inserted\\n        \"\"\"\\n        data = [dict(zip(keys, row)) for row in data_iter]\\n        conn.execute(self.table.insert(), data)',\n 'def _query_iterator(self, result, chunksize, columns, coerce_float=True,\\n                        parse_dates=None):\\n        \"\"\"Return generator through chunked result set.\"\"\"\\n\\n        while True:\\n            data = result.fetchmany(chunksize)\\n            if not data:\\n                break\\n            else:\\n                self.frame = DataFrame.from_records(\\n                    data, columns=columns, coerce_float=coerce_float)\\n\\n                self._harmonize_columns(parse_dates=parse_dates)\\n\\n                if self.index is not None:\\n                    self.frame.set_index(self.index, inplace=True)\\n\\n                yield self.frame',\n 'def _harmonize_columns(self, parse_dates=None):\\n        \"\"\"\\n        Make the DataFrame\\'s column types align with the SQL table\\n        column types.\\n        Need to work around limited NA value support. Floats are always\\n        fine, ints must always be floats if there are Null values.\\n        Booleans are hard because converting bool column with None replaces\\n        all Nones with false. Therefore only convert bool if there are no\\n        NA values.\\n        Datetimes should already be converted to np.datetime64 if supported,\\n        but here we also force conversion if required.\\n        \"\"\"\\n        parse_dates = _process_parse_dates_argument(parse_dates)\\n\\n        for sql_col in self.table.columns:\\n            col_name = sql_col.name\\n            try:\\n                df_col = self.frame[col_name]\\n\\n                # Handle date parsing upfront; don\\'t try to convert columns\\n                # twice\\n                if col_name in parse_dates:\\n                    try:\\n                        fmt = parse_dates[col_name]\\n                    except TypeError:\\n                        fmt = None\\n                    self.frame[col_name] = _handle_date_column(\\n                        df_col, format=fmt)\\n                    continue\\n\\n                # the type the dataframe column should have\\n                col_type = self._get_dtype(sql_col.type)\\n\\n                if (col_type is datetime or col_type is date or\\n                        col_type is DatetimeTZDtype):\\n                    # Convert tz-aware Datetime SQL columns to UTC\\n                    utc = col_type is DatetimeTZDtype\\n                    self.frame[col_name] = _handle_date_column(df_col, utc=utc)\\n                elif col_type is float:\\n                    # floats support NA, can always convert!\\n                    self.frame[col_name] = df_col.astype(col_type, copy=False)\\n\\n                elif len(df_col) == df_col.count():\\n                    # No NA values, can convert ints and bools\\n                    if col_type is np.dtype(\\'int64\\') or col_type is bool:\\n                        self.frame[col_name] = df_col.astype(\\n                            col_type, copy=False)\\n            except KeyError:\\n                pass',\n 'def read_table(self, table_name, index_col=None, coerce_float=True,\\n                   parse_dates=None, columns=None, schema=None,\\n                   chunksize=None):\\n        \"\"\"Read SQL database table into a DataFrame.\\n\\n        Parameters\\n        ----------\\n        table_name : string\\n            Name of SQL table in database.\\n        index_col : string, optional, default: None\\n            Column to set as index.\\n        coerce_float : boolean, default True\\n            Attempts to convert values of non-string, non-numeric objects\\n            (like decimal.Decimal) to floating point. This can result in\\n            loss of precision.\\n        parse_dates : list or dict, default: None\\n            - List of column names to parse as dates.\\n            - Dict of ``{column_name: format string}`` where format string is\\n              strftime compatible in case of parsing string times, or is one of\\n              (D, s, ns, ms, us) in case of parsing integer timestamps.\\n            - Dict of ``{column_name: arg}``, where the arg corresponds\\n              to the keyword arguments of :func:`pandas.to_datetime`.\\n              Especially useful with databases without native Datetime support,\\n              such as SQLite.\\n        columns : list, default: None\\n            List of column names to select from SQL table.\\n        schema : string, default None\\n            Name of SQL schema in database to query (if database flavor\\n            supports this).  If specified, this overwrites the default\\n            schema of the SQL database object.\\n        chunksize : int, default None\\n            If specified, return an iterator where `chunksize` is the number\\n            of rows to include in each chunk.\\n\\n        Returns\\n        -------\\n        DataFrame\\n\\n        See Also\\n        --------\\n        pandas.read_sql_table\\n        SQLDatabase.read_query\\n\\n        \"\"\"\\n        table = SQLTable(table_name, self, index=index_col, schema=schema)\\n        return table.read(coerce_float=coerce_float,\\n                          parse_dates=parse_dates, columns=columns,\\n                          chunksize=chunksize)',\n 'def _query_iterator(result, chunksize, columns, index_col=None,\\n                        coerce_float=True, parse_dates=None):\\n        \"\"\"Return generator through chunked result set\"\"\"\\n\\n        while True:\\n            data = result.fetchmany(chunksize)\\n            if not data:\\n                break\\n            else:\\n                yield _wrap_result(data, columns, index_col=index_col,\\n                                   coerce_float=coerce_float,\\n                                   parse_dates=parse_dates)',\n 'def read_query(self, sql, index_col=None, coerce_float=True,\\n                   parse_dates=None, params=None, chunksize=None):\\n        \"\"\"Read SQL query into a DataFrame.\\n\\n        Parameters\\n        ----------\\n        sql : string\\n            SQL query to be executed.\\n        index_col : string, optional, default: None\\n            Column name to use as index for the returned DataFrame object.\\n        coerce_float : boolean, default True\\n            Attempt to convert values of non-string, non-numeric objects (like\\n            decimal.Decimal) to floating point, useful for SQL result sets.\\n        params : list, tuple or dict, optional, default: None\\n            List of parameters to pass to execute method.  The syntax used\\n            to pass parameters is database driver dependent. Check your\\n            database driver documentation for which of the five syntax styles,\\n            described in PEP 249\\'s paramstyle, is supported.\\n            Eg. for psycopg2, uses %(name)s so use params={\\'name\\' : \\'value\\'}\\n        parse_dates : list or dict, default: None\\n            - List of column names to parse as dates.\\n            - Dict of ``{column_name: format string}`` where format string is\\n              strftime compatible in case of parsing string times, or is one of\\n              (D, s, ns, ms, us) in case of parsing integer timestamps.\\n            - Dict of ``{column_name: arg dict}``, where the arg dict\\n              corresponds to the keyword arguments of\\n              :func:`pandas.to_datetime` Especially useful with databases\\n              without native Datetime support, such as SQLite.\\n        chunksize : int, default None\\n            If specified, return an iterator where `chunksize` is the number\\n            of rows to include in each chunk.\\n\\n        Returns\\n        -------\\n        DataFrame\\n\\n        See Also\\n        --------\\n        read_sql_table : Read SQL database table into a DataFrame.\\n        read_sql\\n\\n        \"\"\"\\n        args = _convert_params(sql, params)\\n\\n        result = self.execute(*args)\\n        columns = result.keys()\\n\\n        if chunksize is not None:\\n            return self._query_iterator(result, chunksize, columns,\\n                                        index_col=index_col,\\n                                        coerce_float=coerce_float,\\n                                        parse_dates=parse_dates)\\n        else:\\n            data = result.fetchall()\\n            frame = _wrap_result(data, columns, index_col=index_col,\\n                                 coerce_float=coerce_float,\\n                                 parse_dates=parse_dates)\\n            return frame',\n 'def to_sql(self, frame, name, if_exists=\\'fail\\', index=True,\\n               index_label=None, schema=None, chunksize=None, dtype=None,\\n               method=None):\\n        \"\"\"\\n        Write records stored in a DataFrame to a SQL database.\\n\\n        Parameters\\n        ----------\\n        frame : DataFrame\\n        name : string\\n            Name of SQL table.\\n        if_exists : {\\'fail\\', \\'replace\\', \\'append\\'}, default \\'fail\\'\\n            - fail: If table exists, do nothing.\\n            - replace: If table exists, drop it, recreate it, and insert data.\\n            - append: If table exists, insert data. Create if does not exist.\\n        index : boolean, default True\\n            Write DataFrame index as a column.\\n        index_label : string or sequence, default None\\n            Column label for index column(s). If None is given (default) and\\n            `index` is True, then the index names are used.\\n            A sequence should be given if the DataFrame uses MultiIndex.\\n        schema : string, default None\\n            Name of SQL schema in database to write to (if database flavor\\n            supports this). If specified, this overwrites the default\\n            schema of the SQLDatabase object.\\n        chunksize : int, default None\\n            If not None, then rows will be written in batches of this size at a\\n            time.  If None, all rows will be written at once.\\n        dtype : single type or dict of column name to SQL type, default None\\n            Optional specifying the datatype for columns. The SQL type should\\n            be a SQLAlchemy type. If all columns are of the same type, one\\n            single value can be used.\\n        method : {None\\', \\'multi\\', callable}, default None\\n            Controls the SQL insertion clause used:\\n\\n            * None : Uses standard SQL ``INSERT`` clause (one per row).\\n            * \\'multi\\': Pass multiple values in a single ``INSERT`` clause.\\n            * callable with signature ``(pd_table, conn, keys, data_iter)``.\\n\\n            Details and a sample callable implementation can be found in the\\n            section :ref:`insert method <io.sql.method>`.\\n\\n            .. versionadded:: 0.24.0\\n        \"\"\"\\n        if dtype and not is_dict_like(dtype):\\n            dtype = {col_name: dtype for col_name in frame}\\n\\n        if dtype is not None:\\n            from sqlalchemy.types import to_instance, TypeEngine\\n            for col, my_type in dtype.items():\\n                if not isinstance(to_instance(my_type), TypeEngine):\\n                    raise ValueError(\\'The type of {column} is not a \\'\\n                                     \\'SQLAlchemy type \\'.format(column=col))\\n\\n        table = SQLTable(name, self, frame=frame, index=index,\\n                         if_exists=if_exists, index_label=index_label,\\n                         schema=schema, dtype=dtype)\\n        table.create()\\n        table.insert(chunksize, method=method)\\n        if (not name.isdigit() and not name.islower()):\\n            # check for potentially case sensitivity issues (GH7815)\\n            # Only check when name is not a number and name is not lower case\\n            engine = self.connectable.engine\\n            with self.connectable.connect() as conn:\\n                table_names = engine.table_names(\\n                    schema=schema or self.meta.schema,\\n                    connection=conn,\\n                )\\n            if name not in table_names:\\n                msg = (\\n                    \"The provided table name \\'{0}\\' is not found exactly as \"\\n                    \"such in the database after writing the table, possibly \"\\n                    \"due to case sensitivity issues. Consider using lower \"\\n                    \"case table names.\"\\n                ).format(name)\\n                warnings.warn(msg, UserWarning)',\n 'def _create_table_setup(self):\\n        \"\"\"\\n        Return a list of SQL statements that creates a table reflecting the\\n        structure of a DataFrame.  The first entry will be a CREATE TABLE\\n        statement while the rest will be CREATE INDEX statements.\\n        \"\"\"\\n        column_names_and_types = self._get_column_names_and_types(\\n            self._sql_type_name\\n        )\\n\\n        pat = re.compile(r\\'\\\\s+\\')\\n        column_names = [col_name for col_name, _, _ in column_names_and_types]\\n        if any(map(pat.search, column_names)):\\n            warnings.warn(_SAFE_NAMES_WARNING, stacklevel=6)\\n\\n        escape = _get_valid_sqlite_name\\n\\n        create_tbl_stmts = [escape(cname) + \\' \\' + ctype\\n                            for cname, ctype, _ in column_names_and_types]\\n\\n        if self.keys is not None and len(self.keys):\\n            if not is_list_like(self.keys):\\n                keys = [self.keys]\\n            else:\\n                keys = self.keys\\n            cnames_br = \", \".join(escape(c) for c in keys)\\n            create_tbl_stmts.append(\\n                \"CONSTRAINT {tbl}_pk PRIMARY KEY ({cnames_br})\".format(\\n                    tbl=self.name, cnames_br=cnames_br))\\n\\n        create_stmts = [\"CREATE TABLE \" + escape(self.name) + \" (\\\\n\" +\\n                        \\',\\\\n  \\'.join(create_tbl_stmts) + \"\\\\n)\"]\\n\\n        ix_cols = [cname for cname, _, is_index in column_names_and_types\\n                   if is_index]\\n        if len(ix_cols):\\n            cnames = \"_\".join(ix_cols)\\n            cnames_br = \",\".join(escape(c) for c in ix_cols)\\n            create_stmts.append(\\n                \"CREATE INDEX \" + escape(\"ix_\" + self.name + \"_\" + cnames) +\\n                \"ON \" + escape(self.name) + \" (\" + cnames_br + \")\")\\n\\n        return create_stmts',\n 'def _query_iterator(cursor, chunksize, columns, index_col=None,\\n                        coerce_float=True, parse_dates=None):\\n        \"\"\"Return generator through chunked result set\"\"\"\\n\\n        while True:\\n            data = cursor.fetchmany(chunksize)\\n            if type(data) == tuple:\\n                data = list(data)\\n            if not data:\\n                cursor.close()\\n                break\\n            else:\\n                yield _wrap_result(data, columns, index_col=index_col,\\n                                   coerce_float=coerce_float,\\n                                   parse_dates=parse_dates)',\n 'def to_sql(self, frame, name, if_exists=\\'fail\\', index=True,\\n               index_label=None, schema=None, chunksize=None, dtype=None,\\n               method=None):\\n        \"\"\"\\n        Write records stored in a DataFrame to a SQL database.\\n\\n        Parameters\\n        ----------\\n        frame: DataFrame\\n        name: string\\n            Name of SQL table.\\n        if_exists: {\\'fail\\', \\'replace\\', \\'append\\'}, default \\'fail\\'\\n            fail: If table exists, do nothing.\\n            replace: If table exists, drop it, recreate it, and insert data.\\n            append: If table exists, insert data. Create if it does not exist.\\n        index : boolean, default True\\n            Write DataFrame index as a column\\n        index_label : string or sequence, default None\\n            Column label for index column(s). If None is given (default) and\\n            `index` is True, then the index names are used.\\n            A sequence should be given if the DataFrame uses MultiIndex.\\n        schema : string, default None\\n            Ignored parameter included for compatibility with SQLAlchemy\\n            version of ``to_sql``.\\n        chunksize : int, default None\\n            If not None, then rows will be written in batches of this\\n            size at a time. If None, all rows will be written at once.\\n        dtype : single type or dict of column name to SQL type, default None\\n            Optional specifying the datatype for columns. The SQL type should\\n            be a string. If all columns are of the same type, one single value\\n            can be used.\\n        method : {None, \\'multi\\', callable}, default None\\n            Controls the SQL insertion clause used:\\n\\n            * None : Uses standard SQL ``INSERT`` clause (one per row).\\n            * \\'multi\\': Pass multiple values in a single ``INSERT`` clause.\\n            * callable with signature ``(pd_table, conn, keys, data_iter)``.\\n\\n            Details and a sample callable implementation can be found in the\\n            section :ref:`insert method <io.sql.method>`.\\n\\n            .. versionadded:: 0.24.0\\n        \"\"\"\\n        if dtype and not is_dict_like(dtype):\\n            dtype = {col_name: dtype for col_name in frame}\\n\\n        if dtype is not None:\\n            for col, my_type in dtype.items():\\n                if not isinstance(my_type, str):\\n                    raise ValueError(\\'{column} ({type!s}) not a string\\'.format(\\n                        column=col, type=my_type))\\n\\n        table = SQLiteTable(name, self, frame=frame, index=index,\\n                            if_exists=if_exists, index_label=index_label,\\n                            dtype=dtype)\\n        table.create()\\n        table.insert(chunksize, method)',\n 'def _maybe_to_categorical(array):\\n    \"\"\"\\n    Coerce to a categorical if a series is given.\\n\\n    Internal use ONLY.\\n    \"\"\"\\n    if isinstance(array, (ABCSeries, ABCCategoricalIndex)):\\n        return array._values\\n    elif isinstance(array, np.ndarray):\\n        return Categorical(array)\\n    return array',\n 'def contains(cat, key, container):\\n    \"\"\"\\n    Helper for membership check for ``key`` in ``cat``.\\n\\n    This is a helper method for :method:`__contains__`\\n    and :class:`CategoricalIndex.__contains__`.\\n\\n    Returns True if ``key`` is in ``cat.categories`` and the\\n    location of ``key`` in ``categories`` is in ``container``.\\n\\n    Parameters\\n    ----------\\n    cat : :class:`Categorical`or :class:`categoricalIndex`\\n    key : a hashable object\\n        The key to check membership for.\\n    container : Container (e.g. list-like or mapping)\\n        The container to check for membership in.\\n\\n    Returns\\n    -------\\n    is_in : bool\\n        True if ``key`` is in ``self.categories`` and location of\\n        ``key`` in ``categories`` is in ``container``, else False.\\n\\n    Notes\\n    -----\\n    This method does not check for NaN values. Do that separately\\n    before calling this method.\\n    \"\"\"\\n    hash(key)\\n\\n    # get location of key in categories.\\n    # If a KeyError, the key isn\\'t in categories, so logically\\n    #  can\\'t be in container either.\\n    try:\\n        loc = cat.categories.get_loc(key)\\n    except KeyError:\\n        return False\\n\\n    # loc is the location of key in categories, but also the *value*\\n    # for key in container. So, `key` may be in categories,\\n    # but still not in `container`. Example (\\'b\\' in categories,\\n    # but not in values):\\n    # \\'b\\' in Categorical([\\'a\\'], categories=[\\'a\\', \\'b\\'])  # False\\n    if is_scalar(loc):\\n        return loc in container\\n    else:\\n        # if categories is an IntervalIndex, loc is an array.\\n        return any(loc_ in container for loc_ in loc)',\n 'def _get_codes_for_values(values, categories):\\n    \"\"\"\\n    utility routine to turn values into codes given the specified categories\\n    \"\"\"\\n    from pandas.core.algorithms import _get_data_algo, _hashtables\\n    dtype_equal = is_dtype_equal(values.dtype, categories.dtype)\\n\\n    if dtype_equal:\\n        # To prevent erroneous dtype coercion in _get_data_algo, retrieve\\n        # the underlying numpy array. gh-22702\\n        values = getattr(values, \\'_ndarray_values\\', values)\\n        categories = getattr(categories, \\'_ndarray_values\\', categories)\\n    elif (is_extension_array_dtype(categories.dtype) and\\n          is_object_dtype(values)):\\n        # Support inferring the correct extension dtype from an array of\\n        # scalar objects. e.g.\\n        # Categorical(array[Period, Period], categories=PeriodIndex(...))\\n        try:\\n            values = (\\n                categories.dtype.construct_array_type()._from_sequence(values)\\n            )\\n        except Exception:\\n            # but that may fail for any reason, so fall back to object\\n            values = ensure_object(values)\\n            categories = ensure_object(categories)\\n    else:\\n        values = ensure_object(values)\\n        categories = ensure_object(categories)\\n\\n    (hash_klass, vec_klass), vals = _get_data_algo(values, _hashtables)\\n    (_, _), cats = _get_data_algo(categories, _hashtables)\\n    t = hash_klass(len(cats))\\n    t.map_locations(cats)\\n    return coerce_indexer_dtype(t.lookup(vals), cats)',\n 'def _recode_for_categories(codes, old_categories, new_categories):\\n    \"\"\"\\n    Convert a set of codes for to a new set of categories\\n\\n    Parameters\\n    ----------\\n    codes : array\\n    old_categories, new_categories : Index\\n\\n    Returns\\n    -------\\n    new_codes : array\\n\\n    Examples\\n    --------\\n    >>> old_cat = pd.Index([\\'b\\', \\'a\\', \\'c\\'])\\n    >>> new_cat = pd.Index([\\'a\\', \\'b\\'])\\n    >>> codes = np.array([0, 1, 1, 2])\\n    >>> _recode_for_categories(codes, old_cat, new_cat)\\n    array([ 1,  0,  0, -1])\\n    \"\"\"\\n    from pandas.core.algorithms import take_1d\\n\\n    if len(old_categories) == 0:\\n        # All null anyway, so just retain the nulls\\n        return codes.copy()\\n    elif new_categories.equals(old_categories):\\n        # Same categories, so no need to actually recode\\n        return codes.copy()\\n    indexer = coerce_indexer_dtype(new_categories.get_indexer(old_categories),\\n                                   new_categories)\\n    new_codes = take_1d(indexer, codes.copy(), fill_value=-1)\\n    return new_codes',\n 'def _factorize_from_iterable(values):\\n    \"\"\"\\n    Factorize an input `values` into `categories` and `codes`. Preserves\\n    categorical dtype in `categories`.\\n\\n    *This is an internal function*\\n\\n    Parameters\\n    ----------\\n    values : list-like\\n\\n    Returns\\n    -------\\n    codes : ndarray\\n    categories : Index\\n        If `values` has a categorical dtype, then `categories` is\\n        a CategoricalIndex keeping the categories and order of `values`.\\n    \"\"\"\\n    from pandas.core.indexes.category import CategoricalIndex\\n\\n    if not is_list_like(values):\\n        raise TypeError(\"Input must be list-like\")\\n\\n    if is_categorical(values):\\n        if isinstance(values, (ABCCategoricalIndex, ABCSeries)):\\n            values = values._values\\n        categories = CategoricalIndex(values.categories, dtype=values.dtype)\\n        codes = values.codes\\n    else:\\n        # The value of ordered is irrelevant since we don\\'t use cat as such,\\n        # but only the resulting categories, the order of which is independent\\n        # from ordered. Set ordered to False as default. See GH #15457\\n        cat = Categorical(values, ordered=False)\\n        categories = cat.categories\\n        codes = cat.codes\\n    return codes, categories',\n 'def _factorize_from_iterables(iterables):\\n    \"\"\"\\n    A higher-level wrapper over `_factorize_from_iterable`.\\n\\n    *This is an internal function*\\n\\n    Parameters\\n    ----------\\n    iterables : list-like of list-likes\\n\\n    Returns\\n    -------\\n    codes_list : list of ndarrays\\n    categories_list : list of Indexes\\n\\n    Notes\\n    -----\\n    See `_factorize_from_iterable` for more info.\\n    \"\"\"\\n    if len(iterables) == 0:\\n        # For consistency, it should return a list of 2 lists.\\n        return [[], []]\\n    return map(list, lzip(*[_factorize_from_iterable(it) for it in iterables]))',\n 'def copy(self):\\n        \"\"\"\\n        Copy constructor.\\n        \"\"\"\\n        return self._constructor(values=self._codes.copy(),\\n                                 dtype=self.dtype,\\n                                 fastpath=True)',\n 'def astype(self, dtype, copy=True):\\n        \"\"\"\\n        Coerce this type to another dtype\\n\\n        Parameters\\n        ----------\\n        dtype : numpy dtype or pandas type\\n        copy : bool, default True\\n            By default, astype always returns a newly allocated object.\\n            If copy is set to False and dtype is categorical, the original\\n            object is returned.\\n\\n            .. versionadded:: 0.19.0\\n\\n        \"\"\"\\n        if is_categorical_dtype(dtype):\\n            # GH 10696/18593\\n            dtype = self.dtype.update_dtype(dtype)\\n            self = self.copy() if copy else self\\n            if dtype == self.dtype:\\n                return self\\n            return self._set_dtype(dtype)\\n        return np.array(self, dtype=dtype, copy=copy)',\n 'def _from_inferred_categories(cls, inferred_categories, inferred_codes,\\n                                  dtype, true_values=None):\\n        \"\"\"\\n        Construct a Categorical from inferred values.\\n\\n        For inferred categories (`dtype` is None) the categories are sorted.\\n        For explicit `dtype`, the `inferred_categories` are cast to the\\n        appropriate type.\\n\\n        Parameters\\n        ----------\\n        inferred_categories : Index\\n        inferred_codes : Index\\n        dtype : CategoricalDtype or \\'category\\'\\n        true_values : list, optional\\n            If none are provided, the default ones are\\n            \"True\", \"TRUE\", and \"true.\"\\n\\n        Returns\\n        -------\\n        Categorical\\n        \"\"\"\\n        from pandas import Index, to_numeric, to_datetime, to_timedelta\\n\\n        cats = Index(inferred_categories)\\n        known_categories = (isinstance(dtype, CategoricalDtype) and\\n                            dtype.categories is not None)\\n\\n        if known_categories:\\n            # Convert to a specialized type with `dtype` if specified.\\n            if dtype.categories.is_numeric():\\n                cats = to_numeric(inferred_categories, errors=\"coerce\")\\n            elif is_datetime64_dtype(dtype.categories):\\n                cats = to_datetime(inferred_categories, errors=\"coerce\")\\n            elif is_timedelta64_dtype(dtype.categories):\\n                cats = to_timedelta(inferred_categories, errors=\"coerce\")\\n            elif dtype.categories.is_boolean():\\n                if true_values is None:\\n                    true_values = [\"True\", \"TRUE\", \"true\"]\\n\\n                cats = cats.isin(true_values)\\n\\n        if known_categories:\\n            # Recode from observation order to dtype.categories order.\\n            categories = dtype.categories\\n            codes = _recode_for_categories(inferred_codes, cats, categories)\\n        elif not cats.is_monotonic_increasing:\\n            # Sort categories and recode for unknown categories.\\n            unsorted = cats.copy()\\n            categories = cats.sort_values()\\n\\n            codes = _recode_for_categories(inferred_codes, unsorted,\\n                                           categories)\\n            dtype = CategoricalDtype(categories, ordered=False)\\n        else:\\n            dtype = CategoricalDtype(cats, ordered=False)\\n            codes = inferred_codes\\n\\n        return cls(codes, dtype=dtype, fastpath=True)',\n 'def from_codes(cls, codes, categories=None, ordered=None, dtype=None):\\n        \"\"\"\\n        Make a Categorical type from codes and categories or dtype.\\n\\n        This constructor is useful if you already have codes and\\n        categories/dtype and so do not need the (computation intensive)\\n        factorization step, which is usually done on the constructor.\\n\\n        If your data does not follow this convention, please use the normal\\n        constructor.\\n\\n        Parameters\\n        ----------\\n        codes : array-like, integers\\n            An integer array, where each integer points to a category in\\n            categories or dtype.categories, or else is -1 for NaN.\\n        categories : index-like, optional\\n            The categories for the categorical. Items need to be unique.\\n            If the categories are not given here, then they must be provided\\n            in `dtype`.\\n        ordered : bool, optional\\n            Whether or not this categorical is treated as an ordered\\n            categorical. If not given here or in `dtype`, the resulting\\n            categorical will be unordered.\\n        dtype : CategoricalDtype or the string \"category\", optional\\n            If :class:`CategoricalDtype`, cannot be used together with\\n            `categories` or `ordered`.\\n\\n            .. versionadded:: 0.24.0\\n\\n               When `dtype` is provided, neither `categories` nor `ordered`\\n               should be provided.\\n\\n        Examples\\n        --------\\n        >>> dtype = pd.CategoricalDtype([\\'a\\', \\'b\\'], ordered=True)\\n        >>> pd.Categorical.from_codes(codes=[0, 1, 0, 1], dtype=dtype)\\n        [a, b, a, b]\\n        Categories (2, object): [a < b]\\n        \"\"\"\\n        dtype = CategoricalDtype._from_values_or_dtype(categories=categories,\\n                                                       ordered=ordered,\\n                                                       dtype=dtype)\\n        if dtype.categories is None:\\n            msg = (\"The categories must be provided in \\'categories\\' or \"\\n                   \"\\'dtype\\'. Both were None.\")\\n            raise ValueError(msg)\\n\\n        codes = np.asarray(codes)  # #21767\\n        if not is_integer_dtype(codes):\\n            msg = \"codes need to be array-like integers\"\\n            if is_float_dtype(codes):\\n                icodes = codes.astype(\\'i8\\')\\n                if (icodes == codes).all():\\n                    msg = None\\n                    codes = icodes\\n                    warn((\"float codes will be disallowed in the future and \"\\n                          \"raise a ValueError\"), FutureWarning, stacklevel=2)\\n            if msg:\\n                raise ValueError(msg)\\n\\n        if len(codes) and (\\n                codes.max() >= len(dtype.categories) or codes.min() < -1):\\n            raise ValueError(\"codes need to be between -1 and \"\\n                             \"len(categories)-1\")\\n\\n        return cls(codes, dtype=dtype, fastpath=True)',\n 'def _get_codes(self):\\n        \"\"\"\\n        Get the codes.\\n\\n        Returns\\n        -------\\n        codes : integer array view\\n            A non writable view of the `codes` array.\\n        \"\"\"\\n        v = self._codes.view()\\n        v.flags.writeable = False\\n        return v',\n 'def _set_categories(self, categories, fastpath=False):\\n        \"\"\"\\n        Sets new categories inplace\\n\\n        Parameters\\n        ----------\\n        fastpath : bool, default False\\n           Don\\'t perform validation of the categories for uniqueness or nulls\\n\\n        Examples\\n        --------\\n        >>> c = pd.Categorical([\\'a\\', \\'b\\'])\\n        >>> c\\n        [a, b]\\n        Categories (2, object): [a, b]\\n\\n        >>> c._set_categories(pd.Index([\\'a\\', \\'c\\']))\\n        >>> c\\n        [a, c]\\n        Categories (2, object): [a, c]\\n        \"\"\"\\n\\n        if fastpath:\\n            new_dtype = CategoricalDtype._from_fastpath(categories,\\n                                                        self.ordered)\\n        else:\\n            new_dtype = CategoricalDtype(categories, ordered=self.ordered)\\n        if (not fastpath and self.dtype.categories is not None and\\n                len(new_dtype.categories) != len(self.dtype.categories)):\\n            raise ValueError(\"new categories need to have the same number of \"\\n                             \"items than the old categories!\")\\n\\n        self._dtype = new_dtype',\n 'def _set_dtype(self, dtype):\\n        \"\"\"\\n        Internal method for directly updating the CategoricalDtype\\n\\n        Parameters\\n        ----------\\n        dtype : CategoricalDtype\\n\\n        Notes\\n        -----\\n        We don\\'t do any validation here. It\\'s assumed that the dtype is\\n        a (valid) instance of `CategoricalDtype`.\\n        \"\"\"\\n        codes = _recode_for_categories(self.codes, self.categories,\\n                                       dtype.categories)\\n        return type(self)(codes, dtype=dtype, fastpath=True)',\n 'def set_ordered(self, value, inplace=False):\\n        \"\"\"\\n        Set the ordered attribute to the boolean value.\\n\\n        Parameters\\n        ----------\\n        value : bool\\n           Set whether this categorical is ordered (True) or not (False).\\n        inplace : bool, default False\\n           Whether or not to set the ordered attribute in-place or return\\n           a copy of this categorical with ordered set to the value.\\n        \"\"\"\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        new_dtype = CategoricalDtype(self.categories, ordered=value)\\n        cat = self if inplace else self.copy()\\n        cat._dtype = new_dtype\\n        if not inplace:\\n            return cat',\n 'def as_ordered(self, inplace=False):\\n        \"\"\"\\n        Set the Categorical to be ordered.\\n\\n        Parameters\\n        ----------\\n        inplace : bool, default False\\n           Whether or not to set the ordered attribute in-place or return\\n           a copy of this categorical with ordered set to True.\\n        \"\"\"\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        return self.set_ordered(True, inplace=inplace)',\n 'def as_unordered(self, inplace=False):\\n        \"\"\"\\n        Set the Categorical to be unordered.\\n\\n        Parameters\\n        ----------\\n        inplace : bool, default False\\n           Whether or not to set the ordered attribute in-place or return\\n           a copy of this categorical with ordered set to False.\\n        \"\"\"\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        return self.set_ordered(False, inplace=inplace)',\n 'def set_categories(self, new_categories, ordered=None, rename=False,\\n                       inplace=False):\\n        \"\"\"\\n        Set the categories to the specified new_categories.\\n\\n        `new_categories` can include new categories (which will result in\\n        unused categories) or remove old categories (which results in values\\n        set to NaN). If `rename==True`, the categories will simple be renamed\\n        (less or more items than in old categories will result in values set to\\n        NaN or in unused categories respectively).\\n\\n        This method can be used to perform more than one action of adding,\\n        removing, and reordering simultaneously and is therefore faster than\\n        performing the individual steps via the more specialised methods.\\n\\n        On the other hand this methods does not do checks (e.g., whether the\\n        old categories are included in the new categories on a reorder), which\\n        can result in surprising changes, for example when using special string\\n        dtypes on python3, which does not considers a S1 string equal to a\\n        single char python string.\\n\\n        Parameters\\n        ----------\\n        new_categories : Index-like\\n           The categories in new order.\\n        ordered : bool, default False\\n           Whether or not the categorical is treated as a ordered categorical.\\n           If not given, do not change the ordered information.\\n        rename : bool, default False\\n           Whether or not the new_categories should be considered as a rename\\n           of the old categories or as reordered categories.\\n        inplace : bool, default False\\n           Whether or not to reorder the categories in-place or return a copy\\n           of this categorical with reordered categories.\\n\\n        Returns\\n        -------\\n        Categorical with reordered categories or None if inplace.\\n\\n        Raises\\n        ------\\n        ValueError\\n            If new_categories does not validate as categories\\n\\n        See Also\\n        --------\\n        rename_categories\\n        reorder_categories\\n        add_categories\\n        remove_categories\\n        remove_unused_categories\\n        \"\"\"\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        if ordered is None:\\n            ordered = self.dtype.ordered\\n        new_dtype = CategoricalDtype(new_categories, ordered=ordered)\\n\\n        cat = self if inplace else self.copy()\\n        if rename:\\n            if (cat.dtype.categories is not None and\\n                    len(new_dtype.categories) < len(cat.dtype.categories)):\\n                # remove all _codes which are larger and set to -1/NaN\\n                cat._codes[cat._codes >= len(new_dtype.categories)] = -1\\n        else:\\n            codes = _recode_for_categories(cat.codes, cat.categories,\\n                                           new_dtype.categories)\\n            cat._codes = codes\\n        cat._dtype = new_dtype\\n\\n        if not inplace:\\n            return cat',\n 'def rename_categories(self, new_categories, inplace=False):\\n        \"\"\"\\n        Rename categories.\\n\\n        Parameters\\n        ----------\\n        new_categories : list-like, dict-like or callable\\n\\n           * list-like: all items must be unique and the number of items in\\n             the new categories must match the existing number of categories.\\n\\n           * dict-like: specifies a mapping from\\n             old categories to new. Categories not contained in the mapping\\n             are passed through and extra categories in the mapping are\\n             ignored.\\n\\n             .. versionadded:: 0.21.0\\n\\n           * callable : a callable that is called on all items in the old\\n             categories and whose return values comprise the new categories.\\n\\n             .. versionadded:: 0.23.0\\n\\n           .. warning::\\n\\n              Currently, Series are considered list like. In a future version\\n              of pandas they\\'ll be considered dict-like.\\n\\n        inplace : bool, default False\\n           Whether or not to rename the categories inplace or return a copy of\\n           this categorical with renamed categories.\\n\\n        Returns\\n        -------\\n        cat : Categorical or None\\n           With ``inplace=False``, the new categorical is returned.\\n           With ``inplace=True``, there is no return value.\\n\\n        Raises\\n        ------\\n        ValueError\\n            If new categories are list-like and do not have the same number of\\n            items than the current categories or do not validate as categories\\n\\n        See Also\\n        --------\\n        reorder_categories\\n        add_categories\\n        remove_categories\\n        remove_unused_categories\\n        set_categories\\n\\n        Examples\\n        --------\\n        >>> c = pd.Categorical([\\'a\\', \\'a\\', \\'b\\'])\\n        >>> c.rename_categories([0, 1])\\n        [0, 0, 1]\\n        Categories (2, int64): [0, 1]\\n\\n        For dict-like ``new_categories``, extra keys are ignored and\\n        categories not in the dictionary are passed through\\n\\n        >>> c.rename_categories({\\'a\\': \\'A\\', \\'c\\': \\'C\\'})\\n        [A, A, b]\\n        Categories (2, object): [A, b]\\n\\n        You may also provide a callable to create the new categories\\n\\n        >>> c.rename_categories(lambda x: x.upper())\\n        [A, A, B]\\n        Categories (2, object): [A, B]\\n        \"\"\"\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        cat = self if inplace else self.copy()\\n\\n        if isinstance(new_categories, ABCSeries):\\n            msg = (\"Treating Series \\'new_categories\\' as a list-like and using \"\\n                   \"the values. In a future version, \\'rename_categories\\' will \"\\n                   \"treat Series like a dictionary.\\\\n\"\\n                   \"For dict-like, use \\'new_categories.to_dict()\\'\\\\n\"\\n                   \"For list-like, use \\'new_categories.values\\'.\")\\n            warn(msg, FutureWarning, stacklevel=2)\\n            new_categories = list(new_categories)\\n\\n        if is_dict_like(new_categories):\\n            cat.categories = [new_categories.get(item, item)\\n                              for item in cat.categories]\\n        elif callable(new_categories):\\n            cat.categories = [new_categories(item) for item in cat.categories]\\n        else:\\n            cat.categories = new_categories\\n        if not inplace:\\n            return cat',\n 'def reorder_categories(self, new_categories, ordered=None, inplace=False):\\n        \"\"\"\\n        Reorder categories as specified in new_categories.\\n\\n        `new_categories` need to include all old categories and no new category\\n        items.\\n\\n        Parameters\\n        ----------\\n        new_categories : Index-like\\n           The categories in new order.\\n        ordered : bool, optional\\n           Whether or not the categorical is treated as a ordered categorical.\\n           If not given, do not change the ordered information.\\n        inplace : bool, default False\\n           Whether or not to reorder the categories inplace or return a copy of\\n           this categorical with reordered categories.\\n\\n        Returns\\n        -------\\n        cat : Categorical with reordered categories or None if inplace.\\n\\n        Raises\\n        ------\\n        ValueError\\n            If the new categories do not contain all old category items or any\\n            new ones\\n\\n        See Also\\n        --------\\n        rename_categories\\n        add_categories\\n        remove_categories\\n        remove_unused_categories\\n        set_categories\\n        \"\"\"\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        if set(self.dtype.categories) != set(new_categories):\\n            raise ValueError(\"items in new_categories are not the same as in \"\\n                             \"old categories\")\\n        return self.set_categories(new_categories, ordered=ordered,\\n                                   inplace=inplace)',\n 'def add_categories(self, new_categories, inplace=False):\\n        \"\"\"\\n        Add new categories.\\n\\n        `new_categories` will be included at the last/highest place in the\\n        categories and will be unused directly after this call.\\n\\n        Parameters\\n        ----------\\n        new_categories : category or list-like of category\\n           The new categories to be included.\\n        inplace : bool, default False\\n           Whether or not to add the categories inplace or return a copy of\\n           this categorical with added categories.\\n\\n        Returns\\n        -------\\n        cat : Categorical with new categories added or None if inplace.\\n\\n        Raises\\n        ------\\n        ValueError\\n            If the new categories include old categories or do not validate as\\n            categories\\n\\n        See Also\\n        --------\\n        rename_categories\\n        reorder_categories\\n        remove_categories\\n        remove_unused_categories\\n        set_categories\\n        \"\"\"\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        if not is_list_like(new_categories):\\n            new_categories = [new_categories]\\n        already_included = set(new_categories) & set(self.dtype.categories)\\n        if len(already_included) != 0:\\n            msg = (\"new categories must not include old categories: \"\\n                   \"{already_included!s}\")\\n            raise ValueError(msg.format(already_included=already_included))\\n        new_categories = list(self.dtype.categories) + list(new_categories)\\n        new_dtype = CategoricalDtype(new_categories, self.ordered)\\n\\n        cat = self if inplace else self.copy()\\n        cat._dtype = new_dtype\\n        cat._codes = coerce_indexer_dtype(cat._codes, new_dtype.categories)\\n        if not inplace:\\n            return cat',\n 'def remove_categories(self, removals, inplace=False):\\n        \"\"\"\\n        Remove the specified categories.\\n\\n        `removals` must be included in the old categories. Values which were in\\n        the removed categories will be set to NaN\\n\\n        Parameters\\n        ----------\\n        removals : category or list of categories\\n           The categories which should be removed.\\n        inplace : bool, default False\\n           Whether or not to remove the categories inplace or return a copy of\\n           this categorical with removed categories.\\n\\n        Returns\\n        -------\\n        cat : Categorical with removed categories or None if inplace.\\n\\n        Raises\\n        ------\\n        ValueError\\n            If the removals are not contained in the categories\\n\\n        See Also\\n        --------\\n        rename_categories\\n        reorder_categories\\n        add_categories\\n        remove_unused_categories\\n        set_categories\\n        \"\"\"\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        if not is_list_like(removals):\\n            removals = [removals]\\n\\n        removal_set = set(list(removals))\\n        not_included = removal_set - set(self.dtype.categories)\\n        new_categories = [c for c in self.dtype.categories\\n                          if c not in removal_set]\\n\\n        # GH 10156\\n        if any(isna(removals)):\\n            not_included = [x for x in not_included if notna(x)]\\n            new_categories = [x for x in new_categories if notna(x)]\\n\\n        if len(not_included) != 0:\\n            msg = \"removals must all be in old categories: {not_included!s}\"\\n            raise ValueError(msg.format(not_included=not_included))\\n\\n        return self.set_categories(new_categories, ordered=self.ordered,\\n                                   rename=False, inplace=inplace)',\n 'def remove_unused_categories(self, inplace=False):\\n        \"\"\"\\n        Remove categories which are not used.\\n\\n        Parameters\\n        ----------\\n        inplace : bool, default False\\n           Whether or not to drop unused categories inplace or return a copy of\\n           this categorical with unused categories dropped.\\n\\n        Returns\\n        -------\\n        cat : Categorical with unused categories dropped or None if inplace.\\n\\n        See Also\\n        --------\\n        rename_categories\\n        reorder_categories\\n        add_categories\\n        remove_categories\\n        set_categories\\n        \"\"\"\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        cat = self if inplace else self.copy()\\n        idx, inv = np.unique(cat._codes, return_inverse=True)\\n\\n        if idx.size != 0 and idx[0] == -1:  # na sentinel\\n            idx, inv = idx[1:], inv - 1\\n\\n        new_categories = cat.dtype.categories.take(idx)\\n        new_dtype = CategoricalDtype._from_fastpath(new_categories,\\n                                                    ordered=self.ordered)\\n        cat._dtype = new_dtype\\n        cat._codes = coerce_indexer_dtype(inv, new_dtype.categories)\\n\\n        if not inplace:\\n            return cat',\n 'def map(self, mapper):\\n        \"\"\"\\n        Map categories using input correspondence (dict, Series, or function).\\n\\n        Maps the categories to new categories. If the mapping correspondence is\\n        one-to-one the result is a :class:`~pandas.Categorical` which has the\\n        same order property as the original, otherwise a :class:`~pandas.Index`\\n        is returned. NaN values are unaffected.\\n\\n        If a `dict` or :class:`~pandas.Series` is used any unmapped category is\\n        mapped to `NaN`. Note that if this happens an :class:`~pandas.Index`\\n        will be returned.\\n\\n        Parameters\\n        ----------\\n        mapper : function, dict, or Series\\n            Mapping correspondence.\\n\\n        Returns\\n        -------\\n        pandas.Categorical or pandas.Index\\n            Mapped categorical.\\n\\n        See Also\\n        --------\\n        CategoricalIndex.map : Apply a mapping correspondence on a\\n            :class:`~pandas.CategoricalIndex`.\\n        Index.map : Apply a mapping correspondence on an\\n            :class:`~pandas.Index`.\\n        Series.map : Apply a mapping correspondence on a\\n            :class:`~pandas.Series`.\\n        Series.apply : Apply more complex functions on a\\n            :class:`~pandas.Series`.\\n\\n        Examples\\n        --------\\n        >>> cat = pd.Categorical([\\'a\\', \\'b\\', \\'c\\'])\\n        >>> cat\\n        [a, b, c]\\n        Categories (3, object): [a, b, c]\\n        >>> cat.map(lambda x: x.upper())\\n        [A, B, C]\\n        Categories (3, object): [A, B, C]\\n        >>> cat.map({\\'a\\': \\'first\\', \\'b\\': \\'second\\', \\'c\\': \\'third\\'})\\n        [first, second, third]\\n        Categories (3, object): [first, second, third]\\n\\n        If the mapping is one-to-one the ordering of the categories is\\n        preserved:\\n\\n        >>> cat = pd.Categorical([\\'a\\', \\'b\\', \\'c\\'], ordered=True)\\n        >>> cat\\n        [a, b, c]\\n        Categories (3, object): [a < b < c]\\n        >>> cat.map({\\'a\\': 3, \\'b\\': 2, \\'c\\': 1})\\n        [3, 2, 1]\\n        Categories (3, int64): [3 < 2 < 1]\\n\\n        If the mapping is not one-to-one an :class:`~pandas.Index` is returned:\\n\\n        >>> cat.map({\\'a\\': \\'first\\', \\'b\\': \\'second\\', \\'c\\': \\'first\\'})\\n        Index([\\'first\\', \\'second\\', \\'first\\'], dtype=\\'object\\')\\n\\n        If a `dict` is used, all unmapped categories are mapped to `NaN` and\\n        the result is an :class:`~pandas.Index`:\\n\\n        >>> cat.map({\\'a\\': \\'first\\', \\'b\\': \\'second\\'})\\n        Index([\\'first\\', \\'second\\', nan], dtype=\\'object\\')\\n        \"\"\"\\n        new_categories = self.categories.map(mapper)\\n        try:\\n            return self.from_codes(self._codes.copy(),\\n                                   categories=new_categories,\\n                                   ordered=self.ordered)\\n        except ValueError:\\n            # NA values are represented in self._codes with -1\\n            # np.take causes NA values to take final element in new_categories\\n            if np.any(self._codes == -1):\\n                new_categories = new_categories.insert(len(new_categories),\\n                                                       np.nan)\\n            return np.take(new_categories, self._codes)',\n 'def shift(self, periods, fill_value=None):\\n        \"\"\"\\n        Shift Categorical by desired number of periods.\\n\\n        Parameters\\n        ----------\\n        periods : int\\n            Number of periods to move, can be positive or negative\\n        fill_value : object, optional\\n            The scalar value to use for newly introduced missing values.\\n\\n            .. versionadded:: 0.24.0\\n\\n        Returns\\n        -------\\n        shifted : Categorical\\n        \"\"\"\\n        # since categoricals always have ndim == 1, an axis parameter\\n        # doesn\\'t make any sense here.\\n        codes = self.codes\\n        if codes.ndim > 1:\\n            raise NotImplementedError(\"Categorical with ndim > 1.\")\\n        if np.prod(codes.shape) and (periods != 0):\\n            codes = np.roll(codes, ensure_platform_int(periods), axis=0)\\n            if isna(fill_value):\\n                fill_value = -1\\n            elif fill_value in self.categories:\\n                fill_value = self.categories.get_loc(fill_value)\\n            else:\\n                raise ValueError(\"\\'fill_value={}\\' is not present \"\\n                                 \"in this Categorical\\'s \"\\n                                 \"categories\".format(fill_value))\\n            if periods > 0:\\n                codes[:periods] = fill_value\\n            else:\\n                codes[periods:] = fill_value\\n\\n        return self.from_codes(codes, dtype=self.dtype)',\n 'def memory_usage(self, deep=False):\\n        \"\"\"\\n        Memory usage of my values\\n\\n        Parameters\\n        ----------\\n        deep : bool\\n            Introspect the data deeply, interrogate\\n            `object` dtypes for system-level memory consumption\\n\\n        Returns\\n        -------\\n        bytes used\\n\\n        Notes\\n        -----\\n        Memory usage does not include memory consumed by elements that\\n        are not components of the array if deep=False\\n\\n        See Also\\n        --------\\n        numpy.ndarray.nbytes\\n        \"\"\"\\n        return self._codes.nbytes + self.dtype.categories.memory_usage(\\n            deep=deep)',\n 'def value_counts(self, dropna=True):\\n        \"\"\"\\n        Return a Series containing counts of each category.\\n\\n        Every category will have an entry, even those with a count of 0.\\n\\n        Parameters\\n        ----------\\n        dropna : bool, default True\\n            Don\\'t include counts of NaN.\\n\\n        Returns\\n        -------\\n        counts : Series\\n\\n        See Also\\n        --------\\n        Series.value_counts\\n\\n        \"\"\"\\n        from numpy import bincount\\n        from pandas import Series, CategoricalIndex\\n\\n        code, cat = self._codes, self.categories\\n        ncat, mask = len(cat), 0 <= code\\n        ix, clean = np.arange(ncat), mask.all()\\n\\n        if dropna or clean:\\n            obs = code if clean else code[mask]\\n            count = bincount(obs, minlength=ncat or None)\\n        else:\\n            count = bincount(np.where(mask, code, ncat))\\n            ix = np.append(ix, -1)\\n\\n        ix = self._constructor(ix, dtype=self.dtype,\\n                               fastpath=True)\\n\\n        return Series(count, index=CategoricalIndex(ix), dtype=\\'int64\\')',\n 'def get_values(self):\\n        \"\"\"\\n        Return the values.\\n\\n        For internal compatibility with pandas formatting.\\n\\n        Returns\\n        -------\\n        numpy.array\\n            A numpy array of the same dtype as categorical.categories.dtype or\\n            Index if datetime / periods.\\n        \"\"\"\\n        # if we are a datetime and period index, return Index to keep metadata\\n        if is_datetimelike(self.categories):\\n            return self.categories.take(self._codes, fill_value=np.nan)\\n        elif is_integer_dtype(self.categories) and -1 in self._codes:\\n            return self.categories.astype(\"object\").take(self._codes,\\n                                                         fill_value=np.nan)\\n        return np.array(self)',\n 'def sort_values(self, inplace=False, ascending=True, na_position=\\'last\\'):\\n        \"\"\"\\n        Sort the Categorical by category value returning a new\\n        Categorical by default.\\n\\n        While an ordering is applied to the category values, sorting in this\\n        context refers more to organizing and grouping together based on\\n        matching category values. Thus, this function can be called on an\\n        unordered Categorical instance unlike the functions \\'Categorical.min\\'\\n        and \\'Categorical.max\\'.\\n\\n        Parameters\\n        ----------\\n        inplace : bool, default False\\n            Do operation in place.\\n        ascending : bool, default True\\n            Order ascending. Passing False orders descending. The\\n            ordering parameter provides the method by which the\\n            category values are organized.\\n        na_position : {\\'first\\', \\'last\\'} (optional, default=\\'last\\')\\n            \\'first\\' puts NaNs at the beginning\\n            \\'last\\' puts NaNs at the end\\n\\n        Returns\\n        -------\\n        Categorical or None\\n\\n        See Also\\n        --------\\n        Categorical.sort\\n        Series.sort_values\\n\\n        Examples\\n        --------\\n        >>> c = pd.Categorical([1, 2, 2, 1, 5])\\n        >>> c\\n        [1, 2, 2, 1, 5]\\n        Categories (3, int64): [1, 2, 5]\\n        >>> c.sort_values()\\n        [1, 1, 2, 2, 5]\\n        Categories (3, int64): [1, 2, 5]\\n        >>> c.sort_values(ascending=False)\\n        [5, 2, 2, 1, 1]\\n        Categories (3, int64): [1, 2, 5]\\n\\n        Inplace sorting can be done as well:\\n\\n        >>> c.sort_values(inplace=True)\\n        >>> c\\n        [1, 1, 2, 2, 5]\\n        Categories (3, int64): [1, 2, 5]\\n        >>>\\n        >>> c = pd.Categorical([1, 2, 2, 1, 5])\\n\\n        \\'sort_values\\' behaviour with NaNs. Note that \\'na_position\\'\\n        is independent of the \\'ascending\\' parameter:\\n\\n        >>> c = pd.Categorical([np.nan, 2, 2, np.nan, 5])\\n        >>> c\\n        [NaN, 2.0, 2.0, NaN, 5.0]\\n        Categories (2, int64): [2, 5]\\n        >>> c.sort_values()\\n        [2.0, 2.0, 5.0, NaN, NaN]\\n        Categories (2, int64): [2, 5]\\n        >>> c.sort_values(ascending=False)\\n        [5.0, 2.0, 2.0, NaN, NaN]\\n        Categories (2, int64): [2, 5]\\n        >>> c.sort_values(na_position=\\'first\\')\\n        [NaN, NaN, 2.0, 2.0, 5.0]\\n        Categories (2, int64): [2, 5]\\n        >>> c.sort_values(ascending=False, na_position=\\'first\\')\\n        [NaN, NaN, 5.0, 2.0, 2.0]\\n        Categories (2, int64): [2, 5]\\n        \"\"\"\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        if na_position not in [\\'last\\', \\'first\\']:\\n            msg = \\'invalid na_position: {na_position!r}\\'\\n            raise ValueError(msg.format(na_position=na_position))\\n\\n        sorted_idx = nargsort(self,\\n                              ascending=ascending,\\n                              na_position=na_position)\\n\\n        if inplace:\\n            self._codes = self._codes[sorted_idx]\\n        else:\\n            return self._constructor(values=self._codes[sorted_idx],\\n                                     dtype=self.dtype,\\n                                     fastpath=True)',\n 'def _values_for_rank(self):\\n        \"\"\"\\n        For correctly ranking ordered categorical data. See GH#15420\\n\\n        Ordered categorical data should be ranked on the basis of\\n        codes with -1 translated to NaN.\\n\\n        Returns\\n        -------\\n        numpy.array\\n\\n        \"\"\"\\n        from pandas import Series\\n        if self.ordered:\\n            values = self.codes\\n            mask = values == -1\\n            if mask.any():\\n                values = values.astype(\\'float64\\')\\n                values[mask] = np.nan\\n        elif self.categories.is_numeric():\\n            values = np.array(self)\\n        else:\\n            #  reorder the categories (so rank can use the float codes)\\n            #  instead of passing an object array to rank\\n            values = np.array(\\n                self.rename_categories(Series(self.categories).rank().values)\\n            )\\n        return values',\n 'def fillna(self, value=None, method=None, limit=None):\\n        \"\"\"\\n        Fill NA/NaN values using the specified method.\\n\\n        Parameters\\n        ----------\\n        value : scalar, dict, Series\\n            If a scalar value is passed it is used to fill all missing values.\\n            Alternatively, a Series or dict can be used to fill in different\\n            values for each index. The value should not be a list. The\\n            value(s) passed should either be in the categories or should be\\n            NaN.\\n        method : {\\'backfill\\', \\'bfill\\', \\'pad\\', \\'ffill\\', None}, default None\\n            Method to use for filling holes in reindexed Series\\n            pad / ffill: propagate last valid observation forward to next valid\\n            backfill / bfill: use NEXT valid observation to fill gap\\n        limit : int, default None\\n            (Not implemented yet for Categorical!)\\n            If method is specified, this is the maximum number of consecutive\\n            NaN values to forward/backward fill. In other words, if there is\\n            a gap with more than this number of consecutive NaNs, it will only\\n            be partially filled. If method is not specified, this is the\\n            maximum number of entries along the entire axis where NaNs will be\\n            filled.\\n\\n        Returns\\n        -------\\n        filled : Categorical with NA/NaN filled\\n        \"\"\"\\n        value, method = validate_fillna_kwargs(\\n            value, method, validate_scalar_dict_value=False\\n        )\\n\\n        if value is None:\\n            value = np.nan\\n        if limit is not None:\\n            raise NotImplementedError(\"specifying a limit for fillna has not \"\\n                                      \"been implemented yet\")\\n\\n        codes = self._codes\\n\\n        # pad / bfill\\n        if method is not None:\\n\\n            values = self.to_dense().reshape(-1, len(self))\\n            values = interpolate_2d(values, method, 0, None,\\n                                    value).astype(self.categories.dtype)[0]\\n            codes = _get_codes_for_values(values, self.categories)\\n\\n        else:\\n\\n            # If value is a dict or a Series (a dict value has already\\n            # been converted to a Series)\\n            if isinstance(value, ABCSeries):\\n                if not value[~value.isin(self.categories)].isna().all():\\n                    raise ValueError(\"fill value must be in categories\")\\n\\n                values_codes = _get_codes_for_values(value, self.categories)\\n                indexer = np.where(values_codes != -1)\\n                codes[indexer] = values_codes[values_codes != -1]\\n\\n            # If value is not a dict or Series it should be a scalar\\n            elif is_hashable(value):\\n                if not isna(value) and value not in self.categories:\\n                    raise ValueError(\"fill value must be in categories\")\\n\\n                mask = codes == -1\\n                if mask.any():\\n                    codes = codes.copy()\\n                    if isna(value):\\n                        codes[mask] = -1\\n                    else:\\n                        codes[mask] = self.categories.get_loc(value)\\n\\n            else:\\n                raise TypeError(\\'\"value\" parameter must be a scalar, dict \\'\\n                                \\'or Series, but you passed a \\'\\n                                \\'\"{0}\"\\'.format(type(value).__name__))\\n\\n        return self._constructor(codes, dtype=self.dtype, fastpath=True)',\n 'def take_nd(self, indexer, allow_fill=None, fill_value=None):\\n        \"\"\"\\n        Take elements from the Categorical.\\n\\n        Parameters\\n        ----------\\n        indexer : sequence of int\\n            The indices in `self` to take. The meaning of negative values in\\n            `indexer` depends on the value of `allow_fill`.\\n        allow_fill : bool, default None\\n            How to handle negative values in `indexer`.\\n\\n            * False: negative values in `indices` indicate positional indices\\n              from the right. This is similar to\\n              :func:`numpy.take`.\\n\\n            * True: negative values in `indices` indicate missing values\\n              (the default). These values are set to `fill_value`. Any other\\n              other negative values raise a ``ValueError``.\\n\\n            .. versionchanged:: 0.23.0\\n\\n               Deprecated the default value of `allow_fill`. The deprecated\\n               default is ``True``. In the future, this will change to\\n               ``False``.\\n\\n        fill_value : object\\n            The value to use for `indices` that are missing (-1), when\\n            ``allow_fill=True``. This should be the category, i.e. a value\\n            in ``self.categories``, not a code.\\n\\n        Returns\\n        -------\\n        Categorical\\n            This Categorical will have the same categories and ordered as\\n            `self`.\\n\\n        See Also\\n        --------\\n        Series.take : Similar method for Series.\\n        numpy.ndarray.take : Similar method for NumPy arrays.\\n\\n        Examples\\n        --------\\n        >>> cat = pd.Categorical([\\'a\\', \\'a\\', \\'b\\'])\\n        >>> cat\\n        [a, a, b]\\n        Categories (2, object): [a, b]\\n\\n        Specify ``allow_fill==False`` to have negative indices mean indexing\\n        from the right.\\n\\n        >>> cat.take([0, -1, -2], allow_fill=False)\\n        [a, b, a]\\n        Categories (2, object): [a, b]\\n\\n        With ``allow_fill=True``, indices equal to ``-1`` mean \"missing\"\\n        values that should be filled with the `fill_value`, which is\\n        ``np.nan`` by default.\\n\\n        >>> cat.take([0, -1, -1], allow_fill=True)\\n        [a, NaN, NaN]\\n        Categories (2, object): [a, b]\\n\\n        The fill value can be specified.\\n\\n        >>> cat.take([0, -1, -1], allow_fill=True, fill_value=\\'a\\')\\n        [a, a, a]\\n        Categories (3, object): [a, b]\\n\\n        Specifying a fill value that\\'s not in ``self.categories``\\n        will raise a ``TypeError``.\\n        \"\"\"\\n        indexer = np.asarray(indexer, dtype=np.intp)\\n        if allow_fill is None:\\n            if (indexer < 0).any():\\n                warn(_take_msg, FutureWarning, stacklevel=2)\\n                allow_fill = True\\n\\n        dtype = self.dtype\\n\\n        if isna(fill_value):\\n            fill_value = -1\\n        elif allow_fill:\\n            # convert user-provided `fill_value` to codes\\n            if fill_value in self.categories:\\n                fill_value = self.categories.get_loc(fill_value)\\n            else:\\n                msg = (\\n                    \"\\'fill_value\\' (\\'{}\\') is not in this Categorical\\'s \"\\n                    \"categories.\"\\n                )\\n                raise TypeError(msg.format(fill_value))\\n\\n        codes = take(self._codes, indexer, allow_fill=allow_fill,\\n                     fill_value=fill_value)\\n        result = type(self).from_codes(codes, dtype=dtype)\\n        return result',\n 'def _slice(self, slicer):\\n        \"\"\"\\n        Return a slice of myself.\\n\\n        For internal compatibility with numpy arrays.\\n        \"\"\"\\n\\n        # only allow 1 dimensional slicing, but can\\n        # in a 2-d case be passd (slice(None),....)\\n        if isinstance(slicer, tuple) and len(slicer) == 2:\\n            if not com.is_null_slice(slicer[0]):\\n                raise AssertionError(\"invalid slicing for a 1-ndim \"\\n                                     \"categorical\")\\n            slicer = slicer[1]\\n\\n        codes = self._codes[slicer]\\n        return self._constructor(values=codes, dtype=self.dtype, fastpath=True)',\n 'def _tidy_repr(self, max_vals=10, footer=True):\\n        \"\"\" a short repr displaying only max_vals and an optional (but default\\n        footer)\\n        \"\"\"\\n        num = max_vals // 2\\n        head = self[:num]._get_repr(length=False, footer=False)\\n        tail = self[-(max_vals - num):]._get_repr(length=False, footer=False)\\n\\n        result = \\'{head}, ..., {tail}\\'.format(head=head[:-1], tail=tail[1:])\\n        if footer:\\n            result = \\'{result}\\\\n{footer}\\'.format(\\n                result=result, footer=self._repr_footer())\\n\\n        return str(result)',\n 'def _repr_categories(self):\\n        \"\"\"\\n        return the base repr for the categories\\n        \"\"\"\\n        max_categories = (10 if get_option(\"display.max_categories\") == 0 else\\n                          get_option(\"display.max_categories\"))\\n        from pandas.io.formats import format as fmt\\n        if len(self.categories) > max_categories:\\n            num = max_categories // 2\\n            head = fmt.format_array(self.categories[:num], None)\\n            tail = fmt.format_array(self.categories[-num:], None)\\n            category_strs = head + [\"...\"] + tail\\n        else:\\n            category_strs = fmt.format_array(self.categories, None)\\n\\n        # Strip all leading spaces, which format_array adds for columns...\\n        category_strs = [x.strip() for x in category_strs]\\n        return category_strs',\n 'def _repr_categories_info(self):\\n        \"\"\"\\n        Returns a string representation of the footer.\\n        \"\"\"\\n\\n        category_strs = self._repr_categories()\\n        dtype = getattr(self.categories, \\'dtype_str\\',\\n                        str(self.categories.dtype))\\n\\n        levheader = \"Categories ({length}, {dtype}): \".format(\\n            length=len(self.categories), dtype=dtype)\\n        width, height = get_terminal_size()\\n        max_width = get_option(\"display.width\") or width\\n        if console.in_ipython_frontend():\\n            # 0 = no breaks\\n            max_width = 0\\n        levstring = \"\"\\n        start = True\\n        cur_col_len = len(levheader)  # header\\n        sep_len, sep = (3, \" < \") if self.ordered else (2, \", \")\\n        linesep = sep.rstrip() + \"\\\\n\"  # remove whitespace\\n        for val in category_strs:\\n            if max_width != 0 and cur_col_len + sep_len + len(val) > max_width:\\n                levstring += linesep + (\" \" * (len(levheader) + 1))\\n                cur_col_len = len(levheader) + 1  # header + a whitespace\\n            elif not start:\\n                levstring += sep\\n                cur_col_len += len(val)\\n            levstring += val\\n            start = False\\n        # replace to simple save space by\\n        return levheader + \"[\" + levstring.replace(\" < ... < \", \" ... \") + \"]\"',\n 'def _maybe_coerce_indexer(self, indexer):\\n        \"\"\"\\n        return an indexer coerced to the codes dtype\\n        \"\"\"\\n        if isinstance(indexer, np.ndarray) and indexer.dtype.kind == \\'i\\':\\n            indexer = indexer.astype(self._codes.dtype)\\n        return indexer',\n 'def _reverse_indexer(self):\\n        \"\"\"\\n        Compute the inverse of a categorical, returning\\n        a dict of categories -> indexers.\\n\\n        *This is an internal function*\\n\\n        Returns\\n        -------\\n        dict of categories -> indexers\\n\\n        Example\\n        -------\\n        In [1]: c = pd.Categorical(list(\\'aabca\\'))\\n\\n        In [2]: c\\n        Out[2]:\\n        [a, a, b, c, a]\\n        Categories (3, object): [a, b, c]\\n\\n        In [3]: c.categories\\n        Out[3]: Index([\\'a\\', \\'b\\', \\'c\\'], dtype=\\'object\\')\\n\\n        In [4]: c.codes\\n        Out[4]: array([0, 0, 1, 2, 0], dtype=int8)\\n\\n        In [5]: c._reverse_indexer()\\n        Out[5]: {\\'a\\': array([0, 1, 4]), \\'b\\': array([2]), \\'c\\': array([3])}\\n\\n        \"\"\"\\n        categories = self.categories\\n        r, counts = libalgos.groupsort_indexer(self.codes.astype(\\'int64\\'),\\n                                               categories.size)\\n        counts = counts.cumsum()\\n        result = (r[start:end] for start, end in zip(counts, counts[1:]))\\n        result = dict(zip(categories, result))\\n        return result',\n 'def min(self, numeric_only=None, **kwargs):\\n        \"\"\"\\n        The minimum value of the object.\\n\\n        Only ordered `Categoricals` have a minimum!\\n\\n        Raises\\n        ------\\n        TypeError\\n            If the `Categorical` is not `ordered`.\\n\\n        Returns\\n        -------\\n        min : the minimum of this `Categorical`\\n        \"\"\"\\n        self.check_for_ordered(\\'min\\')\\n        if numeric_only:\\n            good = self._codes != -1\\n            pointer = self._codes[good].min(**kwargs)\\n        else:\\n            pointer = self._codes.min(**kwargs)\\n        if pointer == -1:\\n            return np.nan\\n        else:\\n            return self.categories[pointer]',\n 'def mode(self, dropna=True):\\n        \"\"\"\\n        Returns the mode(s) of the Categorical.\\n\\n        Always returns `Categorical` even if only one value.\\n\\n        Parameters\\n        ----------\\n        dropna : bool, default True\\n            Don\\'t consider counts of NaN/NaT.\\n\\n            .. versionadded:: 0.24.0\\n\\n        Returns\\n        -------\\n        modes : `Categorical` (sorted)\\n        \"\"\"\\n\\n        import pandas._libs.hashtable as htable\\n        codes = self._codes\\n        if dropna:\\n            good = self._codes != -1\\n            codes = self._codes[good]\\n        codes = sorted(htable.mode_int64(ensure_int64(codes), dropna))\\n        return self._constructor(values=codes, dtype=self.dtype, fastpath=True)',\n 'def unique(self):\\n        \"\"\"\\n        Return the ``Categorical`` which ``categories`` and ``codes`` are\\n        unique. Unused categories are NOT returned.\\n\\n        - unordered category: values and categories are sorted by appearance\\n          order.\\n        - ordered category: values are sorted by appearance order, categories\\n          keeps existing order.\\n\\n        Returns\\n        -------\\n        unique values : ``Categorical``\\n\\n        Examples\\n        --------\\n        An unordered Categorical will return categories in the\\n        order of appearance.\\n\\n        >>> pd.Categorical(list(\\'baabc\\'))\\n        [b, a, c]\\n        Categories (3, object): [b, a, c]\\n\\n        >>> pd.Categorical(list(\\'baabc\\'), categories=list(\\'abc\\'))\\n        [b, a, c]\\n        Categories (3, object): [b, a, c]\\n\\n        An ordered Categorical preserves the category ordering.\\n\\n        >>> pd.Categorical(list(\\'baabc\\'),\\n        ...                categories=list(\\'abc\\'),\\n        ...                ordered=True)\\n        [b, a, c]\\n        Categories (3, object): [a < b < c]\\n\\n        See Also\\n        --------\\n        unique\\n        CategoricalIndex.unique\\n        Series.unique\\n\\n        \"\"\"\\n\\n        # unlike np.unique, unique1d does not sort\\n        unique_codes = unique1d(self.codes)\\n        cat = self.copy()\\n\\n        # keep nan in codes\\n        cat._codes = unique_codes\\n\\n        # exclude nan from indexer for categories\\n        take_codes = unique_codes[unique_codes != -1]\\n        if self.ordered:\\n            take_codes = np.sort(take_codes)\\n        return cat.set_categories(cat.categories.take(take_codes))',\n 'def equals(self, other):\\n        \"\"\"\\n        Returns True if categorical arrays are equal.\\n\\n        Parameters\\n        ----------\\n        other : `Categorical`\\n\\n        Returns\\n        -------\\n        bool\\n        \"\"\"\\n        if self.is_dtype_equal(other):\\n            if self.categories.equals(other.categories):\\n                # fastpath to avoid re-coding\\n                other_codes = other._codes\\n            else:\\n                other_codes = _recode_for_categories(other.codes,\\n                                                     other.categories,\\n                                                     self.categories)\\n            return np.array_equal(self._codes, other_codes)\\n        return False',\n 'def is_dtype_equal(self, other):\\n        \"\"\"\\n        Returns True if categoricals are the same dtype\\n          same categories, and same ordered\\n\\n        Parameters\\n        ----------\\n        other : Categorical\\n\\n        Returns\\n        -------\\n        bool\\n        \"\"\"\\n\\n        try:\\n            return hash(self.dtype) == hash(other.dtype)\\n        except (AttributeError, TypeError):\\n            return False',\n 'def describe(self):\\n        \"\"\"\\n        Describes this Categorical\\n\\n        Returns\\n        -------\\n        description: `DataFrame`\\n            A dataframe with frequency and counts by category.\\n        \"\"\"\\n        counts = self.value_counts(dropna=False)\\n        freqs = counts / float(counts.sum())\\n\\n        from pandas.core.reshape.concat import concat\\n        result = concat([counts, freqs], axis=1)\\n        result.columns = [\\'counts\\', \\'freqs\\']\\n        result.index.name = \\'categories\\'\\n\\n        return result',\n 'def isin(self, values):\\n        \"\"\"\\n        Check whether `values` are contained in Categorical.\\n\\n        Return a boolean NumPy Array showing whether each element in\\n        the Categorical matches an element in the passed sequence of\\n        `values` exactly.\\n\\n        Parameters\\n        ----------\\n        values : set or list-like\\n            The sequence of values to test. Passing in a single string will\\n            raise a ``TypeError``. Instead, turn a single string into a\\n            list of one element.\\n\\n        Returns\\n        -------\\n        isin : numpy.ndarray (bool dtype)\\n\\n        Raises\\n        ------\\n        TypeError\\n          * If `values` is not a set or list-like\\n\\n        See Also\\n        --------\\n        pandas.Series.isin : Equivalent method on Series.\\n\\n        Examples\\n        --------\\n\\n        >>> s = pd.Categorical([\\'lama\\', \\'cow\\', \\'lama\\', \\'beetle\\', \\'lama\\',\\n        ...                \\'hippo\\'])\\n        >>> s.isin([\\'cow\\', \\'lama\\'])\\n        array([ True,  True,  True, False,  True, False])\\n\\n        Passing a single string as ``s.isin(\\'lama\\')`` will raise an error. Use\\n        a list of one element instead:\\n\\n        >>> s.isin([\\'lama\\'])\\n        array([ True, False,  True, False,  True, False])\\n        \"\"\"\\n        from pandas.core.internals.construction import sanitize_array\\n        if not is_list_like(values):\\n            raise TypeError(\"only list-like objects are allowed to be passed\"\\n                            \" to isin(), you passed a [{values_type}]\"\\n                            .format(values_type=type(values).__name__))\\n        values = sanitize_array(values, None, None)\\n        null_mask = np.asarray(isna(values))\\n        code_values = self.categories.get_indexer(values)\\n        code_values = code_values[null_mask | (code_values >= 0)]\\n        return algorithms.isin(self.codes, code_values)',\n 'def to_timedelta(arg, unit=\\'ns\\', box=True, errors=\\'raise\\'):\\n    \"\"\"\\n    Convert argument to timedelta.\\n\\n    Timedeltas are absolute differences in times, expressed in difference\\n    units (e.g. days, hours, minutes, seconds). This method converts\\n    an argument from a recognized timedelta format / value into\\n    a Timedelta type.\\n\\n    Parameters\\n    ----------\\n    arg : str, timedelta, list-like or Series\\n        The data to be converted to timedelta.\\n    unit : str, default \\'ns\\'\\n        Denotes the unit of the arg. Possible values:\\n        (\\'Y\\', \\'M\\', \\'W\\', \\'D\\', \\'days\\', \\'day\\', \\'hours\\', hour\\', \\'hr\\',\\n        \\'h\\', \\'m\\', \\'minute\\', \\'min\\', \\'minutes\\', \\'T\\', \\'S\\', \\'seconds\\',\\n        \\'sec\\', \\'second\\', \\'ms\\', \\'milliseconds\\', \\'millisecond\\',\\n        \\'milli\\', \\'millis\\', \\'L\\', \\'us\\', \\'microseconds\\', \\'microsecond\\',\\n        \\'micro\\', \\'micros\\', \\'U\\', \\'ns\\', \\'nanoseconds\\', \\'nano\\', \\'nanos\\',\\n        \\'nanosecond\\', \\'N\\').\\n    box : bool, default True\\n        - If True returns a Timedelta/TimedeltaIndex of the results.\\n        - If False returns a numpy.timedelta64 or numpy.darray of\\n          values of dtype timedelta64[ns].\\n\\n        .. deprecated:: 0.25.0\\n            Use :meth:`.to_numpy` or :meth:`Timedelta.to_timedelta64`\\n            instead to get an ndarray of values or numpy.timedelta64,\\n            respectively.\\n\\n    errors : {\\'ignore\\', \\'raise\\', \\'coerce\\'}, default \\'raise\\'\\n        - If \\'raise\\', then invalid parsing will raise an exception.\\n        - If \\'coerce\\', then invalid parsing will be set as NaT.\\n        - If \\'ignore\\', then invalid parsing will return the input.\\n\\n    Returns\\n    -------\\n    timedelta64 or numpy.array of timedelta64\\n        Output type returned if parsing succeeded.\\n\\n    See Also\\n    --------\\n    DataFrame.astype : Cast argument to a specified dtype.\\n    to_datetime : Convert argument to datetime.\\n\\n    Examples\\n    --------\\n\\n    Parsing a single string to a Timedelta:\\n\\n    >>> pd.to_timedelta(\\'1 days 06:05:01.00003\\')\\n    Timedelta(\\'1 days 06:05:01.000030\\')\\n    >>> pd.to_timedelta(\\'15.5us\\')\\n    Timedelta(\\'0 days 00:00:00.000015\\')\\n\\n    Parsing a list or array of strings:\\n\\n    >>> pd.to_timedelta([\\'1 days 06:05:01.00003\\', \\'15.5us\\', \\'nan\\'])\\n    TimedeltaIndex([\\'1 days 06:05:01.000030\\', \\'0 days 00:00:00.000015\\', NaT],\\n                   dtype=\\'timedelta64[ns]\\', freq=None)\\n\\n    Converting numbers by specifying the `unit` keyword argument:\\n\\n    >>> pd.to_timedelta(np.arange(5), unit=\\'s\\')\\n    TimedeltaIndex([\\'00:00:00\\', \\'00:00:01\\', \\'00:00:02\\',\\n                    \\'00:00:03\\', \\'00:00:04\\'],\\n                   dtype=\\'timedelta64[ns]\\', freq=None)\\n    >>> pd.to_timedelta(np.arange(5), unit=\\'d\\')\\n    TimedeltaIndex([\\'0 days\\', \\'1 days\\', \\'2 days\\', \\'3 days\\', \\'4 days\\'],\\n                   dtype=\\'timedelta64[ns]\\', freq=None)\\n\\n    Returning an ndarray by using the \\'box\\' keyword argument:\\n\\n    >>> pd.to_timedelta(np.arange(5), box=False)\\n    array([0, 1, 2, 3, 4], dtype=\\'timedelta64[ns]\\')\\n    \"\"\"\\n    unit = parse_timedelta_unit(unit)\\n\\n    if errors not in (\\'ignore\\', \\'raise\\', \\'coerce\\'):\\n        raise ValueError(\"errors must be one of \\'ignore\\', \"\\n                         \"\\'raise\\', or \\'coerce\\'}\")\\n\\n    if unit in {\\'Y\\', \\'y\\', \\'M\\'}:\\n        warnings.warn(\"M and Y units are deprecated and \"\\n                      \"will be removed in a future version.\",\\n                      FutureWarning, stacklevel=2)\\n\\n    if arg is None:\\n        return arg\\n    elif isinstance(arg, ABCSeries):\\n        values = _convert_listlike(arg._values, unit=unit,\\n                                   box=False, errors=errors)\\n        return arg._constructor(values, index=arg.index, name=arg.name)\\n    elif isinstance(arg, ABCIndexClass):\\n        return _convert_listlike(arg, unit=unit, box=box,\\n                                 errors=errors, name=arg.name)\\n    elif isinstance(arg, np.ndarray) and arg.ndim == 0:\\n        # extract array scalar and process below\\n        arg = arg.item()\\n    elif is_list_like(arg) and getattr(arg, \\'ndim\\', 1) == 1:\\n        return _convert_listlike(arg, unit=unit, box=box, errors=errors)\\n    elif getattr(arg, \\'ndim\\', 1) > 1:\\n        raise TypeError(\\'arg must be a string, timedelta, list, tuple, \\'\\n                        \\'1-d array, or Series\\')\\n\\n    # ...so it must be a scalar value. Return scalar.\\n    return _coerce_scalar_to_timedelta_type(arg, unit=unit,\\n                                            box=box, errors=errors)',\n 'def _coerce_scalar_to_timedelta_type(r, unit=\\'ns\\', box=True, errors=\\'raise\\'):\\n    \"\"\"Convert string \\'r\\' to a timedelta object.\"\"\"\\n\\n    try:\\n        result = Timedelta(r, unit)\\n        if not box:\\n            # explicitly view as timedelta64 for case when result is pd.NaT\\n            result = result.asm8.view(\\'timedelta64[ns]\\')\\n    except ValueError:\\n        if errors == \\'raise\\':\\n            raise\\n        elif errors == \\'ignore\\':\\n            return r\\n\\n        # coerce\\n        result = NaT\\n\\n    return result',\n 'def _convert_listlike(arg, unit=\\'ns\\', box=True, errors=\\'raise\\', name=None):\\n    \"\"\"Convert a list of objects to a timedelta index object.\"\"\"\\n\\n    if isinstance(arg, (list, tuple)) or not hasattr(arg, \\'dtype\\'):\\n        # This is needed only to ensure that in the case where we end up\\n        #  returning arg (errors == \"ignore\"), and where the input is a\\n        #  generator, we return a useful list-like instead of a\\n        #  used-up generator\\n        arg = np.array(list(arg), dtype=object)\\n\\n    try:\\n        value = sequence_to_td64ns(arg, unit=unit,\\n                                   errors=errors, copy=False)[0]\\n    except ValueError:\\n        if errors == \\'ignore\\':\\n            return arg\\n        else:\\n            # This else-block accounts for the cases when errors=\\'raise\\'\\n            # and errors=\\'coerce\\'. If errors == \\'raise\\', these errors\\n            # should be raised. If errors == \\'coerce\\', we shouldn\\'t\\n            # expect any errors to be raised, since all parsing errors\\n            # cause coercion to pd.NaT. However, if an error / bug is\\n            # introduced that causes an Exception to be raised, we would\\n            # like to surface it.\\n            raise\\n\\n    if box:\\n        from pandas import TimedeltaIndex\\n        value = TimedeltaIndex(value, unit=\\'ns\\', name=name)\\n    return value',\n 'def generate_range(start=None, end=None, periods=None, offset=BDay()):\\n    \"\"\"\\n    Generates a sequence of dates corresponding to the specified time\\n    offset. Similar to dateutil.rrule except uses pandas DateOffset\\n    objects to represent time increments.\\n\\n    Parameters\\n    ----------\\n    start : datetime (default None)\\n    end : datetime (default None)\\n    periods : int, (default None)\\n    offset : DateOffset, (default BDay())\\n\\n    Notes\\n    -----\\n    * This method is faster for generating weekdays than dateutil.rrule\\n    * At least two of (start, end, periods) must be specified.\\n    * If both start and end are specified, the returned dates will\\n    satisfy start <= date <= end.\\n\\n    Returns\\n    -------\\n    dates : generator object\\n    \"\"\"\\n    from pandas.tseries.frequencies import to_offset\\n    offset = to_offset(offset)\\n\\n    start = to_datetime(start)\\n    end = to_datetime(end)\\n\\n    if start and not offset.onOffset(start):\\n        start = offset.rollforward(start)\\n\\n    elif end and not offset.onOffset(end):\\n        end = offset.rollback(end)\\n\\n    if periods is None and end < start and offset.n >= 0:\\n        end = None\\n        periods = 0\\n\\n    if end is None:\\n        end = start + (periods - 1) * offset\\n\\n    if start is None:\\n        start = end - (periods - 1) * offset\\n\\n    cur = start\\n    if offset.n >= 0:\\n        while cur <= end:\\n            yield cur\\n\\n            # faster than cur + offset\\n            next_date = offset.apply(cur)\\n            if next_date <= cur:\\n                raise ValueError(\\'Offset {offset} did not increment date\\'\\n                                 .format(offset=offset))\\n            cur = next_date\\n    else:\\n        while cur >= end:\\n            yield cur\\n\\n            # faster than cur + offset\\n            next_date = offset.apply(cur)\\n            if next_date >= cur:\\n                raise ValueError(\\'Offset {offset} did not decrement date\\'\\n                                 .format(offset=offset))\\n            cur = next_date',\n 'def apply_index(self, i):\\n        \"\"\"\\n        Vectorized apply of DateOffset to DatetimeIndex,\\n        raises NotImplentedError for offsets without a\\n        vectorized implementation.\\n\\n        Parameters\\n        ----------\\n        i : DatetimeIndex\\n\\n        Returns\\n        -------\\n        y : DatetimeIndex\\n        \"\"\"\\n\\n        if type(self) is not DateOffset:\\n            raise NotImplementedError(\"DateOffset subclass {name} \"\\n                                      \"does not have a vectorized \"\\n                                      \"implementation\".format(\\n                                          name=self.__class__.__name__))\\n        kwds = self.kwds\\n        relativedelta_fast = {\\'years\\', \\'months\\', \\'weeks\\', \\'days\\', \\'hours\\',\\n                              \\'minutes\\', \\'seconds\\', \\'microseconds\\'}\\n        # relativedelta/_offset path only valid for base DateOffset\\n        if (self._use_relativedelta and\\n                set(kwds).issubset(relativedelta_fast)):\\n\\n            months = ((kwds.get(\\'years\\', 0) * 12 +\\n                       kwds.get(\\'months\\', 0)) * self.n)\\n            if months:\\n                shifted = liboffsets.shift_months(i.asi8, months)\\n                i = type(i)(shifted, freq=i.freq, dtype=i.dtype)\\n\\n            weeks = (kwds.get(\\'weeks\\', 0)) * self.n\\n            if weeks:\\n                # integer addition on PeriodIndex is deprecated,\\n                #   so we directly use _time_shift instead\\n                asper = i.to_period(\\'W\\')\\n                if not isinstance(asper._data, np.ndarray):\\n                    # unwrap PeriodIndex --> PeriodArray\\n                    asper = asper._data\\n                shifted = asper._time_shift(weeks)\\n                i = shifted.to_timestamp() + i.to_perioddelta(\\'W\\')\\n\\n            timedelta_kwds = {k: v for k, v in kwds.items()\\n                              if k in [\\'days\\', \\'hours\\', \\'minutes\\',\\n                                       \\'seconds\\', \\'microseconds\\']}\\n            if timedelta_kwds:\\n                delta = Timedelta(**timedelta_kwds)\\n                i = i + (self.n * delta)\\n            return i\\n        elif not self._use_relativedelta and hasattr(self, \\'_offset\\'):\\n            # timedelta\\n            return i + (self._offset * self.n)\\n        else:\\n            # relativedelta with other keywords\\n            kwd = set(kwds) - relativedelta_fast\\n            raise NotImplementedError(\"DateOffset with relativedelta \"\\n                                      \"keyword(s) {kwd} not able to be \"\\n                                      \"applied vectorized\".format(kwd=kwd))',\n 'def rollback(self, dt):\\n        \"\"\"\\n        Roll provided date backward to next offset only if not on offset.\\n        \"\"\"\\n        dt = as_timestamp(dt)\\n        if not self.onOffset(dt):\\n            dt = dt - self.__class__(1, normalize=self.normalize, **self.kwds)\\n        return dt',\n 'def rollforward(self, dt):\\n        \"\"\"\\n        Roll provided date forward to next offset only if not on offset.\\n        \"\"\"\\n        dt = as_timestamp(dt)\\n        if not self.onOffset(dt):\\n            dt = dt + self.__class__(1, normalize=self.normalize, **self.kwds)\\n        return dt',\n 'def next_bday(self):\\n        \"\"\"\\n        Used for moving to next business day.\\n        \"\"\"\\n        if self.n >= 0:\\n            nb_offset = 1\\n        else:\\n            nb_offset = -1\\n        if self._prefix.startswith(\\'C\\'):\\n            # CustomBusinessHour\\n            return CustomBusinessDay(n=nb_offset,\\n                                     weekmask=self.weekmask,\\n                                     holidays=self.holidays,\\n                                     calendar=self.calendar)\\n        else:\\n            return BusinessDay(n=nb_offset)',\n 'def _next_opening_time(self, other):\\n        \"\"\"\\n        If n is positive, return tomorrow\\'s business day opening time.\\n        Otherwise yesterday\\'s business day\\'s opening time.\\n\\n        Opening time always locates on BusinessDay.\\n        Otherwise, closing time may not if business hour extends over midnight.\\n        \"\"\"\\n        if not self.next_bday.onOffset(other):\\n            other = other + self.next_bday\\n        else:\\n            if self.n >= 0 and self.start < other.time():\\n                other = other + self.next_bday\\n            elif self.n < 0 and other.time() < self.start:\\n                other = other + self.next_bday\\n        return datetime(other.year, other.month, other.day,\\n                        self.start.hour, self.start.minute)',\n 'def _get_business_hours_by_sec(self):\\n        \"\"\"\\n        Return business hours in a day by seconds.\\n        \"\"\"\\n        if self._get_daytime_flag:\\n            # create dummy datetime to calculate businesshours in a day\\n            dtstart = datetime(2014, 4, 1, self.start.hour, self.start.minute)\\n            until = datetime(2014, 4, 1, self.end.hour, self.end.minute)\\n            return (until - dtstart).total_seconds()\\n        else:\\n            dtstart = datetime(2014, 4, 1, self.start.hour, self.start.minute)\\n            until = datetime(2014, 4, 2, self.end.hour, self.end.minute)\\n            return (until - dtstart).total_seconds()',\n 'def rollback(self, dt):\\n        \"\"\"\\n        Roll provided date backward to next offset only if not on offset.\\n        \"\"\"\\n        if not self.onOffset(dt):\\n            businesshours = self._get_business_hours_by_sec\\n            if self.n >= 0:\\n                dt = self._prev_opening_time(\\n                    dt) + timedelta(seconds=businesshours)\\n            else:\\n                dt = self._next_opening_time(\\n                    dt) + timedelta(seconds=businesshours)\\n        return dt',\n 'def rollforward(self, dt):\\n        \"\"\"\\n        Roll provided date forward to next offset only if not on offset.\\n        \"\"\"\\n        if not self.onOffset(dt):\\n            if self.n >= 0:\\n                return self._next_opening_time(dt)\\n            else:\\n                return self._prev_opening_time(dt)\\n        return dt',\n 'def _onOffset(self, dt, businesshours):\\n        \"\"\"\\n        Slight speedups using calculated values.\\n        \"\"\"\\n        # if self.normalize and not _is_normalized(dt):\\n        #     return False\\n        # Valid BH can be on the different BusinessDay during midnight\\n        # Distinguish by the time spent from previous opening time\\n        if self.n >= 0:\\n            op = self._prev_opening_time(dt)\\n        else:\\n            op = self._next_opening_time(dt)\\n        span = (dt - op).total_seconds()\\n        if span <= businesshours:\\n            return True\\n        else:\\n            return False',\n 'def cbday_roll(self):\\n        \"\"\"\\n        Define default roll function to be called in apply method.\\n        \"\"\"\\n        cbday = CustomBusinessDay(n=self.n, normalize=False, **self.kwds)\\n\\n        if self._prefix.endswith(\\'S\\'):\\n            # MonthBegin\\n            roll_func = cbday.rollforward\\n        else:\\n            # MonthEnd\\n            roll_func = cbday.rollback\\n        return roll_func',\n 'def month_roll(self):\\n        \"\"\"\\n        Define default roll function to be called in apply method.\\n        \"\"\"\\n        if self._prefix.endswith(\\'S\\'):\\n            # MonthBegin\\n            roll_func = self.m_offset.rollback\\n        else:\\n            # MonthEnd\\n            roll_func = self.m_offset.rollforward\\n        return roll_func',\n 'def _apply_index_days(self, i, roll):\\n        \"\"\"\\n        Add days portion of offset to DatetimeIndex i.\\n\\n        Parameters\\n        ----------\\n        i : DatetimeIndex\\n        roll : ndarray[int64_t]\\n\\n        Returns\\n        -------\\n        result : DatetimeIndex\\n        \"\"\"\\n        nanos = (roll % 2) * Timedelta(days=self.day_of_month - 1).value\\n        return i + nanos.astype(\\'timedelta64[ns]\\')',\n 'def _end_apply_index(self, dtindex):\\n        \"\"\"\\n        Add self to the given DatetimeIndex, specialized for case where\\n        self.weekday is non-null.\\n\\n        Parameters\\n        ----------\\n        dtindex : DatetimeIndex\\n\\n        Returns\\n        -------\\n        result : DatetimeIndex\\n        \"\"\"\\n        off = dtindex.to_perioddelta(\\'D\\')\\n\\n        base, mult = libfrequencies.get_freq_code(self.freqstr)\\n        base_period = dtindex.to_period(base)\\n        if not isinstance(base_period._data, np.ndarray):\\n            # unwrap PeriodIndex --> PeriodArray\\n            base_period = base_period._data\\n\\n        if self.n > 0:\\n            # when adding, dates on end roll to next\\n            normed = dtindex - off + Timedelta(1, \\'D\\') - Timedelta(1, \\'ns\\')\\n            roll = np.where(base_period.to_timestamp(how=\\'end\\') == normed,\\n                            self.n, self.n - 1)\\n            # integer-array addition on PeriodIndex is deprecated,\\n            #  so we use _addsub_int_array directly\\n            shifted = base_period._addsub_int_array(roll, operator.add)\\n            base = shifted.to_timestamp(how=\\'end\\')\\n        else:\\n            # integer addition on PeriodIndex is deprecated,\\n            #  so we use _time_shift directly\\n            roll = self.n\\n            base = base_period._time_shift(roll).to_timestamp(how=\\'end\\')\\n\\n        return base + off + Timedelta(1, \\'ns\\') - Timedelta(1, \\'D\\')',\n 'def _get_offset_day(self, other):\\n        \"\"\"\\n        Find the day in the same month as other that has the same\\n        weekday as self.weekday and is the self.week\\'th such day in the month.\\n\\n        Parameters\\n        ----------\\n        other : datetime\\n\\n        Returns\\n        -------\\n        day : int\\n        \"\"\"\\n        mstart = datetime(other.year, other.month, 1)\\n        wday = mstart.weekday()\\n        shift_days = (self.weekday - wday) % 7\\n        return 1 + shift_days + self.week * 7',\n 'def _get_offset_day(self, other):\\n        \"\"\"\\n        Find the day in the same month as other that has the same\\n        weekday as self.weekday and is the last such day in the month.\\n\\n        Parameters\\n        ----------\\n        other: datetime\\n\\n        Returns\\n        -------\\n        day: int\\n        \"\"\"\\n        dim = ccalendar.get_days_in_month(other.year, other.month)\\n        mend = datetime(other.year, other.month, dim)\\n        wday = mend.weekday()\\n        shift_days = (wday - self.weekday) % 7\\n        return dim - shift_days',\n 'def _rollback_to_year(self, other):\\n        \"\"\"\\n        Roll `other` back to the most recent date that was on a fiscal year\\n        end.\\n\\n        Return the date of that year-end, the number of full quarters\\n        elapsed between that year-end and other, and the remaining Timedelta\\n        since the most recent quarter-end.\\n\\n        Parameters\\n        ----------\\n        other : datetime or Timestamp\\n\\n        Returns\\n        -------\\n        tuple of\\n        prev_year_end : Timestamp giving most recent fiscal year end\\n        num_qtrs : int\\n        tdelta : Timedelta\\n        \"\"\"\\n        num_qtrs = 0\\n\\n        norm = Timestamp(other).tz_localize(None)\\n        start = self._offset.rollback(norm)\\n        # Note: start <= norm and self._offset.onOffset(start)\\n\\n        if start < norm:\\n            # roll adjustment\\n            qtr_lens = self.get_weeks(norm)\\n\\n            # check thet qtr_lens is consistent with self._offset addition\\n            end = liboffsets.shift_day(start, days=7 * sum(qtr_lens))\\n            assert self._offset.onOffset(end), (start, end, qtr_lens)\\n\\n            tdelta = norm - start\\n            for qlen in qtr_lens:\\n                if qlen * 7 <= tdelta.days:\\n                    num_qtrs += 1\\n                    tdelta -= Timedelta(days=qlen * 7)\\n                else:\\n                    break\\n        else:\\n            tdelta = Timedelta(0)\\n\\n        # Note: we always have tdelta.value >= 0\\n        return start, num_qtrs, tdelta',\n 'def concat(objs, axis=0, join=\\'outer\\', join_axes=None, ignore_index=False,\\n           keys=None, levels=None, names=None, verify_integrity=False,\\n           sort=None, copy=True):\\n    \"\"\"\\n    Concatenate pandas objects along a particular axis with optional set logic\\n    along the other axes.\\n\\n    Can also add a layer of hierarchical indexing on the concatenation axis,\\n    which may be useful if the labels are the same (or overlapping) on\\n    the passed axis number.\\n\\n    Parameters\\n    ----------\\n    objs : a sequence or mapping of Series, DataFrame, or Panel objects\\n        If a dict is passed, the sorted keys will be used as the `keys`\\n        argument, unless it is passed, in which case the values will be\\n        selected (see below). Any None objects will be dropped silently unless\\n        they are all None in which case a ValueError will be raised.\\n    axis : {0/\\'index\\', 1/\\'columns\\'}, default 0\\n        The axis to concatenate along.\\n    join : {\\'inner\\', \\'outer\\'}, default \\'outer\\'\\n        How to handle indexes on other axis (or axes).\\n    join_axes : list of Index objects\\n        Specific indexes to use for the other n - 1 axes instead of performing\\n        inner/outer set logic.\\n    ignore_index : bool, default False\\n        If True, do not use the index values along the concatenation axis. The\\n        resulting axis will be labeled 0, ..., n - 1. This is useful if you are\\n        concatenating objects where the concatenation axis does not have\\n        meaningful indexing information. Note the index values on the other\\n        axes are still respected in the join.\\n    keys : sequence, default None\\n        If multiple levels passed, should contain tuples. Construct\\n        hierarchical index using the passed keys as the outermost level.\\n    levels : list of sequences, default None\\n        Specific levels (unique values) to use for constructing a\\n        MultiIndex. Otherwise they will be inferred from the keys.\\n    names : list, default None\\n        Names for the levels in the resulting hierarchical index.\\n    verify_integrity : bool, default False\\n        Check whether the new concatenated axis contains duplicates. This can\\n        be very expensive relative to the actual data concatenation.\\n    sort : bool, default None\\n        Sort non-concatenation axis if it is not already aligned when `join`\\n        is \\'outer\\'. The current default of sorting is deprecated and will\\n        change to not-sorting in a future version of pandas.\\n\\n        Explicitly pass ``sort=True`` to silence the warning and sort.\\n        Explicitly pass ``sort=False`` to silence the warning and not sort.\\n\\n        This has no effect when ``join=\\'inner\\'``, which already preserves\\n        the order of the non-concatenation axis.\\n\\n        .. versionadded:: 0.23.0\\n\\n    copy : bool, default True\\n        If False, do not copy data unnecessarily.\\n\\n    Returns\\n    -------\\n    object, type of objs\\n        When concatenating all ``Series`` along the index (axis=0), a\\n        ``Series`` is returned. When ``objs`` contains at least one\\n        ``DataFrame``, a ``DataFrame`` is returned. When concatenating along\\n        the columns (axis=1), a ``DataFrame`` is returned.\\n\\n    See Also\\n    --------\\n    Series.append : Concatenate Series.\\n    DataFrame.append : Concatenate DataFrames.\\n    DataFrame.join : Join DataFrames using indexes.\\n    DataFrame.merge : Merge DataFrames by indexes or columns.\\n\\n    Notes\\n    -----\\n    The keys, levels, and names arguments are all optional.\\n\\n    A walkthrough of how this method fits in with other tools for combining\\n    pandas objects can be found `here\\n    <http://pandas.pydata.org/pandas-docs/stable/merging.html>`__.\\n\\n    Examples\\n    --------\\n    Combine two ``Series``.\\n\\n    >>> s1 = pd.Series([\\'a\\', \\'b\\'])\\n    >>> s2 = pd.Series([\\'c\\', \\'d\\'])\\n    >>> pd.concat([s1, s2])\\n    0    a\\n    1    b\\n    0    c\\n    1    d\\n    dtype: object\\n\\n    Clear the existing index and reset it in the result\\n    by setting the ``ignore_index`` option to ``True``.\\n\\n    >>> pd.concat([s1, s2], ignore_index=True)\\n    0    a\\n    1    b\\n    2    c\\n    3    d\\n    dtype: object\\n\\n    Add a hierarchical index at the outermost level of\\n    the data with the ``keys`` option.\\n\\n    >>> pd.concat([s1, s2], keys=[\\'s1\\', \\'s2\\'])\\n    s1  0    a\\n        1    b\\n    s2  0    c\\n        1    d\\n    dtype: object\\n\\n    Label the index keys you create with the ``names`` option.\\n\\n    >>> pd.concat([s1, s2], keys=[\\'s1\\', \\'s2\\'],\\n    ...           names=[\\'Series name\\', \\'Row ID\\'])\\n    Series name  Row ID\\n    s1           0         a\\n                 1         b\\n    s2           0         c\\n                 1         d\\n    dtype: object\\n\\n    Combine two ``DataFrame`` objects with identical columns.\\n\\n    >>> df1 = pd.DataFrame([[\\'a\\', 1], [\\'b\\', 2]],\\n    ...                    columns=[\\'letter\\', \\'number\\'])\\n    >>> df1\\n      letter  number\\n    0      a       1\\n    1      b       2\\n    >>> df2 = pd.DataFrame([[\\'c\\', 3], [\\'d\\', 4]],\\n    ...                    columns=[\\'letter\\', \\'number\\'])\\n    >>> df2\\n      letter  number\\n    0      c       3\\n    1      d       4\\n    >>> pd.concat([df1, df2])\\n      letter  number\\n    0      a       1\\n    1      b       2\\n    0      c       3\\n    1      d       4\\n\\n    Combine ``DataFrame`` objects with overlapping columns\\n    and return everything. Columns outside the intersection will\\n    be filled with ``NaN`` values.\\n\\n    >>> df3 = pd.DataFrame([[\\'c\\', 3, \\'cat\\'], [\\'d\\', 4, \\'dog\\']],\\n    ...                    columns=[\\'letter\\', \\'number\\', \\'animal\\'])\\n    >>> df3\\n      letter  number animal\\n    0      c       3    cat\\n    1      d       4    dog\\n    >>> pd.concat([df1, df3], sort=False)\\n      letter  number animal\\n    0      a       1    NaN\\n    1      b       2    NaN\\n    0      c       3    cat\\n    1      d       4    dog\\n\\n    Combine ``DataFrame`` objects with overlapping columns\\n    and return only those that are shared by passing ``inner`` to\\n    the ``join`` keyword argument.\\n\\n    >>> pd.concat([df1, df3], join=\"inner\")\\n      letter  number\\n    0      a       1\\n    1      b       2\\n    0      c       3\\n    1      d       4\\n\\n    Combine ``DataFrame`` objects horizontally along the x axis by\\n    passing in ``axis=1``.\\n\\n    >>> df4 = pd.DataFrame([[\\'bird\\', \\'polly\\'], [\\'monkey\\', \\'george\\']],\\n    ...                    columns=[\\'animal\\', \\'name\\'])\\n    >>> pd.concat([df1, df4], axis=1)\\n      letter  number  animal    name\\n    0      a       1    bird   polly\\n    1      b       2  monkey  george\\n\\n    Prevent the result from including duplicate index values with the\\n    ``verify_integrity`` option.\\n\\n    >>> df5 = pd.DataFrame([1], index=[\\'a\\'])\\n    >>> df5\\n       0\\n    a  1\\n    >>> df6 = pd.DataFrame([2], index=[\\'a\\'])\\n    >>> df6\\n       0\\n    a  2\\n    >>> pd.concat([df5, df6], verify_integrity=True)\\n    Traceback (most recent call last):\\n        ...\\n    ValueError: Indexes have overlapping values: [\\'a\\']\\n    \"\"\"\\n    op = _Concatenator(objs, axis=axis, join_axes=join_axes,\\n                       ignore_index=ignore_index, join=join,\\n                       keys=keys, levels=levels, names=names,\\n                       verify_integrity=verify_integrity,\\n                       copy=copy, sort=sort)\\n    return op.get_result()',\n 'def _get_concat_axis(self):\\n        \"\"\"\\n        Return index to be used along concatenation axis.\\n        \"\"\"\\n        if self._is_series:\\n            if self.axis == 0:\\n                indexes = [x.index for x in self.objs]\\n            elif self.ignore_index:\\n                idx = ibase.default_index(len(self.objs))\\n                return idx\\n            elif self.keys is None:\\n                names = [None] * len(self.objs)\\n                num = 0\\n                has_names = False\\n                for i, x in enumerate(self.objs):\\n                    if not isinstance(x, Series):\\n                        raise TypeError(\"Cannot concatenate type \\'Series\\' \"\\n                                        \"with object of type {type!r}\"\\n                                        .format(type=type(x).__name__))\\n                    if x.name is not None:\\n                        names[i] = x.name\\n                        has_names = True\\n                    else:\\n                        names[i] = num\\n                        num += 1\\n                if has_names:\\n                    return Index(names)\\n                else:\\n                    return ibase.default_index(len(self.objs))\\n            else:\\n                return ensure_index(self.keys).set_names(self.names)\\n        else:\\n            indexes = [x._data.axes[self.axis] for x in self.objs]\\n\\n        if self.ignore_index:\\n            idx = ibase.default_index(sum(len(i) for i in indexes))\\n            return idx\\n\\n        if self.keys is None:\\n            concat_axis = _concat_indexes(indexes)\\n        else:\\n            concat_axis = _make_concat_multiindex(indexes, self.keys,\\n                                                  self.levels, self.names)\\n\\n        self._maybe_check_integrity(concat_axis)\\n\\n        return concat_axis',\n 'def _in(x, y):\\n    \"\"\"Compute the vectorized membership of ``x in y`` if possible, otherwise\\n    use Python.\\n    \"\"\"\\n    try:\\n        return x.isin(y)\\n    except AttributeError:\\n        if is_list_like(x):\\n            try:\\n                return y.isin(x)\\n            except AttributeError:\\n                pass\\n        return x in y',\n 'def _not_in(x, y):\\n    \"\"\"Compute the vectorized membership of ``x not in y`` if possible,\\n    otherwise use Python.\\n    \"\"\"\\n    try:\\n        return ~x.isin(y)\\n    except AttributeError:\\n        if is_list_like(x):\\n            try:\\n                return ~y.isin(x)\\n            except AttributeError:\\n                pass\\n        return x not in y',\n 'def _cast_inplace(terms, acceptable_dtypes, dtype):\\n    \"\"\"Cast an expression inplace.\\n\\n    Parameters\\n    ----------\\n    terms : Op\\n        The expression that should cast.\\n    acceptable_dtypes : list of acceptable numpy.dtype\\n        Will not cast if term\\'s dtype in this list.\\n\\n        .. versionadded:: 0.19.0\\n\\n    dtype : str or numpy.dtype\\n        The dtype to cast to.\\n    \"\"\"\\n    dt = np.dtype(dtype)\\n    for term in terms:\\n        if term.type in acceptable_dtypes:\\n            continue\\n\\n        try:\\n            new_value = term.value.astype(dt)\\n        except AttributeError:\\n            new_value = dt.type(term.value)\\n        term.update(new_value)',\n 'def update(self, value):\\n        \"\"\"\\n        search order for local (i.e., @variable) variables:\\n\\n        scope, key_variable\\n        [(\\'locals\\', \\'local_name\\'),\\n         (\\'globals\\', \\'local_name\\'),\\n         (\\'locals\\', \\'key\\'),\\n         (\\'globals\\', \\'key\\')]\\n        \"\"\"\\n        key = self.name\\n\\n        # if it\\'s a variable name (otherwise a constant)\\n        if isinstance(key, str):\\n            self.env.swapkey(self.local_name, key, new_value=value)\\n\\n        self.value = value',\n 'def evaluate(self, env, engine, parser, term_type, eval_in_python):\\n        \"\"\"Evaluate a binary operation *before* being passed to the engine.\\n\\n        Parameters\\n        ----------\\n        env : Scope\\n        engine : str\\n        parser : str\\n        term_type : type\\n        eval_in_python : list\\n\\n        Returns\\n        -------\\n        term_type\\n            The \"pre-evaluated\" expression as an instance of ``term_type``\\n        \"\"\"\\n        if engine == \\'python\\':\\n            res = self(env)\\n        else:\\n            # recurse over the left/right nodes\\n            left = self.lhs.evaluate(env, engine=engine, parser=parser,\\n                                     term_type=term_type,\\n                                     eval_in_python=eval_in_python)\\n            right = self.rhs.evaluate(env, engine=engine, parser=parser,\\n                                      term_type=term_type,\\n                                      eval_in_python=eval_in_python)\\n\\n            # base cases\\n            if self.op in eval_in_python:\\n                res = self.func(left.value, right.value)\\n            else:\\n                from pandas.core.computation.eval import eval\\n                res = eval(self, local_dict=env, engine=engine,\\n                           parser=parser)\\n\\n        name = env.add_tmp(res)\\n        return term_type(name, env=env)',\n 'def convert_values(self):\\n        \"\"\"Convert datetimes to a comparable value in an expression.\\n        \"\"\"\\n        def stringify(value):\\n            if self.encoding is not None:\\n                encoder = partial(pprint_thing_encoded,\\n                                  encoding=self.encoding)\\n            else:\\n                encoder = pprint_thing\\n            return encoder(value)\\n\\n        lhs, rhs = self.lhs, self.rhs\\n\\n        if is_term(lhs) and lhs.is_datetime and is_term(rhs) and rhs.is_scalar:\\n            v = rhs.value\\n            if isinstance(v, (int, float)):\\n                v = stringify(v)\\n            v = Timestamp(_ensure_decoded(v))\\n            if v.tz is not None:\\n                v = v.tz_convert(\\'UTC\\')\\n            self.rhs.update(v)\\n\\n        if is_term(rhs) and rhs.is_datetime and is_term(lhs) and lhs.is_scalar:\\n            v = lhs.value\\n            if isinstance(v, (int, float)):\\n                v = stringify(v)\\n            v = Timestamp(_ensure_decoded(v))\\n            if v.tz is not None:\\n                v = v.tz_convert(\\'UTC\\')\\n            self.lhs.update(v)',\n 'def crosstab(index, columns, values=None, rownames=None, colnames=None,\\n             aggfunc=None, margins=False, margins_name=\\'All\\', dropna=True,\\n             normalize=False):\\n    \"\"\"\\n    Compute a simple cross tabulation of two (or more) factors. By default\\n    computes a frequency table of the factors unless an array of values and an\\n    aggregation function are passed.\\n\\n    Parameters\\n    ----------\\n    index : array-like, Series, or list of arrays/Series\\n        Values to group by in the rows.\\n    columns : array-like, Series, or list of arrays/Series\\n        Values to group by in the columns.\\n    values : array-like, optional\\n        Array of values to aggregate according to the factors.\\n        Requires `aggfunc` be specified.\\n    rownames : sequence, default None\\n        If passed, must match number of row arrays passed.\\n    colnames : sequence, default None\\n        If passed, must match number of column arrays passed.\\n    aggfunc : function, optional\\n        If specified, requires `values` be specified as well.\\n    margins : bool, default False\\n        Add row/column margins (subtotals).\\n    margins_name : str, default \\'All\\'\\n        Name of the row/column that will contain the totals\\n        when margins is True.\\n\\n        .. versionadded:: 0.21.0\\n\\n    dropna : bool, default True\\n        Do not include columns whose entries are all NaN.\\n    normalize : bool, {\\'all\\', \\'index\\', \\'columns\\'}, or {0,1}, default False\\n        Normalize by dividing all values by the sum of values.\\n\\n        - If passed \\'all\\' or `True`, will normalize over all values.\\n        - If passed \\'index\\' will normalize over each row.\\n        - If passed \\'columns\\' will normalize over each column.\\n        - If margins is `True`, will also normalize margin values.\\n\\n        .. versionadded:: 0.18.1\\n\\n    Returns\\n    -------\\n    DataFrame\\n        Cross tabulation of the data.\\n\\n    See Also\\n    --------\\n    DataFrame.pivot : Reshape data based on column values.\\n    pivot_table : Create a pivot table as a DataFrame.\\n\\n    Notes\\n    -----\\n    Any Series passed will have their name attributes used unless row or column\\n    names for the cross-tabulation are specified.\\n\\n    Any input passed containing Categorical data will have **all** of its\\n    categories included in the cross-tabulation, even if the actual data does\\n    not contain any instances of a particular category.\\n\\n    In the event that there aren\\'t overlapping indexes an empty DataFrame will\\n    be returned.\\n\\n    Examples\\n    --------\\n    >>> a = np.array([\"foo\", \"foo\", \"foo\", \"foo\", \"bar\", \"bar\",\\n    ...               \"bar\", \"bar\", \"foo\", \"foo\", \"foo\"], dtype=object)\\n    >>> b = np.array([\"one\", \"one\", \"one\", \"two\", \"one\", \"one\",\\n    ...               \"one\", \"two\", \"two\", \"two\", \"one\"], dtype=object)\\n    >>> c = np.array([\"dull\", \"dull\", \"shiny\", \"dull\", \"dull\", \"shiny\",\\n    ...               \"shiny\", \"dull\", \"shiny\", \"shiny\", \"shiny\"],\\n    ...              dtype=object)\\n    >>> pd.crosstab(a, [b, c], rownames=[\\'a\\'], colnames=[\\'b\\', \\'c\\'])\\n    b   one        two\\n    c   dull shiny dull shiny\\n    a\\n    bar    1     2    1     0\\n    foo    2     2    1     2\\n\\n    Here \\'c\\' and \\'f\\' are not represented in the data and will not be\\n    shown in the output because dropna is True by default. Set\\n    dropna=False to preserve categories with no data.\\n\\n    >>> foo = pd.Categorical([\\'a\\', \\'b\\'], categories=[\\'a\\', \\'b\\', \\'c\\'])\\n    >>> bar = pd.Categorical([\\'d\\', \\'e\\'], categories=[\\'d\\', \\'e\\', \\'f\\'])\\n    >>> pd.crosstab(foo, bar)\\n    col_0  d  e\\n    row_0\\n    a      1  0\\n    b      0  1\\n    >>> pd.crosstab(foo, bar, dropna=False)\\n    col_0  d  e  f\\n    row_0\\n    a      1  0  0\\n    b      0  1  0\\n    c      0  0  0\\n    \"\"\"\\n\\n    index = com.maybe_make_list(index)\\n    columns = com.maybe_make_list(columns)\\n\\n    rownames = _get_names(index, rownames, prefix=\\'row\\')\\n    colnames = _get_names(columns, colnames, prefix=\\'col\\')\\n\\n    common_idx = _get_objs_combined_axis(index + columns, intersect=True,\\n                                         sort=False)\\n\\n    data = {}\\n    data.update(zip(rownames, index))\\n    data.update(zip(colnames, columns))\\n\\n    if values is None and aggfunc is not None:\\n        raise ValueError(\"aggfunc cannot be used without values.\")\\n\\n    if values is not None and aggfunc is None:\\n        raise ValueError(\"values cannot be used without an aggfunc.\")\\n\\n    from pandas import DataFrame\\n    df = DataFrame(data, index=common_idx)\\n    if values is None:\\n        df[\\'__dummy__\\'] = 0\\n        kwargs = {\\'aggfunc\\': len, \\'fill_value\\': 0}\\n    else:\\n        df[\\'__dummy__\\'] = values\\n        kwargs = {\\'aggfunc\\': aggfunc}\\n\\n    table = df.pivot_table(\\'__dummy__\\', index=rownames, columns=colnames,\\n                           margins=margins, margins_name=margins_name,\\n                           dropna=dropna, **kwargs)\\n\\n    # Post-process\\n    if normalize is not False:\\n        table = _normalize(table, normalize=normalize, margins=margins,\\n                           margins_name=margins_name)\\n\\n    return table',\n 'def _shape(self, df):\\n        \"\"\"\\n        Calculate table chape considering index levels.\\n        \"\"\"\\n\\n        row, col = df.shape\\n        return row + df.columns.nlevels, col + df.index.nlevels',\n 'def _get_cells(self, left, right, vertical):\\n        \"\"\"\\n        Calculate appropriate figure size based on left and right data.\\n        \"\"\"\\n\\n        if vertical:\\n            # calculate required number of cells\\n            vcells = max(sum(self._shape(l)[0] for l in left),\\n                         self._shape(right)[0])\\n            hcells = (max(self._shape(l)[1] for l in left) +\\n                      self._shape(right)[1])\\n        else:\\n            vcells = max([self._shape(l)[0] for l in left] +\\n                         [self._shape(right)[0]])\\n            hcells = sum([self._shape(l)[1] for l in left] +\\n                         [self._shape(right)[1]])\\n        return hcells, vcells',\n 'def plot(self, left, right, labels=None, vertical=True):\\n        \"\"\"\\n        Plot left / right DataFrames in specified layout.\\n\\n        Parameters\\n        ----------\\n        left : list of DataFrames before operation is applied\\n        right : DataFrame of operation result\\n        labels : list of str to be drawn as titles of left DataFrames\\n        vertical : bool\\n            If True, use vertical layout. If False, use horizontal layout.\\n        \"\"\"\\n        import matplotlib.pyplot as plt\\n        import matplotlib.gridspec as gridspec\\n\\n        if not isinstance(left, list):\\n            left = [left]\\n        left = [self._conv(l) for l in left]\\n        right = self._conv(right)\\n\\n        hcells, vcells = self._get_cells(left, right, vertical)\\n\\n        if vertical:\\n            figsize = self.cell_width * hcells, self.cell_height * vcells\\n        else:\\n            # include margin for titles\\n            figsize = self.cell_width * hcells, self.cell_height * vcells\\n        fig = plt.figure(figsize=figsize)\\n\\n        if vertical:\\n            gs = gridspec.GridSpec(len(left), hcells)\\n            # left\\n            max_left_cols = max(self._shape(l)[1] for l in left)\\n            max_left_rows = max(self._shape(l)[0] for l in left)\\n            for i, (l, label) in enumerate(zip(left, labels)):\\n                ax = fig.add_subplot(gs[i, 0:max_left_cols])\\n                self._make_table(ax, l, title=label,\\n                                 height=1.0 / max_left_rows)\\n            # right\\n            ax = plt.subplot(gs[:, max_left_cols:])\\n            self._make_table(ax, right, title=\\'Result\\', height=1.05 / vcells)\\n            fig.subplots_adjust(top=0.9, bottom=0.05, left=0.05, right=0.95)\\n        else:\\n            max_rows = max(self._shape(df)[0] for df in left + [right])\\n            height = 1.0 / np.max(max_rows)\\n            gs = gridspec.GridSpec(1, hcells)\\n            # left\\n            i = 0\\n            for l, label in zip(left, labels):\\n                sp = self._shape(l)\\n                ax = fig.add_subplot(gs[0, i:i + sp[1]])\\n                self._make_table(ax, l, title=label, height=height)\\n                i += sp[1]\\n            # right\\n            ax = plt.subplot(gs[0, i:])\\n            self._make_table(ax, right, title=\\'Result\\', height=height)\\n            fig.subplots_adjust(top=0.85, bottom=0.05, left=0.05, right=0.95)\\n\\n        return fig',\n 'def _conv(self, data):\\n        \"\"\"Convert each input to appropriate for table outplot\"\"\"\\n        if isinstance(data, pd.Series):\\n            if data.name is None:\\n                data = data.to_frame(name=\\'\\')\\n            else:\\n                data = data.to_frame()\\n        data = data.fillna(\\'NaN\\')\\n        return data',\n 'def cut(x, bins, right=True, labels=None, retbins=False, precision=3,\\n        include_lowest=False, duplicates=\\'raise\\'):\\n    \"\"\"\\n    Bin values into discrete intervals.\\n\\n    Use `cut` when you need to segment and sort data values into bins. This\\n    function is also useful for going from a continuous variable to a\\n    categorical variable. For example, `cut` could convert ages to groups of\\n    age ranges. Supports binning into an equal number of bins, or a\\n    pre-specified array of bins.\\n\\n    Parameters\\n    ----------\\n    x : array-like\\n        The input array to be binned. Must be 1-dimensional.\\n    bins : int, sequence of scalars, or IntervalIndex\\n        The criteria to bin by.\\n\\n        * int : Defines the number of equal-width bins in the range of `x`. The\\n          range of `x` is extended by .1% on each side to include the minimum\\n          and maximum values of `x`.\\n        * sequence of scalars : Defines the bin edges allowing for non-uniform\\n          width. No extension of the range of `x` is done.\\n        * IntervalIndex : Defines the exact bins to be used. Note that\\n          IntervalIndex for `bins` must be non-overlapping.\\n\\n    right : bool, default True\\n        Indicates whether `bins` includes the rightmost edge or not. If\\n        ``right == True`` (the default), then the `bins` ``[1, 2, 3, 4]``\\n        indicate (1,2], (2,3], (3,4]. This argument is ignored when\\n        `bins` is an IntervalIndex.\\n    labels : array or bool, optional\\n        Specifies the labels for the returned bins. Must be the same length as\\n        the resulting bins. If False, returns only integer indicators of the\\n        bins. This affects the type of the output container (see below).\\n        This argument is ignored when `bins` is an IntervalIndex.\\n    retbins : bool, default False\\n        Whether to return the bins or not. Useful when bins is provided\\n        as a scalar.\\n    precision : int, default 3\\n        The precision at which to store and display the bins labels.\\n    include_lowest : bool, default False\\n        Whether the first interval should be left-inclusive or not.\\n    duplicates : {default \\'raise\\', \\'drop\\'}, optional\\n        If bin edges are not unique, raise ValueError or drop non-uniques.\\n\\n        .. versionadded:: 0.23.0\\n\\n    Returns\\n    -------\\n    out : Categorical, Series, or ndarray\\n        An array-like object representing the respective bin for each value\\n        of `x`. The type depends on the value of `labels`.\\n\\n        * True (default) : returns a Series for Series `x` or a\\n          Categorical for all other inputs. The values stored within\\n          are Interval dtype.\\n\\n        * sequence of scalars : returns a Series for Series `x` or a\\n          Categorical for all other inputs. The values stored within\\n          are whatever the type in the sequence is.\\n\\n        * False : returns an ndarray of integers.\\n\\n    bins : numpy.ndarray or IntervalIndex.\\n        The computed or specified bins. Only returned when `retbins=True`.\\n        For scalar or sequence `bins`, this is an ndarray with the computed\\n        bins. If set `duplicates=drop`, `bins` will drop non-unique bin. For\\n        an IntervalIndex `bins`, this is equal to `bins`.\\n\\n    See Also\\n    --------\\n    qcut : Discretize variable into equal-sized buckets based on rank\\n        or based on sample quantiles.\\n    Categorical : Array type for storing data that come from a\\n        fixed set of values.\\n    Series : One-dimensional array with axis labels (including time series).\\n    IntervalIndex : Immutable Index implementing an ordered, sliceable set.\\n\\n    Notes\\n    -----\\n    Any NA values will be NA in the result. Out of bounds values will be NA in\\n    the resulting Series or Categorical object.\\n\\n    Examples\\n    --------\\n    Discretize into three equal-sized bins.\\n\\n    >>> pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3)\\n    ... # doctest: +ELLIPSIS\\n    [(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ...\\n    Categories (3, interval[float64]): [(0.994, 3.0] < (3.0, 5.0] ...\\n\\n    >>> pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3, retbins=True)\\n    ... # doctest: +ELLIPSIS\\n    ([(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ...\\n    Categories (3, interval[float64]): [(0.994, 3.0] < (3.0, 5.0] ...\\n    array([0.994, 3.   , 5.   , 7.   ]))\\n\\n    Discovers the same bins, but assign them specific labels. Notice that\\n    the returned Categorical\\'s categories are `labels` and is ordered.\\n\\n    >>> pd.cut(np.array([1, 7, 5, 4, 6, 3]),\\n    ...        3, labels=[\"bad\", \"medium\", \"good\"])\\n    [bad, good, medium, medium, good, bad]\\n    Categories (3, object): [bad < medium < good]\\n\\n    ``labels=False`` implies you just want the bins back.\\n\\n    >>> pd.cut([0, 1, 1, 2], bins=4, labels=False)\\n    array([0, 1, 1, 3])\\n\\n    Passing a Series as an input returns a Series with categorical dtype:\\n\\n    >>> s = pd.Series(np.array([2, 4, 6, 8, 10]),\\n    ...               index=[\\'a\\', \\'b\\', \\'c\\', \\'d\\', \\'e\\'])\\n    >>> pd.cut(s, 3)\\n    ... # doctest: +ELLIPSIS\\n    a    (1.992, 4.667]\\n    b    (1.992, 4.667]\\n    c    (4.667, 7.333]\\n    d     (7.333, 10.0]\\n    e     (7.333, 10.0]\\n    dtype: category\\n    Categories (3, interval[float64]): [(1.992, 4.667] < (4.667, ...\\n\\n    Passing a Series as an input returns a Series with mapping value.\\n    It is used to map numerically to intervals based on bins.\\n\\n    >>> s = pd.Series(np.array([2, 4, 6, 8, 10]),\\n    ...               index=[\\'a\\', \\'b\\', \\'c\\', \\'d\\', \\'e\\'])\\n    >>> pd.cut(s, [0, 2, 4, 6, 8, 10], labels=False, retbins=True, right=False)\\n    ... # doctest: +ELLIPSIS\\n    (a    0.0\\n     b    1.0\\n     c    2.0\\n     d    3.0\\n     e    4.0\\n     dtype: float64, array([0, 2, 4, 6, 8]))\\n\\n    Use `drop` optional when bins is not unique\\n\\n    >>> pd.cut(s, [0, 2, 4, 6, 10, 10], labels=False, retbins=True,\\n    ...        right=False, duplicates=\\'drop\\')\\n    ... # doctest: +ELLIPSIS\\n    (a    0.0\\n     b    1.0\\n     c    2.0\\n     d    3.0\\n     e    3.0\\n     dtype: float64, array([0, 2, 4, 6, 8]))\\n\\n    Passing an IntervalIndex for `bins` results in those categories exactly.\\n    Notice that values not covered by the IntervalIndex are set to NaN. 0\\n    is to the left of the first bin (which is closed on the right), and 1.5\\n    falls between two bins.\\n\\n    >>> bins = pd.IntervalIndex.from_tuples([(0, 1), (2, 3), (4, 5)])\\n    >>> pd.cut([0, 0.5, 1.5, 2.5, 4.5], bins)\\n    [NaN, (0, 1], NaN, (2, 3], (4, 5]]\\n    Categories (3, interval[int64]): [(0, 1] < (2, 3] < (4, 5]]\\n    \"\"\"\\n    # NOTE: this binning code is changed a bit from histogram for var(x) == 0\\n\\n    # for handling the cut for datetime and timedelta objects\\n    x_is_series, series_index, name, x = _preprocess_for_cut(x)\\n    x, dtype = _coerce_to_type(x)\\n\\n    if not np.iterable(bins):\\n        if is_scalar(bins) and bins < 1:\\n            raise ValueError(\"`bins` should be a positive integer.\")\\n\\n        try:  # for array-like\\n            sz = x.size\\n        except AttributeError:\\n            x = np.asarray(x)\\n            sz = x.size\\n\\n        if sz == 0:\\n            raise ValueError(\\'Cannot cut empty array\\')\\n\\n        rng = (nanops.nanmin(x), nanops.nanmax(x))\\n        mn, mx = [mi + 0.0 for mi in rng]\\n\\n        if np.isinf(mn) or np.isinf(mx):\\n            # GH 24314\\n            raise ValueError(\\'cannot specify integer `bins` when input data \\'\\n                             \\'contains infinity\\')\\n        elif mn == mx:  # adjust end points before binning\\n            mn -= .001 * abs(mn) if mn != 0 else .001\\n            mx += .001 * abs(mx) if mx != 0 else .001\\n            bins = np.linspace(mn, mx, bins + 1, endpoint=True)\\n        else:  # adjust end points after binning\\n            bins = np.linspace(mn, mx, bins + 1, endpoint=True)\\n            adj = (mx - mn) * 0.001  # 0.1% of the range\\n            if right:\\n                bins[0] -= adj\\n            else:\\n                bins[-1] += adj\\n\\n    elif isinstance(bins, IntervalIndex):\\n        if bins.is_overlapping:\\n            raise ValueError(\\'Overlapping IntervalIndex is not accepted.\\')\\n\\n    else:\\n        if is_datetime64tz_dtype(bins):\\n            bins = np.asarray(bins, dtype=_NS_DTYPE)\\n        else:\\n            bins = np.asarray(bins)\\n        bins = _convert_bin_to_numeric_type(bins, dtype)\\n\\n        # GH 26045: cast to float64 to avoid an overflow\\n        if (np.diff(bins.astype(\\'float64\\')) < 0).any():\\n            raise ValueError(\\'bins must increase monotonically.\\')\\n\\n    fac, bins = _bins_to_cuts(x, bins, right=right, labels=labels,\\n                              precision=precision,\\n                              include_lowest=include_lowest,\\n                              dtype=dtype,\\n                              duplicates=duplicates)\\n\\n    return _postprocess_for_cut(fac, bins, retbins, x_is_series,\\n                                series_index, name, dtype)',\n 'def qcut(x, q, labels=None, retbins=False, precision=3, duplicates=\\'raise\\'):\\n    \"\"\"\\n    Quantile-based discretization function. Discretize variable into\\n    equal-sized buckets based on rank or based on sample quantiles. For example\\n    1000 values for 10 quantiles would produce a Categorical object indicating\\n    quantile membership for each data point.\\n\\n    Parameters\\n    ----------\\n    x : 1d ndarray or Series\\n    q : integer or array of quantiles\\n        Number of quantiles. 10 for deciles, 4 for quartiles, etc. Alternately\\n        array of quantiles, e.g. [0, .25, .5, .75, 1.] for quartiles\\n    labels : array or boolean, default None\\n        Used as labels for the resulting bins. Must be of the same length as\\n        the resulting bins. If False, return only integer indicators of the\\n        bins.\\n    retbins : bool, optional\\n        Whether to return the (bins, labels) or not. Can be useful if bins\\n        is given as a scalar.\\n    precision : int, optional\\n        The precision at which to store and display the bins labels\\n    duplicates : {default \\'raise\\', \\'drop\\'}, optional\\n        If bin edges are not unique, raise ValueError or drop non-uniques.\\n\\n        .. versionadded:: 0.20.0\\n\\n    Returns\\n    -------\\n    out : Categorical or Series or array of integers if labels is False\\n        The return type (Categorical or Series) depends on the input: a Series\\n        of type category if input is a Series else Categorical. Bins are\\n        represented as categories when categorical data is returned.\\n    bins : ndarray of floats\\n        Returned only if `retbins` is True.\\n\\n    Notes\\n    -----\\n    Out of bounds values will be NA in the resulting Categorical object\\n\\n    Examples\\n    --------\\n    >>> pd.qcut(range(5), 4)\\n    ... # doctest: +ELLIPSIS\\n    [(-0.001, 1.0], (-0.001, 1.0], (1.0, 2.0], (2.0, 3.0], (3.0, 4.0]]\\n    Categories (4, interval[float64]): [(-0.001, 1.0] < (1.0, 2.0] ...\\n\\n    >>> pd.qcut(range(5), 3, labels=[\"good\", \"medium\", \"bad\"])\\n    ... # doctest: +SKIP\\n    [good, good, medium, bad, bad]\\n    Categories (3, object): [good < medium < bad]\\n\\n    >>> pd.qcut(range(5), 4, labels=False)\\n    array([0, 0, 1, 2, 3])\\n    \"\"\"\\n    x_is_series, series_index, name, x = _preprocess_for_cut(x)\\n\\n    x, dtype = _coerce_to_type(x)\\n\\n    if is_integer(q):\\n        quantiles = np.linspace(0, 1, q + 1)\\n    else:\\n        quantiles = q\\n    bins = algos.quantile(x, quantiles)\\n    fac, bins = _bins_to_cuts(x, bins, labels=labels,\\n                              precision=precision, include_lowest=True,\\n                              dtype=dtype, duplicates=duplicates)\\n\\n    return _postprocess_for_cut(fac, bins, retbins, x_is_series,\\n                                series_index, name, dtype)',\n 'def _coerce_to_type(x):\\n    \"\"\"\\n    if the passed data is of datetime/timedelta type,\\n    this method converts it to numeric so that cut method can\\n    handle it\\n    \"\"\"\\n    dtype = None\\n\\n    if is_datetime64tz_dtype(x):\\n        dtype = x.dtype\\n    elif is_datetime64_dtype(x):\\n        x = to_datetime(x)\\n        dtype = np.dtype(\\'datetime64[ns]\\')\\n    elif is_timedelta64_dtype(x):\\n        x = to_timedelta(x)\\n        dtype = np.dtype(\\'timedelta64[ns]\\')\\n\\n    if dtype is not None:\\n        # GH 19768: force NaT to NaN during integer conversion\\n        x = np.where(x.notna(), x.view(np.int64), np.nan)\\n\\n    return x, dtype',\n 'def _convert_bin_to_numeric_type(bins, dtype):\\n    \"\"\"\\n    if the passed bin is of datetime/timedelta type,\\n    this method converts it to integer\\n\\n    Parameters\\n    ----------\\n    bins : list-like of bins\\n    dtype : dtype of data\\n\\n    Raises\\n    ------\\n    ValueError if bins are not of a compat dtype to dtype\\n    \"\"\"\\n    bins_dtype = infer_dtype(bins, skipna=False)\\n    if is_timedelta64_dtype(dtype):\\n        if bins_dtype in [\\'timedelta\\', \\'timedelta64\\']:\\n            bins = to_timedelta(bins).view(np.int64)\\n        else:\\n            raise ValueError(\"bins must be of timedelta64 dtype\")\\n    elif is_datetime64_dtype(dtype) or is_datetime64tz_dtype(dtype):\\n        if bins_dtype in [\\'datetime\\', \\'datetime64\\']:\\n            bins = to_datetime(bins).view(np.int64)\\n        else:\\n            raise ValueError(\"bins must be of datetime64 dtype\")\\n\\n    return bins',\n 'def _convert_bin_to_datelike_type(bins, dtype):\\n    \"\"\"\\n    Convert bins to a DatetimeIndex or TimedeltaIndex if the orginal dtype is\\n    datelike\\n\\n    Parameters\\n    ----------\\n    bins : list-like of bins\\n    dtype : dtype of data\\n\\n    Returns\\n    -------\\n    bins : Array-like of bins, DatetimeIndex or TimedeltaIndex if dtype is\\n           datelike\\n    \"\"\"\\n    if is_datetime64tz_dtype(dtype):\\n        bins = to_datetime(bins.astype(np.int64),\\n                           utc=True).tz_convert(dtype.tz)\\n    elif is_datetime_or_timedelta_dtype(dtype):\\n        bins = Index(bins.astype(np.int64), dtype=dtype)\\n    return bins',\n 'def _format_labels(bins, precision, right=True,\\n                   include_lowest=False, dtype=None):\\n    \"\"\" based on the dtype, return our labels \"\"\"\\n\\n    closed = \\'right\\' if right else \\'left\\'\\n\\n    if is_datetime64tz_dtype(dtype):\\n        formatter = partial(Timestamp, tz=dtype.tz)\\n        adjust = lambda x: x - Timedelta(\\'1ns\\')\\n    elif is_datetime64_dtype(dtype):\\n        formatter = Timestamp\\n        adjust = lambda x: x - Timedelta(\\'1ns\\')\\n    elif is_timedelta64_dtype(dtype):\\n        formatter = Timedelta\\n        adjust = lambda x: x - Timedelta(\\'1ns\\')\\n    else:\\n        precision = _infer_precision(precision, bins)\\n        formatter = lambda x: _round_frac(x, precision)\\n        adjust = lambda x: x - 10 ** (-precision)\\n\\n    breaks = [formatter(b) for b in bins]\\n    labels = IntervalIndex.from_breaks(breaks, closed=closed)\\n\\n    if right and include_lowest:\\n        # we will adjust the left hand side by precision to\\n        # account that we are all right closed\\n        v = adjust(labels[0].left)\\n\\n        i = IntervalIndex([Interval(v, labels[0].right, closed=\\'right\\')])\\n        labels = i.append(labels[1:])\\n\\n    return labels',\n 'def _preprocess_for_cut(x):\\n    \"\"\"\\n    handles preprocessing for cut where we convert passed\\n    input to array, strip the index information and store it\\n    separately\\n    \"\"\"\\n    x_is_series = isinstance(x, Series)\\n    series_index = None\\n    name = None\\n\\n    if x_is_series:\\n        series_index = x.index\\n        name = x.name\\n\\n    # Check that the passed array is a Pandas or Numpy object\\n    # We don\\'t want to strip away a Pandas data-type here (e.g. datetimetz)\\n    ndim = getattr(x, \\'ndim\\', None)\\n    if ndim is None:\\n        x = np.asarray(x)\\n    if x.ndim != 1:\\n        raise ValueError(\"Input array must be 1 dimensional\")\\n\\n    return x_is_series, series_index, name, x',\n 'def _postprocess_for_cut(fac, bins, retbins, x_is_series,\\n                         series_index, name, dtype):\\n    \"\"\"\\n    handles post processing for the cut method where\\n    we combine the index information if the originally passed\\n    datatype was a series\\n    \"\"\"\\n    if x_is_series:\\n        fac = Series(fac, index=series_index, name=name)\\n\\n    if not retbins:\\n        return fac\\n\\n    bins = _convert_bin_to_datelike_type(bins, dtype)\\n\\n    return fac, bins',\n 'def _round_frac(x, precision):\\n    \"\"\"\\n    Round the fractional part of the given number\\n    \"\"\"\\n    if not np.isfinite(x) or x == 0:\\n        return x\\n    else:\\n        frac, whole = np.modf(x)\\n        if whole == 0:\\n            digits = -int(np.floor(np.log10(abs(frac)))) - 1 + precision\\n        else:\\n            digits = precision\\n        return np.around(x, digits)',\n 'def _infer_precision(base_precision, bins):\\n    \"\"\"Infer an appropriate precision for _round_frac\\n    \"\"\"\\n    for precision in range(base_precision, 20):\\n        levels = [_round_frac(b, precision) for b in bins]\\n        if algos.unique(levels).size == bins.size:\\n            return precision\\n    return base_precision',\n 'def detect_console_encoding():\\n    \"\"\"\\n    Try to find the most capable encoding supported by the console.\\n    slightly modified from the way IPython handles the same issue.\\n    \"\"\"\\n    global _initial_defencoding\\n\\n    encoding = None\\n    try:\\n        encoding = sys.stdout.encoding or sys.stdin.encoding\\n    except (AttributeError, IOError):\\n        pass\\n\\n    # try again for something better\\n    if not encoding or \\'ascii\\' in encoding.lower():\\n        try:\\n            encoding = locale.getpreferredencoding()\\n        except Exception:\\n            pass\\n\\n    # when all else fails. this will usually be \"ascii\"\\n    if not encoding or \\'ascii\\' in encoding.lower():\\n        encoding = sys.getdefaultencoding()\\n\\n    # GH#3360, save the reported defencoding at import time\\n    # MPL backends may change it. Make available for debugging.\\n    if not _initial_defencoding:\\n        _initial_defencoding = sys.getdefaultencoding()\\n\\n    return encoding',\n 'def _check_arg_length(fname, args, max_fname_arg_count, compat_args):\\n    \"\"\"\\n    Checks whether \\'args\\' has length of at most \\'compat_args\\'. Raises\\n    a TypeError if that is not the case, similar to in Python when a\\n    function is called with too many arguments.\\n\\n    \"\"\"\\n    if max_fname_arg_count < 0:\\n        raise ValueError(\"\\'max_fname_arg_count\\' must be non-negative\")\\n\\n    if len(args) > len(compat_args):\\n        max_arg_count = len(compat_args) + max_fname_arg_count\\n        actual_arg_count = len(args) + max_fname_arg_count\\n        argument = \\'argument\\' if max_arg_count == 1 else \\'arguments\\'\\n\\n        raise TypeError(\\n            \"{fname}() takes at most {max_arg} {argument} \"\\n            \"({given_arg} given)\".format(\\n                fname=fname, max_arg=max_arg_count,\\n                argument=argument, given_arg=actual_arg_count))',\n 'def _check_for_default_values(fname, arg_val_dict, compat_args):\\n    \"\"\"\\n    Check that the keys in `arg_val_dict` are mapped to their\\n    default values as specified in `compat_args`.\\n\\n    Note that this function is to be called only when it has been\\n    checked that arg_val_dict.keys() is a subset of compat_args\\n\\n    \"\"\"\\n    for key in arg_val_dict:\\n        # try checking equality directly with \\'=\\' operator,\\n        # as comparison may have been overridden for the left\\n        # hand object\\n        try:\\n            v1 = arg_val_dict[key]\\n            v2 = compat_args[key]\\n\\n            # check for None-ness otherwise we could end up\\n            # comparing a numpy array vs None\\n            if (v1 is not None and v2 is None) or \\\\\\n               (v1 is None and v2 is not None):\\n                match = False\\n            else:\\n                match = (v1 == v2)\\n\\n            if not is_bool(match):\\n                raise ValueError(\"\\'match\\' is not a boolean\")\\n\\n        # could not compare them directly, so try comparison\\n        # using the \\'is\\' operator\\n        except ValueError:\\n            match = (arg_val_dict[key] is compat_args[key])\\n\\n        if not match:\\n            raise ValueError((\"the \\'{arg}\\' parameter is not \"\\n                              \"supported in the pandas \"\\n                              \"implementation of {fname}()\".\\n                              format(fname=fname, arg=key)))',\n 'def validate_args(fname, args, max_fname_arg_count, compat_args):\\n    \"\"\"\\n    Checks whether the length of the `*args` argument passed into a function\\n    has at most `len(compat_args)` arguments and whether or not all of these\\n    elements in `args` are set to their default values.\\n\\n    fname: str\\n        The name of the function being passed the `*args` parameter\\n\\n    args: tuple\\n        The `*args` parameter passed into a function\\n\\n    max_fname_arg_count: int\\n        The maximum number of arguments that the function `fname`\\n        can accept, excluding those in `args`. Used for displaying\\n        appropriate error messages. Must be non-negative.\\n\\n    compat_args: OrderedDict\\n        A ordered dictionary of keys and their associated default values.\\n        In order to accommodate buggy behaviour in some versions of `numpy`,\\n        where a signature displayed keyword arguments but then passed those\\n        arguments **positionally** internally when calling downstream\\n        implementations, an ordered dictionary ensures that the original\\n        order of the keyword arguments is enforced. Note that if there is\\n        only one key, a generic dict can be passed in as well.\\n\\n    Raises\\n    ------\\n    TypeError if `args` contains more values than there are `compat_args`\\n    ValueError if `args` contains values that do not correspond to those\\n    of the default values specified in `compat_args`\\n\\n    \"\"\"\\n    _check_arg_length(fname, args, max_fname_arg_count, compat_args)\\n\\n    # We do this so that we can provide a more informative\\n    # error message about the parameters that we are not\\n    # supporting in the pandas implementation of \\'fname\\'\\n    kwargs = dict(zip(compat_args, args))\\n    _check_for_default_values(fname, kwargs, compat_args)',\n 'def _check_for_invalid_keys(fname, kwargs, compat_args):\\n    \"\"\"\\n    Checks whether \\'kwargs\\' contains any keys that are not\\n    in \\'compat_args\\' and raises a TypeError if there is one.\\n\\n    \"\"\"\\n    # set(dict) --> set of the dictionary\\'s keys\\n    diff = set(kwargs) - set(compat_args)\\n\\n    if diff:\\n        bad_arg = list(diff)[0]\\n        raise TypeError((\"{fname}() got an unexpected \"\\n                         \"keyword argument \\'{arg}\\'\".\\n                         format(fname=fname, arg=bad_arg)))',\n 'def validate_kwargs(fname, kwargs, compat_args):\\n    \"\"\"\\n    Checks whether parameters passed to the **kwargs argument in a\\n    function `fname` are valid parameters as specified in `*compat_args`\\n    and whether or not they are set to their default values.\\n\\n    Parameters\\n    ----------\\n    fname: str\\n        The name of the function being passed the `**kwargs` parameter\\n\\n    kwargs: dict\\n        The `**kwargs` parameter passed into `fname`\\n\\n    compat_args: dict\\n        A dictionary of keys that `kwargs` is allowed to have and their\\n        associated default values\\n\\n    Raises\\n    ------\\n    TypeError if `kwargs` contains keys not in `compat_args`\\n    ValueError if `kwargs` contains keys in `compat_args` that do not\\n    map to the default values specified in `compat_args`\\n\\n    \"\"\"\\n    kwds = kwargs.copy()\\n    _check_for_invalid_keys(fname, kwargs, compat_args)\\n    _check_for_default_values(fname, kwds, compat_args)',\n 'def validate_args_and_kwargs(fname, args, kwargs,\\n                             max_fname_arg_count,\\n                             compat_args):\\n    \"\"\"\\n    Checks whether parameters passed to the *args and **kwargs argument in a\\n    function `fname` are valid parameters as specified in `*compat_args`\\n    and whether or not they are set to their default values.\\n\\n    Parameters\\n    ----------\\n    fname: str\\n        The name of the function being passed the `**kwargs` parameter\\n\\n    args: tuple\\n        The `*args` parameter passed into a function\\n\\n    kwargs: dict\\n        The `**kwargs` parameter passed into `fname`\\n\\n    max_fname_arg_count: int\\n        The minimum number of arguments that the function `fname`\\n        requires, excluding those in `args`. Used for displaying\\n        appropriate error messages. Must be non-negative.\\n\\n    compat_args: OrderedDict\\n        A ordered dictionary of keys that `kwargs` is allowed to\\n        have and their associated default values. Note that if there\\n        is only one key, a generic dict can be passed in as well.\\n\\n    Raises\\n    ------\\n    TypeError if `args` contains more values than there are\\n    `compat_args` OR `kwargs` contains keys not in `compat_args`\\n    ValueError if `args` contains values not at the default value (`None`)\\n    `kwargs` contains keys in `compat_args` that do not map to the default\\n    value as specified in `compat_args`\\n\\n    See Also\\n    --------\\n    validate_args : Purely args validation.\\n    validate_kwargs : Purely kwargs validation.\\n\\n    \"\"\"\\n    # Check that the total number of arguments passed in (i.e.\\n    # args and kwargs) does not exceed the length of compat_args\\n    _check_arg_length(fname, args + tuple(kwargs.values()),\\n                      max_fname_arg_count, compat_args)\\n\\n    # Check there is no overlap with the positional and keyword\\n    # arguments, similar to what is done in actual Python functions\\n    args_dict = dict(zip(compat_args, args))\\n\\n    for key in args_dict:\\n        if key in kwargs:\\n            raise TypeError(\"{fname}() got multiple values for keyword \"\\n                            \"argument \\'{arg}\\'\".format(fname=fname, arg=key))\\n\\n    kwargs.update(args_dict)\\n    validate_kwargs(fname, kwargs, compat_args)',\n 'def validate_bool_kwarg(value, arg_name):\\n    \"\"\" Ensures that argument passed in arg_name is of type bool. \"\"\"\\n    if not (is_bool(value) or value is None):\\n        raise ValueError(\\'For argument \"{arg}\" expected type bool, received \\'\\n                         \\'type {typ}.\\'.format(arg=arg_name,\\n                                              typ=type(value).__name__))\\n    return value',\n 'def validate_axis_style_args(data, args, kwargs, arg_name, method_name):\\n    \"\"\"Argument handler for mixed index, columns / axis functions\\n\\n    In an attempt to handle both `.method(index, columns)`, and\\n    `.method(arg, axis=.)`, we have to do some bad things to argument\\n    parsing. This translates all arguments to `{index=., columns=.}` style.\\n\\n    Parameters\\n    ----------\\n    data : DataFrame or Panel\\n    args : tuple\\n        All positional arguments from the user\\n    kwargs : dict\\n        All keyword arguments from the user\\n    arg_name, method_name : str\\n        Used for better error messages\\n\\n    Returns\\n    -------\\n    kwargs : dict\\n        A dictionary of keyword arguments. Doesn\\'t modify ``kwargs``\\n        inplace, so update them with the return value here.\\n\\n    Examples\\n    --------\\n    >>> df._validate_axis_style_args((str.upper,), {\\'columns\\': id},\\n    ...                              \\'mapper\\', \\'rename\\')\\n    {\\'columns\\': <function id>, \\'index\\': <method \\'upper\\' of \\'str\\' objects>}\\n\\n    This emits a warning\\n    >>> df._validate_axis_style_args((str.upper, id), {},\\n    ...                              \\'mapper\\', \\'rename\\')\\n    {\\'columns\\': <function id>, \\'index\\': <method \\'upper\\' of \\'str\\' objects>}\\n    \"\"\"\\n    # TODO: Change to keyword-only args and remove all this\\n\\n    out = {}\\n    # Goal: fill \\'out\\' with index/columns-style arguments\\n    # like out = {\\'index\\': foo, \\'columns\\': bar}\\n\\n    # Start by validating for consistency\\n    if \\'axis\\' in kwargs and any(x in kwargs for x in data._AXIS_NUMBERS):\\n        msg = \"Cannot specify both \\'axis\\' and any of \\'index\\' or \\'columns\\'.\"\\n        raise TypeError(msg)\\n\\n    # First fill with explicit values provided by the user...\\n    if arg_name in kwargs:\\n        if args:\\n            msg = (\"{} got multiple values for argument \"\\n                   \"\\'{}\\'\".format(method_name, arg_name))\\n            raise TypeError(msg)\\n\\n        axis = data._get_axis_name(kwargs.get(\\'axis\\', 0))\\n        out[axis] = kwargs[arg_name]\\n\\n    # More user-provided arguments, now from kwargs\\n    for k, v in kwargs.items():\\n        try:\\n            ax = data._get_axis_name(k)\\n        except ValueError:\\n            pass\\n        else:\\n            out[ax] = v\\n\\n    # All user-provided kwargs have been handled now.\\n    # Now we supplement with positional arguments, emitting warnings\\n    # when there\\'s ambiguity and raising when there\\'s conflicts\\n\\n    if len(args) == 0:\\n        pass  # It\\'s up to the function to decide if this is valid\\n    elif len(args) == 1:\\n        axis = data._get_axis_name(kwargs.get(\\'axis\\', 0))\\n        out[axis] = args[0]\\n    elif len(args) == 2:\\n        if \\'axis\\' in kwargs:\\n            # Unambiguously wrong\\n            msg = (\"Cannot specify both \\'axis\\' and any of \\'index\\' \"\\n                   \"or \\'columns\\'\")\\n            raise TypeError(msg)\\n\\n        msg = (\"Interpreting call\\\\n\\\\t\\'.{method_name}(a, b)\\' as \"\\n               \"\\\\n\\\\t\\'.{method_name}(index=a, columns=b)\\'.\\\\nUse named \"\\n               \"arguments to remove any ambiguity. In the future, using \"\\n               \"positional arguments for \\'index\\' or \\'columns\\' will raise \"\\n               \" a \\'TypeError\\'.\")\\n        warnings.warn(msg.format(method_name=method_name,), FutureWarning,\\n                      stacklevel=4)\\n        out[data._AXIS_NAMES[0]] = args[0]\\n        out[data._AXIS_NAMES[1]] = args[1]\\n    else:\\n        msg = \"Cannot specify all of \\'{}\\', \\'index\\', \\'columns\\'.\"\\n        raise TypeError(msg.format(arg_name))\\n    return out',\n 'def validate_fillna_kwargs(value, method, validate_scalar_dict_value=True):\\n    \"\"\"Validate the keyword arguments to \\'fillna\\'.\\n\\n    This checks that exactly one of \\'value\\' and \\'method\\' is specified.\\n    If \\'method\\' is specified, this validates that it\\'s a valid method.\\n\\n    Parameters\\n    ----------\\n    value, method : object\\n        The \\'value\\' and \\'method\\' keyword arguments for \\'fillna\\'.\\n    validate_scalar_dict_value : bool, default True\\n        Whether to validate that \\'value\\' is a scalar or dict. Specifically,\\n        validate that it is not a list or tuple.\\n\\n    Returns\\n    -------\\n    value, method : object\\n    \"\"\"\\n    from pandas.core.missing import clean_fill_method\\n\\n    if value is None and method is None:\\n        raise ValueError(\"Must specify a fill \\'value\\' or \\'method\\'.\")\\n    elif value is None and method is not None:\\n        method = clean_fill_method(method)\\n\\n    elif value is not None and method is None:\\n        if validate_scalar_dict_value and isinstance(value, (list, tuple)):\\n            raise TypeError(\\'\"value\" parameter must be a scalar or dict, but \\'\\n                            \\'you passed a \"{0}\"\\'.format(type(value).__name__))\\n\\n    elif value is not None and method is not None:\\n        raise ValueError(\"Cannot specify both \\'value\\' and \\'method\\'.\")\\n\\n    return value, method',\n 'def _maybe_process_deprecations(r, how=None, fill_method=None, limit=None):\\n    \"\"\"\\n    Potentially we might have a deprecation warning, show it\\n    but call the appropriate methods anyhow.\\n    \"\"\"\\n\\n    if how is not None:\\n\\n        # .resample(..., how=\\'sum\\')\\n        if isinstance(how, str):\\n            method = \"{0}()\".format(how)\\n\\n            # .resample(..., how=lambda x: ....)\\n        else:\\n            method = \".apply(<func>)\"\\n\\n        # if we have both a how and fill_method, then show\\n        # the following warning\\n        if fill_method is None:\\n            warnings.warn(\"how in .resample() is deprecated\\\\n\"\\n                          \"the new syntax is \"\\n                          \".resample(...).{method}\".format(\\n                              method=method),\\n                          FutureWarning, stacklevel=3)\\n        r = r.aggregate(how)\\n\\n    if fill_method is not None:\\n\\n        # show the prior function call\\n        method = \\'.\\' + method if how is not None else \\'\\'\\n\\n        args = \"limit={0}\".format(limit) if limit is not None else \"\"\\n        warnings.warn(\"fill_method is deprecated to .resample()\\\\n\"\\n                      \"the new syntax is .resample(...){method}\"\\n                      \".{fill_method}({args})\".format(\\n                          method=method,\\n                          fill_method=fill_method,\\n                          args=args),\\n                      FutureWarning, stacklevel=3)\\n\\n        if how is not None:\\n            r = getattr(r, fill_method)(limit=limit)\\n        else:\\n            r = r.aggregate(fill_method, limit=limit)\\n\\n    return r',\n 'def resample(obj, kind=None, **kwds):\\n    \"\"\"\\n    Create a TimeGrouper and return our resampler.\\n    \"\"\"\\n    tg = TimeGrouper(**kwds)\\n    return tg._get_resampler(obj, kind=kind)',\n 'def get_resampler_for_grouping(groupby, rule, how=None, fill_method=None,\\n                               limit=None, kind=None, **kwargs):\\n    \"\"\"\\n    Return our appropriate resampler when grouping as well.\\n    \"\"\"\\n\\n    # .resample uses \\'on\\' similar to how .groupby uses \\'key\\'\\n    kwargs[\\'key\\'] = kwargs.pop(\\'on\\', None)\\n\\n    tg = TimeGrouper(freq=rule, **kwargs)\\n    resampler = tg._get_resampler(groupby.obj, kind=kind)\\n    r = resampler._get_resampler_for_grouping(groupby=groupby)\\n    return _maybe_process_deprecations(r,\\n                                       how=how,\\n                                       fill_method=fill_method,\\n                                       limit=limit)',\n 'def _get_timestamp_range_edges(first, last, offset, closed=\\'left\\', base=0):\\n    \"\"\"\\n    Adjust the `first` Timestamp to the preceeding Timestamp that resides on\\n    the provided offset. Adjust the `last` Timestamp to the following\\n    Timestamp that resides on the provided offset. Input Timestamps that\\n    already reside on the offset will be adjusted depending on the type of\\n    offset and the `closed` parameter.\\n\\n    Parameters\\n    ----------\\n    first : pd.Timestamp\\n        The beginning Timestamp of the range to be adjusted.\\n    last : pd.Timestamp\\n        The ending Timestamp of the range to be adjusted.\\n    offset : pd.DateOffset\\n        The dateoffset to which the Timestamps will be adjusted.\\n    closed : {\\'right\\', \\'left\\'}, default None\\n        Which side of bin interval is closed.\\n    base : int, default 0\\n        The \"origin\" of the adjusted Timestamps.\\n\\n    Returns\\n    -------\\n    A tuple of length 2, containing the adjusted pd.Timestamp objects.\\n    \"\"\"\\n    if isinstance(offset, Tick):\\n        if isinstance(offset, Day):\\n            # _adjust_dates_anchored assumes \\'D\\' means 24H, but first/last\\n            # might contain a DST transition (23H, 24H, or 25H).\\n            # So \"pretend\" the dates are naive when adjusting the endpoints\\n            tz = first.tz\\n            first = first.tz_localize(None)\\n            last = last.tz_localize(None)\\n\\n        first, last = _adjust_dates_anchored(first, last, offset,\\n                                             closed=closed, base=base)\\n        if isinstance(offset, Day):\\n            first = first.tz_localize(tz)\\n            last = last.tz_localize(tz)\\n        return first, last\\n\\n    else:\\n        first = first.normalize()\\n        last = last.normalize()\\n\\n    if closed == \\'left\\':\\n        first = Timestamp(offset.rollback(first))\\n    else:\\n        first = Timestamp(first - offset)\\n\\n    last = Timestamp(last + offset)\\n\\n    return first, last',\n 'def _get_period_range_edges(first, last, offset, closed=\\'left\\', base=0):\\n    \"\"\"\\n    Adjust the provided `first` and `last` Periods to the respective Period of\\n    the given offset that encompasses them.\\n\\n    Parameters\\n    ----------\\n    first : pd.Period\\n        The beginning Period of the range to be adjusted.\\n    last : pd.Period\\n        The ending Period of the range to be adjusted.\\n    offset : pd.DateOffset\\n        The dateoffset to which the Periods will be adjusted.\\n    closed : {\\'right\\', \\'left\\'}, default None\\n        Which side of bin interval is closed.\\n    base : int, default 0\\n        The \"origin\" of the adjusted Periods.\\n\\n    Returns\\n    -------\\n    A tuple of length 2, containing the adjusted pd.Period objects.\\n    \"\"\"\\n    if not all(isinstance(obj, pd.Period) for obj in [first, last]):\\n        raise TypeError(\"\\'first\\' and \\'last\\' must be instances of type Period\")\\n\\n    # GH 23882\\n    first = first.to_timestamp()\\n    last = last.to_timestamp()\\n    adjust_first = not offset.onOffset(first)\\n    adjust_last = offset.onOffset(last)\\n\\n    first, last = _get_timestamp_range_edges(first, last, offset,\\n                                             closed=closed, base=base)\\n\\n    first = (first + adjust_first * offset).to_period(offset)\\n    last = (last - adjust_last * offset).to_period(offset)\\n    return first, last',\n 'def asfreq(obj, freq, method=None, how=None, normalize=False, fill_value=None):\\n    \"\"\"\\n    Utility frequency conversion method for Series/DataFrame.\\n    \"\"\"\\n    if isinstance(obj.index, PeriodIndex):\\n        if method is not None:\\n            raise NotImplementedError(\"\\'method\\' argument is not supported\")\\n\\n        if how is None:\\n            how = \\'E\\'\\n\\n        new_obj = obj.copy()\\n        new_obj.index = obj.index.asfreq(freq, how=how)\\n\\n    elif len(obj.index) == 0:\\n        new_obj = obj.copy()\\n        new_obj.index = obj.index._shallow_copy(freq=to_offset(freq))\\n\\n    else:\\n        dti = date_range(obj.index[0], obj.index[-1], freq=freq)\\n        dti.name = obj.index.name\\n        new_obj = obj.reindex(dti, method=method, fill_value=fill_value)\\n        if normalize:\\n            new_obj.index = new_obj.index.normalize()\\n\\n    return new_obj',\n 'def _from_selection(self):\\n        \"\"\"\\n        Is the resampling from a DataFrame column or MultiIndex level.\\n        \"\"\"\\n        # upsampling and PeriodIndex resampling do not work\\n        # with selection, this state used to catch and raise an error\\n        return (self.groupby is not None and\\n                (self.groupby.key is not None or\\n                 self.groupby.level is not None))',\n 'def _set_binner(self):\\n        \"\"\"\\n        Setup our binners.\\n\\n        Cache these as we are an immutable object\\n        \"\"\"\\n        if self.binner is None:\\n            self.binner, self.grouper = self._get_binner()',\n 'def _get_binner(self):\\n        \"\"\"\\n        Create the BinGrouper, assume that self.set_grouper(obj)\\n        has already been called.\\n        \"\"\"\\n\\n        binner, bins, binlabels = self._get_binner_for_time()\\n        bin_grouper = BinGrouper(bins, binlabels, indexer=self.groupby.indexer)\\n        return binner, bin_grouper',\n 'def transform(self, arg, *args, **kwargs):\\n        \"\"\"\\n        Call function producing a like-indexed Series on each group and return\\n        a Series with the transformed values.\\n\\n        Parameters\\n        ----------\\n        arg : function\\n            To apply to each group. Should return a Series with the same index.\\n\\n        Returns\\n        -------\\n        transformed : Series\\n\\n        Examples\\n        --------\\n        >>> resampled.transform(lambda x: (x - x.mean()) / x.std())\\n        \"\"\"\\n        return self._selected_obj.groupby(self.groupby).transform(\\n            arg, *args, **kwargs)',\n 'def _gotitem(self, key, ndim, subset=None):\\n        \"\"\"\\n        Sub-classes to define. Return a sliced object.\\n\\n        Parameters\\n        ----------\\n        key : string / list of selections\\n        ndim : 1,2\\n            requested ndim of result\\n        subset : object, default None\\n            subset to act on\\n        \"\"\"\\n        self._set_binner()\\n        grouper = self.grouper\\n        if subset is None:\\n            subset = self.obj\\n        grouped = groupby(subset, by=None, grouper=grouper, axis=self.axis)\\n\\n        # try the key selection\\n        try:\\n            return grouped[key]\\n        except KeyError:\\n            return grouped',\n 'def _groupby_and_aggregate(self, how, grouper=None, *args, **kwargs):\\n        \"\"\"\\n        Re-evaluate the obj with a groupby aggregation.\\n        \"\"\"\\n\\n        if grouper is None:\\n            self._set_binner()\\n            grouper = self.grouper\\n\\n        obj = self._selected_obj\\n\\n        grouped = groupby(obj, by=None, grouper=grouper, axis=self.axis)\\n\\n        try:\\n            if isinstance(obj, ABCDataFrame) and callable(how):\\n                # Check if the function is reducing or not.\\n                result = grouped._aggregate_item_by_item(how, *args, **kwargs)\\n            else:\\n                result = grouped.aggregate(how, *args, **kwargs)\\n        except Exception:\\n\\n            # we have a non-reducing function\\n            # try to evaluate\\n            result = grouped.apply(how, *args, **kwargs)\\n\\n        result = self._apply_loffset(result)\\n        return self._wrap_result(result)',\n 'def _apply_loffset(self, result):\\n        \"\"\"\\n        If loffset is set, offset the result index.\\n\\n        This is NOT an idempotent routine, it will be applied\\n        exactly once to the result.\\n\\n        Parameters\\n        ----------\\n        result : Series or DataFrame\\n            the result of resample\\n        \"\"\"\\n\\n        needs_offset = (\\n            isinstance(self.loffset, (DateOffset, timedelta,\\n                                      np.timedelta64)) and\\n            isinstance(result.index, DatetimeIndex) and\\n            len(result.index) > 0\\n        )\\n\\n        if needs_offset:\\n            result.index = result.index + self.loffset\\n\\n        self.loffset = None\\n        return result',\n 'def _get_resampler_for_grouping(self, groupby, **kwargs):\\n        \"\"\"\\n        Return the correct class for resampling with groupby.\\n        \"\"\"\\n        return self._resampler_for_grouping(self, groupby=groupby, **kwargs)',\n 'def _wrap_result(self, result):\\n        \"\"\"\\n        Potentially wrap any results.\\n        \"\"\"\\n        if isinstance(result, ABCSeries) and self._selection is not None:\\n            result.name = self._selection\\n\\n        if isinstance(result, ABCSeries) and result.empty:\\n            obj = self.obj\\n            if isinstance(obj.index, PeriodIndex):\\n                result.index = obj.index.asfreq(self.freq)\\n            else:\\n                result.index = obj.index._shallow_copy(freq=self.freq)\\n            result.name = getattr(obj, \\'name\\', None)\\n\\n        return result',\n 'def interpolate(self, method=\\'linear\\', axis=0, limit=None, inplace=False,\\n                    limit_direction=\\'forward\\', limit_area=None,\\n                    downcast=None, **kwargs):\\n        \"\"\"\\n        Interpolate values according to different methods.\\n\\n        .. versionadded:: 0.18.1\\n        \"\"\"\\n        result = self._upsample(None)\\n        return result.interpolate(method=method, axis=axis, limit=limit,\\n                                  inplace=inplace,\\n                                  limit_direction=limit_direction,\\n                                  limit_area=limit_area,\\n                                  downcast=downcast, **kwargs)',\n 'def std(self, ddof=1, *args, **kwargs):\\n        \"\"\"\\n        Compute standard deviation of groups, excluding missing values.\\n\\n        Parameters\\n        ----------\\n        ddof : integer, default 1\\n            Degrees of freedom.\\n        \"\"\"\\n        nv.validate_resampler_func(\\'std\\', args, kwargs)\\n        return self._downsample(\\'std\\', ddof=ddof)',\n 'def var(self, ddof=1, *args, **kwargs):\\n        \"\"\"\\n        Compute variance of groups, excluding missing values.\\n\\n        Parameters\\n        ----------\\n        ddof : integer, default 1\\n            degrees of freedom\\n        \"\"\"\\n        nv.validate_resampler_func(\\'var\\', args, kwargs)\\n        return self._downsample(\\'var\\', ddof=ddof)',\n 'def _apply(self, f, grouper=None, *args, **kwargs):\\n        \"\"\"\\n        Dispatch to _upsample; we are stripping all of the _upsample kwargs and\\n        performing the original function call on the grouped object.\\n        \"\"\"\\n\\n        def func(x):\\n            x = self._shallow_copy(x, groupby=self.groupby)\\n\\n            if isinstance(f, str):\\n                return getattr(x, f)(**kwargs)\\n\\n            return x.apply(f, *args, **kwargs)\\n\\n        result = self._groupby.apply(func)\\n        return self._wrap_result(result)',\n 'def _downsample(self, how, **kwargs):\\n        \"\"\"\\n        Downsample the cython defined function.\\n\\n        Parameters\\n        ----------\\n        how : string / cython mapped function\\n        **kwargs : kw args passed to how function\\n        \"\"\"\\n        self._set_binner()\\n        how = self._is_cython_func(how) or how\\n        ax = self.ax\\n        obj = self._selected_obj\\n\\n        if not len(ax):\\n            # reset to the new freq\\n            obj = obj.copy()\\n            obj.index.freq = self.freq\\n            return obj\\n\\n        # do we have a regular frequency\\n        if ax.freq is not None or ax.inferred_freq is not None:\\n\\n            if len(self.grouper.binlabels) > len(ax) and how is None:\\n\\n                # let\\'s do an asfreq\\n                return self.asfreq()\\n\\n        # we are downsampling\\n        # we want to call the actual grouper method here\\n        result = obj.groupby(\\n            self.grouper, axis=self.axis).aggregate(how, **kwargs)\\n\\n        result = self._apply_loffset(result)\\n        return self._wrap_result(result)',\n 'def _adjust_binner_for_upsample(self, binner):\\n        \"\"\"\\n        Adjust our binner when upsampling.\\n\\n        The range of a new index should not be outside specified range\\n        \"\"\"\\n        if self.closed == \\'right\\':\\n            binner = binner[1:]\\n        else:\\n            binner = binner[:-1]\\n        return binner',\n 'def _upsample(self, method, limit=None, fill_value=None):\\n        \"\"\"\\n        Parameters\\n        ----------\\n        method : string {\\'backfill\\', \\'bfill\\', \\'pad\\',\\n            \\'ffill\\', \\'asfreq\\'} method for upsampling\\n        limit : int, default None\\n            Maximum size gap to fill when reindexing\\n        fill_value : scalar, default None\\n            Value to use for missing values\\n\\n        See Also\\n        --------\\n        .fillna\\n\\n        \"\"\"\\n        self._set_binner()\\n        if self.axis:\\n            raise AssertionError(\\'axis must be 0\\')\\n        if self._from_selection:\\n            raise ValueError(\"Upsampling from level= or on= selection\"\\n                             \" is not supported, use .set_index(...)\"\\n                             \" to explicitly set index to\"\\n                             \" datetime-like\")\\n\\n        ax = self.ax\\n        obj = self._selected_obj\\n        binner = self.binner\\n        res_index = self._adjust_binner_for_upsample(binner)\\n\\n        # if we have the same frequency as our axis, then we are equal sampling\\n        if limit is None and to_offset(ax.inferred_freq) == self.freq:\\n            result = obj.copy()\\n            result.index = res_index\\n        else:\\n            result = obj.reindex(res_index, method=method,\\n                                 limit=limit, fill_value=fill_value)\\n\\n        result = self._apply_loffset(result)\\n        return self._wrap_result(result)',\n 'def _downsample(self, how, **kwargs):\\n        \"\"\"\\n        Downsample the cython defined function.\\n\\n        Parameters\\n        ----------\\n        how : string / cython mapped function\\n        **kwargs : kw args passed to how function\\n        \"\"\"\\n\\n        # we may need to actually resample as if we are timestamps\\n        if self.kind == \\'timestamp\\':\\n            return super()._downsample(how, **kwargs)\\n\\n        how = self._is_cython_func(how) or how\\n        ax = self.ax\\n\\n        if is_subperiod(ax.freq, self.freq):\\n            # Downsampling\\n            return self._groupby_and_aggregate(how, grouper=self.grouper,\\n                                               **kwargs)\\n        elif is_superperiod(ax.freq, self.freq):\\n            if how == \\'ohlc\\':\\n                # GH #13083\\n                # upsampling to subperiods is handled as an asfreq, which works\\n                # for pure aggregating/reducing methods\\n                # OHLC reduces along the time dimension, but creates multiple\\n                # values for each period -> handle by _groupby_and_aggregate()\\n                return self._groupby_and_aggregate(how, grouper=self.grouper)\\n            return self.asfreq()\\n        elif ax.freq == self.freq:\\n            return self.asfreq()\\n\\n        raise IncompatibleFrequency(\\n            \\'Frequency {} cannot be resampled to {}, as they are not \\'\\n            \\'sub or super periods\\'.format(ax.freq, self.freq))',\n 'def _upsample(self, method, limit=None, fill_value=None):\\n        \"\"\"\\n        Parameters\\n        ----------\\n        method : string {\\'backfill\\', \\'bfill\\', \\'pad\\', \\'ffill\\'}\\n            method for upsampling\\n        limit : int, default None\\n            Maximum size gap to fill when reindexing\\n        fill_value : scalar, default None\\n            Value to use for missing values\\n\\n        See Also\\n        --------\\n        .fillna\\n\\n        \"\"\"\\n\\n        # we may need to actually resample as if we are timestamps\\n        if self.kind == \\'timestamp\\':\\n            return super()._upsample(method, limit=limit,\\n                                     fill_value=fill_value)\\n\\n        self._set_binner()\\n        ax = self.ax\\n        obj = self.obj\\n        new_index = self.binner\\n\\n        # Start vs. end of period\\n        memb = ax.asfreq(self.freq, how=self.convention)\\n\\n        # Get the fill indexer\\n        indexer = memb.get_indexer(new_index, method=method, limit=limit)\\n        return self._wrap_result(_take_new_index(\\n            obj, indexer, new_index, axis=self.axis))',\n 'def _get_resampler(self, obj, kind=None):\\n        \"\"\"\\n        Return my resampler or raise if we have an invalid axis.\\n\\n        Parameters\\n        ----------\\n        obj : input object\\n        kind : string, optional\\n            \\'period\\',\\'timestamp\\',\\'timedelta\\' are valid\\n\\n        Returns\\n        -------\\n        a Resampler\\n\\n        Raises\\n        ------\\n        TypeError if incompatible axis\\n\\n        \"\"\"\\n        self._set_grouper(obj)\\n\\n        ax = self.ax\\n        if isinstance(ax, DatetimeIndex):\\n            return DatetimeIndexResampler(obj,\\n                                          groupby=self,\\n                                          kind=kind,\\n                                          axis=self.axis)\\n        elif isinstance(ax, PeriodIndex) or kind == \\'period\\':\\n            return PeriodIndexResampler(obj,\\n                                        groupby=self,\\n                                        kind=kind,\\n                                        axis=self.axis)\\n        elif isinstance(ax, TimedeltaIndex):\\n            return TimedeltaIndexResampler(obj,\\n                                           groupby=self,\\n                                           axis=self.axis)\\n\\n        raise TypeError(\"Only valid with DatetimeIndex, \"\\n                        \"TimedeltaIndex or PeriodIndex, \"\\n                        \"but got an instance of %r\" % type(ax).__name__)',\n 'def _combine_hash_arrays(arrays, num_items):\\n    \"\"\"\\n    Parameters\\n    ----------\\n    arrays : generator\\n    num_items : int\\n\\n    Should be the same as CPython\\'s tupleobject.c\\n    \"\"\"\\n    try:\\n        first = next(arrays)\\n    except StopIteration:\\n        return np.array([], dtype=np.uint64)\\n\\n    arrays = itertools.chain([first], arrays)\\n\\n    mult = np.uint64(1000003)\\n    out = np.zeros_like(first) + np.uint64(0x345678)\\n    for i, a in enumerate(arrays):\\n        inverse_i = num_items - i\\n        out ^= a\\n        out *= mult\\n        mult += np.uint64(82520 + inverse_i + inverse_i)\\n    assert i + 1 == num_items, \\'Fed in wrong num_items\\'\\n    out += np.uint64(97531)\\n    return out',\n 'def hash_pandas_object(obj, index=True, encoding=\\'utf8\\', hash_key=None,\\n                       categorize=True):\\n    \"\"\"\\n    Return a data hash of the Index/Series/DataFrame\\n\\n    .. versionadded:: 0.19.2\\n\\n    Parameters\\n    ----------\\n    index : boolean, default True\\n        include the index in the hash (if Series/DataFrame)\\n    encoding : string, default \\'utf8\\'\\n        encoding for data & key when strings\\n    hash_key : string key to encode, default to _default_hash_key\\n    categorize : bool, default True\\n        Whether to first categorize object arrays before hashing. This is more\\n        efficient when the array contains duplicate values.\\n\\n        .. versionadded:: 0.20.0\\n\\n    Returns\\n    -------\\n    Series of uint64, same length as the object\\n    \"\"\"\\n    from pandas import Series\\n    if hash_key is None:\\n        hash_key = _default_hash_key\\n\\n    if isinstance(obj, ABCMultiIndex):\\n        return Series(hash_tuples(obj, encoding, hash_key),\\n                      dtype=\\'uint64\\', copy=False)\\n\\n    if isinstance(obj, ABCIndexClass):\\n        h = hash_array(obj.values, encoding, hash_key,\\n                       categorize).astype(\\'uint64\\', copy=False)\\n        h = Series(h, index=obj, dtype=\\'uint64\\', copy=False)\\n    elif isinstance(obj, ABCSeries):\\n        h = hash_array(obj.values, encoding, hash_key,\\n                       categorize).astype(\\'uint64\\', copy=False)\\n        if index:\\n            index_iter = (hash_pandas_object(obj.index,\\n                                             index=False,\\n                                             encoding=encoding,\\n                                             hash_key=hash_key,\\n                                             categorize=categorize).values\\n                          for _ in [None])\\n            arrays = itertools.chain([h], index_iter)\\n            h = _combine_hash_arrays(arrays, 2)\\n\\n        h = Series(h, index=obj.index, dtype=\\'uint64\\', copy=False)\\n\\n    elif isinstance(obj, ABCDataFrame):\\n        hashes = (hash_array(series.values) for _, series in obj.iteritems())\\n        num_items = len(obj.columns)\\n        if index:\\n            index_hash_generator = (hash_pandas_object(obj.index,\\n                                                       index=False,\\n                                                       encoding=encoding,\\n                                                       hash_key=hash_key,\\n                                                       categorize=categorize).values  # noqa\\n                                    for _ in [None])\\n            num_items += 1\\n            hashes = itertools.chain(hashes, index_hash_generator)\\n        h = _combine_hash_arrays(hashes, num_items)\\n\\n        h = Series(h, index=obj.index, dtype=\\'uint64\\', copy=False)\\n    else:\\n        raise TypeError(\"Unexpected type for hashing %s\" % type(obj))\\n    return h',\n 'def hash_tuples(vals, encoding=\\'utf8\\', hash_key=None):\\n    \"\"\"\\n    Hash an MultiIndex / list-of-tuples efficiently\\n\\n    .. versionadded:: 0.20.0\\n\\n    Parameters\\n    ----------\\n    vals : MultiIndex, list-of-tuples, or single tuple\\n    encoding : string, default \\'utf8\\'\\n    hash_key : string key to encode, default to _default_hash_key\\n\\n    Returns\\n    -------\\n    ndarray of hashed values array\\n    \"\"\"\\n    is_tuple = False\\n    if isinstance(vals, tuple):\\n        vals = [vals]\\n        is_tuple = True\\n    elif not is_list_like(vals):\\n        raise TypeError(\"must be convertible to a list-of-tuples\")\\n\\n    from pandas import Categorical, MultiIndex\\n\\n    if not isinstance(vals, ABCMultiIndex):\\n        vals = MultiIndex.from_tuples(vals)\\n\\n    # create a list-of-Categoricals\\n    vals = [Categorical(vals.codes[level],\\n                        vals.levels[level],\\n                        ordered=False,\\n                        fastpath=True)\\n            for level in range(vals.nlevels)]\\n\\n    # hash the list-of-ndarrays\\n    hashes = (_hash_categorical(cat,\\n                                encoding=encoding,\\n                                hash_key=hash_key)\\n              for cat in vals)\\n    h = _combine_hash_arrays(hashes, len(vals))\\n    if is_tuple:\\n        h = h[0]\\n\\n    return h',\n 'def hash_tuple(val, encoding=\\'utf8\\', hash_key=None):\\n    \"\"\"\\n    Hash a single tuple efficiently\\n\\n    Parameters\\n    ----------\\n    val : single tuple\\n    encoding : string, default \\'utf8\\'\\n    hash_key : string key to encode, default to _default_hash_key\\n\\n    Returns\\n    -------\\n    hash\\n\\n    \"\"\"\\n    hashes = (_hash_scalar(v, encoding=encoding, hash_key=hash_key)\\n              for v in val)\\n\\n    h = _combine_hash_arrays(hashes, len(val))[0]\\n\\n    return h',\n 'def _hash_categorical(c, encoding, hash_key):\\n    \"\"\"\\n    Hash a Categorical by hashing its categories, and then mapping the codes\\n    to the hashes\\n\\n    Parameters\\n    ----------\\n    c : Categorical\\n    encoding : string, default \\'utf8\\'\\n    hash_key : string key to encode, default to _default_hash_key\\n\\n    Returns\\n    -------\\n    ndarray of hashed values array, same size as len(c)\\n    \"\"\"\\n    # Convert ExtensionArrays to ndarrays\\n    values = np.asarray(c.categories.values)\\n    hashed = hash_array(values, encoding, hash_key,\\n                        categorize=False)\\n\\n    # we have uint64, as we don\\'t directly support missing values\\n    # we don\\'t want to use take_nd which will coerce to float\\n    # instead, directly construct the result with a\\n    # max(np.uint64) as the missing value indicator\\n    #\\n    # TODO: GH 15362\\n\\n    mask = c.isna()\\n    if len(hashed):\\n        result = hashed.take(c.codes)\\n    else:\\n        result = np.zeros(len(mask), dtype=\\'uint64\\')\\n\\n    if mask.any():\\n        result[mask] = np.iinfo(np.uint64).max\\n\\n    return result',\n 'def hash_array(vals, encoding=\\'utf8\\', hash_key=None, categorize=True):\\n    \"\"\"\\n    Given a 1d array, return an array of deterministic integers.\\n\\n    .. versionadded:: 0.19.2\\n\\n    Parameters\\n    ----------\\n    vals : ndarray, Categorical\\n    encoding : string, default \\'utf8\\'\\n        encoding for data & key when strings\\n    hash_key : string key to encode, default to _default_hash_key\\n    categorize : bool, default True\\n        Whether to first categorize object arrays before hashing. This is more\\n        efficient when the array contains duplicate values.\\n\\n        .. versionadded:: 0.20.0\\n\\n    Returns\\n    -------\\n    1d uint64 numpy array of hash values, same length as the vals\\n    \"\"\"\\n\\n    if not hasattr(vals, \\'dtype\\'):\\n        raise TypeError(\"must pass a ndarray-like\")\\n    dtype = vals.dtype\\n\\n    if hash_key is None:\\n        hash_key = _default_hash_key\\n\\n    # For categoricals, we hash the categories, then remap the codes to the\\n    # hash values. (This check is above the complex check so that we don\\'t ask\\n    # numpy if categorical is a subdtype of complex, as it will choke).\\n    if is_categorical_dtype(dtype):\\n        return _hash_categorical(vals, encoding, hash_key)\\n    elif is_extension_array_dtype(dtype):\\n        vals, _ = vals._values_for_factorize()\\n        dtype = vals.dtype\\n\\n    # we\\'ll be working with everything as 64-bit values, so handle this\\n    # 128-bit value early\\n    if np.issubdtype(dtype, np.complex128):\\n        return hash_array(vals.real) + 23 * hash_array(vals.imag)\\n\\n    # First, turn whatever array this is into unsigned 64-bit ints, if we can\\n    # manage it.\\n    elif isinstance(dtype, np.bool):\\n        vals = vals.astype(\\'u8\\')\\n    elif issubclass(dtype.type, (np.datetime64, np.timedelta64)):\\n        vals = vals.view(\\'i8\\').astype(\\'u8\\', copy=False)\\n    elif issubclass(dtype.type, np.number) and dtype.itemsize <= 8:\\n        vals = vals.view(\\'u{}\\'.format(vals.dtype.itemsize)).astype(\\'u8\\')\\n    else:\\n        # With repeated values, its MUCH faster to categorize object dtypes,\\n        # then hash and rename categories. We allow skipping the categorization\\n        # when the values are known/likely to be unique.\\n        if categorize:\\n            from pandas import factorize, Categorical, Index\\n            codes, categories = factorize(vals, sort=False)\\n            cat = Categorical(codes, Index(categories),\\n                              ordered=False, fastpath=True)\\n            return _hash_categorical(cat, encoding, hash_key)\\n\\n        try:\\n            vals = hashing.hash_object_array(vals, hash_key, encoding)\\n        except TypeError:\\n            # we have mixed types\\n            vals = hashing.hash_object_array(vals.astype(str).astype(object),\\n                                             hash_key, encoding)\\n\\n    # Then, redistribute these 64-bit ints within the space of 64-bit ints\\n    vals ^= vals >> 30\\n    vals *= np.uint64(0xbf58476d1ce4e5b9)\\n    vals ^= vals >> 27\\n    vals *= np.uint64(0x94d049bb133111eb)\\n    vals ^= vals >> 31\\n    return vals',\n 'def _hash_scalar(val, encoding=\\'utf8\\', hash_key=None):\\n    \"\"\"\\n    Hash scalar value\\n\\n    Returns\\n    -------\\n    1d uint64 numpy array of hash value, of length 1\\n    \"\"\"\\n\\n    if isna(val):\\n        # this is to be consistent with the _hash_categorical implementation\\n        return np.array([np.iinfo(np.uint64).max], dtype=\\'u8\\')\\n\\n    if getattr(val, \\'tzinfo\\', None) is not None:\\n        # for tz-aware datetimes, we need the underlying naive UTC value and\\n        # not the tz aware object or pd extension type (as\\n        # infer_dtype_from_scalar would do)\\n        if not isinstance(val, tslibs.Timestamp):\\n            val = tslibs.Timestamp(val)\\n        val = val.tz_convert(None)\\n\\n    dtype, val = infer_dtype_from_scalar(val)\\n    vals = np.array([val], dtype=dtype)\\n\\n    return hash_array(vals, hash_key=hash_key, encoding=encoding,\\n                      categorize=False)',\n 'def _process_single_doc(self, single_doc):\\n        \"\"\"\\n        Make sure the provided value for --single is a path to an existing\\n        .rst/.ipynb file, or a pandas object that can be imported.\\n\\n        For example, categorial.rst or pandas.DataFrame.head. For the latter,\\n        return the corresponding file path\\n        (e.g. reference/api/pandas.DataFrame.head.rst).\\n        \"\"\"\\n        base_name, extension = os.path.splitext(single_doc)\\n        if extension in (\\'.rst\\', \\'.ipynb\\'):\\n            if os.path.exists(os.path.join(SOURCE_PATH, single_doc)):\\n                return single_doc\\n            else:\\n                raise FileNotFoundError(\\'File {} not found\\'.format(single_doc))\\n\\n        elif single_doc.startswith(\\'pandas.\\'):\\n            try:\\n                obj = pandas  # noqa: F821\\n                for name in single_doc.split(\\'.\\'):\\n                    obj = getattr(obj, name)\\n            except AttributeError:\\n                raise ImportError(\\'Could not import {}\\'.format(single_doc))\\n            else:\\n                return single_doc[len(\\'pandas.\\'):]\\n        else:\\n            raise ValueError((\\'--single={} not understood. Value should be a \\'\\n                              \\'valid path to a .rst or .ipynb file, or a \\'\\n                              \\'valid pandas object (e.g. categorical.rst or \\'\\n                              \\'pandas.DataFrame.head)\\').format(single_doc))',\n 'def _run_os(*args):\\n        \"\"\"\\n        Execute a command as a OS terminal.\\n\\n        Parameters\\n        ----------\\n        *args : list of str\\n            Command and parameters to be executed\\n\\n        Examples\\n        --------\\n        >>> DocBuilder()._run_os(\\'python\\', \\'--version\\')\\n        \"\"\"\\n        subprocess.check_call(args, stdout=sys.stdout, stderr=sys.stderr)',\n 'def _sphinx_build(self, kind):\\n        \"\"\"\\n        Call sphinx to build documentation.\\n\\n        Attribute `num_jobs` from the class is used.\\n\\n        Parameters\\n        ----------\\n        kind : {\\'html\\', \\'latex\\'}\\n\\n        Examples\\n        --------\\n        >>> DocBuilder(num_jobs=4)._sphinx_build(\\'html\\')\\n        \"\"\"\\n        if kind not in (\\'html\\', \\'latex\\'):\\n            raise ValueError(\\'kind must be html or latex, \\'\\n                             \\'not {}\\'.format(kind))\\n\\n        cmd = [\\'sphinx-build\\', \\'-b\\', kind]\\n        if self.num_jobs:\\n            cmd += [\\'-j\\', str(self.num_jobs)]\\n        if self.warnings_are_errors:\\n            cmd += [\\'-W\\', \\'--keep-going\\']\\n        if self.verbosity:\\n            cmd.append(\\'-{}\\'.format(\\'v\\' * self.verbosity))\\n        cmd += [\\'-d\\', os.path.join(BUILD_PATH, \\'doctrees\\'),\\n                SOURCE_PATH, os.path.join(BUILD_PATH, kind)]\\n        return subprocess.call(cmd)',\n 'def _open_browser(self, single_doc_html):\\n        \"\"\"\\n        Open a browser tab showing single\\n        \"\"\"\\n        url = os.path.join(\\'file://\\', DOC_PATH, \\'build\\', \\'html\\',\\n                           single_doc_html)\\n        webbrowser.open(url, new=2)',\n 'def _get_page_title(self, page):\\n        \"\"\"\\n        Open the rst file `page` and extract its title.\\n        \"\"\"\\n        fname = os.path.join(SOURCE_PATH, \\'{}.rst\\'.format(page))\\n        option_parser = docutils.frontend.OptionParser(\\n            components=(docutils.parsers.rst.Parser,))\\n        doc = docutils.utils.new_document(\\n            \\'<doc>\\',\\n            option_parser.get_default_values())\\n        with open(fname) as f:\\n            data = f.read()\\n\\n        parser = docutils.parsers.rst.Parser()\\n        # do not generate any warning when parsing the rst\\n        with open(os.devnull, \\'a\\') as f:\\n            doc.reporter.stream = f\\n            parser.parse(data, doc)\\n\\n        section = next(node for node in doc.children\\n                       if isinstance(node, docutils.nodes.section))\\n        title = next(node for node in section.children\\n                     if isinstance(node, docutils.nodes.title))\\n\\n        return title.astext()',\n 'def _add_redirects(self):\\n        \"\"\"\\n        Create in the build directory an html file with a redirect,\\n        for every row in REDIRECTS_FILE.\\n        \"\"\"\\n        html = \\'\\'\\'\\n        <html>\\n            <head>\\n                <meta http-equiv=\"refresh\" content=\"0;URL={url}\"/>\\n            </head>\\n            <body>\\n                <p>\\n                    The page has been moved to <a href=\"{url}\">{title}</a>\\n                </p>\\n            </body>\\n        <html>\\n        \\'\\'\\'\\n        with open(REDIRECTS_FILE) as mapping_fd:\\n            reader = csv.reader(mapping_fd)\\n            for row in reader:\\n                if not row or row[0].strip().startswith(\\'#\\'):\\n                    continue\\n\\n                path = os.path.join(BUILD_PATH,\\n                                    \\'html\\',\\n                                    *row[0].split(\\'/\\')) + \\'.html\\'\\n\\n                try:\\n                    title = self._get_page_title(row[1])\\n                except Exception:\\n                    # the file can be an ipynb and not an rst, or docutils\\n                    # may not be able to read the rst because it has some\\n                    # sphinx specific stuff\\n                    title = \\'this page\\'\\n\\n                if os.path.exists(path):\\n                    raise RuntimeError((\\n                        \\'Redirection would overwrite an existing file: \\'\\n                        \\'{}\\').format(path))\\n\\n                with open(path, \\'w\\') as moved_page_fd:\\n                    moved_page_fd.write(\\n                        html.format(url=\\'{}.html\\'.format(row[1]),\\n                                    title=title))',\n 'def html(self):\\n        \"\"\"\\n        Build HTML documentation.\\n        \"\"\"\\n        ret_code = self._sphinx_build(\\'html\\')\\n        zip_fname = os.path.join(BUILD_PATH, \\'html\\', \\'pandas.zip\\')\\n        if os.path.exists(zip_fname):\\n            os.remove(zip_fname)\\n\\n        if self.single_doc_html is not None:\\n            self._open_browser(self.single_doc_html)\\n        else:\\n            self._add_redirects()\\n        return ret_code',\n 'def latex(self, force=False):\\n        \"\"\"\\n        Build PDF documentation.\\n        \"\"\"\\n        if sys.platform == \\'win32\\':\\n            sys.stderr.write(\\'latex build has not been tested on windows\\\\n\\')\\n        else:\\n            ret_code = self._sphinx_build(\\'latex\\')\\n            os.chdir(os.path.join(BUILD_PATH, \\'latex\\'))\\n            if force:\\n                for i in range(3):\\n                    self._run_os(\\'pdflatex\\',\\n                                 \\'-interaction=nonstopmode\\',\\n                                 \\'pandas.tex\\')\\n                raise SystemExit(\\'You should check the file \\'\\n                                 \\'\"build/latex/pandas.pdf\" for problems.\\')\\n            else:\\n                self._run_os(\\'make\\')\\n            return ret_code',\n 'def clean():\\n        \"\"\"\\n        Clean documentation generated files.\\n        \"\"\"\\n        shutil.rmtree(BUILD_PATH, ignore_errors=True)\\n        shutil.rmtree(os.path.join(SOURCE_PATH, \\'reference\\', \\'api\\'),\\n                      ignore_errors=True)',\n 'def zip_html(self):\\n        \"\"\"\\n        Compress HTML documentation into a zip file.\\n        \"\"\"\\n        zip_fname = os.path.join(BUILD_PATH, \\'html\\', \\'pandas.zip\\')\\n        if os.path.exists(zip_fname):\\n            os.remove(zip_fname)\\n        dirname = os.path.join(BUILD_PATH, \\'html\\')\\n        fnames = os.listdir(dirname)\\n        os.chdir(dirname)\\n        self._run_os(\\'zip\\',\\n                     zip_fname,\\n                     \\'-r\\',\\n                     \\'-q\\',\\n                     *fnames)',\n 'def write_result(self, buf):\\n        \"\"\"\\n        Render a DataFrame to a LaTeX tabular/longtable environment output.\\n        \"\"\"\\n\\n        # string representation of the columns\\n        if len(self.frame.columns) == 0 or len(self.frame.index) == 0:\\n            info_line = (\\'Empty {name}\\\\nColumns: {col}\\\\nIndex: {idx}\\'\\n                         .format(name=type(self.frame).__name__,\\n                                 col=self.frame.columns,\\n                                 idx=self.frame.index))\\n            strcols = [[info_line]]\\n        else:\\n            strcols = self.fmt._to_str_columns()\\n\\n        def get_col_type(dtype):\\n            if issubclass(dtype.type, np.number):\\n                return \\'r\\'\\n            else:\\n                return \\'l\\'\\n\\n        # reestablish the MultiIndex that has been joined by _to_str_column\\n        if self.fmt.index and isinstance(self.frame.index, ABCMultiIndex):\\n            out = self.frame.index.format(\\n                adjoin=False, sparsify=self.fmt.sparsify,\\n                names=self.fmt.has_index_names, na_rep=self.fmt.na_rep\\n            )\\n\\n            # index.format will sparsify repeated entries with empty strings\\n            # so pad these with some empty space\\n            def pad_empties(x):\\n                for pad in reversed(x):\\n                    if pad:\\n                        break\\n                return [x[0]] + [i if i else \\' \\' * len(pad) for i in x[1:]]\\n            out = (pad_empties(i) for i in out)\\n\\n            # Add empty spaces for each column level\\n            clevels = self.frame.columns.nlevels\\n            out = [[\\' \\' * len(i[-1])] * clevels + i for i in out]\\n\\n            # Add the column names to the last index column\\n            cnames = self.frame.columns.names\\n            if any(cnames):\\n                new_names = [i if i else \\'{}\\' for i in cnames]\\n                out[self.frame.index.nlevels - 1][:clevels] = new_names\\n\\n            # Get rid of old multiindex column and add new ones\\n            strcols = out + strcols[1:]\\n\\n        column_format = self.column_format\\n        if column_format is None:\\n            dtypes = self.frame.dtypes._values\\n            column_format = \\'\\'.join(map(get_col_type, dtypes))\\n            if self.fmt.index:\\n                index_format = \\'l\\' * self.frame.index.nlevels\\n                column_format = index_format + column_format\\n        elif not isinstance(column_format, str):  # pragma: no cover\\n            raise AssertionError(\\'column_format must be str or unicode, \\'\\n                                 \\'not {typ}\\'.format(typ=type(column_format)))\\n\\n        if not self.longtable:\\n            buf.write(\\'\\\\\\\\begin{{tabular}}{{{fmt}}}\\\\n\\'\\n                      .format(fmt=column_format))\\n            buf.write(\\'\\\\\\\\toprule\\\\n\\')\\n        else:\\n            buf.write(\\'\\\\\\\\begin{{longtable}}{{{fmt}}}\\\\n\\'\\n                      .format(fmt=column_format))\\n            buf.write(\\'\\\\\\\\toprule\\\\n\\')\\n\\n        ilevels = self.frame.index.nlevels\\n        clevels = self.frame.columns.nlevels\\n        nlevels = clevels\\n        if self.fmt.has_index_names and self.fmt.show_index_names:\\n            nlevels += 1\\n        strrows = list(zip(*strcols))\\n        self.clinebuf = []\\n\\n        for i, row in enumerate(strrows):\\n            if i == nlevels and self.fmt.header:\\n                buf.write(\\'\\\\\\\\midrule\\\\n\\')  # End of header\\n                if self.longtable:\\n                    buf.write(\\'\\\\\\\\endhead\\\\n\\')\\n                    buf.write(\\'\\\\\\\\midrule\\\\n\\')\\n                    buf.write(\\'\\\\\\\\multicolumn{{{n}}}{{r}}{{{{Continued on next \\'\\n                              \\'page}}}} \\\\\\\\\\\\\\\\\\\\n\\'.format(n=len(row)))\\n                    buf.write(\\'\\\\\\\\midrule\\\\n\\')\\n                    buf.write(\\'\\\\\\\\endfoot\\\\n\\\\n\\')\\n                    buf.write(\\'\\\\\\\\bottomrule\\\\n\\')\\n                    buf.write(\\'\\\\\\\\endlastfoot\\\\n\\')\\n            if self.fmt.kwds.get(\\'escape\\', True):\\n                # escape backslashes first\\n                crow = [(x.replace(\\'\\\\\\\\\\', \\'\\\\\\\\textbackslash \\')\\n                         .replace(\\'_\\', \\'\\\\\\\\_\\')\\n                         .replace(\\'%\\', \\'\\\\\\\\%\\').replace(\\'$\\', \\'\\\\\\\\$\\')\\n                         .replace(\\'#\\', \\'\\\\\\\\#\\').replace(\\'{\\', \\'\\\\\\\\{\\')\\n                         .replace(\\'}\\', \\'\\\\\\\\}\\').replace(\\'~\\', \\'\\\\\\\\textasciitilde \\')\\n                         .replace(\\'^\\', \\'\\\\\\\\textasciicircum \\')\\n                         .replace(\\'&\\', \\'\\\\\\\\&\\')\\n                         if (x and x != \\'{}\\') else \\'{}\\') for x in row]\\n            else:\\n                crow = [x if x else \\'{}\\' for x in row]\\n            if self.bold_rows and self.fmt.index:\\n                # bold row labels\\n                crow = [\\'\\\\\\\\textbf{{{x}}}\\'.format(x=x)\\n                        if j < ilevels and x.strip() not in [\\'\\', \\'{}\\'] else x\\n                        for j, x in enumerate(crow)]\\n            if i < clevels and self.fmt.header and self.multicolumn:\\n                # sum up columns to multicolumns\\n                crow = self._format_multicolumn(crow, ilevels)\\n            if (i >= nlevels and self.fmt.index and self.multirow and\\n                    ilevels > 1):\\n                # sum up rows to multirows\\n                crow = self._format_multirow(crow, ilevels, i, strrows)\\n            buf.write(\\' & \\'.join(crow))\\n            buf.write(\\' \\\\\\\\\\\\\\\\\\\\n\\')\\n            if self.multirow and i < len(strrows) - 1:\\n                self._print_cline(buf, i, len(strcols))\\n\\n        if not self.longtable:\\n            buf.write(\\'\\\\\\\\bottomrule\\\\n\\')\\n            buf.write(\\'\\\\\\\\end{tabular}\\\\n\\')\\n        else:\\n            buf.write(\\'\\\\\\\\end{longtable}\\\\n\\')',\n 'def _format_multicolumn(self, row, ilevels):\\n        r\"\"\"\\n        Combine columns belonging to a group to a single multicolumn entry\\n        according to self.multicolumn_format\\n\\n        e.g.:\\n        a &  &  & b & c &\\n        will become\\n        \\\\multicolumn{3}{l}{a} & b & \\\\multicolumn{2}{l}{c}\\n        \"\"\"\\n        row2 = list(row[:ilevels])\\n        ncol = 1\\n        coltext = \\'\\'\\n\\n        def append_col():\\n            # write multicolumn if needed\\n            if ncol > 1:\\n                row2.append(\\'\\\\\\\\multicolumn{{{ncol:d}}}{{{fmt:s}}}{{{txt:s}}}\\'\\n                            .format(ncol=ncol, fmt=self.multicolumn_format,\\n                                    txt=coltext.strip()))\\n            # don\\'t modify where not needed\\n            else:\\n                row2.append(coltext)\\n        for c in row[ilevels:]:\\n            # if next col has text, write the previous\\n            if c.strip():\\n                if coltext:\\n                    append_col()\\n                coltext = c\\n                ncol = 1\\n            # if not, add it to the previous multicolumn\\n            else:\\n                ncol += 1\\n        # write last column name\\n        if coltext:\\n            append_col()\\n        return row2',\n 'def _format_multirow(self, row, ilevels, i, rows):\\n        r\"\"\"\\n        Check following rows, whether row should be a multirow\\n\\n        e.g.:     becomes:\\n        a & 0 &   \\\\multirow{2}{*}{a} & 0 &\\n          & 1 &     & 1 &\\n        b & 0 &   \\\\cline{1-2}\\n                  b & 0 &\\n        \"\"\"\\n        for j in range(ilevels):\\n            if row[j].strip():\\n                nrow = 1\\n                for r in rows[i + 1:]:\\n                    if not r[j].strip():\\n                        nrow += 1\\n                    else:\\n                        break\\n                if nrow > 1:\\n                    # overwrite non-multirow entry\\n                    row[j] = \\'\\\\\\\\multirow{{{nrow:d}}}{{*}}{{{row:s}}}\\'.format(\\n                        nrow=nrow, row=row[j].strip())\\n                    # save when to end the current block with \\\\cline\\n                    self.clinebuf.append([i + nrow - 1, j + 1])\\n        return row',\n 'def _print_cline(self, buf, i, icol):\\n        \"\"\"\\n        Print clines after multirow-blocks are finished\\n        \"\"\"\\n        for cl in self.clinebuf:\\n            if cl[0] == i:\\n                buf.write(\\'\\\\\\\\cline{{{cl:d}-{icol:d}}}\\\\n\\'\\n                          .format(cl=cl[1], icol=icol))\\n        # remove entries that have been written to buffer\\n        self.clinebuf = [x for x in self.clinebuf if x[0] != i]',\n 'def _validate_integer(name, val, min_val=0):\\n    \"\"\"\\n    Checks whether the \\'name\\' parameter for parsing is either\\n    an integer OR float that can SAFELY be cast to an integer\\n    without losing accuracy. Raises a ValueError if that is\\n    not the case.\\n\\n    Parameters\\n    ----------\\n    name : string\\n        Parameter name (used for error reporting)\\n    val : int or float\\n        The value to check\\n    min_val : int\\n        Minimum allowed value (val < min_val will result in a ValueError)\\n    \"\"\"\\n    msg = \"\\'{name:s}\\' must be an integer >={min_val:d}\".format(name=name,\\n                                                               min_val=min_val)\\n\\n    if val is not None:\\n        if is_float(val):\\n            if int(val) != val:\\n                raise ValueError(msg)\\n            val = int(val)\\n        elif not (is_integer(val) and val >= min_val):\\n            raise ValueError(msg)\\n\\n    return val',\n 'def _validate_names(names):\\n    \"\"\"\\n    Check if the `names` parameter contains duplicates.\\n\\n    If duplicates are found, we issue a warning before returning.\\n\\n    Parameters\\n    ----------\\n    names : array-like or None\\n        An array containing a list of the names used for the output DataFrame.\\n\\n    Returns\\n    -------\\n    names : array-like or None\\n        The original `names` parameter.\\n    \"\"\"\\n\\n    if names is not None:\\n        if len(names) != len(set(names)):\\n            msg = (\"Duplicate names specified. This \"\\n                   \"will raise an error in the future.\")\\n            warnings.warn(msg, UserWarning, stacklevel=3)\\n\\n    return names',\n 'def _read(filepath_or_buffer: FilePathOrBuffer, kwds):\\n    \"\"\"Generic reader of line files.\"\"\"\\n    encoding = kwds.get(\\'encoding\\', None)\\n    if encoding is not None:\\n        encoding = re.sub(\\'_\\', \\'-\\', encoding).lower()\\n        kwds[\\'encoding\\'] = encoding\\n\\n    compression = kwds.get(\\'compression\\', \\'infer\\')\\n    compression = _infer_compression(filepath_or_buffer, compression)\\n\\n    # TODO: get_filepath_or_buffer could return\\n    # Union[FilePathOrBuffer, s3fs.S3File, gcsfs.GCSFile]\\n    # though mypy handling of conditional imports is difficult.\\n    # See https://github.com/python/mypy/issues/1297\\n    fp_or_buf, _, compression, should_close = get_filepath_or_buffer(\\n        filepath_or_buffer, encoding, compression)\\n    kwds[\\'compression\\'] = compression\\n\\n    if kwds.get(\\'date_parser\\', None) is not None:\\n        if isinstance(kwds[\\'parse_dates\\'], bool):\\n            kwds[\\'parse_dates\\'] = True\\n\\n    # Extract some of the arguments (pass chunksize on).\\n    iterator = kwds.get(\\'iterator\\', False)\\n    chunksize = _validate_integer(\\'chunksize\\', kwds.get(\\'chunksize\\', None), 1)\\n    nrows = kwds.get(\\'nrows\\', None)\\n\\n    # Check for duplicates in names.\\n    _validate_names(kwds.get(\"names\", None))\\n\\n    # Create the parser.\\n    parser = TextFileReader(fp_or_buf, **kwds)\\n\\n    if chunksize or iterator:\\n        return parser\\n\\n    try:\\n        data = parser.read(nrows)\\n    finally:\\n        parser.close()\\n\\n    if should_close:\\n        try:\\n            fp_or_buf.close()\\n        except ValueError:\\n            pass\\n\\n    return data',\n 'def read_fwf(filepath_or_buffer: FilePathOrBuffer,\\n             colspecs=\\'infer\\',\\n             widths=None,\\n             infer_nrows=100,\\n             **kwds):\\n\\n    r\"\"\"\\n    Read a table of fixed-width formatted lines into DataFrame.\\n\\n    Also supports optionally iterating or breaking of the file\\n    into chunks.\\n\\n    Additional help can be found in the `online docs for IO Tools\\n    <http://pandas.pydata.org/pandas-docs/stable/io.html>`_.\\n\\n    Parameters\\n    ----------\\n    filepath_or_buffer : str, path object, or file-like object\\n        Any valid string path is acceptable. The string could be a URL. Valid\\n        URL schemes include http, ftp, s3, and file. For file URLs, a host is\\n        expected. A local file could be: file://localhost/path/to/table.csv.\\n\\n        If you want to pass in a path object, pandas accepts either\\n        ``pathlib.Path`` or ``py._path.local.LocalPath``.\\n\\n        By file-like object, we refer to objects with a ``read()`` method,\\n        such as a file handler (e.g. via builtin ``open`` function)\\n        or ``StringIO``.\\n    colspecs : list of tuple (int, int) or \\'infer\\'. optional\\n        A list of tuples giving the extents of the fixed-width\\n        fields of each line as half-open intervals (i.e.,  [from, to[ ).\\n        String value \\'infer\\' can be used to instruct the parser to try\\n        detecting the column specifications from the first 100 rows of\\n        the data which are not being skipped via skiprows (default=\\'infer\\').\\n    widths : list of int, optional\\n        A list of field widths which can be used instead of \\'colspecs\\' if\\n        the intervals are contiguous.\\n    infer_nrows : int, default 100\\n        The number of rows to consider when letting the parser determine the\\n        `colspecs`.\\n\\n        .. versionadded:: 0.24.0\\n    **kwds : optional\\n        Optional keyword arguments can be passed to ``TextFileReader``.\\n\\n    Returns\\n    -------\\n    DataFrame or TextParser\\n        A comma-separated values (csv) file is returned as two-dimensional\\n        data structure with labeled axes.\\n\\n    See Also\\n    --------\\n    to_csv : Write DataFrame to a comma-separated values (csv) file.\\n    read_csv : Read a comma-separated values (csv) file into DataFrame.\\n\\n    Examples\\n    --------\\n    >>> pd.read_fwf(\\'data.csv\\')  # doctest: +SKIP\\n    \"\"\"\\n\\n    # Check input arguments.\\n    if colspecs is None and widths is None:\\n        raise ValueError(\"Must specify either colspecs or widths\")\\n    elif colspecs not in (None, \\'infer\\') and widths is not None:\\n        raise ValueError(\"You must specify only one of \\'widths\\' and \"\\n                         \"\\'colspecs\\'\")\\n\\n    # Compute \\'colspecs\\' from \\'widths\\', if specified.\\n    if widths is not None:\\n        colspecs, col = [], 0\\n        for w in widths:\\n            colspecs.append((col, col + w))\\n            col += w\\n\\n    kwds[\\'colspecs\\'] = colspecs\\n    kwds[\\'infer_nrows\\'] = infer_nrows\\n    kwds[\\'engine\\'] = \\'python-fwf\\'\\n    return _read(filepath_or_buffer, kwds)',\n 'def _is_potential_multi_index(columns):\\n    \"\"\"\\n    Check whether or not the `columns` parameter\\n    could be converted into a MultiIndex.\\n\\n    Parameters\\n    ----------\\n    columns : array-like\\n        Object which may or may not be convertible into a MultiIndex\\n\\n    Returns\\n    -------\\n    boolean : Whether or not columns could become a MultiIndex\\n    \"\"\"\\n    return (len(columns) and not isinstance(columns, MultiIndex) and\\n            all(isinstance(c, tuple) for c in columns))',\n 'def _evaluate_usecols(usecols, names):\\n    \"\"\"\\n    Check whether or not the \\'usecols\\' parameter\\n    is a callable.  If so, enumerates the \\'names\\'\\n    parameter and returns a set of indices for\\n    each entry in \\'names\\' that evaluates to True.\\n    If not a callable, returns \\'usecols\\'.\\n    \"\"\"\\n    if callable(usecols):\\n        return {i for i, name in enumerate(names) if usecols(name)}\\n    return usecols',\n 'def _validate_usecols_names(usecols, names):\\n    \"\"\"\\n    Validates that all usecols are present in a given\\n    list of names. If not, raise a ValueError that\\n    shows what usecols are missing.\\n\\n    Parameters\\n    ----------\\n    usecols : iterable of usecols\\n        The columns to validate are present in names.\\n    names : iterable of names\\n        The column names to check against.\\n\\n    Returns\\n    -------\\n    usecols : iterable of usecols\\n        The `usecols` parameter if the validation succeeds.\\n\\n    Raises\\n    ------\\n    ValueError : Columns were missing. Error message will list them.\\n    \"\"\"\\n    missing = [c for c in usecols if c not in names]\\n    if len(missing) > 0:\\n        raise ValueError(\\n            \"Usecols do not match columns, \"\\n            \"columns expected but not found: {missing}\".format(missing=missing)\\n        )\\n\\n    return usecols',\n 'def _validate_usecols_arg(usecols):\\n    \"\"\"\\n    Validate the \\'usecols\\' parameter.\\n\\n    Checks whether or not the \\'usecols\\' parameter contains all integers\\n    (column selection by index), strings (column by name) or is a callable.\\n    Raises a ValueError if that is not the case.\\n\\n    Parameters\\n    ----------\\n    usecols : list-like, callable, or None\\n        List of columns to use when parsing or a callable that can be used\\n        to filter a list of table columns.\\n\\n    Returns\\n    -------\\n    usecols_tuple : tuple\\n        A tuple of (verified_usecols, usecols_dtype).\\n\\n        \\'verified_usecols\\' is either a set if an array-like is passed in or\\n        \\'usecols\\' if a callable or None is passed in.\\n\\n        \\'usecols_dtype` is the inferred dtype of \\'usecols\\' if an array-like\\n        is passed in or None if a callable or None is passed in.\\n    \"\"\"\\n    msg = (\"\\'usecols\\' must either be list-like of all strings, all unicode, \"\\n           \"all integers or a callable.\")\\n    if usecols is not None:\\n        if callable(usecols):\\n            return usecols, None\\n\\n        if not is_list_like(usecols):\\n            # see gh-20529\\n            #\\n            # Ensure it is iterable container but not string.\\n            raise ValueError(msg)\\n\\n        usecols_dtype = lib.infer_dtype(usecols, skipna=False)\\n\\n        if usecols_dtype not in (\"empty\", \"integer\",\\n                                 \"string\", \"unicode\"):\\n            raise ValueError(msg)\\n\\n        usecols = set(usecols)\\n\\n        return usecols, usecols_dtype\\n    return usecols, None',\n 'def _validate_parse_dates_arg(parse_dates):\\n    \"\"\"\\n    Check whether or not the \\'parse_dates\\' parameter\\n    is a non-boolean scalar. Raises a ValueError if\\n    that is the case.\\n    \"\"\"\\n    msg = (\"Only booleans, lists, and \"\\n           \"dictionaries are accepted \"\\n           \"for the \\'parse_dates\\' parameter\")\\n\\n    if parse_dates is not None:\\n        if is_scalar(parse_dates):\\n            if not lib.is_bool(parse_dates):\\n                raise TypeError(msg)\\n\\n        elif not isinstance(parse_dates, (list, dict)):\\n            raise TypeError(msg)\\n\\n    return parse_dates',\n 'def _stringify_na_values(na_values):\\n    \"\"\" return a stringified and numeric for these values \"\"\"\\n    result = []\\n    for x in na_values:\\n        result.append(str(x))\\n        result.append(x)\\n        try:\\n            v = float(x)\\n\\n            # we are like 999 here\\n            if v == int(v):\\n                v = int(v)\\n                result.append(\"{value}.0\".format(value=v))\\n                result.append(str(v))\\n\\n            result.append(v)\\n        except (TypeError, ValueError, OverflowError):\\n            pass\\n        try:\\n            result.append(int(x))\\n        except (TypeError, ValueError, OverflowError):\\n            pass\\n    return set(result)',\n 'def _get_na_values(col, na_values, na_fvalues, keep_default_na):\\n    \"\"\"\\n    Get the NaN values for a given column.\\n\\n    Parameters\\n    ----------\\n    col : str\\n        The name of the column.\\n    na_values : array-like, dict\\n        The object listing the NaN values as strings.\\n    na_fvalues : array-like, dict\\n        The object listing the NaN values as floats.\\n    keep_default_na : bool\\n        If `na_values` is a dict, and the column is not mapped in the\\n        dictionary, whether to return the default NaN values or the empty set.\\n\\n    Returns\\n    -------\\n    nan_tuple : A length-two tuple composed of\\n\\n        1) na_values : the string NaN values for that column.\\n        2) na_fvalues : the float NaN values for that column.\\n    \"\"\"\\n\\n    if isinstance(na_values, dict):\\n        if col in na_values:\\n            return na_values[col], na_fvalues[col]\\n        else:\\n            if keep_default_na:\\n                return _NA_VALUES, set()\\n\\n            return set(), set()\\n    else:\\n        return na_values, na_fvalues',\n 'def _extract_multi_indexer_columns(self, header, index_names, col_names,\\n                                       passed_names=False):\\n        \"\"\" extract and return the names, index_names, col_names\\n            header is a list-of-lists returned from the parsers \"\"\"\\n        if len(header) < 2:\\n            return header[0], index_names, col_names, passed_names\\n\\n        # the names are the tuples of the header that are not the index cols\\n        # 0 is the name of the index, assuming index_col is a list of column\\n        # numbers\\n        ic = self.index_col\\n        if ic is None:\\n            ic = []\\n\\n        if not isinstance(ic, (list, tuple, np.ndarray)):\\n            ic = [ic]\\n        sic = set(ic)\\n\\n        # clean the index_names\\n        index_names = header.pop(-1)\\n        index_names, names, index_col = _clean_index_names(index_names,\\n                                                           self.index_col,\\n                                                           self.unnamed_cols)\\n\\n        # extract the columns\\n        field_count = len(header[0])\\n\\n        def extract(r):\\n            return tuple(r[i] for i in range(field_count) if i not in sic)\\n\\n        columns = lzip(*[extract(r) for r in header])\\n        names = ic + columns\\n\\n        # If we find unnamed columns all in a single\\n        # level, then our header was too long.\\n        for n in range(len(columns[0])):\\n            if all(compat.to_str(c[n]) in self.unnamed_cols for c in columns):\\n                raise ParserError(\\n                    \"Passed header=[{header}] are too many rows for this \"\\n                    \"multi_index of columns\"\\n                    .format(header=\\',\\'.join(str(x) for x in self.header))\\n                )\\n\\n        # Clean the column names (if we have an index_col).\\n        if len(ic):\\n            col_names = [r[0] if (len(r[0]) and\\n                                  r[0] not in self.unnamed_cols) else None\\n                         for r in header]\\n        else:\\n            col_names = [None] * len(header)\\n\\n        passed_names = True\\n\\n        return names, index_names, col_names, passed_names',\n 'def _infer_types(self, values, na_values, try_num_bool=True):\\n        \"\"\"\\n        Infer types of values, possibly casting\\n\\n        Parameters\\n        ----------\\n        values : ndarray\\n        na_values : set\\n        try_num_bool : bool, default try\\n           try to cast values to numeric (first preference) or boolean\\n\\n        Returns:\\n        --------\\n        converted : ndarray\\n        na_count : int\\n        \"\"\"\\n        na_count = 0\\n        if issubclass(values.dtype.type, (np.number, np.bool_)):\\n            mask = algorithms.isin(values, list(na_values))\\n            na_count = mask.sum()\\n            if na_count > 0:\\n                if is_integer_dtype(values):\\n                    values = values.astype(np.float64)\\n                np.putmask(values, mask, np.nan)\\n            return values, na_count\\n\\n        if try_num_bool:\\n            try:\\n                result = lib.maybe_convert_numeric(values, na_values, False)\\n                na_count = isna(result).sum()\\n            except Exception:\\n                result = values\\n                if values.dtype == np.object_:\\n                    na_count = parsers.sanitize_objects(result,\\n                                                        na_values, False)\\n        else:\\n            result = values\\n            if values.dtype == np.object_:\\n                na_count = parsers.sanitize_objects(values, na_values, False)\\n\\n        if result.dtype == np.object_ and try_num_bool:\\n            result = libops.maybe_convert_bool(np.asarray(values),\\n                                               true_values=self.true_values,\\n                                               false_values=self.false_values)\\n\\n        return result, na_count',\n 'def _cast_types(self, values, cast_type, column):\\n        \"\"\"\\n        Cast values to specified type\\n\\n        Parameters\\n        ----------\\n        values : ndarray\\n        cast_type : string or np.dtype\\n           dtype to cast values to\\n        column : string\\n            column name - used only for error reporting\\n\\n        Returns\\n        -------\\n        converted : ndarray\\n        \"\"\"\\n\\n        if is_categorical_dtype(cast_type):\\n            known_cats = (isinstance(cast_type, CategoricalDtype) and\\n                          cast_type.categories is not None)\\n\\n            if not is_object_dtype(values) and not known_cats:\\n                # XXX this is for consistency with\\n                # c-parser which parses all categories\\n                # as strings\\n                values = astype_nansafe(values, str)\\n\\n            cats = Index(values).unique().dropna()\\n            values = Categorical._from_inferred_categories(\\n                cats, cats.get_indexer(values), cast_type,\\n                true_values=self.true_values)\\n\\n        # use the EA\\'s implementation of casting\\n        elif is_extension_array_dtype(cast_type):\\n            # ensure cast_type is an actual dtype and not a string\\n            cast_type = pandas_dtype(cast_type)\\n            array_type = cast_type.construct_array_type()\\n            try:\\n                return array_type._from_sequence_of_strings(values,\\n                                                            dtype=cast_type)\\n            except NotImplementedError:\\n                raise NotImplementedError(\\n                    \"Extension Array: {ea} must implement \"\\n                    \"_from_sequence_of_strings in order \"\\n                    \"to be used in parser methods\".format(ea=array_type))\\n\\n        else:\\n            try:\\n                values = astype_nansafe(values, cast_type,\\n                                        copy=True, skipna=True)\\n            except ValueError:\\n                raise ValueError(\\n                    \"Unable to convert column {column} to type \"\\n                    \"{cast_type}\".format(\\n                        column=column, cast_type=cast_type))\\n        return values',\n 'def _set_noconvert_columns(self):\\n        \"\"\"\\n        Set the columns that should not undergo dtype conversions.\\n\\n        Currently, any column that is involved with date parsing will not\\n        undergo such conversions.\\n        \"\"\"\\n        names = self.orig_names\\n        if self.usecols_dtype == \\'integer\\':\\n            # A set of integers will be converted to a list in\\n            # the correct order every single time.\\n            usecols = list(self.usecols)\\n            usecols.sort()\\n        elif (callable(self.usecols) or\\n                self.usecols_dtype not in (\\'empty\\', None)):\\n            # The names attribute should have the correct columns\\n            # in the proper order for indexing with parse_dates.\\n            usecols = self.names[:]\\n        else:\\n            # Usecols is empty.\\n            usecols = None\\n\\n        def _set(x):\\n            if usecols is not None and is_integer(x):\\n                x = usecols[x]\\n\\n            if not is_integer(x):\\n                x = names.index(x)\\n\\n            self._reader.set_noconvert(x)\\n\\n        if isinstance(self.parse_dates, list):\\n            for val in self.parse_dates:\\n                if isinstance(val, list):\\n                    for k in val:\\n                        _set(k)\\n                else:\\n                    _set(val)\\n\\n        elif isinstance(self.parse_dates, dict):\\n            for val in self.parse_dates.values():\\n                if isinstance(val, list):\\n                    for k in val:\\n                        _set(k)\\n                else:\\n                    _set(val)\\n\\n        elif self.parse_dates:\\n            if isinstance(self.index_col, list):\\n                for k in self.index_col:\\n                    _set(k)\\n            elif self.index_col is not None:\\n                _set(self.index_col)',\n 'def _handle_usecols(self, columns, usecols_key):\\n        \"\"\"\\n        Sets self._col_indices\\n\\n        usecols_key is used if there are string usecols.\\n        \"\"\"\\n        if self.usecols is not None:\\n            if callable(self.usecols):\\n                col_indices = _evaluate_usecols(self.usecols, usecols_key)\\n            elif any(isinstance(u, str) for u in self.usecols):\\n                if len(columns) > 1:\\n                    raise ValueError(\"If using multiple headers, usecols must \"\\n                                     \"be integers.\")\\n                col_indices = []\\n\\n                for col in self.usecols:\\n                    if isinstance(col, str):\\n                        try:\\n                            col_indices.append(usecols_key.index(col))\\n                        except ValueError:\\n                            _validate_usecols_names(self.usecols, usecols_key)\\n                    else:\\n                        col_indices.append(col)\\n            else:\\n                col_indices = self.usecols\\n\\n            columns = [[n for i, n in enumerate(column) if i in col_indices]\\n                       for column in columns]\\n            self._col_indices = col_indices\\n        return columns',\n 'def _check_for_bom(self, first_row):\\n        \"\"\"\\n        Checks whether the file begins with the BOM character.\\n        If it does, remove it. In addition, if there is quoting\\n        in the field subsequent to the BOM, remove it as well\\n        because it technically takes place at the beginning of\\n        the name, not the middle of it.\\n        \"\"\"\\n        # first_row will be a list, so we need to check\\n        # that that list is not empty before proceeding.\\n        if not first_row:\\n            return first_row\\n\\n        # The first element of this row is the one that could have the\\n        # BOM that we want to remove. Check that the first element is a\\n        # string before proceeding.\\n        if not isinstance(first_row[0], str):\\n            return first_row\\n\\n        # Check that the string is not empty, as that would\\n        # obviously not have a BOM at the start of it.\\n        if not first_row[0]:\\n            return first_row\\n\\n        # Since the string is non-empty, check that it does\\n        # in fact begin with a BOM.\\n        first_elt = first_row[0][0]\\n        if first_elt != _BOM:\\n            return first_row\\n\\n        first_row = first_row[0]\\n\\n        if len(first_row) > 1 and first_row[1] == self.quotechar:\\n            start = 2\\n            quote = first_row[1]\\n            end = first_row[2:].index(quote) + 2\\n\\n            # Extract the data between the quotation marks\\n            new_row = first_row[start:end]\\n\\n            # Extract any remaining data after the second\\n            # quotation mark.\\n            if len(first_row) > end + 1:\\n                new_row += first_row[end + 1:]\\n            return [new_row]\\n        elif len(first_row) > 1:\\n            return [first_row[1:]]\\n        else:\\n            # First row is just the BOM, so we\\n            # return an empty string.\\n            return [\"\"]',\n 'def _alert_malformed(self, msg, row_num):\\n        \"\"\"\\n        Alert a user about a malformed row.\\n\\n        If `self.error_bad_lines` is True, the alert will be `ParserError`.\\n        If `self.warn_bad_lines` is True, the alert will be printed out.\\n\\n        Parameters\\n        ----------\\n        msg : The error message to display.\\n        row_num : The row number where the parsing error occurred.\\n                  Because this row number is displayed, we 1-index,\\n                  even though we 0-index internally.\\n        \"\"\"\\n\\n        if self.error_bad_lines:\\n            raise ParserError(msg)\\n        elif self.warn_bad_lines:\\n            base = \\'Skipping line {row_num}: \\'.format(row_num=row_num)\\n            sys.stderr.write(base + msg + \\'\\\\n\\')',\n 'def _next_iter_line(self, row_num):\\n        \"\"\"\\n        Wrapper around iterating through `self.data` (CSV source).\\n\\n        When a CSV error is raised, we check for specific\\n        error messages that allow us to customize the\\n        error message displayed to the user.\\n\\n        Parameters\\n        ----------\\n        row_num : The row number of the line being parsed.\\n        \"\"\"\\n\\n        try:\\n            return next(self.data)\\n        except csv.Error as e:\\n            if self.warn_bad_lines or self.error_bad_lines:\\n                msg = str(e)\\n\\n                if \\'NULL byte\\' in msg:\\n                    msg = (\\'NULL byte detected. This byte \\'\\n                           \\'cannot be processed in Python\\\\\\'s \\'\\n                           \\'native csv library at the moment, \\'\\n                           \\'so please pass in engine=\\\\\\'c\\\\\\' instead\\')\\n\\n                if self.skipfooter > 0:\\n                    reason = (\\'Error could possibly be due to \\'\\n                              \\'parsing errors in the skipped footer rows \\'\\n                              \\'(the skipfooter keyword is only applied \\'\\n                              \\'after Python\\\\\\'s csv library has parsed \\'\\n                              \\'all rows).\\')\\n                    msg += \\'. \\' + reason\\n\\n                self._alert_malformed(msg, row_num)\\n            return None',\n 'def _remove_empty_lines(self, lines):\\n        \"\"\"\\n        Iterate through the lines and remove any that are\\n        either empty or contain only one whitespace value\\n\\n        Parameters\\n        ----------\\n        lines : array-like\\n            The array of lines that we are to filter.\\n\\n        Returns\\n        -------\\n        filtered_lines : array-like\\n            The same array of lines with the \"empty\" ones removed.\\n        \"\"\"\\n\\n        ret = []\\n        for l in lines:\\n            # Remove empty lines and lines with only one whitespace value\\n            if (len(l) > 1 or len(l) == 1 and\\n                    (not isinstance(l[0], str) or l[0].strip())):\\n                ret.append(l)\\n        return ret',\n 'def _get_index_name(self, columns):\\n        \"\"\"\\n        Try several cases to get lines:\\n\\n        0) There are headers on row 0 and row 1 and their\\n        total summed lengths equals the length of the next line.\\n        Treat row 0 as columns and row 1 as indices\\n        1) Look for implicit index: there are more columns\\n        on row 1 than row 0. If this is true, assume that row\\n        1 lists index columns and row 0 lists normal columns.\\n        2) Get index from the columns if it was listed.\\n        \"\"\"\\n        orig_names = list(columns)\\n        columns = list(columns)\\n\\n        try:\\n            line = self._next_line()\\n        except StopIteration:\\n            line = None\\n\\n        try:\\n            next_line = self._next_line()\\n        except StopIteration:\\n            next_line = None\\n\\n        # implicitly index_col=0 b/c 1 fewer column names\\n        implicit_first_cols = 0\\n        if line is not None:\\n            # leave it 0, #2442\\n            # Case 1\\n            if self.index_col is not False:\\n                implicit_first_cols = len(line) - self.num_original_columns\\n\\n            # Case 0\\n            if next_line is not None:\\n                if len(next_line) == len(line) + self.num_original_columns:\\n                    # column and index names on diff rows\\n                    self.index_col = lrange(len(line))\\n                    self.buf = self.buf[1:]\\n\\n                    for c in reversed(line):\\n                        columns.insert(0, c)\\n\\n                    # Update list of original names to include all indices.\\n                    orig_names = list(columns)\\n                    self.num_original_columns = len(columns)\\n                    return line, orig_names, columns\\n\\n        if implicit_first_cols > 0:\\n            # Case 1\\n            self._implicit_index = True\\n            if self.index_col is None:\\n                self.index_col = lrange(implicit_first_cols)\\n\\n            index_name = None\\n\\n        else:\\n            # Case 2\\n            (index_name, columns_,\\n             self.index_col) = _clean_index_names(columns, self.index_col,\\n                                                  self.unnamed_cols)\\n\\n        return index_name, orig_names, columns',\n 'def get_rows(self, infer_nrows, skiprows=None):\\n        \"\"\"\\n        Read rows from self.f, skipping as specified.\\n\\n        We distinguish buffer_rows (the first <= infer_nrows\\n        lines) from the rows returned to detect_colspecs\\n        because it\\'s simpler to leave the other locations\\n        with skiprows logic alone than to modify them to\\n        deal with the fact we skipped some rows here as\\n        well.\\n\\n        Parameters\\n        ----------\\n        infer_nrows : int\\n            Number of rows to read from self.f, not counting\\n            rows that are skipped.\\n        skiprows: set, optional\\n            Indices of rows to skip.\\n\\n        Returns\\n        -------\\n        detect_rows : list of str\\n            A list containing the rows to read.\\n\\n        \"\"\"\\n        if skiprows is None:\\n            skiprows = set()\\n        buffer_rows = []\\n        detect_rows = []\\n        for i, row in enumerate(self.f):\\n            if i not in skiprows:\\n                detect_rows.append(row)\\n            buffer_rows.append(row)\\n            if len(detect_rows) >= infer_nrows:\\n                break\\n        self.buffer = iter(buffer_rows)\\n        return detect_rows',\n 'def linkcode_resolve(domain, info):\\n    \"\"\"\\n    Determine the URL corresponding to Python object\\n    \"\"\"\\n    if domain != \\'py\\':\\n        return None\\n\\n    modname = info[\\'module\\']\\n    fullname = info[\\'fullname\\']\\n\\n    submod = sys.modules.get(modname)\\n    if submod is None:\\n        return None\\n\\n    obj = submod\\n    for part in fullname.split(\\'.\\'):\\n        try:\\n            obj = getattr(obj, part)\\n        except AttributeError:\\n            return None\\n\\n    try:\\n        # inspect.unwrap() was added in Python version 3.4\\n        if sys.version_info >= (3, 5):\\n            fn = inspect.getsourcefile(inspect.unwrap(obj))\\n        else:\\n            fn = inspect.getsourcefile(obj)\\n    except TypeError:\\n        fn = None\\n    if not fn:\\n        return None\\n\\n    try:\\n        source, lineno = inspect.getsourcelines(obj)\\n    except OSError:\\n        lineno = None\\n\\n    if lineno:\\n        linespec = \"#L{:d}-L{:d}\".format(lineno, lineno + len(source) - 1)\\n    else:\\n        linespec = \"\"\\n\\n    fn = os.path.relpath(fn, start=os.path.dirname(pandas.__file__))\\n\\n    if \\'+\\' in pandas.__version__:\\n        return (\"http://github.com/pandas-dev/pandas/blob/master/pandas/\"\\n                \"{}{}\".format(fn, linespec))\\n    else:\\n        return (\"http://github.com/pandas-dev/pandas/blob/\"\\n                \"v{}/pandas/{}{}\".format(pandas.__version__, fn, linespec))',\n 'def process_class_docstrings(app, what, name, obj, options, lines):\\n    \"\"\"\\n    For those classes for which we use ::\\n\\n    :template: autosummary/class_without_autosummary.rst\\n\\n    the documented attributes/methods have to be listed in the class\\n    docstring. However, if one of those lists is empty, we use \\'None\\',\\n    which then generates warnings in sphinx / ugly html output.\\n    This \"autodoc-process-docstring\" event connector removes that part\\n    from the processed docstring.\\n\\n    \"\"\"\\n    if what == \"class\":\\n        joined = \\'\\\\n\\'.join(lines)\\n\\n        templates = [\\n            \"\"\".. rubric:: Attributes\\n\\n.. autosummary::\\n   :toctree:\\n\\n   None\\n\"\"\",\\n            \"\"\".. rubric:: Methods\\n\\n.. autosummary::\\n   :toctree:\\n\\n   None\\n\"\"\"\\n        ]\\n\\n        for template in templates:\\n            if template in joined:\\n                joined = joined.replace(template, \\'\\')\\n        lines[:] = joined.split(\\'\\\\n\\')',\n 'def pack(o, stream, **kwargs):\\n    \"\"\"\\n    Pack object `o` and write it to `stream`\\n\\n    See :class:`Packer` for options.\\n    \"\"\"\\n    packer = Packer(**kwargs)\\n    stream.write(packer.pack(o))',\n 'def get_mgr_concatenation_plan(mgr, indexers):\\n    \"\"\"\\n    Construct concatenation plan for given block manager and indexers.\\n\\n    Parameters\\n    ----------\\n    mgr : BlockManager\\n    indexers : dict of {axis: indexer}\\n\\n    Returns\\n    -------\\n    plan : list of (BlockPlacement, JoinUnit) tuples\\n\\n    \"\"\"\\n    # Calculate post-reindex shape , save for item axis which will be separate\\n    # for each block anyway.\\n    mgr_shape = list(mgr.shape)\\n    for ax, indexer in indexers.items():\\n        mgr_shape[ax] = len(indexer)\\n    mgr_shape = tuple(mgr_shape)\\n\\n    if 0 in indexers:\\n        ax0_indexer = indexers.pop(0)\\n        blknos = algos.take_1d(mgr._blknos, ax0_indexer, fill_value=-1)\\n        blklocs = algos.take_1d(mgr._blklocs, ax0_indexer, fill_value=-1)\\n    else:\\n\\n        if mgr._is_single_block:\\n            blk = mgr.blocks[0]\\n            return [(blk.mgr_locs, JoinUnit(blk, mgr_shape, indexers))]\\n\\n        ax0_indexer = None\\n        blknos = mgr._blknos\\n        blklocs = mgr._blklocs\\n\\n    plan = []\\n    for blkno, placements in libinternals.get_blkno_placements(blknos,\\n                                                               mgr.nblocks,\\n                                                               group=False):\\n\\n        assert placements.is_slice_like\\n\\n        join_unit_indexers = indexers.copy()\\n\\n        shape = list(mgr_shape)\\n        shape[0] = len(placements)\\n        shape = tuple(shape)\\n\\n        if blkno == -1:\\n            unit = JoinUnit(None, shape)\\n        else:\\n            blk = mgr.blocks[blkno]\\n            ax0_blk_indexer = blklocs[placements.indexer]\\n\\n            unit_no_ax0_reindexing = (len(placements) == len(blk.mgr_locs) and\\n                                      # Fastpath detection of join unit not\\n                                      # needing to reindex its block: no ax0\\n                                      # reindexing took place and block\\n                                      # placement was sequential before.\\n                                      ((ax0_indexer is None and\\n                                        blk.mgr_locs.is_slice_like and\\n                                        blk.mgr_locs.as_slice.step == 1) or\\n                                       # Slow-ish detection: all indexer locs\\n                                       # are sequential (and length match is\\n                                       # checked above).\\n                                       (np.diff(ax0_blk_indexer) == 1).all()))\\n\\n            # Omit indexer if no item reindexing is required.\\n            if unit_no_ax0_reindexing:\\n                join_unit_indexers.pop(0, None)\\n            else:\\n                join_unit_indexers[0] = ax0_blk_indexer\\n\\n            unit = JoinUnit(blk, shape, join_unit_indexers)\\n\\n        plan.append((placements, unit))\\n\\n    return plan',\n 'def concatenate_join_units(join_units, concat_axis, copy):\\n    \"\"\"\\n    Concatenate values from several join units along selected axis.\\n    \"\"\"\\n    if concat_axis == 0 and len(join_units) > 1:\\n        # Concatenating join units along ax0 is handled in _merge_blocks.\\n        raise AssertionError(\"Concatenating join units along axis0\")\\n\\n    empty_dtype, upcasted_na = get_empty_dtype_and_na(join_units)\\n\\n    to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\\n                                         upcasted_na=upcasted_na)\\n                 for ju in join_units]\\n\\n    if len(to_concat) == 1:\\n        # Only one block, nothing to concatenate.\\n        concat_values = to_concat[0]\\n        if copy:\\n            if isinstance(concat_values, np.ndarray):\\n                # non-reindexed (=not yet copied) arrays are made into a view\\n                # in JoinUnit.get_reindexed_values\\n                if concat_values.base is not None:\\n                    concat_values = concat_values.copy()\\n            else:\\n                concat_values = concat_values.copy()\\n    else:\\n        concat_values = _concat._concat_compat(to_concat, axis=concat_axis)\\n\\n    return concat_values',\n 'def get_empty_dtype_and_na(join_units):\\n    \"\"\"\\n    Return dtype and N/A values to use when concatenating specified units.\\n\\n    Returned N/A value may be None which means there was no casting involved.\\n\\n    Returns\\n    -------\\n    dtype\\n    na\\n    \"\"\"\\n    if len(join_units) == 1:\\n        blk = join_units[0].block\\n        if blk is None:\\n            return np.float64, np.nan\\n\\n    if is_uniform_reindex(join_units):\\n        # XXX: integrate property\\n        empty_dtype = join_units[0].block.dtype\\n        upcasted_na = join_units[0].block.fill_value\\n        return empty_dtype, upcasted_na\\n\\n    has_none_blocks = False\\n    dtypes = [None] * len(join_units)\\n    for i, unit in enumerate(join_units):\\n        if unit.block is None:\\n            has_none_blocks = True\\n        else:\\n            dtypes[i] = unit.dtype\\n\\n    upcast_classes = defaultdict(list)\\n    null_upcast_classes = defaultdict(list)\\n    for dtype, unit in zip(dtypes, join_units):\\n        if dtype is None:\\n            continue\\n\\n        if is_categorical_dtype(dtype):\\n            upcast_cls = \\'category\\'\\n        elif is_datetime64tz_dtype(dtype):\\n            upcast_cls = \\'datetimetz\\'\\n        elif issubclass(dtype.type, np.bool_):\\n            upcast_cls = \\'bool\\'\\n        elif issubclass(dtype.type, np.object_):\\n            upcast_cls = \\'object\\'\\n        elif is_datetime64_dtype(dtype):\\n            upcast_cls = \\'datetime\\'\\n        elif is_timedelta64_dtype(dtype):\\n            upcast_cls = \\'timedelta\\'\\n        elif is_sparse(dtype):\\n            upcast_cls = dtype.subtype.name\\n        elif is_extension_array_dtype(dtype):\\n            upcast_cls = \\'object\\'\\n        elif is_float_dtype(dtype) or is_numeric_dtype(dtype):\\n            upcast_cls = dtype.name\\n        else:\\n            upcast_cls = \\'float\\'\\n\\n        # Null blocks should not influence upcast class selection, unless there\\n        # are only null blocks, when same upcasting rules must be applied to\\n        # null upcast classes.\\n        if unit.is_na:\\n            null_upcast_classes[upcast_cls].append(dtype)\\n        else:\\n            upcast_classes[upcast_cls].append(dtype)\\n\\n    if not upcast_classes:\\n        upcast_classes = null_upcast_classes\\n\\n    # create the result\\n    if \\'object\\' in upcast_classes:\\n        return np.dtype(np.object_), np.nan\\n    elif \\'bool\\' in upcast_classes:\\n        if has_none_blocks:\\n            return np.dtype(np.object_), np.nan\\n        else:\\n            return np.dtype(np.bool_), None\\n    elif \\'category\\' in upcast_classes:\\n        return np.dtype(np.object_), np.nan\\n    elif \\'datetimetz\\' in upcast_classes:\\n        # GH-25014. We use NaT instead of iNaT, since this eventually\\n        # ends up in DatetimeArray.take, which does not allow iNaT.\\n        dtype = upcast_classes[\\'datetimetz\\']\\n        return dtype[0], tslibs.NaT\\n    elif \\'datetime\\' in upcast_classes:\\n        return np.dtype(\\'M8[ns]\\'), tslibs.iNaT\\n    elif \\'timedelta\\' in upcast_classes:\\n        return np.dtype(\\'m8[ns]\\'), tslibs.iNaT\\n    else:  # pragma\\n        try:\\n            g = np.find_common_type(upcast_classes, [])\\n        except TypeError:\\n            # At least one is an ExtensionArray\\n            return np.dtype(np.object_), np.nan\\n        else:\\n            if is_float_dtype(g):\\n                return g, g.type(np.nan)\\n            elif is_numeric_dtype(g):\\n                if has_none_blocks:\\n                    return np.float64, np.nan\\n                else:\\n                    return g, None\\n\\n    msg = \"invalid dtype determination in get_concat_dtype\"\\n    raise AssertionError(msg)',\n 'def is_uniform_join_units(join_units):\\n    \"\"\"\\n    Check if the join units consist of blocks of uniform type that can\\n    be concatenated using Block.concat_same_type instead of the generic\\n    concatenate_join_units (which uses `_concat._concat_compat`).\\n\\n    \"\"\"\\n    return (\\n        # all blocks need to have the same type\\n        all(type(ju.block) is type(join_units[0].block) for ju in join_units) and  # noqa\\n        # no blocks that would get missing values (can lead to type upcasts)\\n        # unless we\\'re an extension dtype.\\n        all(not ju.is_na or ju.block.is_extension for ju in join_units) and\\n        # no blocks with indexers (as then the dimensions do not fit)\\n        all(not ju.indexers for ju in join_units) and\\n        # disregard Panels\\n        all(ju.block.ndim <= 2 for ju in join_units) and\\n        # only use this path when there is something to concatenate\\n        len(join_units) > 1)',\n 'def trim_join_unit(join_unit, length):\\n    \"\"\"\\n    Reduce join_unit\\'s shape along item axis to length.\\n\\n    Extra items that didn\\'t fit are returned as a separate block.\\n    \"\"\"\\n\\n    if 0 not in join_unit.indexers:\\n        extra_indexers = join_unit.indexers\\n\\n        if join_unit.block is None:\\n            extra_block = None\\n        else:\\n            extra_block = join_unit.block.getitem_block(slice(length, None))\\n            join_unit.block = join_unit.block.getitem_block(slice(length))\\n    else:\\n        extra_block = join_unit.block\\n\\n        extra_indexers = copy.copy(join_unit.indexers)\\n        extra_indexers[0] = extra_indexers[0][length:]\\n        join_unit.indexers[0] = join_unit.indexers[0][:length]\\n\\n    extra_shape = (join_unit.shape[0] - length,) + join_unit.shape[1:]\\n    join_unit.shape = (length,) + join_unit.shape[1:]\\n\\n    return JoinUnit(block=extra_block, indexers=extra_indexers,\\n                    shape=extra_shape)',\n 'def combine_concat_plans(plans, concat_axis):\\n    \"\"\"\\n    Combine multiple concatenation plans into one.\\n\\n    existing_plan is updated in-place.\\n    \"\"\"\\n    if len(plans) == 1:\\n        for p in plans[0]:\\n            yield p[0], [p[1]]\\n\\n    elif concat_axis == 0:\\n        offset = 0\\n        for plan in plans:\\n            last_plc = None\\n\\n            for plc, unit in plan:\\n                yield plc.add(offset), [unit]\\n                last_plc = plc\\n\\n            if last_plc is not None:\\n                offset += last_plc.as_slice.stop\\n\\n    else:\\n        num_ended = [0]\\n\\n        def _next_or_none(seq):\\n            retval = next(seq, None)\\n            if retval is None:\\n                num_ended[0] += 1\\n            return retval\\n\\n        plans = list(map(iter, plans))\\n        next_items = list(map(_next_or_none, plans))\\n\\n        while num_ended[0] != len(next_items):\\n            if num_ended[0] > 0:\\n                raise ValueError(\"Plan shapes are not aligned\")\\n\\n            placements, units = zip(*next_items)\\n\\n            lengths = list(map(len, placements))\\n            min_len, max_len = min(lengths), max(lengths)\\n\\n            if min_len == max_len:\\n                yield placements[0], units\\n                next_items[:] = map(_next_or_none, plans)\\n            else:\\n                yielded_placement = None\\n                yielded_units = [None] * len(next_items)\\n                for i, (plc, unit) in enumerate(next_items):\\n                    yielded_units[i] = unit\\n                    if len(plc) > min_len:\\n                        # trim_join_unit updates unit in place, so only\\n                        # placement needs to be sliced to skip min_len.\\n                        next_items[i] = (plc[min_len:],\\n                                         trim_join_unit(unit, min_len))\\n                    else:\\n                        yielded_placement = plc\\n                        next_items[i] = _next_or_none(plans[i])\\n\\n                yield yielded_placement, yielded_units',\n 'def use(self, key, value):\\n        \"\"\"\\n        Temporarily set a parameter value using the with statement.\\n        Aliasing allowed.\\n        \"\"\"\\n        old_value = self[key]\\n        try:\\n            self[key] = value\\n            yield self\\n        finally:\\n            self[key] = old_value',\n 'def _stata_elapsed_date_to_datetime_vec(dates, fmt):\\n    \"\"\"\\n    Convert from SIF to datetime. http://www.stata.com/help.cgi?datetime\\n\\n    Parameters\\n    ----------\\n    dates : Series\\n        The Stata Internal Format date to convert to datetime according to fmt\\n    fmt : str\\n        The format to convert to. Can be, tc, td, tw, tm, tq, th, ty\\n        Returns\\n\\n    Returns\\n    -------\\n    converted : Series\\n        The converted dates\\n\\n    Examples\\n    --------\\n    >>> dates = pd.Series([52])\\n    >>> _stata_elapsed_date_to_datetime_vec(dates , \"%tw\")\\n    0   1961-01-01\\n    dtype: datetime64[ns]\\n\\n    Notes\\n    -----\\n    datetime/c - tc\\n        milliseconds since 01jan1960 00:00:00.000, assuming 86,400 s/day\\n    datetime/C - tC - NOT IMPLEMENTED\\n        milliseconds since 01jan1960 00:00:00.000, adjusted for leap seconds\\n    date - td\\n        days since 01jan1960 (01jan1960 = 0)\\n    weekly date - tw\\n        weeks since 1960w1\\n        This assumes 52 weeks in a year, then adds 7 * remainder of the weeks.\\n        The datetime value is the start of the week in terms of days in the\\n        year, not ISO calendar weeks.\\n    monthly date - tm\\n        months since 1960m1\\n    quarterly date - tq\\n        quarters since 1960q1\\n    half-yearly date - th\\n        half-years since 1960h1 yearly\\n    date - ty\\n        years since 0000\\n\\n    If you don\\'t have pandas with datetime support, then you can\\'t do\\n    milliseconds accurately.\\n    \"\"\"\\n    MIN_YEAR, MAX_YEAR = Timestamp.min.year, Timestamp.max.year\\n    MAX_DAY_DELTA = (Timestamp.max - datetime.datetime(1960, 1, 1)).days\\n    MIN_DAY_DELTA = (Timestamp.min - datetime.datetime(1960, 1, 1)).days\\n    MIN_MS_DELTA = MIN_DAY_DELTA * 24 * 3600 * 1000\\n    MAX_MS_DELTA = MAX_DAY_DELTA * 24 * 3600 * 1000\\n\\n    def convert_year_month_safe(year, month):\\n        \"\"\"\\n        Convert year and month to datetimes, using pandas vectorized versions\\n        when the date range falls within the range supported by pandas.\\n        Otherwise it falls back to a slower but more robust method\\n        using datetime.\\n        \"\"\"\\n        if year.max() < MAX_YEAR and year.min() > MIN_YEAR:\\n            return to_datetime(100 * year + month, format=\\'%Y%m\\')\\n        else:\\n            index = getattr(year, \\'index\\', None)\\n            return Series(\\n                [datetime.datetime(y, m, 1) for y, m in zip(year, month)],\\n                index=index)\\n\\n    def convert_year_days_safe(year, days):\\n        \"\"\"\\n        Converts year (e.g. 1999) and days since the start of the year to a\\n        datetime or datetime64 Series\\n        \"\"\"\\n        if year.max() < (MAX_YEAR - 1) and year.min() > MIN_YEAR:\\n            return (to_datetime(year, format=\\'%Y\\') +\\n                    to_timedelta(days, unit=\\'d\\'))\\n        else:\\n            index = getattr(year, \\'index\\', None)\\n            value = [datetime.datetime(y, 1, 1) + relativedelta(days=int(d))\\n                     for y, d in zip(year, days)]\\n            return Series(value, index=index)\\n\\n    def convert_delta_safe(base, deltas, unit):\\n        \"\"\"\\n        Convert base dates and deltas to datetimes, using pandas vectorized\\n        versions if the deltas satisfy restrictions required to be expressed\\n        as dates in pandas.\\n        \"\"\"\\n        index = getattr(deltas, \\'index\\', None)\\n        if unit == \\'d\\':\\n            if deltas.max() > MAX_DAY_DELTA or deltas.min() < MIN_DAY_DELTA:\\n                values = [base + relativedelta(days=int(d)) for d in deltas]\\n                return Series(values, index=index)\\n        elif unit == \\'ms\\':\\n            if deltas.max() > MAX_MS_DELTA or deltas.min() < MIN_MS_DELTA:\\n                values = [base + relativedelta(microseconds=(int(d) * 1000))\\n                          for d in deltas]\\n                return Series(values, index=index)\\n        else:\\n            raise ValueError(\\'format not understood\\')\\n        base = to_datetime(base)\\n        deltas = to_timedelta(deltas, unit=unit)\\n        return base + deltas\\n\\n    # TODO: If/when pandas supports more than datetime64[ns], this should be\\n    # improved to use correct range, e.g. datetime[Y] for yearly\\n    bad_locs = np.isnan(dates)\\n    has_bad_values = False\\n    if bad_locs.any():\\n        has_bad_values = True\\n        data_col = Series(dates)\\n        data_col[bad_locs] = 1.0  # Replace with NaT\\n    dates = dates.astype(np.int64)\\n\\n    if fmt.startswith((\"%tc\", \"tc\")):  # Delta ms relative to base\\n        base = stata_epoch\\n        ms = dates\\n        conv_dates = convert_delta_safe(base, ms, \\'ms\\')\\n    elif fmt.startswith((\"%tC\", \"tC\")):\\n\\n        warnings.warn(\"Encountered %tC format. Leaving in Stata \"\\n                      \"Internal Format.\")\\n        conv_dates = Series(dates, dtype=np.object)\\n        if has_bad_values:\\n            conv_dates[bad_locs] = NaT\\n        return conv_dates\\n    # Delta days relative to base\\n    elif fmt.startswith((\"%td\", \"td\", \"%d\", \"d\")):\\n        base = stata_epoch\\n        days = dates\\n        conv_dates = convert_delta_safe(base, days, \\'d\\')\\n    # does not count leap days - 7 days is a week.\\n    # 52nd week may have more than 7 days\\n    elif fmt.startswith((\"%tw\", \"tw\")):\\n        year = stata_epoch.year + dates // 52\\n        days = (dates % 52) * 7\\n        conv_dates = convert_year_days_safe(year, days)\\n    elif fmt.startswith((\"%tm\", \"tm\")):  # Delta months relative to base\\n        year = stata_epoch.year + dates // 12\\n        month = (dates % 12) + 1\\n        conv_dates = convert_year_month_safe(year, month)\\n    elif fmt.startswith((\"%tq\", \"tq\")):  # Delta quarters relative to base\\n        year = stata_epoch.year + dates // 4\\n        month = (dates % 4) * 3 + 1\\n        conv_dates = convert_year_month_safe(year, month)\\n    elif fmt.startswith((\"%th\", \"th\")):  # Delta half-years relative to base\\n        year = stata_epoch.year + dates // 2\\n        month = (dates % 2) * 6 + 1\\n        conv_dates = convert_year_month_safe(year, month)\\n    elif fmt.startswith((\"%ty\", \"ty\")):  # Years -- not delta\\n        year = dates\\n        month = np.ones_like(dates)\\n        conv_dates = convert_year_month_safe(year, month)\\n    else:\\n        raise ValueError(\"Date fmt {fmt} not understood\".format(fmt=fmt))\\n\\n    if has_bad_values:  # Restore NaT for bad values\\n        conv_dates[bad_locs] = NaT\\n\\n    return conv_dates',\n 'def _datetime_to_stata_elapsed_vec(dates, fmt):\\n    \"\"\"\\n    Convert from datetime to SIF. http://www.stata.com/help.cgi?datetime\\n\\n    Parameters\\n    ----------\\n    dates : Series\\n        Series or array containing datetime.datetime or datetime64[ns] to\\n        convert to the Stata Internal Format given by fmt\\n    fmt : str\\n        The format to convert to. Can be, tc, td, tw, tm, tq, th, ty\\n    \"\"\"\\n    index = dates.index\\n    NS_PER_DAY = 24 * 3600 * 1000 * 1000 * 1000\\n    US_PER_DAY = NS_PER_DAY / 1000\\n\\n    def parse_dates_safe(dates, delta=False, year=False, days=False):\\n        d = {}\\n        if is_datetime64_dtype(dates.values):\\n            if delta:\\n                delta = dates - stata_epoch\\n                d[\\'delta\\'] = delta.values.astype(\\n                    np.int64) // 1000  # microseconds\\n            if days or year:\\n                dates = DatetimeIndex(dates)\\n                d[\\'year\\'], d[\\'month\\'] = dates.year, dates.month\\n            if days:\\n                days = (dates.astype(np.int64) -\\n                        to_datetime(d[\\'year\\'], format=\\'%Y\\').astype(np.int64))\\n                d[\\'days\\'] = days // NS_PER_DAY\\n\\n        elif infer_dtype(dates, skipna=False) == \\'datetime\\':\\n            if delta:\\n                delta = dates.values - stata_epoch\\n                f = lambda x: \\\\\\n                    US_PER_DAY * x.days + 1000000 * x.seconds + x.microseconds\\n                v = np.vectorize(f)\\n                d[\\'delta\\'] = v(delta)\\n            if year:\\n                year_month = dates.apply(lambda x: 100 * x.year + x.month)\\n                d[\\'year\\'] = year_month.values // 100\\n                d[\\'month\\'] = (year_month.values - d[\\'year\\'] * 100)\\n            if days:\\n                f = lambda x: (x - datetime.datetime(x.year, 1, 1)).days\\n                v = np.vectorize(f)\\n                d[\\'days\\'] = v(dates)\\n        else:\\n            raise ValueError(\\'Columns containing dates must contain either \\'\\n                             \\'datetime64, datetime.datetime or null values.\\')\\n\\n        return DataFrame(d, index=index)\\n\\n    bad_loc = isna(dates)\\n    index = dates.index\\n    if bad_loc.any():\\n        dates = Series(dates)\\n        if is_datetime64_dtype(dates):\\n            dates[bad_loc] = to_datetime(stata_epoch)\\n        else:\\n            dates[bad_loc] = stata_epoch\\n\\n    if fmt in [\"%tc\", \"tc\"]:\\n        d = parse_dates_safe(dates, delta=True)\\n        conv_dates = d.delta / 1000\\n    elif fmt in [\"%tC\", \"tC\"]:\\n        warnings.warn(\"Stata Internal Format tC not supported.\")\\n        conv_dates = dates\\n    elif fmt in [\"%td\", \"td\"]:\\n        d = parse_dates_safe(dates, delta=True)\\n        conv_dates = d.delta // US_PER_DAY\\n    elif fmt in [\"%tw\", \"tw\"]:\\n        d = parse_dates_safe(dates, year=True, days=True)\\n        conv_dates = (52 * (d.year - stata_epoch.year) + d.days // 7)\\n    elif fmt in [\"%tm\", \"tm\"]:\\n        d = parse_dates_safe(dates, year=True)\\n        conv_dates = (12 * (d.year - stata_epoch.year) + d.month - 1)\\n    elif fmt in [\"%tq\", \"tq\"]:\\n        d = parse_dates_safe(dates, year=True)\\n        conv_dates = 4 * (d.year - stata_epoch.year) + (d.month - 1) // 3\\n    elif fmt in [\"%th\", \"th\"]:\\n        d = parse_dates_safe(dates, year=True)\\n        conv_dates = (2 * (d.year - stata_epoch.year) +\\n                      (d.month > 6).astype(np.int))\\n    elif fmt in [\"%ty\", \"ty\"]:\\n        d = parse_dates_safe(dates, year=True)\\n        conv_dates = d.year\\n    else:\\n        raise ValueError(\\n            \"Format {fmt} is not a known Stata date format\".format(fmt=fmt))\\n\\n    conv_dates = Series(conv_dates, dtype=np.float64)\\n    missing_value = struct.unpack(\\'<d\\', b\\'\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\xe0\\\\x7f\\')[0]\\n    conv_dates[bad_loc] = missing_value\\n\\n    return Series(conv_dates, index=index)',\n 'def _cast_to_stata_types(data):\\n    \"\"\"Checks the dtypes of the columns of a pandas DataFrame for\\n    compatibility with the data types and ranges supported by Stata, and\\n    converts if necessary.\\n\\n    Parameters\\n    ----------\\n    data : DataFrame\\n        The DataFrame to check and convert\\n\\n    Notes\\n    -----\\n    Numeric columns in Stata must be one of int8, int16, int32, float32 or\\n    float64, with some additional value restrictions.  int8 and int16 columns\\n    are checked for violations of the value restrictions and upcast if needed.\\n    int64 data is not usable in Stata, and so it is downcast to int32 whenever\\n    the value are in the int32 range, and sidecast to float64 when larger than\\n    this range.  If the int64 values are outside of the range of those\\n    perfectly representable as float64 values, a warning is raised.\\n\\n    bool columns are cast to int8.  uint columns are converted to int of the\\n    same size if there is no loss in precision, otherwise are upcast to a\\n    larger type.  uint64 is currently not supported since it is concerted to\\n    object in a DataFrame.\\n    \"\"\"\\n    ws = \\'\\'\\n    #                  original, if small, if large\\n    conversion_data = ((np.bool, np.int8, np.int8),\\n                       (np.uint8, np.int8, np.int16),\\n                       (np.uint16, np.int16, np.int32),\\n                       (np.uint32, np.int32, np.int64))\\n\\n    float32_max = struct.unpack(\\'<f\\', b\\'\\\\xff\\\\xff\\\\xff\\\\x7e\\')[0]\\n    float64_max = struct.unpack(\\'<d\\', b\\'\\\\xff\\\\xff\\\\xff\\\\xff\\\\xff\\\\xff\\\\xdf\\\\x7f\\')[0]\\n\\n    for col in data:\\n        dtype = data[col].dtype\\n        # Cast from unsupported types to supported types\\n        for c_data in conversion_data:\\n            if dtype == c_data[0]:\\n                if data[col].max() <= np.iinfo(c_data[1]).max:\\n                    dtype = c_data[1]\\n                else:\\n                    dtype = c_data[2]\\n                if c_data[2] == np.float64:  # Warn if necessary\\n                    if data[col].max() >= 2 ** 53:\\n                        ws = precision_loss_doc % (\\'uint64\\', \\'float64\\')\\n\\n                data[col] = data[col].astype(dtype)\\n\\n        # Check values and upcast if necessary\\n        if dtype == np.int8:\\n            if data[col].max() > 100 or data[col].min() < -127:\\n                data[col] = data[col].astype(np.int16)\\n        elif dtype == np.int16:\\n            if data[col].max() > 32740 or data[col].min() < -32767:\\n                data[col] = data[col].astype(np.int32)\\n        elif dtype == np.int64:\\n            if (data[col].max() <= 2147483620 and\\n                    data[col].min() >= -2147483647):\\n                data[col] = data[col].astype(np.int32)\\n            else:\\n                data[col] = data[col].astype(np.float64)\\n                if data[col].max() >= 2 ** 53 or data[col].min() <= -2 ** 53:\\n                    ws = precision_loss_doc % (\\'int64\\', \\'float64\\')\\n        elif dtype in (np.float32, np.float64):\\n            value = data[col].max()\\n            if np.isinf(value):\\n                raise ValueError(\\'Column {col} has a maximum value of \\'\\n                                 \\'infinity which is outside the range \\'\\n                                 \\'supported by Stata.\\'.format(col=col))\\n            if dtype == np.float32 and value > float32_max:\\n                data[col] = data[col].astype(np.float64)\\n            elif dtype == np.float64:\\n                if value > float64_max:\\n                    raise ValueError(\\'Column {col} has a maximum value \\'\\n                                     \\'({val}) outside the range supported by \\'\\n                                     \\'Stata ({float64_max})\\'\\n                                     .format(col=col, val=value,\\n                                             float64_max=float64_max))\\n\\n    if ws:\\n        warnings.warn(ws, PossiblePrecisionLoss)\\n\\n    return data',\n 'def _dtype_to_stata_type(dtype, column):\\n    \"\"\"\\n    Convert dtype types to stata types. Returns the byte of the given ordinal.\\n    See TYPE_MAP and comments for an explanation. This is also explained in\\n    the dta spec.\\n    1 - 244 are strings of this length\\n                         Pandas    Stata\\n    251 - for int8      byte\\n    252 - for int16     int\\n    253 - for int32     long\\n    254 - for float32   float\\n    255 - for double    double\\n\\n    If there are dates to convert, then dtype will already have the correct\\n    type inserted.\\n    \"\"\"\\n    # TODO: expand to handle datetime to integer conversion\\n    if dtype.type == np.object_:  # try to coerce it to the biggest string\\n        # not memory efficient, what else could we\\n        # do?\\n        itemsize = max_len_string_array(ensure_object(column.values))\\n        return max(itemsize, 1)\\n    elif dtype == np.float64:\\n        return 255\\n    elif dtype == np.float32:\\n        return 254\\n    elif dtype == np.int32:\\n        return 253\\n    elif dtype == np.int16:\\n        return 252\\n    elif dtype == np.int8:\\n        return 251\\n    else:  # pragma : no cover\\n        raise NotImplementedError(\\n            \"Data type {dtype} not supported.\".format(dtype=dtype))',\n 'def _dtype_to_default_stata_fmt(dtype, column, dta_version=114,\\n                                force_strl=False):\\n    \"\"\"\\n    Map numpy dtype to stata\\'s default format for this type. Not terribly\\n    important since users can change this in Stata. Semantics are\\n\\n    object  -> \"%DDs\" where DD is the length of the string.  If not a string,\\n                raise ValueError\\n    float64 -> \"%10.0g\"\\n    float32 -> \"%9.0g\"\\n    int64   -> \"%9.0g\"\\n    int32   -> \"%12.0g\"\\n    int16   -> \"%8.0g\"\\n    int8    -> \"%8.0g\"\\n    strl    -> \"%9s\"\\n    \"\"\"\\n    # TODO: Refactor to combine type with format\\n    # TODO: expand this to handle a default datetime format?\\n    if dta_version < 117:\\n        max_str_len = 244\\n    else:\\n        max_str_len = 2045\\n        if force_strl:\\n            return \\'%9s\\'\\n    if dtype.type == np.object_:\\n        inferred_dtype = infer_dtype(column, skipna=True)\\n        if not (inferred_dtype in (\\'string\\', \\'unicode\\') or\\n                len(column) == 0):\\n            raise ValueError(\\'Column `{col}` cannot be exported.\\\\n\\\\nOnly \\'\\n                             \\'string-like object arrays containing all \\'\\n                             \\'strings or a mix of strings and None can be \\'\\n                             \\'exported. Object arrays containing only null \\'\\n                             \\'values are prohibited. Other object types\\'\\n                             \\'cannot be exported and must first be converted \\'\\n                             \\'to one of the supported \\'\\n                             \\'types.\\'.format(col=column.name))\\n        itemsize = max_len_string_array(ensure_object(column.values))\\n        if itemsize > max_str_len:\\n            if dta_version >= 117:\\n                return \\'%9s\\'\\n            else:\\n                raise ValueError(excessive_string_length_error % column.name)\\n        return \"%\" + str(max(itemsize, 1)) + \"s\"\\n    elif dtype == np.float64:\\n        return \"%10.0g\"\\n    elif dtype == np.float32:\\n        return \"%9.0g\"\\n    elif dtype == np.int32:\\n        return \"%12.0g\"\\n    elif dtype == np.int8 or dtype == np.int16:\\n        return \"%8.0g\"\\n    else:  # pragma : no cover\\n        raise NotImplementedError(\\n            \"Data type {dtype} not supported.\".format(dtype=dtype))',\n 'def _pad_bytes_new(name, length):\\n    \"\"\"\\n    Takes a bytes instance and pads it with null bytes until it\\'s length chars.\\n    \"\"\"\\n    if isinstance(name, str):\\n        name = bytes(name, \\'utf-8\\')\\n    return name + b\\'\\\\x00\\' * (length - len(name))',\n 'def generate_value_label(self, byteorder, encoding):\\n        \"\"\"\\n        Parameters\\n        ----------\\n        byteorder : str\\n            Byte order of the output\\n        encoding : str\\n            File encoding\\n\\n        Returns\\n        -------\\n        value_label : bytes\\n            Bytes containing the formatted value label\\n        \"\"\"\\n\\n        self._encoding = encoding\\n        bio = BytesIO()\\n        null_string = \\'\\\\x00\\'\\n        null_byte = b\\'\\\\x00\\'\\n\\n        # len\\n        bio.write(struct.pack(byteorder + \\'i\\', self.len))\\n\\n        # labname\\n        labname = self._encode(_pad_bytes(self.labname[:32], 33))\\n        bio.write(labname)\\n\\n        # padding - 3 bytes\\n        for i in range(3):\\n            bio.write(struct.pack(\\'c\\', null_byte))\\n\\n        # value_label_table\\n        # n - int32\\n        bio.write(struct.pack(byteorder + \\'i\\', self.n))\\n\\n        # textlen  - int32\\n        bio.write(struct.pack(byteorder + \\'i\\', self.text_len))\\n\\n        # off - int32 array (n elements)\\n        for offset in self.off:\\n            bio.write(struct.pack(byteorder + \\'i\\', offset))\\n\\n        # val - int32 array (n elements)\\n        for value in self.val:\\n            bio.write(struct.pack(byteorder + \\'i\\', value))\\n\\n        # txt - Text labels, null terminated\\n        for text in self.txt:\\n            bio.write(self._encode(text + null_string))\\n\\n        bio.seek(0)\\n        return bio.read()',\n 'def _setup_dtype(self):\\n        \"\"\"Map between numpy and state dtypes\"\"\"\\n        if self._dtype is not None:\\n            return self._dtype\\n\\n        dtype = []  # Convert struct data types to numpy data type\\n        for i, typ in enumerate(self.typlist):\\n            if typ in self.NUMPY_TYPE_MAP:\\n                dtype.append((\\'s\\' + str(i), self.byteorder +\\n                              self.NUMPY_TYPE_MAP[typ]))\\n            else:\\n                dtype.append((\\'s\\' + str(i), \\'S\\' + str(typ)))\\n        dtype = np.dtype(dtype)\\n        self._dtype = dtype\\n\\n        return self._dtype',\n 'def _do_convert_categoricals(self, data, value_label_dict, lbllist,\\n                                 order_categoricals):\\n        \"\"\"\\n        Converts categorical columns to Categorical type.\\n        \"\"\"\\n        value_labels = list(value_label_dict.keys())\\n        cat_converted_data = []\\n        for col, label in zip(data, lbllist):\\n            if label in value_labels:\\n                # Explicit call with ordered=True\\n                cat_data = Categorical(data[col], ordered=order_categoricals)\\n                categories = []\\n                for category in cat_data.categories:\\n                    if category in value_label_dict[label]:\\n                        categories.append(value_label_dict[label][category])\\n                    else:\\n                        categories.append(category)  # Partially labeled\\n                try:\\n                    cat_data.categories = categories\\n                except ValueError:\\n                    vc = Series(categories).value_counts()\\n                    repeats = list(vc.index[vc > 1])\\n                    repeats = \\'-\\' * 80 + \\'\\\\n\\' + \\'\\\\n\\'.join(repeats)\\n                    # GH 25772\\n                    msg = \"\"\"\\nValue labels for column {col} are not unique. These cannot be converted to\\npandas categoricals.\\n\\nEither read the file with `convert_categoricals` set to False or use the\\nlow level interface in `StataReader` to separately read the values and the\\nvalue_labels.\\n\\nThe repeated labels are:\\n{repeats}\\n\"\"\"\\n                    raise ValueError(msg.format(col=col, repeats=repeats))\\n                # TODO: is the next line needed above in the data(...) method?\\n                cat_data = Series(cat_data, index=data.index)\\n                cat_converted_data.append((col, cat_data))\\n            else:\\n                cat_converted_data.append((col, data[col]))\\n        data = DataFrame.from_dict(OrderedDict(cat_converted_data))\\n        return data',\n 'def _write(self, to_write):\\n        \"\"\"\\n        Helper to call encode before writing to file for Python 3 compat.\\n        \"\"\"\\n        self._file.write(to_write.encode(self._encoding or\\n                                         self._default_encoding))',\n 'def _prepare_categoricals(self, data):\\n        \"\"\"Check for categorical columns, retain categorical information for\\n        Stata file and convert categorical data to int\"\"\"\\n\\n        is_cat = [is_categorical_dtype(data[col]) for col in data]\\n        self._is_col_cat = is_cat\\n        self._value_labels = []\\n        if not any(is_cat):\\n            return data\\n\\n        get_base_missing_value = StataMissingValue.get_base_missing_value\\n        data_formatted = []\\n        for col, col_is_cat in zip(data, is_cat):\\n            if col_is_cat:\\n                self._value_labels.append(StataValueLabel(data[col]))\\n                dtype = data[col].cat.codes.dtype\\n                if dtype == np.int64:\\n                    raise ValueError(\\'It is not possible to export \\'\\n                                     \\'int64-based categorical data to Stata.\\')\\n                values = data[col].cat.codes.values.copy()\\n\\n                # Upcast if needed so that correct missing values can be set\\n                if values.max() >= get_base_missing_value(dtype):\\n                    if dtype == np.int8:\\n                        dtype = np.int16\\n                    elif dtype == np.int16:\\n                        dtype = np.int32\\n                    else:\\n                        dtype = np.float64\\n                    values = np.array(values, dtype=dtype)\\n\\n                # Replace missing values with Stata missing value for type\\n                values[values == -1] = get_base_missing_value(dtype)\\n                data_formatted.append((col, values))\\n            else:\\n                data_formatted.append((col, data[col]))\\n        return DataFrame.from_dict(OrderedDict(data_formatted))',\n 'def _replace_nans(self, data):\\n        # return data\\n        \"\"\"Checks floating point data columns for nans, and replaces these with\\n        the generic Stata for missing value (.)\"\"\"\\n        for c in data:\\n            dtype = data[c].dtype\\n            if dtype in (np.float32, np.float64):\\n                if dtype == np.float32:\\n                    replacement = self.MISSING_VALUES[\\'f\\']\\n                else:\\n                    replacement = self.MISSING_VALUES[\\'d\\']\\n                data[c] = data[c].fillna(replacement)\\n\\n        return data',\n 'def _check_column_names(self, data):\\n        \"\"\"\\n        Checks column names to ensure that they are valid Stata column names.\\n        This includes checks for:\\n            * Non-string names\\n            * Stata keywords\\n            * Variables that start with numbers\\n            * Variables with names that are too long\\n\\n        When an illegal variable name is detected, it is converted, and if\\n        dates are exported, the variable name is propagated to the date\\n        conversion dictionary\\n        \"\"\"\\n        converted_names = {}\\n        columns = list(data.columns)\\n        original_columns = columns[:]\\n\\n        duplicate_var_id = 0\\n        for j, name in enumerate(columns):\\n            orig_name = name\\n            if not isinstance(name, str):\\n                name = str(name)\\n\\n            for c in name:\\n                if ((c < \\'A\\' or c > \\'Z\\') and (c < \\'a\\' or c > \\'z\\') and\\n                        (c < \\'0\\' or c > \\'9\\') and c != \\'_\\'):\\n                    name = name.replace(c, \\'_\\')\\n\\n            # Variable name must not be a reserved word\\n            if name in self.RESERVED_WORDS:\\n                name = \\'_\\' + name\\n\\n            # Variable name may not start with a number\\n            if name[0] >= \\'0\\' and name[0] <= \\'9\\':\\n                name = \\'_\\' + name\\n\\n            name = name[:min(len(name), 32)]\\n\\n            if not name == orig_name:\\n                # check for duplicates\\n                while columns.count(name) > 0:\\n                    # prepend ascending number to avoid duplicates\\n                    name = \\'_\\' + str(duplicate_var_id) + name\\n                    name = name[:min(len(name), 32)]\\n                    duplicate_var_id += 1\\n                converted_names[orig_name] = name\\n\\n            columns[j] = name\\n\\n        data.columns = columns\\n\\n        # Check date conversion, and fix key if needed\\n        if self._convert_dates:\\n            for c, o in zip(columns, original_columns):\\n                if c != o:\\n                    self._convert_dates[c] = self._convert_dates[o]\\n                    del self._convert_dates[o]\\n\\n        if converted_names:\\n            conversion_warning = []\\n            for orig_name, name in converted_names.items():\\n                # need to possibly encode the orig name if its unicode\\n                try:\\n                    orig_name = orig_name.encode(\\'utf-8\\')\\n                except (UnicodeDecodeError, AttributeError):\\n                    pass\\n                msg = \\'{0}   ->   {1}\\'.format(orig_name, name)\\n                conversion_warning.append(msg)\\n\\n            ws = invalid_name_doc.format(\\'\\\\n    \\'.join(conversion_warning))\\n            warnings.warn(ws, InvalidColumnName)\\n\\n        self._converted_names = converted_names\\n        self._update_strl_names()\\n\\n        return data',\n 'def _close(self):\\n        \"\"\"\\n        Close the file if it was created by the writer.\\n\\n        If a buffer or file-like object was passed in, for example a GzipFile,\\n        then leave this file open for the caller to close. In either case,\\n        attempt to flush the file contents to ensure they are written to disk\\n        (if supported)\\n        \"\"\"\\n        # Some file-like objects might not support flush\\n        try:\\n            self._file.flush()\\n        except AttributeError:\\n            pass\\n        if self._own_file:\\n            self._file.close()',\n 'def generate_table(self):\\n        \"\"\"\\n        Generates the GSO lookup table for the DataFRame\\n\\n        Returns\\n        -------\\n        gso_table : OrderedDict\\n            Ordered dictionary using the string found as keys\\n            and their lookup position (v,o) as values\\n        gso_df : DataFrame\\n            DataFrame where strl columns have been converted to\\n            (v,o) values\\n\\n        Notes\\n        -----\\n        Modifies the DataFrame in-place.\\n\\n        The DataFrame returned encodes the (v,o) values as uint64s. The\\n        encoding depends on teh dta version, and can be expressed as\\n\\n        enc = v + o * 2 ** (o_size * 8)\\n\\n        so that v is stored in the lower bits and o is in the upper\\n        bits. o_size is\\n\\n          * 117: 4\\n          * 118: 6\\n          * 119: 5\\n        \"\"\"\\n\\n        gso_table = self._gso_table\\n        gso_df = self.df\\n        columns = list(gso_df.columns)\\n        selected = gso_df[self.columns]\\n        col_index = [(col, columns.index(col)) for col in self.columns]\\n        keys = np.empty(selected.shape, dtype=np.uint64)\\n        for o, (idx, row) in enumerate(selected.iterrows()):\\n            for j, (col, v) in enumerate(col_index):\\n                val = row[col]\\n                # Allow columns with mixed str and None (GH 23633)\\n                val = \\'\\' if val is None else val\\n                key = gso_table.get(val, None)\\n                if key is None:\\n                    # Stata prefers human numbers\\n                    key = (v + 1, o + 1)\\n                    gso_table[val] = key\\n                keys[o, j] = self._convert_key(key)\\n        for i, col in enumerate(self.columns):\\n            gso_df[col] = keys[:, i]\\n\\n        return gso_table, gso_df',\n 'def generate_blob(self, gso_table):\\n        \"\"\"\\n        Generates the binary blob of GSOs that is written to the dta file.\\n\\n        Parameters\\n        ----------\\n        gso_table : OrderedDict\\n            Ordered dictionary (str, vo)\\n\\n        Returns\\n        -------\\n        gso : bytes\\n            Binary content of dta file to be placed between strl tags\\n\\n        Notes\\n        -----\\n        Output format depends on dta version.  117 uses two uint32s to\\n        express v and o while 118+ uses a uint32 for v and a uint64 for o.\\n        \"\"\"\\n        # Format information\\n        # Length includes null term\\n        # 117\\n        # GSOvvvvooootllllxxxxxxxxxxxxxxx...x\\n        #  3  u4  u4 u1 u4  string + null term\\n        #\\n        # 118, 119\\n        # GSOvvvvooooooootllllxxxxxxxxxxxxxxx...x\\n        #  3  u4   u8   u1 u4    string + null term\\n\\n        bio = BytesIO()\\n        gso = bytes(\\'GSO\\', \\'ascii\\')\\n        gso_type = struct.pack(self._byteorder + \\'B\\', 130)\\n        null = struct.pack(self._byteorder + \\'B\\', 0)\\n        v_type = self._byteorder + self._gso_v_type\\n        o_type = self._byteorder + self._gso_o_type\\n        len_type = self._byteorder + \\'I\\'\\n        for strl, vo in gso_table.items():\\n            if vo == (0, 0):\\n                continue\\n            v, o = vo\\n\\n            # GSO\\n            bio.write(gso)\\n\\n            # vvvv\\n            bio.write(struct.pack(v_type, v))\\n\\n            # oooo / oooooooo\\n            bio.write(struct.pack(o_type, o))\\n\\n            # t\\n            bio.write(gso_type)\\n\\n            # llll\\n            utf8_string = bytes(strl, \\'utf-8\\')\\n            bio.write(struct.pack(len_type, len(utf8_string) + 1))\\n\\n            # xxx...xxx\\n            bio.write(utf8_string)\\n            bio.write(null)\\n\\n        bio.seek(0)\\n        return bio.read()',\n 'def _tag(val, tag):\\n        \"\"\"Surround val with <tag></tag>\"\"\"\\n        if isinstance(val, str):\\n            val = bytes(val, \\'utf-8\\')\\n        return (bytes(\\'<\\' + tag + \\'>\\', \\'utf-8\\') + val +\\n                bytes(\\'</\\' + tag + \\'>\\', \\'utf-8\\'))',\n 'def _write_header(self, data_label=None, time_stamp=None):\\n        \"\"\"Write the file header\"\"\"\\n        byteorder = self._byteorder\\n        self._file.write(bytes(\\'<stata_dta>\\', \\'utf-8\\'))\\n        bio = BytesIO()\\n        # ds_format - 117\\n        bio.write(self._tag(bytes(\\'117\\', \\'utf-8\\'), \\'release\\'))\\n        # byteorder\\n        bio.write(self._tag(byteorder == \">\" and \"MSF\" or \"LSF\", \\'byteorder\\'))\\n        # number of vars, 2 bytes\\n        assert self.nvar < 2 ** 16\\n        bio.write(self._tag(struct.pack(byteorder + \"H\", self.nvar), \\'K\\'))\\n        # number of obs, 4 bytes\\n        bio.write(self._tag(struct.pack(byteorder + \"I\", self.nobs), \\'N\\'))\\n        # data label 81 bytes, char, null terminated\\n        label = data_label[:80] if data_label is not None else \\'\\'\\n        label_len = struct.pack(byteorder + \"B\", len(label))\\n        label = label_len + bytes(label, \\'utf-8\\')\\n        bio.write(self._tag(label, \\'label\\'))\\n        # time stamp, 18 bytes, char, null terminated\\n        # format dd Mon yyyy hh:mm\\n        if time_stamp is None:\\n            time_stamp = datetime.datetime.now()\\n        elif not isinstance(time_stamp, datetime.datetime):\\n            raise ValueError(\"time_stamp should be datetime type\")\\n        # Avoid locale-specific month conversion\\n        months = [\\'Jan\\', \\'Feb\\', \\'Mar\\', \\'Apr\\', \\'May\\', \\'Jun\\', \\'Jul\\', \\'Aug\\',\\n                  \\'Sep\\', \\'Oct\\', \\'Nov\\', \\'Dec\\']\\n        month_lookup = {i + 1: month for i, month in enumerate(months)}\\n        ts = (time_stamp.strftime(\"%d \") +\\n              month_lookup[time_stamp.month] +\\n              time_stamp.strftime(\" %Y %H:%M\"))\\n        # \\'\\\\x11\\' added due to inspection of Stata file\\n        ts = b\\'\\\\x11\\' + bytes(ts, \\'utf8\\')\\n        bio.write(self._tag(ts, \\'timestamp\\'))\\n        bio.seek(0)\\n        self._file.write(self._tag(bio.read(), \\'header\\'))',\n 'def _write_map(self):\\n        \"\"\"Called twice during file write. The first populates the values in\\n        the map with 0s.  The second call writes the final map locations when\\n        all blocks have been written.\"\"\"\\n        if self._map is None:\\n            self._map = OrderedDict(((\\'stata_data\\', 0),\\n                                     (\\'map\\', self._file.tell()),\\n                                     (\\'variable_types\\', 0),\\n                                     (\\'varnames\\', 0),\\n                                     (\\'sortlist\\', 0),\\n                                     (\\'formats\\', 0),\\n                                     (\\'value_label_names\\', 0),\\n                                     (\\'variable_labels\\', 0),\\n                                     (\\'characteristics\\', 0),\\n                                     (\\'data\\', 0),\\n                                     (\\'strls\\', 0),\\n                                     (\\'value_labels\\', 0),\\n                                     (\\'stata_data_close\\', 0),\\n                                     (\\'end-of-file\\', 0)))\\n        # Move to start of map\\n        self._file.seek(self._map[\\'map\\'])\\n        bio = BytesIO()\\n        for val in self._map.values():\\n            bio.write(struct.pack(self._byteorder + \\'Q\\', val))\\n        bio.seek(0)\\n        self._file.write(self._tag(bio.read(), \\'map\\'))',\n 'def _update_strl_names(self):\\n        \"\"\"Update column names for conversion to strl if they might have been\\n        changed to comply with Stata naming rules\"\"\"\\n        # Update convert_strl if names changed\\n        for orig, new in self._converted_names.items():\\n            if orig in self._convert_strl:\\n                idx = self._convert_strl.index(orig)\\n                self._convert_strl[idx] = new',\n 'def _convert_strls(self, data):\\n        \"\"\"Convert columns to StrLs if either very large or in the\\n        convert_strl variable\"\"\"\\n        convert_cols = [\\n            col for i, col in enumerate(data)\\n            if self.typlist[i] == 32768 or col in self._convert_strl]\\n\\n        if convert_cols:\\n            ssw = StataStrLWriter(data, convert_cols)\\n            tab, new_data = ssw.generate_table()\\n            data = new_data\\n            self._strl_blob = ssw.generate_blob(tab)\\n        return data',\n 'def register(explicit=True):\\n    \"\"\"\\n    Register Pandas Formatters and Converters with matplotlib\\n\\n    This function modifies the global ``matplotlib.units.registry``\\n    dictionary. Pandas adds custom converters for\\n\\n    * pd.Timestamp\\n    * pd.Period\\n    * np.datetime64\\n    * datetime.datetime\\n    * datetime.date\\n    * datetime.time\\n\\n    See Also\\n    --------\\n    deregister_matplotlib_converter\\n    \"\"\"\\n    # Renamed in pandas.plotting.__init__\\n    global _WARN\\n\\n    if explicit:\\n        _WARN = False\\n\\n    pairs = get_pairs()\\n    for type_, cls in pairs:\\n        converter = cls()\\n        if type_ in units.registry:\\n            previous = units.registry[type_]\\n            _mpl_units[type_] = previous\\n        units.registry[type_] = converter',\n 'def deregister():\\n    \"\"\"\\n    Remove pandas\\' formatters and converters\\n\\n    Removes the custom converters added by :func:`register`. This\\n    attempts to set the state of the registry back to the state before\\n    pandas registered its own units. Converters for pandas\\' own types like\\n    Timestamp and Period are removed completely. Converters for types\\n    pandas overwrites, like ``datetime.datetime``, are restored to their\\n    original value.\\n\\n    See Also\\n    --------\\n    deregister_matplotlib_converters\\n    \"\"\"\\n    # Renamed in pandas.plotting.__init__\\n    for type_, cls in get_pairs():\\n        # We use type to catch our classes directly, no inheritance\\n        if type(units.registry.get(type_)) is cls:\\n            units.registry.pop(type_)\\n\\n    # restore the old keys\\n    for unit, formatter in _mpl_units.items():\\n        if type(formatter) not in {DatetimeConverter, PeriodConverter,\\n                                   TimeConverter}:\\n            # make it idempotent by excluding ours.\\n            units.registry[unit] = formatter',\n 'def _dt_to_float_ordinal(dt):\\n    \"\"\"\\n    Convert :mod:`datetime` to the Gregorian date as UTC float days,\\n    preserving hours, minutes, seconds and microseconds.  Return value\\n    is a :func:`float`.\\n    \"\"\"\\n    if (isinstance(dt, (np.ndarray, Index, ABCSeries)\\n                   ) and is_datetime64_ns_dtype(dt)):\\n        base = dates.epoch2num(dt.asi8 / 1.0E9)\\n    else:\\n        base = dates.date2num(dt)\\n    return base',\n 'def _get_default_annual_spacing(nyears):\\n    \"\"\"\\n    Returns a default spacing between consecutive ticks for annual data.\\n    \"\"\"\\n    if nyears < 11:\\n        (min_spacing, maj_spacing) = (1, 1)\\n    elif nyears < 20:\\n        (min_spacing, maj_spacing) = (1, 2)\\n    elif nyears < 50:\\n        (min_spacing, maj_spacing) = (1, 5)\\n    elif nyears < 100:\\n        (min_spacing, maj_spacing) = (5, 10)\\n    elif nyears < 200:\\n        (min_spacing, maj_spacing) = (5, 25)\\n    elif nyears < 600:\\n        (min_spacing, maj_spacing) = (10, 50)\\n    else:\\n        factor = nyears // 1000 + 1\\n        (min_spacing, maj_spacing) = (factor * 20, factor * 100)\\n    return (min_spacing, maj_spacing)',\n 'def period_break(dates, period):\\n    \"\"\"\\n    Returns the indices where the given period changes.\\n\\n    Parameters\\n    ----------\\n    dates : PeriodIndex\\n        Array of intervals to monitor.\\n    period : string\\n        Name of the period to monitor.\\n    \"\"\"\\n    current = getattr(dates, period)\\n    previous = getattr(dates - 1 * dates.freq, period)\\n    return np.nonzero(current - previous)[0]',\n 'def has_level_label(label_flags, vmin):\\n    \"\"\"\\n    Returns true if the ``label_flags`` indicate there is at least one label\\n    for this level.\\n\\n    if the minimum view limit is not an exact integer, then the first tick\\n    label won\\'t be shown, so we must adjust for that.\\n    \"\"\"\\n    if label_flags.size == 0 or (label_flags.size == 1 and\\n                                 label_flags[0] == 0 and\\n                                 vmin % 1 > 0.0):\\n        return False\\n    else:\\n        return True',\n 'def axisinfo(unit, axis):\\n        \"\"\"\\n        Return the :class:`~matplotlib.units.AxisInfo` for *unit*.\\n\\n        *unit* is a tzinfo instance or None.\\n        The *axis* argument is required but not used.\\n        \"\"\"\\n        tz = unit\\n\\n        majloc = PandasAutoDateLocator(tz=tz)\\n        majfmt = PandasAutoDateFormatter(majloc, tz=tz)\\n        datemin = pydt.date(2000, 1, 1)\\n        datemax = pydt.date(2010, 1, 1)\\n\\n        return units.AxisInfo(majloc=majloc, majfmt=majfmt, label=\\'\\',\\n                              default_limits=(datemin, datemax))',\n \"def get_locator(self, dmin, dmax):\\n        'Pick the best locator based on a distance.'\\n        _check_implicitly_registered()\\n        delta = relativedelta(dmax, dmin)\\n\\n        num_days = (delta.years * 12.0 + delta.months) * 31.0 + delta.days\\n        num_sec = (delta.hours * 60.0 + delta.minutes) * 60.0 + delta.seconds\\n        tot_sec = num_days * 86400. + num_sec\\n\\n        if abs(tot_sec) < self.minticks:\\n            self._freq = -1\\n            locator = MilliSecondLocator(self.tz)\\n            locator.set_axis(self.axis)\\n\\n            locator.set_view_interval(*self.axis.get_view_interval())\\n            locator.set_data_interval(*self.axis.get_data_interval())\\n            return locator\\n\\n        return dates.AutoDateLocator.get_locator(self, dmin, dmax)\",\n 'def autoscale(self):\\n        \"\"\"\\n        Set the view limits to include the data range.\\n        \"\"\"\\n        dmin, dmax = self.datalim_to_dt()\\n        if dmin > dmax:\\n            dmax, dmin = dmin, dmax\\n\\n        # We need to cap at the endpoints of valid datetime\\n\\n        # TODO(wesm): unused?\\n\\n        # delta = relativedelta(dmax, dmin)\\n        # try:\\n        #     start = dmin - delta\\n        # except ValueError:\\n        #     start = _from_ordinal(1.0)\\n\\n        # try:\\n        #     stop = dmax + delta\\n        # except ValueError:\\n        #     # The magic number!\\n        #     stop = _from_ordinal(3652059.9999999)\\n\\n        dmin, dmax = self.datalim_to_dt()\\n\\n        vmin = dates.date2num(dmin)\\n        vmax = dates.date2num(dmax)\\n\\n        return self.nonsingular(vmin, vmax)',\n 'def _get_default_locs(self, vmin, vmax):\\n        \"Returns the default locations of ticks.\"\\n\\n        if self.plot_obj.date_axis_info is None:\\n            self.plot_obj.date_axis_info = self.finder(vmin, vmax, self.freq)\\n\\n        locator = self.plot_obj.date_axis_info\\n\\n        if self.isminor:\\n            return np.compress(locator[\\'min\\'], locator[\\'val\\'])\\n        return np.compress(locator[\\'maj\\'], locator[\\'val\\'])',\n 'def autoscale(self):\\n        \"\"\"\\n        Sets the view limits to the nearest multiples of base that contain the\\n        data.\\n        \"\"\"\\n        # requires matplotlib >= 0.98.0\\n        (vmin, vmax) = self.axis.get_data_interval()\\n\\n        locs = self._get_default_locs(vmin, vmax)\\n        (vmin, vmax) = locs[[0, -1]]\\n        if vmin == vmax:\\n            vmin -= 1\\n            vmax += 1\\n        return nonsingular(vmin, vmax)',\n 'def _set_default_format(self, vmin, vmax):\\n        \"Returns the default ticks spacing.\"\\n\\n        if self.plot_obj.date_axis_info is None:\\n            self.plot_obj.date_axis_info = self.finder(vmin, vmax, self.freq)\\n        info = self.plot_obj.date_axis_info\\n\\n        if self.isminor:\\n            format = np.compress(info[\\'min\\'] & np.logical_not(info[\\'maj\\']),\\n                                 info)\\n        else:\\n            format = np.compress(info[\\'maj\\'], info)\\n        self.formatdict = {x: f for (x, _, _, f) in format}\\n        return self.formatdict',\n \"def set_locs(self, locs):\\n        'Sets the locations of the ticks'\\n        # don't actually use the locs. This is just needed to work with\\n        # matplotlib. Force to use vmin, vmax\\n        _check_implicitly_registered()\\n\\n        self.locs = locs\\n\\n        (vmin, vmax) = vi = tuple(self.axis.get_view_interval())\\n        if vi != self.plot_obj.view_interval:\\n            self.plot_obj.date_axis_info = None\\n        self.plot_obj.view_interval = vi\\n        if vmax < vmin:\\n            (vmin, vmax) = (vmax, vmin)\\n        self._set_default_format(vmin, vmax)\",\n 'def set_default_names(data):\\n    \"\"\"Sets index names to \\'index\\' for regular, or \\'level_x\\' for Multi\"\"\"\\n    if com._all_not_none(*data.index.names):\\n        nms = data.index.names\\n        if len(nms) == 1 and data.index.name == \\'index\\':\\n            warnings.warn(\"Index name of \\'index\\' is not round-trippable\")\\n        elif len(nms) > 1 and any(x.startswith(\\'level_\\') for x in nms):\\n            warnings.warn(\"Index names beginning with \\'level_\\' are not \"\\n                          \"round-trippable\")\\n        return data\\n\\n    data = data.copy()\\n    if data.index.nlevels > 1:\\n        names = [name if name is not None else \\'level_{}\\'.format(i)\\n                 for i, name in enumerate(data.index.names)]\\n        data.index.names = names\\n    else:\\n        data.index.name = data.index.name or \\'index\\'\\n    return data',\n 'def convert_json_field_to_pandas_type(field):\\n    \"\"\"\\n    Converts a JSON field descriptor into its corresponding NumPy / pandas type\\n\\n    Parameters\\n    ----------\\n    field\\n        A JSON field descriptor\\n\\n    Returns\\n    -------\\n    dtype\\n\\n    Raises\\n    -----\\n    ValueError\\n        If the type of the provided field is unknown or currently unsupported\\n\\n    Examples\\n    --------\\n    >>> convert_json_field_to_pandas_type({\\'name\\': \\'an_int\\',\\n                                           \\'type\\': \\'integer\\'})\\n    \\'int64\\'\\n    >>> convert_json_field_to_pandas_type({\\'name\\': \\'a_categorical\\',\\n                                           \\'type\\': \\'any\\',\\n                                           \\'contraints\\': {\\'enum\\': [\\n                                                          \\'a\\', \\'b\\', \\'c\\']},\\n                                           \\'ordered\\': True})\\n    \\'CategoricalDtype(categories=[\\'a\\', \\'b\\', \\'c\\'], ordered=True)\\'\\n    >>> convert_json_field_to_pandas_type({\\'name\\': \\'a_datetime\\',\\n                                           \\'type\\': \\'datetime\\'})\\n    \\'datetime64[ns]\\'\\n    >>> convert_json_field_to_pandas_type({\\'name\\': \\'a_datetime_with_tz\\',\\n                                           \\'type\\': \\'datetime\\',\\n                                           \\'tz\\': \\'US/Central\\'})\\n    \\'datetime64[ns, US/Central]\\'\\n    \"\"\"\\n    typ = field[\\'type\\']\\n    if typ == \\'string\\':\\n        return \\'object\\'\\n    elif typ == \\'integer\\':\\n        return \\'int64\\'\\n    elif typ == \\'number\\':\\n        return \\'float64\\'\\n    elif typ == \\'boolean\\':\\n        return \\'bool\\'\\n    elif typ == \\'duration\\':\\n        return \\'timedelta64\\'\\n    elif typ == \\'datetime\\':\\n        if field.get(\\'tz\\'):\\n            return \\'datetime64[ns, {tz}]\\'.format(tz=field[\\'tz\\'])\\n        else:\\n            return \\'datetime64[ns]\\'\\n    elif typ == \\'any\\':\\n        if \\'constraints\\' in field and \\'ordered\\' in field:\\n            return CategoricalDtype(categories=field[\\'constraints\\'][\\'enum\\'],\\n                                    ordered=field[\\'ordered\\'])\\n        else:\\n            return \\'object\\'\\n\\n    raise ValueError(\"Unsupported or invalid field type: {}\".format(typ))',\n 'def build_table_schema(data, index=True, primary_key=None, version=True):\\n    \"\"\"\\n    Create a Table schema from ``data``.\\n\\n    Parameters\\n    ----------\\n    data : Series, DataFrame\\n    index : bool, default True\\n        Whether to include ``data.index`` in the schema.\\n    primary_key : bool or None, default True\\n        column names to designate as the primary key.\\n        The default `None` will set `\\'primaryKey\\'` to the index\\n        level or levels if the index is unique.\\n    version : bool, default True\\n        Whether to include a field `pandas_version` with the version\\n        of pandas that generated the schema.\\n\\n    Returns\\n    -------\\n    schema : dict\\n\\n    Notes\\n    -----\\n    See `_as_json_table_type` for conversion types.\\n    Timedeltas as converted to ISO8601 duration format with\\n    9 decimal places after the seconds field for nanosecond precision.\\n\\n    Categoricals are converted to the `any` dtype, and use the `enum` field\\n    constraint to list the allowed values. The `ordered` attribute is included\\n    in an `ordered` field.\\n\\n    Examples\\n    --------\\n    >>> df = pd.DataFrame(\\n    ...     {\\'A\\': [1, 2, 3],\\n    ...      \\'B\\': [\\'a\\', \\'b\\', \\'c\\'],\\n    ...      \\'C\\': pd.date_range(\\'2016-01-01\\', freq=\\'d\\', periods=3),\\n    ...     }, index=pd.Index(range(3), name=\\'idx\\'))\\n    >>> build_table_schema(df)\\n    {\\'fields\\': [{\\'name\\': \\'idx\\', \\'type\\': \\'integer\\'},\\n    {\\'name\\': \\'A\\', \\'type\\': \\'integer\\'},\\n    {\\'name\\': \\'B\\', \\'type\\': \\'string\\'},\\n    {\\'name\\': \\'C\\', \\'type\\': \\'datetime\\'}],\\n    \\'pandas_version\\': \\'0.20.0\\',\\n    \\'primaryKey\\': [\\'idx\\']}\\n    \"\"\"\\n    if index is True:\\n        data = set_default_names(data)\\n\\n    schema = {}\\n    fields = []\\n\\n    if index:\\n        if data.index.nlevels > 1:\\n            for level in data.index.levels:\\n                fields.append(convert_pandas_type_to_json_field(level))\\n        else:\\n            fields.append(convert_pandas_type_to_json_field(data.index))\\n\\n    if data.ndim > 1:\\n        for column, s in data.iteritems():\\n            fields.append(convert_pandas_type_to_json_field(s))\\n    else:\\n        fields.append(convert_pandas_type_to_json_field(data))\\n\\n    schema[\\'fields\\'] = fields\\n    if index and data.index.is_unique and primary_key is None:\\n        if data.index.nlevels == 1:\\n            schema[\\'primaryKey\\'] = [data.index.name]\\n        else:\\n            schema[\\'primaryKey\\'] = data.index.names\\n    elif primary_key is not None:\\n        schema[\\'primaryKey\\'] = primary_key\\n\\n    if version:\\n        schema[\\'pandas_version\\'] = \\'0.20.0\\'\\n    return schema',\n 'def parse_table_schema(json, precise_float):\\n    \"\"\"\\n    Builds a DataFrame from a given schema\\n\\n    Parameters\\n    ----------\\n    json :\\n        A JSON table schema\\n    precise_float : boolean\\n        Flag controlling precision when decoding string to double values, as\\n        dictated by ``read_json``\\n\\n    Returns\\n    -------\\n    df : DataFrame\\n\\n    Raises\\n    ------\\n    NotImplementedError\\n        If the JSON table schema contains either timezone or timedelta data\\n\\n    Notes\\n    -----\\n        Because :func:`DataFrame.to_json` uses the string \\'index\\' to denote a\\n        name-less :class:`Index`, this function sets the name of the returned\\n        :class:`DataFrame` to ``None`` when said string is encountered with a\\n        normal :class:`Index`. For a :class:`MultiIndex`, the same limitation\\n        applies to any strings beginning with \\'level_\\'. Therefore, an\\n        :class:`Index` name of \\'index\\'  and :class:`MultiIndex` names starting\\n        with \\'level_\\' are not supported.\\n\\n    See Also\\n    --------\\n    build_table_schema : Inverse function.\\n    pandas.read_json\\n    \"\"\"\\n    table = loads(json, precise_float=precise_float)\\n    col_order = [field[\\'name\\'] for field in table[\\'schema\\'][\\'fields\\']]\\n    df = DataFrame(table[\\'data\\'], columns=col_order)[col_order]\\n\\n    dtypes = {field[\\'name\\']: convert_json_field_to_pandas_type(field)\\n              for field in table[\\'schema\\'][\\'fields\\']}\\n\\n    # Cannot directly use as_type with timezone data on object; raise for now\\n    if any(str(x).startswith(\\'datetime64[ns, \\') for x in dtypes.values()):\\n        raise NotImplementedError(\\'table=\"orient\" can not yet read timezone \\'\\n                                  \\'data\\')\\n\\n    # No ISO constructor for Timedelta as of yet, so need to raise\\n    if \\'timedelta64\\' in dtypes.values():\\n        raise NotImplementedError(\\'table=\"orient\" can not yet read \\'\\n                                  \\'ISO-formatted Timedelta data\\')\\n\\n    df = df.astype(dtypes)\\n\\n    if \\'primaryKey\\' in table[\\'schema\\']:\\n        df = df.set_index(table[\\'schema\\'][\\'primaryKey\\'])\\n        if len(df.index.names) == 1:\\n            if df.index.name == \\'index\\':\\n                df.index.name = None\\n        else:\\n            df.index.names = [None if x.startswith(\\'level_\\') else x for x in\\n                              df.index.names]\\n\\n    return df',\n 'def get_op_result_name(left, right):\\n    \"\"\"\\n    Find the appropriate name to pin to an operation result.  This result\\n    should always be either an Index or a Series.\\n\\n    Parameters\\n    ----------\\n    left : {Series, Index}\\n    right : object\\n\\n    Returns\\n    -------\\n    name : object\\n        Usually a string\\n    \"\"\"\\n    # `left` is always a pd.Series when called from within ops\\n    if isinstance(right, (ABCSeries, pd.Index)):\\n        name = _maybe_match_name(left, right)\\n    else:\\n        name = left.name\\n    return name',\n 'def _maybe_match_name(a, b):\\n    \"\"\"\\n    Try to find a name to attach to the result of an operation between\\n    a and b.  If only one of these has a `name` attribute, return that\\n    name.  Otherwise return a consensus name if they match of None if\\n    they have different names.\\n\\n    Parameters\\n    ----------\\n    a : object\\n    b : object\\n\\n    Returns\\n    -------\\n    name : str or None\\n\\n    See Also\\n    --------\\n    pandas.core.common.consensus_name_attr\\n    \"\"\"\\n    a_has = hasattr(a, \\'name\\')\\n    b_has = hasattr(b, \\'name\\')\\n    if a_has and b_has:\\n        if a.name == b.name:\\n            return a.name\\n        else:\\n            # TODO: what if they both have np.nan for their names?\\n            return None\\n    elif a_has:\\n        return a.name\\n    elif b_has:\\n        return b.name\\n    return None',\n 'def maybe_upcast_for_op(obj):\\n    \"\"\"\\n    Cast non-pandas objects to pandas types to unify behavior of arithmetic\\n    and comparison operations.\\n\\n    Parameters\\n    ----------\\n    obj: object\\n\\n    Returns\\n    -------\\n    out : object\\n\\n    Notes\\n    -----\\n    Be careful to call this *after* determining the `name` attribute to be\\n    attached to the result of the arithmetic operation.\\n    \"\"\"\\n    if type(obj) is datetime.timedelta:\\n        # GH#22390  cast up to Timedelta to rely on Timedelta\\n        # implementation; otherwise operation against numeric-dtype\\n        # raises TypeError\\n        return pd.Timedelta(obj)\\n    elif isinstance(obj, np.timedelta64) and not isna(obj):\\n        # In particular non-nanosecond timedelta64 needs to be cast to\\n        #  nanoseconds, or else we get undesired behavior like\\n        #  np.timedelta64(3, \\'D\\') / 2 == np.timedelta64(1, \\'D\\')\\n        # The isna check is to avoid casting timedelta64(\"NaT\"), which would\\n        #  return NaT and incorrectly be treated as a datetime-NaT.\\n        return pd.Timedelta(obj)\\n    elif isinstance(obj, np.ndarray) and is_timedelta64_dtype(obj):\\n        # GH#22390 Unfortunately we need to special-case right-hand\\n        # timedelta64 dtypes because numpy casts integer dtypes to\\n        # timedelta64 when operating with timedelta64\\n        return pd.TimedeltaIndex(obj)\\n    return obj',\n 'def make_invalid_op(name):\\n    \"\"\"\\n    Return a binary method that always raises a TypeError.\\n\\n    Parameters\\n    ----------\\n    name : str\\n\\n    Returns\\n    -------\\n    invalid_op : function\\n    \"\"\"\\n    def invalid_op(self, other=None):\\n        raise TypeError(\"cannot perform {name} with this index type: \"\\n                        \"{typ}\".format(name=name, typ=type(self).__name__))\\n\\n    invalid_op.__name__ = name\\n    return invalid_op',\n 'def _gen_eval_kwargs(name):\\n    \"\"\"\\n    Find the keyword arguments to pass to numexpr for the given operation.\\n\\n    Parameters\\n    ----------\\n    name : str\\n\\n    Returns\\n    -------\\n    eval_kwargs : dict\\n\\n    Examples\\n    --------\\n    >>> _gen_eval_kwargs(\"__add__\")\\n    {}\\n\\n    >>> _gen_eval_kwargs(\"rtruediv\")\\n    {\\'reversed\\': True, \\'truediv\\': True}\\n    \"\"\"\\n    kwargs = {}\\n\\n    # Series and Panel appear to only pass __add__, __radd__, ...\\n    # but DataFrame gets both these dunder names _and_ non-dunder names\\n    # add, radd, ...\\n    name = name.replace(\\'__\\', \\'\\')\\n\\n    if name.startswith(\\'r\\'):\\n        if name not in [\\'radd\\', \\'rand\\', \\'ror\\', \\'rxor\\']:\\n            # Exclude commutative operations\\n            kwargs[\\'reversed\\'] = True\\n\\n    if name in [\\'truediv\\', \\'rtruediv\\']:\\n        kwargs[\\'truediv\\'] = True\\n\\n    if name in [\\'ne\\']:\\n        kwargs[\\'masker\\'] = True\\n\\n    return kwargs',\n 'def _gen_fill_zeros(name):\\n    \"\"\"\\n    Find the appropriate fill value to use when filling in undefined values\\n    in the results of the given operation caused by operating on\\n    (generally dividing by) zero.\\n\\n    Parameters\\n    ----------\\n    name : str\\n\\n    Returns\\n    -------\\n    fill_value : {None, np.nan, np.inf}\\n    \"\"\"\\n    name = name.strip(\\'__\\')\\n    if \\'div\\' in name:\\n        # truediv, floordiv, div, and reversed variants\\n        fill_value = np.inf\\n    elif \\'mod\\' in name:\\n        # mod, rmod\\n        fill_value = np.nan\\n    else:\\n        fill_value = None\\n    return fill_value',\n 'def _get_opstr(op, cls):\\n    \"\"\"\\n    Find the operation string, if any, to pass to numexpr for this\\n    operation.\\n\\n    Parameters\\n    ----------\\n    op : binary operator\\n    cls : class\\n\\n    Returns\\n    -------\\n    op_str : string or None\\n    \"\"\"\\n    # numexpr is available for non-sparse classes\\n    subtyp = getattr(cls, \\'_subtyp\\', \\'\\')\\n    use_numexpr = \\'sparse\\' not in subtyp\\n\\n    if not use_numexpr:\\n        # if we\\'re not using numexpr, then don\\'t pass a str_rep\\n        return None\\n\\n    return {operator.add: \\'+\\',\\n            radd: \\'+\\',\\n            operator.mul: \\'*\\',\\n            rmul: \\'*\\',\\n            operator.sub: \\'-\\',\\n            rsub: \\'-\\',\\n            operator.truediv: \\'/\\',\\n            rtruediv: \\'/\\',\\n            operator.floordiv: \\'//\\',\\n            rfloordiv: \\'//\\',\\n            operator.mod: None,  # TODO: Why None for mod but \\'%\\' for rmod?\\n            rmod: \\'%\\',\\n            operator.pow: \\'**\\',\\n            rpow: \\'**\\',\\n            operator.eq: \\'==\\',\\n            operator.ne: \\'!=\\',\\n            operator.le: \\'<=\\',\\n            operator.lt: \\'<\\',\\n            operator.ge: \\'>=\\',\\n            operator.gt: \\'>\\',\\n            operator.and_: \\'&\\',\\n            rand_: \\'&\\',\\n            operator.or_: \\'|\\',\\n            ror_: \\'|\\',\\n            operator.xor: \\'^\\',\\n            rxor: \\'^\\',\\n            divmod: None,\\n            rdivmod: None}[op]',\n 'def _get_op_name(op, special):\\n    \"\"\"\\n    Find the name to attach to this method according to conventions\\n    for special and non-special methods.\\n\\n    Parameters\\n    ----------\\n    op : binary operator\\n    special : bool\\n\\n    Returns\\n    -------\\n    op_name : str\\n    \"\"\"\\n    opname = op.__name__.strip(\\'_\\')\\n    if special:\\n        opname = \\'__{opname}__\\'.format(opname=opname)\\n    return opname',\n 'def _make_flex_doc(op_name, typ):\\n    \"\"\"\\n    Make the appropriate substitutions for the given operation and class-typ\\n    into either _flex_doc_SERIES or _flex_doc_FRAME to return the docstring\\n    to attach to a generated method.\\n\\n    Parameters\\n    ----------\\n    op_name : str {\\'__add__\\', \\'__sub__\\', ... \\'__eq__\\', \\'__ne__\\', ...}\\n    typ : str {series, \\'dataframe\\']}\\n\\n    Returns\\n    -------\\n    doc : str\\n    \"\"\"\\n    op_name = op_name.replace(\\'__\\', \\'\\')\\n    op_desc = _op_descriptions[op_name]\\n\\n    if op_desc[\\'reversed\\']:\\n        equiv = \\'other \\' + op_desc[\\'op\\'] + \\' \\' + typ\\n    else:\\n        equiv = typ + \\' \\' + op_desc[\\'op\\'] + \\' other\\'\\n\\n    if typ == \\'series\\':\\n        base_doc = _flex_doc_SERIES\\n        doc_no_examples = base_doc.format(\\n            desc=op_desc[\\'desc\\'],\\n            op_name=op_name,\\n            equiv=equiv,\\n            reverse=op_desc[\\'reverse\\']\\n        )\\n        if op_desc[\\'series_examples\\']:\\n            doc = doc_no_examples + op_desc[\\'series_examples\\']\\n        else:\\n            doc = doc_no_examples\\n    elif typ == \\'dataframe\\':\\n        base_doc = _flex_doc_FRAME\\n        doc = base_doc.format(\\n            desc=op_desc[\\'desc\\'],\\n            op_name=op_name,\\n            equiv=equiv,\\n            reverse=op_desc[\\'reverse\\']\\n        )\\n    elif typ == \\'panel\\':\\n        base_doc = _flex_doc_PANEL\\n        doc = base_doc.format(\\n            desc=op_desc[\\'desc\\'],\\n            op_name=op_name,\\n            equiv=equiv,\\n            reverse=op_desc[\\'reverse\\']\\n        )\\n    else:\\n        raise AssertionError(\\'Invalid typ argument.\\')\\n    return doc',\n 'def fill_binop(left, right, fill_value):\\n    \"\"\"\\n    If a non-None fill_value is given, replace null entries in left and right\\n    with this value, but only in positions where _one_ of left/right is null,\\n    not both.\\n\\n    Parameters\\n    ----------\\n    left : array-like\\n    right : array-like\\n    fill_value : object\\n\\n    Returns\\n    -------\\n    left : array-like\\n    right : array-like\\n\\n    Notes\\n    -----\\n    Makes copies if fill_value is not None\\n    \"\"\"\\n    # TODO: can we make a no-copy implementation?\\n    if fill_value is not None:\\n        left_mask = isna(left)\\n        right_mask = isna(right)\\n        left = left.copy()\\n        right = right.copy()\\n\\n        # one but not both\\n        mask = left_mask ^ right_mask\\n        left[left_mask & mask] = fill_value\\n        right[right_mask & mask] = fill_value\\n    return left, right',\n 'def mask_cmp_op(x, y, op, allowed_types):\\n    \"\"\"\\n    Apply the function `op` to only non-null points in x and y.\\n\\n    Parameters\\n    ----------\\n    x : array-like\\n    y : array-like\\n    op : binary operation\\n    allowed_types : class or tuple of classes\\n\\n    Returns\\n    -------\\n    result : ndarray[bool]\\n    \"\"\"\\n    # TODO: Can we make the allowed_types arg unnecessary?\\n    xrav = x.ravel()\\n    result = np.empty(x.size, dtype=bool)\\n    if isinstance(y, allowed_types):\\n        yrav = y.ravel()\\n        mask = notna(xrav) & notna(yrav)\\n        result[mask] = op(np.array(list(xrav[mask])),\\n                          np.array(list(yrav[mask])))\\n    else:\\n        mask = notna(xrav)\\n        result[mask] = op(np.array(list(xrav[mask])), y)\\n\\n    if op == operator.ne:  # pragma: no cover\\n        np.putmask(result, ~mask, True)\\n    else:\\n        np.putmask(result, ~mask, False)\\n    result = result.reshape(x.shape)\\n    return result',\n 'def masked_arith_op(x, y, op):\\n    \"\"\"\\n    If the given arithmetic operation fails, attempt it again on\\n    only the non-null elements of the input array(s).\\n\\n    Parameters\\n    ----------\\n    x : np.ndarray\\n    y : np.ndarray, Series, Index\\n    op : binary operator\\n    \"\"\"\\n    # For Series `x` is 1D so ravel() is a no-op; calling it anyway makes\\n    # the logic valid for both Series and DataFrame ops.\\n    xrav = x.ravel()\\n    assert isinstance(x, (np.ndarray, ABCSeries)), type(x)\\n    if isinstance(y, (np.ndarray, ABCSeries, ABCIndexClass)):\\n        dtype = find_common_type([x.dtype, y.dtype])\\n        result = np.empty(x.size, dtype=dtype)\\n\\n        # PeriodIndex.ravel() returns int64 dtype, so we have\\n        # to work around that case.  See GH#19956\\n        yrav = y if is_period_dtype(y) else y.ravel()\\n        mask = notna(xrav) & notna(yrav)\\n\\n        if yrav.shape != mask.shape:\\n            # FIXME: GH#5284, GH#5035, GH#19448\\n            # Without specifically raising here we get mismatched\\n            # errors in Py3 (TypeError) vs Py2 (ValueError)\\n            # Note: Only = an issue in DataFrame case\\n            raise ValueError(\\'Cannot broadcast operands together.\\')\\n\\n        if mask.any():\\n            with np.errstate(all=\\'ignore\\'):\\n                result[mask] = op(xrav[mask],\\n                                  com.values_from_object(yrav[mask]))\\n\\n    else:\\n        assert is_scalar(y), type(y)\\n        assert isinstance(x, np.ndarray), type(x)\\n        # mask is only meaningful for x\\n        result = np.empty(x.size, dtype=x.dtype)\\n        mask = notna(xrav)\\n\\n        # 1 ** np.nan is 1. So we have to unmask those.\\n        if op == pow:\\n            mask = np.where(x == 1, False, mask)\\n        elif op == rpow:\\n            mask = np.where(y == 1, False, mask)\\n\\n        if mask.any():\\n            with np.errstate(all=\\'ignore\\'):\\n                result[mask] = op(xrav[mask], y)\\n\\n    result, changed = maybe_upcast_putmask(result, ~mask, np.nan)\\n    result = result.reshape(x.shape)  # 2D compat\\n    return result',\n 'def invalid_comparison(left, right, op):\\n    \"\"\"\\n    If a comparison has mismatched types and is not necessarily meaningful,\\n    follow python3 conventions by:\\n\\n        - returning all-False for equality\\n        - returning all-True for inequality\\n        - raising TypeError otherwise\\n\\n    Parameters\\n    ----------\\n    left : array-like\\n    right : scalar, array-like\\n    op : operator.{eq, ne, lt, le, gt}\\n\\n    Raises\\n    ------\\n    TypeError : on inequality comparisons\\n    \"\"\"\\n    if op is operator.eq:\\n        res_values = np.zeros(left.shape, dtype=bool)\\n    elif op is operator.ne:\\n        res_values = np.ones(left.shape, dtype=bool)\\n    else:\\n        raise TypeError(\"Invalid comparison between dtype={dtype} and {typ}\"\\n                        .format(dtype=left.dtype, typ=type(right).__name__))\\n    return res_values',\n 'def should_series_dispatch(left, right, op):\\n    \"\"\"\\n    Identify cases where a DataFrame operation should dispatch to its\\n    Series counterpart.\\n\\n    Parameters\\n    ----------\\n    left : DataFrame\\n    right : DataFrame\\n    op : binary operator\\n\\n    Returns\\n    -------\\n    override : bool\\n    \"\"\"\\n    if left._is_mixed_type or right._is_mixed_type:\\n        return True\\n\\n    if not len(left.columns) or not len(right.columns):\\n        # ensure obj.dtypes[0] exists for each obj\\n        return False\\n\\n    ldtype = left.dtypes.iloc[0]\\n    rdtype = right.dtypes.iloc[0]\\n\\n    if ((is_timedelta64_dtype(ldtype) and is_integer_dtype(rdtype)) or\\n            (is_timedelta64_dtype(rdtype) and is_integer_dtype(ldtype))):\\n        # numpy integer dtypes as timedelta64 dtypes in this scenario\\n        return True\\n\\n    if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):\\n        # in particular case where right is an array of DateOffsets\\n        return True\\n\\n    return False',\n 'def dispatch_to_series(left, right, func, str_rep=None, axis=None):\\n    \"\"\"\\n    Evaluate the frame operation func(left, right) by evaluating\\n    column-by-column, dispatching to the Series implementation.\\n\\n    Parameters\\n    ----------\\n    left : DataFrame\\n    right : scalar or DataFrame\\n    func : arithmetic or comparison operator\\n    str_rep : str or None, default None\\n    axis : {None, 0, 1, \"index\", \"columns\"}\\n\\n    Returns\\n    -------\\n    DataFrame\\n    \"\"\"\\n    # Note: we use iloc to access columns for compat with cases\\n    #       with non-unique columns.\\n    import pandas.core.computation.expressions as expressions\\n\\n    right = lib.item_from_zerodim(right)\\n    if lib.is_scalar(right) or np.ndim(right) == 0:\\n\\n        def column_op(a, b):\\n            return {i: func(a.iloc[:, i], b)\\n                    for i in range(len(a.columns))}\\n\\n    elif isinstance(right, ABCDataFrame):\\n        assert right._indexed_same(left)\\n\\n        def column_op(a, b):\\n            return {i: func(a.iloc[:, i], b.iloc[:, i])\\n                    for i in range(len(a.columns))}\\n\\n    elif isinstance(right, ABCSeries) and axis == \"columns\":\\n        # We only get here if called via left._combine_match_columns,\\n        # in which case we specifically want to operate row-by-row\\n        assert right.index.equals(left.columns)\\n\\n        def column_op(a, b):\\n            return {i: func(a.iloc[:, i], b.iloc[i])\\n                    for i in range(len(a.columns))}\\n\\n    elif isinstance(right, ABCSeries):\\n        assert right.index.equals(left.index)  # Handle other cases later\\n\\n        def column_op(a, b):\\n            return {i: func(a.iloc[:, i], b)\\n                    for i in range(len(a.columns))}\\n\\n    else:\\n        # Remaining cases have less-obvious dispatch rules\\n        raise NotImplementedError(right)\\n\\n    new_data = expressions.evaluate(column_op, str_rep, left, right)\\n\\n    result = left._constructor(new_data, index=left.index, copy=False)\\n    # Pin columns instead of passing to constructor for compat with\\n    # non-unique columns case\\n    result.columns = left.columns\\n    return result',\n 'def dispatch_to_index_op(op, left, right, index_class):\\n    \"\"\"\\n    Wrap Series left in the given index_class to delegate the operation op\\n    to the index implementation.  DatetimeIndex and TimedeltaIndex perform\\n    type checking, timezone handling, overflow checks, etc.\\n\\n    Parameters\\n    ----------\\n    op : binary operator (operator.add, operator.sub, ...)\\n    left : Series\\n    right : object\\n    index_class : DatetimeIndex or TimedeltaIndex\\n\\n    Returns\\n    -------\\n    result : object, usually DatetimeIndex, TimedeltaIndex, or Series\\n    \"\"\"\\n    left_idx = index_class(left)\\n\\n    # avoid accidentally allowing integer add/sub.  For datetime64[tz] dtypes,\\n    # left_idx may inherit a freq from a cached DatetimeIndex.\\n    # See discussion in GH#19147.\\n    if getattr(left_idx, \\'freq\\', None) is not None:\\n        left_idx = left_idx._shallow_copy(freq=None)\\n    try:\\n        result = op(left_idx, right)\\n    except NullFrequencyError:\\n        # DatetimeIndex and TimedeltaIndex with freq == None raise ValueError\\n        # on add/sub of integers (or int-like).  We re-raise as a TypeError.\\n        raise TypeError(\\'incompatible type for a datetime/timedelta \\'\\n                        \\'operation [{name}]\\'.format(name=op.__name__))\\n    return result',\n 'def dispatch_to_extension_op(op, left, right):\\n    \"\"\"\\n    Assume that left or right is a Series backed by an ExtensionArray,\\n    apply the operator defined by op.\\n    \"\"\"\\n\\n    # The op calls will raise TypeError if the op is not defined\\n    # on the ExtensionArray\\n\\n    # unbox Series and Index to arrays\\n    if isinstance(left, (ABCSeries, ABCIndexClass)):\\n        new_left = left._values\\n    else:\\n        new_left = left\\n\\n    if isinstance(right, (ABCSeries, ABCIndexClass)):\\n        new_right = right._values\\n    else:\\n        new_right = right\\n\\n    res_values = op(new_left, new_right)\\n    res_name = get_op_result_name(left, right)\\n\\n    if op.__name__ in [\\'divmod\\', \\'rdivmod\\']:\\n        return _construct_divmod_result(\\n            left, res_values, left.index, res_name)\\n\\n    return _construct_result(left, res_values, left.index, res_name)',\n 'def _get_method_wrappers(cls):\\n    \"\"\"\\n    Find the appropriate operation-wrappers to use when defining flex/special\\n    arithmetic, boolean, and comparison operations with the given class.\\n\\n    Parameters\\n    ----------\\n    cls : class\\n\\n    Returns\\n    -------\\n    arith_flex : function or None\\n    comp_flex : function or None\\n    arith_special : function\\n    comp_special : function\\n    bool_special : function\\n\\n    Notes\\n    -----\\n    None is only returned for SparseArray\\n    \"\"\"\\n    if issubclass(cls, ABCSparseSeries):\\n        # Be sure to catch this before ABCSeries and ABCSparseArray,\\n        # as they will both come see SparseSeries as a subclass\\n        arith_flex = _flex_method_SERIES\\n        comp_flex = _flex_method_SERIES\\n        arith_special = _arith_method_SPARSE_SERIES\\n        comp_special = _arith_method_SPARSE_SERIES\\n        bool_special = _bool_method_SERIES\\n        # TODO: I don\\'t think the functions defined by bool_method are tested\\n    elif issubclass(cls, ABCSeries):\\n        # Just Series; SparseSeries is caught above\\n        arith_flex = _flex_method_SERIES\\n        comp_flex = _flex_method_SERIES\\n        arith_special = _arith_method_SERIES\\n        comp_special = _comp_method_SERIES\\n        bool_special = _bool_method_SERIES\\n    elif issubclass(cls, ABCSparseArray):\\n        arith_flex = None\\n        comp_flex = None\\n        arith_special = _arith_method_SPARSE_ARRAY\\n        comp_special = _arith_method_SPARSE_ARRAY\\n        bool_special = _arith_method_SPARSE_ARRAY\\n    elif issubclass(cls, ABCPanel):\\n        arith_flex = _flex_method_PANEL\\n        comp_flex = _comp_method_PANEL\\n        arith_special = _arith_method_PANEL\\n        comp_special = _comp_method_PANEL\\n        bool_special = _arith_method_PANEL\\n    elif issubclass(cls, ABCDataFrame):\\n        # Same for DataFrame and SparseDataFrame\\n        arith_flex = _arith_method_FRAME\\n        comp_flex = _flex_comp_method_FRAME\\n        arith_special = _arith_method_FRAME\\n        comp_special = _comp_method_FRAME\\n        bool_special = _arith_method_FRAME\\n    return arith_flex, comp_flex, arith_special, comp_special, bool_special',\n 'def add_special_arithmetic_methods(cls):\\n    \"\"\"\\n    Adds the full suite of special arithmetic methods (``__add__``,\\n    ``__sub__``, etc.) to the class.\\n\\n    Parameters\\n    ----------\\n    cls : class\\n        special methods will be defined and pinned to this class\\n    \"\"\"\\n    _, _, arith_method, comp_method, bool_method = _get_method_wrappers(cls)\\n    new_methods = _create_methods(cls, arith_method, comp_method, bool_method,\\n                                  special=True)\\n    # inplace operators (I feel like these should get passed an `inplace=True`\\n    # or just be removed\\n\\n    def _wrap_inplace_method(method):\\n        \"\"\"\\n        return an inplace wrapper for this method\\n        \"\"\"\\n\\n        def f(self, other):\\n            result = method(self, other)\\n\\n            # this makes sure that we are aligned like the input\\n            # we are updating inplace so we want to ignore is_copy\\n            self._update_inplace(result.reindex_like(self, copy=False)._data,\\n                                 verify_is_copy=False)\\n\\n            return self\\n\\n        f.__name__ = \"__i{name}__\".format(name=method.__name__.strip(\"__\"))\\n        return f\\n\\n    new_methods.update(\\n        dict(__iadd__=_wrap_inplace_method(new_methods[\"__add__\"]),\\n             __isub__=_wrap_inplace_method(new_methods[\"__sub__\"]),\\n             __imul__=_wrap_inplace_method(new_methods[\"__mul__\"]),\\n             __itruediv__=_wrap_inplace_method(new_methods[\"__truediv__\"]),\\n             __ifloordiv__=_wrap_inplace_method(new_methods[\"__floordiv__\"]),\\n             __imod__=_wrap_inplace_method(new_methods[\"__mod__\"]),\\n             __ipow__=_wrap_inplace_method(new_methods[\"__pow__\"])))\\n\\n    new_methods.update(\\n        dict(__iand__=_wrap_inplace_method(new_methods[\"__and__\"]),\\n             __ior__=_wrap_inplace_method(new_methods[\"__or__\"]),\\n             __ixor__=_wrap_inplace_method(new_methods[\"__xor__\"])))\\n\\n    add_methods(cls, new_methods=new_methods)',\n 'def add_flex_arithmetic_methods(cls):\\n    \"\"\"\\n    Adds the full suite of flex arithmetic methods (``pow``, ``mul``, ``add``)\\n    to the class.\\n\\n    Parameters\\n    ----------\\n    cls : class\\n        flex methods will be defined and pinned to this class\\n    \"\"\"\\n    flex_arith_method, flex_comp_method, _, _, _ = _get_method_wrappers(cls)\\n    new_methods = _create_methods(cls, flex_arith_method,\\n                                  flex_comp_method, bool_method=None,\\n                                  special=False)\\n    new_methods.update(dict(multiply=new_methods[\\'mul\\'],\\n                            subtract=new_methods[\\'sub\\'],\\n                            divide=new_methods[\\'div\\']))\\n    # opt out of bool flex methods for now\\n    assert not any(kname in new_methods for kname in (\\'ror_\\', \\'rxor\\', \\'rand_\\'))\\n\\n    add_methods(cls, new_methods=new_methods)',\n 'def _align_method_SERIES(left, right, align_asobject=False):\\n    \"\"\" align lhs and rhs Series \"\"\"\\n\\n    # ToDo: Different from _align_method_FRAME, list, tuple and ndarray\\n    # are not coerced here\\n    # because Series has inconsistencies described in #13637\\n\\n    if isinstance(right, ABCSeries):\\n        # avoid repeated alignment\\n        if not left.index.equals(right.index):\\n\\n            if align_asobject:\\n                # to keep original value\\'s dtype for bool ops\\n                left = left.astype(object)\\n                right = right.astype(object)\\n\\n            left, right = left.align(right, copy=False)\\n\\n    return left, right',\n 'def _construct_result(left, result, index, name, dtype=None):\\n    \"\"\"\\n    If the raw op result has a non-None name (e.g. it is an Index object) and\\n    the name argument is None, then passing name to the constructor will\\n    not be enough; we still need to override the name attribute.\\n    \"\"\"\\n    out = left._constructor(result, index=index, dtype=dtype)\\n    out = out.__finalize__(left)\\n    out.name = name\\n    return out',\n 'def _construct_divmod_result(left, result, index, name, dtype=None):\\n    \"\"\"divmod returns a tuple of like indexed series instead of a single series.\\n    \"\"\"\\n    return (\\n        _construct_result(left, result[0], index=index, name=name,\\n                          dtype=dtype),\\n        _construct_result(left, result[1], index=index, name=name,\\n                          dtype=dtype),\\n    )',\n 'def _arith_method_SERIES(cls, op, special):\\n    \"\"\"\\n    Wrapper function for Series arithmetic operations, to avoid\\n    code duplication.\\n    \"\"\"\\n    str_rep = _get_opstr(op, cls)\\n    op_name = _get_op_name(op, special)\\n    eval_kwargs = _gen_eval_kwargs(op_name)\\n    fill_zeros = _gen_fill_zeros(op_name)\\n    construct_result = (_construct_divmod_result\\n                        if op in [divmod, rdivmod] else _construct_result)\\n\\n    def na_op(x, y):\\n        import pandas.core.computation.expressions as expressions\\n        try:\\n            result = expressions.evaluate(op, str_rep, x, y, **eval_kwargs)\\n        except TypeError:\\n            result = masked_arith_op(x, y, op)\\n\\n        result = missing.fill_zeros(result, x, y, op_name, fill_zeros)\\n        return result\\n\\n    def safe_na_op(lvalues, rvalues):\\n        \"\"\"\\n        return the result of evaluating na_op on the passed in values\\n\\n        try coercion to object type if the native types are not compatible\\n\\n        Parameters\\n        ----------\\n        lvalues : array-like\\n        rvalues : array-like\\n\\n        Raises\\n        ------\\n        TypeError: invalid operation\\n        \"\"\"\\n        try:\\n            with np.errstate(all=\\'ignore\\'):\\n                return na_op(lvalues, rvalues)\\n        except Exception:\\n            if is_object_dtype(lvalues):\\n                return libalgos.arrmap_object(lvalues,\\n                                              lambda x: op(x, rvalues))\\n            raise\\n\\n    def wrapper(left, right):\\n        if isinstance(right, ABCDataFrame):\\n            return NotImplemented\\n\\n        left, right = _align_method_SERIES(left, right)\\n        res_name = get_op_result_name(left, right)\\n        right = maybe_upcast_for_op(right)\\n\\n        if is_categorical_dtype(left):\\n            raise TypeError(\"{typ} cannot perform the operation \"\\n                            \"{op}\".format(typ=type(left).__name__, op=str_rep))\\n\\n        elif is_datetime64_dtype(left) or is_datetime64tz_dtype(left):\\n            # Give dispatch_to_index_op a chance for tests like\\n            # test_dt64_series_add_intlike, which the index dispatching handles\\n            # specifically.\\n            result = dispatch_to_index_op(op, left, right, pd.DatetimeIndex)\\n            return construct_result(left, result,\\n                                    index=left.index, name=res_name,\\n                                    dtype=result.dtype)\\n\\n        elif (is_extension_array_dtype(left) or\\n                (is_extension_array_dtype(right) and not is_scalar(right))):\\n            # GH#22378 disallow scalar to exclude e.g. \"category\", \"Int64\"\\n            return dispatch_to_extension_op(op, left, right)\\n\\n        elif is_timedelta64_dtype(left):\\n            result = dispatch_to_index_op(op, left, right, pd.TimedeltaIndex)\\n            return construct_result(left, result,\\n                                    index=left.index, name=res_name)\\n\\n        elif is_timedelta64_dtype(right):\\n            # We should only get here with non-scalar or timedelta64(\\'NaT\\')\\n            #  values for right\\n            # Note: we cannot use dispatch_to_index_op because\\n            #  that may incorrectly raise TypeError when we\\n            #  should get NullFrequencyError\\n            result = op(pd.Index(left), right)\\n            return construct_result(left, result,\\n                                    index=left.index, name=res_name,\\n                                    dtype=result.dtype)\\n\\n        lvalues = left.values\\n        rvalues = right\\n        if isinstance(rvalues, ABCSeries):\\n            rvalues = rvalues.values\\n\\n        result = safe_na_op(lvalues, rvalues)\\n        return construct_result(left, result,\\n                                index=left.index, name=res_name, dtype=None)\\n\\n    wrapper.__name__ = op_name\\n    return wrapper',\n 'def _comp_method_SERIES(cls, op, special):\\n    \"\"\"\\n    Wrapper function for Series arithmetic operations, to avoid\\n    code duplication.\\n    \"\"\"\\n    op_name = _get_op_name(op, special)\\n    masker = _gen_eval_kwargs(op_name).get(\\'masker\\', False)\\n\\n    def na_op(x, y):\\n        # TODO:\\n        # should have guarantess on what x, y can be type-wise\\n        # Extension Dtypes are not called here\\n\\n        # Checking that cases that were once handled here are no longer\\n        # reachable.\\n        assert not (is_categorical_dtype(y) and not is_scalar(y))\\n\\n        if is_object_dtype(x.dtype):\\n            result = _comp_method_OBJECT_ARRAY(op, x, y)\\n\\n        elif is_datetimelike_v_numeric(x, y):\\n            return invalid_comparison(x, y, op)\\n\\n        else:\\n\\n            # we want to compare like types\\n            # we only want to convert to integer like if\\n            # we are not NotImplemented, otherwise\\n            # we would allow datetime64 (but viewed as i8) against\\n            # integer comparisons\\n\\n            # we have a datetime/timedelta and may need to convert\\n            assert not needs_i8_conversion(x)\\n            mask = None\\n            if not is_scalar(y) and needs_i8_conversion(y):\\n                mask = isna(x) | isna(y)\\n                y = y.view(\\'i8\\')\\n                x = x.view(\\'i8\\')\\n\\n            method = getattr(x, op_name, None)\\n            if method is not None:\\n                with np.errstate(all=\\'ignore\\'):\\n                    result = method(y)\\n                if result is NotImplemented:\\n                    return invalid_comparison(x, y, op)\\n            else:\\n                result = op(x, y)\\n\\n            if mask is not None and mask.any():\\n                result[mask] = masker\\n\\n        return result\\n\\n    def wrapper(self, other, axis=None):\\n        # Validate the axis parameter\\n        if axis is not None:\\n            self._get_axis_number(axis)\\n\\n        res_name = get_op_result_name(self, other)\\n\\n        if isinstance(other, list):\\n            # TODO: same for tuples?\\n            other = np.asarray(other)\\n\\n        if isinstance(other, ABCDataFrame):  # pragma: no cover\\n            # Defer to DataFrame implementation; fail early\\n            return NotImplemented\\n\\n        elif isinstance(other, ABCSeries) and not self._indexed_same(other):\\n            raise ValueError(\"Can only compare identically-labeled \"\\n                             \"Series objects\")\\n\\n        elif is_categorical_dtype(self):\\n            # Dispatch to Categorical implementation; pd.CategoricalIndex\\n            # behavior is non-canonical GH#19513\\n            res_values = dispatch_to_index_op(op, self, other, pd.Categorical)\\n            return self._constructor(res_values, index=self.index,\\n                                     name=res_name)\\n\\n        elif is_datetime64_dtype(self) or is_datetime64tz_dtype(self):\\n            # Dispatch to DatetimeIndex to ensure identical\\n            # Series/Index behavior\\n            if (isinstance(other, datetime.date) and\\n                    not isinstance(other, datetime.datetime)):\\n                # https://github.com/pandas-dev/pandas/issues/21152\\n                # Compatibility for difference between Series comparison w/\\n                # datetime and date\\n                msg = (\\n                    \"Comparing Series of datetimes with \\'datetime.date\\'.  \"\\n                    \"Currently, the \\'datetime.date\\' is coerced to a \"\\n                    \"datetime. In the future pandas will not coerce, \"\\n                    \"and {future}. \"\\n                    \"To retain the current behavior, \"\\n                    \"convert the \\'datetime.date\\' to a datetime with \"\\n                    \"\\'pd.Timestamp\\'.\"\\n                )\\n\\n                if op in {operator.lt, operator.le, operator.gt, operator.ge}:\\n                    future = \"a TypeError will be raised\"\\n                else:\\n                    future = (\\n                        \"\\'the values will not compare equal to the \"\\n                        \"\\'datetime.date\\'\"\\n                    )\\n                msg = \\'\\\\n\\'.join(textwrap.wrap(msg.format(future=future)))\\n                warnings.warn(msg, FutureWarning, stacklevel=2)\\n                other = pd.Timestamp(other)\\n\\n            res_values = dispatch_to_index_op(op, self, other,\\n                                              pd.DatetimeIndex)\\n\\n            return self._constructor(res_values, index=self.index,\\n                                     name=res_name)\\n\\n        elif is_timedelta64_dtype(self):\\n            res_values = dispatch_to_index_op(op, self, other,\\n                                              pd.TimedeltaIndex)\\n            return self._constructor(res_values, index=self.index,\\n                                     name=res_name)\\n\\n        elif (is_extension_array_dtype(self) or\\n              (is_extension_array_dtype(other) and not is_scalar(other))):\\n            # Note: the `not is_scalar(other)` condition rules out\\n            # e.g. other == \"category\"\\n            return dispatch_to_extension_op(op, self, other)\\n\\n        elif isinstance(other, ABCSeries):\\n            # By this point we have checked that self._indexed_same(other)\\n            res_values = na_op(self.values, other.values)\\n            # rename is needed in case res_name is None and res_values.name\\n            # is not.\\n            return self._constructor(res_values, index=self.index,\\n                                     name=res_name).rename(res_name)\\n\\n        elif isinstance(other, (np.ndarray, pd.Index)):\\n            # do not check length of zerodim array\\n            # as it will broadcast\\n            if other.ndim != 0 and len(self) != len(other):\\n                raise ValueError(\\'Lengths must match to compare\\')\\n\\n            res_values = na_op(self.values, np.asarray(other))\\n            result = self._constructor(res_values, index=self.index)\\n            # rename is needed in case res_name is None and self.name\\n            # is not.\\n            return result.__finalize__(self).rename(res_name)\\n\\n        elif is_scalar(other) and isna(other):\\n            # numpy does not like comparisons vs None\\n            if op is operator.ne:\\n                res_values = np.ones(len(self), dtype=bool)\\n            else:\\n                res_values = np.zeros(len(self), dtype=bool)\\n            return self._constructor(res_values, index=self.index,\\n                                     name=res_name, dtype=\\'bool\\')\\n\\n        else:\\n            values = self.get_values()\\n\\n            with np.errstate(all=\\'ignore\\'):\\n                res = na_op(values, other)\\n            if is_scalar(res):\\n                raise TypeError(\\'Could not compare {typ} type with Series\\'\\n                                .format(typ=type(other)))\\n\\n            # always return a full value series here\\n            res_values = com.values_from_object(res)\\n            return self._constructor(res_values, index=self.index,\\n                                     name=res_name, dtype=\\'bool\\')\\n\\n    wrapper.__name__ = op_name\\n    return wrapper',\n 'def _bool_method_SERIES(cls, op, special):\\n    \"\"\"\\n    Wrapper function for Series arithmetic operations, to avoid\\n    code duplication.\\n    \"\"\"\\n    op_name = _get_op_name(op, special)\\n\\n    def na_op(x, y):\\n        try:\\n            result = op(x, y)\\n        except TypeError:\\n            assert not isinstance(y, (list, ABCSeries, ABCIndexClass))\\n            if isinstance(y, np.ndarray):\\n                # bool-bool dtype operations should be OK, should not get here\\n                assert not (is_bool_dtype(x) and is_bool_dtype(y))\\n                x = ensure_object(x)\\n                y = ensure_object(y)\\n                result = libops.vec_binop(x, y, op)\\n            else:\\n                # let null fall thru\\n                assert lib.is_scalar(y)\\n                if not isna(y):\\n                    y = bool(y)\\n                try:\\n                    result = libops.scalar_binop(x, y, op)\\n                except (TypeError, ValueError, AttributeError,\\n                        OverflowError, NotImplementedError):\\n                    raise TypeError(\"cannot compare a dtyped [{dtype}] array \"\\n                                    \"with a scalar of type [{typ}]\"\\n                                    .format(dtype=x.dtype,\\n                                            typ=type(y).__name__))\\n\\n        return result\\n\\n    fill_int = lambda x: x.fillna(0)\\n    fill_bool = lambda x: x.fillna(False).astype(bool)\\n\\n    def wrapper(self, other):\\n        is_self_int_dtype = is_integer_dtype(self.dtype)\\n\\n        self, other = _align_method_SERIES(self, other, align_asobject=True)\\n        res_name = get_op_result_name(self, other)\\n\\n        if isinstance(other, ABCDataFrame):\\n            # Defer to DataFrame implementation; fail early\\n            return NotImplemented\\n\\n        elif isinstance(other, (ABCSeries, ABCIndexClass)):\\n            is_other_int_dtype = is_integer_dtype(other.dtype)\\n            other = fill_int(other) if is_other_int_dtype else fill_bool(other)\\n\\n            ovalues = other.values\\n            finalizer = lambda x: x\\n\\n        else:\\n            # scalars, list, tuple, np.array\\n            is_other_int_dtype = is_integer_dtype(np.asarray(other))\\n            if is_list_like(other) and not isinstance(other, np.ndarray):\\n                # TODO: Can we do this before the is_integer_dtype check?\\n                # could the is_integer_dtype check be checking the wrong\\n                # thing?  e.g. other = [[0, 1], [2, 3], [4, 5]]?\\n                other = construct_1d_object_array_from_listlike(other)\\n\\n            ovalues = other\\n            finalizer = lambda x: x.__finalize__(self)\\n\\n        # For int vs int `^`, `|`, `&` are bitwise operators and return\\n        #   integer dtypes.  Otherwise these are boolean ops\\n        filler = (fill_int if is_self_int_dtype and is_other_int_dtype\\n                  else fill_bool)\\n        res_values = na_op(self.values, ovalues)\\n        unfilled = self._constructor(res_values,\\n                                     index=self.index, name=res_name)\\n        filled = filler(unfilled)\\n        return finalizer(filled)\\n\\n    wrapper.__name__ = op_name\\n    return wrapper',\n 'def _combine_series_frame(self, other, func, fill_value=None, axis=None,\\n                          level=None):\\n    \"\"\"\\n    Apply binary operator `func` to self, other using alignment and fill\\n    conventions determined by the fill_value, axis, and level kwargs.\\n\\n    Parameters\\n    ----------\\n    self : DataFrame\\n    other : Series\\n    func : binary operator\\n    fill_value : object, default None\\n    axis : {0, 1, \\'columns\\', \\'index\\', None}, default None\\n    level : int or None, default None\\n\\n    Returns\\n    -------\\n    result : DataFrame\\n    \"\"\"\\n    if fill_value is not None:\\n        raise NotImplementedError(\"fill_value {fill} not supported.\"\\n                                  .format(fill=fill_value))\\n\\n    if axis is not None:\\n        axis = self._get_axis_number(axis)\\n        if axis == 0:\\n            return self._combine_match_index(other, func, level=level)\\n        else:\\n            return self._combine_match_columns(other, func, level=level)\\n    else:\\n        if not len(other):\\n            return self * np.nan\\n\\n        if not len(self):\\n            # Ambiguous case, use _series so works with DataFrame\\n            return self._constructor(data=self._series, index=self.index,\\n                                     columns=self.columns)\\n\\n        # default axis is columns\\n        return self._combine_match_columns(other, func, level=level)',\n 'def _align_method_FRAME(left, right, axis):\\n    \"\"\" convert rhs to meet lhs dims if input is list, tuple or np.ndarray \"\"\"\\n\\n    def to_series(right):\\n        msg = (\\'Unable to coerce to Series, length must be {req_len}: \\'\\n               \\'given {given_len}\\')\\n        if axis is not None and left._get_axis_name(axis) == \\'index\\':\\n            if len(left.index) != len(right):\\n                raise ValueError(msg.format(req_len=len(left.index),\\n                                            given_len=len(right)))\\n            right = left._constructor_sliced(right, index=left.index)\\n        else:\\n            if len(left.columns) != len(right):\\n                raise ValueError(msg.format(req_len=len(left.columns),\\n                                            given_len=len(right)))\\n            right = left._constructor_sliced(right, index=left.columns)\\n        return right\\n\\n    if isinstance(right, np.ndarray):\\n\\n        if right.ndim == 1:\\n            right = to_series(right)\\n\\n        elif right.ndim == 2:\\n            if right.shape == left.shape:\\n                right = left._constructor(right, index=left.index,\\n                                          columns=left.columns)\\n\\n            elif right.shape[0] == left.shape[0] and right.shape[1] == 1:\\n                # Broadcast across columns\\n                right = np.broadcast_to(right, left.shape)\\n                right = left._constructor(right,\\n                                          index=left.index,\\n                                          columns=left.columns)\\n\\n            elif right.shape[1] == left.shape[1] and right.shape[0] == 1:\\n                # Broadcast along rows\\n                right = to_series(right[0, :])\\n\\n            else:\\n                raise ValueError(\"Unable to coerce to DataFrame, shape \"\\n                                 \"must be {req_shape}: given {given_shape}\"\\n                                 .format(req_shape=left.shape,\\n                                         given_shape=right.shape))\\n\\n        elif right.ndim > 2:\\n            raise ValueError(\\'Unable to coerce to Series/DataFrame, dim \\'\\n                             \\'must be <= 2: {dim}\\'.format(dim=right.shape))\\n\\n    elif (is_list_like(right) and\\n          not isinstance(right, (ABCSeries, ABCDataFrame))):\\n        # GH17901\\n        right = to_series(right)\\n\\n    return right',\n 'def _cast_sparse_series_op(left, right, opname):\\n    \"\"\"\\n    For SparseSeries operation, coerce to float64 if the result is expected\\n    to have NaN or inf values\\n\\n    Parameters\\n    ----------\\n    left : SparseArray\\n    right : SparseArray\\n    opname : str\\n\\n    Returns\\n    -------\\n    left : SparseArray\\n    right : SparseArray\\n    \"\"\"\\n    from pandas.core.sparse.api import SparseDtype\\n\\n    opname = opname.strip(\\'_\\')\\n\\n    # TODO: This should be moved to the array?\\n    if is_integer_dtype(left) and is_integer_dtype(right):\\n        # series coerces to float64 if result should have NaN/inf\\n        if opname in (\\'floordiv\\', \\'mod\\') and (right.values == 0).any():\\n            left = left.astype(SparseDtype(np.float64, left.fill_value))\\n            right = right.astype(SparseDtype(np.float64, right.fill_value))\\n        elif opname in (\\'rfloordiv\\', \\'rmod\\') and (left.values == 0).any():\\n            left = left.astype(SparseDtype(np.float64, left.fill_value))\\n            right = right.astype(SparseDtype(np.float64, right.fill_value))\\n\\n    return left, right',\n 'def _arith_method_SPARSE_SERIES(cls, op, special):\\n    \"\"\"\\n    Wrapper function for Series arithmetic operations, to avoid\\n    code duplication.\\n    \"\"\"\\n    op_name = _get_op_name(op, special)\\n\\n    def wrapper(self, other):\\n        if isinstance(other, ABCDataFrame):\\n            return NotImplemented\\n        elif isinstance(other, ABCSeries):\\n            if not isinstance(other, ABCSparseSeries):\\n                other = other.to_sparse(fill_value=self.fill_value)\\n            return _sparse_series_op(self, other, op, op_name)\\n        elif is_scalar(other):\\n            with np.errstate(all=\\'ignore\\'):\\n                new_values = op(self.values, other)\\n            return self._constructor(new_values,\\n                                     index=self.index,\\n                                     name=self.name)\\n        else:  # pragma: no cover\\n            raise TypeError(\\'operation with {other} not supported\\'\\n                            .format(other=type(other)))\\n\\n    wrapper.__name__ = op_name\\n    return wrapper',\n 'def _arith_method_SPARSE_ARRAY(cls, op, special):\\n    \"\"\"\\n    Wrapper function for Series arithmetic operations, to avoid\\n    code duplication.\\n    \"\"\"\\n    op_name = _get_op_name(op, special)\\n\\n    def wrapper(self, other):\\n        from pandas.core.arrays.sparse.array import (\\n            SparseArray, _sparse_array_op, _wrap_result, _get_fill)\\n        if isinstance(other, np.ndarray):\\n            if len(self) != len(other):\\n                raise AssertionError(\"length mismatch: {self} vs. {other}\"\\n                                     .format(self=len(self), other=len(other)))\\n            if not isinstance(other, SparseArray):\\n                dtype = getattr(other, \\'dtype\\', None)\\n                other = SparseArray(other, fill_value=self.fill_value,\\n                                    dtype=dtype)\\n            return _sparse_array_op(self, other, op, op_name)\\n        elif is_scalar(other):\\n            with np.errstate(all=\\'ignore\\'):\\n                fill = op(_get_fill(self), np.asarray(other))\\n                result = op(self.sp_values, other)\\n\\n            return _wrap_result(op_name, result, self.sp_index, fill)\\n        else:  # pragma: no cover\\n            raise TypeError(\\'operation with {other} not supported\\'\\n                            .format(other=type(other)))\\n\\n    wrapper.__name__ = op_name\\n    return wrapper',\n 'def validate_periods(periods):\\n    \"\"\"\\n    If a `periods` argument is passed to the Datetime/Timedelta Array/Index\\n    constructor, cast it to an integer.\\n\\n    Parameters\\n    ----------\\n    periods : None, float, int\\n\\n    Returns\\n    -------\\n    periods : None or int\\n\\n    Raises\\n    ------\\n    TypeError\\n        if periods is None, float, or int\\n    \"\"\"\\n    if periods is not None:\\n        if lib.is_float(periods):\\n            periods = int(periods)\\n        elif not lib.is_integer(periods):\\n            raise TypeError(\\'periods must be a number, got {periods}\\'\\n                            .format(periods=periods))\\n    return periods',\n 'def validate_endpoints(closed):\\n    \"\"\"\\n    Check that the `closed` argument is among [None, \"left\", \"right\"]\\n\\n    Parameters\\n    ----------\\n    closed : {None, \"left\", \"right\"}\\n\\n    Returns\\n    -------\\n    left_closed : bool\\n    right_closed : bool\\n\\n    Raises\\n    ------\\n    ValueError : if argument is not among valid values\\n    \"\"\"\\n    left_closed = False\\n    right_closed = False\\n\\n    if closed is None:\\n        left_closed = True\\n        right_closed = True\\n    elif closed == \"left\":\\n        left_closed = True\\n    elif closed == \"right\":\\n        right_closed = True\\n    else:\\n        raise ValueError(\"Closed has to be either \\'left\\', \\'right\\' or None\")\\n\\n    return left_closed, right_closed',\n 'def validate_inferred_freq(freq, inferred_freq, freq_infer):\\n    \"\"\"\\n    If the user passes a freq and another freq is inferred from passed data,\\n    require that they match.\\n\\n    Parameters\\n    ----------\\n    freq : DateOffset or None\\n    inferred_freq : DateOffset or None\\n    freq_infer : bool\\n\\n    Returns\\n    -------\\n    freq : DateOffset or None\\n    freq_infer : bool\\n\\n    Notes\\n    -----\\n    We assume at this point that `maybe_infer_freq` has been called, so\\n    `freq` is either a DateOffset object or None.\\n    \"\"\"\\n    if inferred_freq is not None:\\n        if freq is not None and freq != inferred_freq:\\n            raise ValueError(\\'Inferred frequency {inferred} from passed \\'\\n                             \\'values does not conform to passed frequency \\'\\n                             \\'{passed}\\'\\n                             .format(inferred=inferred_freq,\\n                                     passed=freq.freqstr))\\n        elif freq is None:\\n            freq = inferred_freq\\n        freq_infer = False\\n\\n    return freq, freq_infer',\n 'def maybe_infer_freq(freq):\\n    \"\"\"\\n    Comparing a DateOffset to the string \"infer\" raises, so we need to\\n    be careful about comparisons.  Make a dummy variable `freq_infer` to\\n    signify the case where the given freq is \"infer\" and set freq to None\\n    to avoid comparison trouble later on.\\n\\n    Parameters\\n    ----------\\n    freq : {DateOffset, None, str}\\n\\n    Returns\\n    -------\\n    freq : {DateOffset, None}\\n    freq_infer : bool\\n    \"\"\"\\n    freq_infer = False\\n    if not isinstance(freq, DateOffset):\\n        # if a passed freq is None, don\\'t infer automatically\\n        if freq != \\'infer\\':\\n            freq = frequencies.to_offset(freq)\\n        else:\\n            freq_infer = True\\n            freq = None\\n    return freq, freq_infer',\n 'def _ensure_datetimelike_to_i8(other, to_utc=False):\\n    \"\"\"\\n    Helper for coercing an input scalar or array to i8.\\n\\n    Parameters\\n    ----------\\n    other : 1d array\\n    to_utc : bool, default False\\n        If True, convert the values to UTC before extracting the i8 values\\n        If False, extract the i8 values directly.\\n\\n    Returns\\n    -------\\n    i8 1d array\\n    \"\"\"\\n    from pandas import Index\\n    from pandas.core.arrays import PeriodArray\\n\\n    if lib.is_scalar(other) and isna(other):\\n        return iNaT\\n    elif isinstance(other, (PeriodArray, ABCIndexClass,\\n                            DatetimeLikeArrayMixin)):\\n        # convert tz if needed\\n        if getattr(other, \\'tz\\', None) is not None:\\n            if to_utc:\\n                other = other.tz_convert(\\'UTC\\')\\n            else:\\n                other = other.tz_localize(None)\\n    else:\\n        try:\\n            return np.array(other, copy=False).view(\\'i8\\')\\n        except TypeError:\\n            # period array cannot be coerced to int\\n            other = Index(other)\\n    return other.asi8',\n 'def _scalar_from_string(\\n            self,\\n            value: str,\\n    ) -> Union[Period, Timestamp, Timedelta, NaTType]:\\n        \"\"\"\\n        Construct a scalar type from a string.\\n\\n        Parameters\\n        ----------\\n        value : str\\n\\n        Returns\\n        -------\\n        Period, Timestamp, or Timedelta, or NaT\\n            Whatever the type of ``self._scalar_type`` is.\\n\\n        Notes\\n        -----\\n        This should call ``self._check_compatible_with`` before\\n        unboxing the result.\\n        \"\"\"\\n        raise AbstractMethodError(self)',\n 'def _unbox_scalar(\\n            self,\\n            value: Union[Period, Timestamp, Timedelta, NaTType],\\n    ) -> int:\\n        \"\"\"\\n        Unbox the integer value of a scalar `value`.\\n\\n        Parameters\\n        ----------\\n        value : Union[Period, Timestamp, Timedelta]\\n\\n        Returns\\n        -------\\n        int\\n\\n        Examples\\n        --------\\n        >>> self._unbox_scalar(Timedelta(\\'10s\\'))  # DOCTEST: +SKIP\\n        10000000000\\n        \"\"\"\\n        raise AbstractMethodError(self)',\n 'def _check_compatible_with(\\n            self,\\n            other: Union[Period, Timestamp, Timedelta, NaTType],\\n    ) -> None:\\n        \"\"\"\\n        Verify that `self` and `other` are compatible.\\n\\n        * DatetimeArray verifies that the timezones (if any) match\\n        * PeriodArray verifies that the freq matches\\n        * Timedelta has no verification\\n\\n        In each case, NaT is considered compatible.\\n\\n        Parameters\\n        ----------\\n        other\\n\\n        Raises\\n        ------\\n        Exception\\n        \"\"\"\\n        raise AbstractMethodError(self)',\n 'def strftime(self, date_format):\\n        \"\"\"\\n        Convert to Index using specified date_format.\\n\\n        Return an Index of formatted strings specified by date_format, which\\n        supports the same string format as the python standard library. Details\\n        of the string format can be found in `python string format\\n        doc <%(URL)s>`__.\\n\\n        Parameters\\n        ----------\\n        date_format : str\\n            Date format string (e.g. \"%%Y-%%m-%%d\").\\n\\n        Returns\\n        -------\\n        Index\\n            Index of formatted strings.\\n\\n        See Also\\n        --------\\n        to_datetime : Convert the given argument to datetime.\\n        DatetimeIndex.normalize : Return DatetimeIndex with times to midnight.\\n        DatetimeIndex.round : Round the DatetimeIndex to the specified freq.\\n        DatetimeIndex.floor : Floor the DatetimeIndex to the specified freq.\\n\\n        Examples\\n        --------\\n        >>> rng = pd.date_range(pd.Timestamp(\"2018-03-10 09:00\"),\\n        ...                     periods=3, freq=\\'s\\')\\n        >>> rng.strftime(\\'%%B %%d, %%Y, %%r\\')\\n        Index([\\'March 10, 2018, 09:00:00 AM\\', \\'March 10, 2018, 09:00:01 AM\\',\\n               \\'March 10, 2018, 09:00:02 AM\\'],\\n              dtype=\\'object\\')\\n        \"\"\"\\n        from pandas import Index\\n        return Index(self._format_native_types(date_format=date_format))',\n 'def searchsorted(self, value, side=\\'left\\', sorter=None):\\n        \"\"\"\\n        Find indices where elements should be inserted to maintain order.\\n\\n        Find the indices into a sorted array `self` such that, if the\\n        corresponding elements in `value` were inserted before the indices,\\n        the order of `self` would be preserved.\\n\\n        Parameters\\n        ----------\\n        value : array_like\\n            Values to insert into `self`.\\n        side : {\\'left\\', \\'right\\'}, optional\\n            If \\'left\\', the index of the first suitable location found is given.\\n            If \\'right\\', return the last such index.  If there is no suitable\\n            index, return either 0 or N (where N is the length of `self`).\\n        sorter : 1-D array_like, optional\\n            Optional array of integer indices that sort `self` into ascending\\n            order. They are typically the result of ``np.argsort``.\\n\\n        Returns\\n        -------\\n        indices : array of ints\\n            Array of insertion points with the same shape as `value`.\\n        \"\"\"\\n        if isinstance(value, str):\\n            value = self._scalar_from_string(value)\\n\\n        if not (isinstance(value, (self._scalar_type, type(self)))\\n                or isna(value)):\\n            raise ValueError(\"Unexpected type for \\'value\\': {valtype}\"\\n                             .format(valtype=type(value)))\\n\\n        self._check_compatible_with(value)\\n        if isinstance(value, type(self)):\\n            value = value.asi8\\n        else:\\n            value = self._unbox_scalar(value)\\n\\n        return self.asi8.searchsorted(value, side=side, sorter=sorter)',\n 'def repeat(self, repeats, *args, **kwargs):\\n        \"\"\"\\n        Repeat elements of an array.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.repeat\\n        \"\"\"\\n        nv.validate_repeat(args, kwargs)\\n        values = self._data.repeat(repeats)\\n        return type(self)(values.view(\\'i8\\'), dtype=self.dtype)',\n 'def value_counts(self, dropna=False):\\n        \"\"\"\\n        Return a Series containing counts of unique values.\\n\\n        Parameters\\n        ----------\\n        dropna : boolean, default True\\n            Don\\'t include counts of NaT values.\\n\\n        Returns\\n        -------\\n        Series\\n        \"\"\"\\n        from pandas import Series, Index\\n\\n        if dropna:\\n            values = self[~self.isna()]._data\\n        else:\\n            values = self._data\\n\\n        cls = type(self)\\n\\n        result = value_counts(values, sort=False, dropna=dropna)\\n        index = Index(cls(result.index.view(\\'i8\\'), dtype=self.dtype),\\n                      name=result.index.name)\\n        return Series(result.values, index=index, name=result.name)',\n 'def _maybe_mask_results(self, result, fill_value=iNaT, convert=None):\\n        \"\"\"\\n        Parameters\\n        ----------\\n        result : a ndarray\\n        fill_value : object, default iNaT\\n        convert : string/dtype or None\\n\\n        Returns\\n        -------\\n        result : ndarray with values replace by the fill_value\\n\\n        mask the result if needed, convert to the provided dtype if its not\\n        None\\n\\n        This is an internal routine.\\n        \"\"\"\\n\\n        if self._hasnans:\\n            if convert:\\n                result = result.astype(convert)\\n            if fill_value is None:\\n                fill_value = np.nan\\n            result[self._isnan] = fill_value\\n        return result',\n 'def _validate_frequency(cls, index, freq, **kwargs):\\n        \"\"\"\\n        Validate that a frequency is compatible with the values of a given\\n        Datetime Array/Index or Timedelta Array/Index\\n\\n        Parameters\\n        ----------\\n        index : DatetimeIndex or TimedeltaIndex\\n            The index on which to determine if the given frequency is valid\\n        freq : DateOffset\\n            The frequency to validate\\n        \"\"\"\\n        if is_period_dtype(cls):\\n            # Frequency validation is not meaningful for Period Array/Index\\n            return None\\n\\n        inferred = index.inferred_freq\\n        if index.size == 0 or inferred == freq.freqstr:\\n            return None\\n\\n        try:\\n            on_freq = cls._generate_range(start=index[0], end=None,\\n                                          periods=len(index), freq=freq,\\n                                          **kwargs)\\n            if not np.array_equal(index.asi8, on_freq.asi8):\\n                raise ValueError\\n        except ValueError as e:\\n            if \"non-fixed\" in str(e):\\n                # non-fixed frequencies are not meaningful for timedelta64;\\n                #  we retain that error message\\n                raise e\\n            # GH#11587 the main way this is reached is if the `np.array_equal`\\n            #  check above is False.  This can also be reached if index[0]\\n            #  is `NaT`, in which case the call to `cls._generate_range` will\\n            #  raise a ValueError, which we re-raise with a more targeted\\n            #  message.\\n            raise ValueError(\\'Inferred frequency {infer} from passed values \\'\\n                             \\'does not conform to passed frequency {passed}\\'\\n                             .format(infer=inferred, passed=freq.freqstr))',\n 'def _add_delta(self, other):\\n        \"\"\"\\n        Add a timedelta-like, Tick or TimedeltaIndex-like object\\n        to self, yielding an int64 numpy array\\n\\n        Parameters\\n        ----------\\n        delta : {timedelta, np.timedelta64, Tick,\\n                 TimedeltaIndex, ndarray[timedelta64]}\\n\\n        Returns\\n        -------\\n        result : ndarray[int64]\\n\\n        Notes\\n        -----\\n        The result\\'s name is set outside of _add_delta by the calling\\n        method (__add__ or __sub__), if necessary (i.e. for Indexes).\\n        \"\"\"\\n        if isinstance(other, (Tick, timedelta, np.timedelta64)):\\n            new_values = self._add_timedeltalike_scalar(other)\\n        elif is_timedelta64_dtype(other):\\n            # ndarray[timedelta64] or TimedeltaArray/index\\n            new_values = self._add_delta_tdi(other)\\n\\n        return new_values',\n 'def _add_timedeltalike_scalar(self, other):\\n        \"\"\"\\n        Add a delta of a timedeltalike\\n        return the i8 result view\\n        \"\"\"\\n        if isna(other):\\n            # i.e np.timedelta64(\"NaT\"), not recognized by delta_to_nanoseconds\\n            new_values = np.empty(len(self), dtype=\\'i8\\')\\n            new_values[:] = iNaT\\n            return new_values\\n\\n        inc = delta_to_nanoseconds(other)\\n        new_values = checked_add_with_arr(self.asi8, inc,\\n                                          arr_mask=self._isnan).view(\\'i8\\')\\n        new_values = self._maybe_mask_results(new_values)\\n        return new_values.view(\\'i8\\')',\n 'def _add_delta_tdi(self, other):\\n        \"\"\"\\n        Add a delta of a TimedeltaIndex\\n        return the i8 result view\\n        \"\"\"\\n        if len(self) != len(other):\\n            raise ValueError(\"cannot add indices of unequal length\")\\n\\n        if isinstance(other, np.ndarray):\\n            # ndarray[timedelta64]; wrap in TimedeltaIndex for op\\n            from pandas import TimedeltaIndex\\n            other = TimedeltaIndex(other)\\n\\n        self_i8 = self.asi8\\n        other_i8 = other.asi8\\n        new_values = checked_add_with_arr(self_i8, other_i8,\\n                                          arr_mask=self._isnan,\\n                                          b_mask=other._isnan)\\n        if self._hasnans or other._hasnans:\\n            mask = (self._isnan) | (other._isnan)\\n            new_values[mask] = iNaT\\n        return new_values.view(\\'i8\\')',\n 'def _add_nat(self):\\n        \"\"\"\\n        Add pd.NaT to self\\n        \"\"\"\\n        if is_period_dtype(self):\\n            raise TypeError(\\'Cannot add {cls} and {typ}\\'\\n                            .format(cls=type(self).__name__,\\n                                    typ=type(NaT).__name__))\\n\\n        # GH#19124 pd.NaT is treated like a timedelta for both timedelta\\n        # and datetime dtypes\\n        result = np.zeros(len(self), dtype=np.int64)\\n        result.fill(iNaT)\\n        return type(self)(result, dtype=self.dtype, freq=None)',\n 'def _sub_nat(self):\\n        \"\"\"\\n        Subtract pd.NaT from self\\n        \"\"\"\\n        # GH#19124 Timedelta - datetime is not in general well-defined.\\n        # We make an exception for pd.NaT, which in this case quacks\\n        # like a timedelta.\\n        # For datetime64 dtypes by convention we treat NaT as a datetime, so\\n        # this subtraction returns a timedelta64 dtype.\\n        # For period dtype, timedelta64 is a close-enough return dtype.\\n        result = np.zeros(len(self), dtype=np.int64)\\n        result.fill(iNaT)\\n        return result.view(\\'timedelta64[ns]\\')',\n 'def _sub_period_array(self, other):\\n        \"\"\"\\n        Subtract a Period Array/Index from self.  This is only valid if self\\n        is itself a Period Array/Index, raises otherwise.  Both objects must\\n        have the same frequency.\\n\\n        Parameters\\n        ----------\\n        other : PeriodIndex or PeriodArray\\n\\n        Returns\\n        -------\\n        result : np.ndarray[object]\\n            Array of DateOffset objects; nulls represented by NaT.\\n        \"\"\"\\n        if not is_period_dtype(self):\\n            raise TypeError(\"cannot subtract {dtype}-dtype from {cls}\"\\n                            .format(dtype=other.dtype,\\n                                    cls=type(self).__name__))\\n\\n        if len(self) != len(other):\\n            raise ValueError(\"cannot subtract arrays/indices of \"\\n                             \"unequal length\")\\n        if self.freq != other.freq:\\n            msg = DIFFERENT_FREQ.format(cls=type(self).__name__,\\n                                        own_freq=self.freqstr,\\n                                        other_freq=other.freqstr)\\n            raise IncompatibleFrequency(msg)\\n\\n        new_values = checked_add_with_arr(self.asi8, -other.asi8,\\n                                          arr_mask=self._isnan,\\n                                          b_mask=other._isnan)\\n\\n        new_values = np.array([self.freq.base * x for x in new_values])\\n        if self._hasnans or other._hasnans:\\n            mask = (self._isnan) | (other._isnan)\\n            new_values[mask] = NaT\\n        return new_values',\n 'def _addsub_int_array(self, other, op):\\n        \"\"\"\\n        Add or subtract array-like of integers equivalent to applying\\n        `_time_shift` pointwise.\\n\\n        Parameters\\n        ----------\\n        other : Index, ExtensionArray, np.ndarray\\n            integer-dtype\\n        op : {operator.add, operator.sub}\\n\\n        Returns\\n        -------\\n        result : same class as self\\n        \"\"\"\\n        # _addsub_int_array is overriden by PeriodArray\\n        assert not is_period_dtype(self)\\n        assert op in [operator.add, operator.sub]\\n\\n        if self.freq is None:\\n            # GH#19123\\n            raise NullFrequencyError(\"Cannot shift with no freq\")\\n\\n        elif isinstance(self.freq, Tick):\\n            # easy case where we can convert to timedelta64 operation\\n            td = Timedelta(self.freq)\\n            return op(self, td * other)\\n\\n        # We should only get here with DatetimeIndex; dispatch\\n        # to _addsub_offset_array\\n        assert not is_timedelta64_dtype(self)\\n        return op(self, np.array(other) * self.freq)',\n 'def _addsub_offset_array(self, other, op):\\n        \"\"\"\\n        Add or subtract array-like of DateOffset objects\\n\\n        Parameters\\n        ----------\\n        other : Index, np.ndarray\\n            object-dtype containing pd.DateOffset objects\\n        op : {operator.add, operator.sub}\\n\\n        Returns\\n        -------\\n        result : same class as self\\n        \"\"\"\\n        assert op in [operator.add, operator.sub]\\n        if len(other) == 1:\\n            return op(self, other[0])\\n\\n        warnings.warn(\"Adding/subtracting array of DateOffsets to \"\\n                      \"{cls} not vectorized\"\\n                      .format(cls=type(self).__name__), PerformanceWarning)\\n\\n        # For EA self.astype(\\'O\\') returns a numpy array, not an Index\\n        left = lib.values_from_object(self.astype(\\'O\\'))\\n\\n        res_values = op(left, np.array(other))\\n        kwargs = {}\\n        if not is_period_dtype(self):\\n            kwargs[\\'freq\\'] = \\'infer\\'\\n        return self._from_sequence(res_values, **kwargs)',\n 'def _time_shift(self, periods, freq=None):\\n        \"\"\"\\n        Shift each value by `periods`.\\n\\n        Note this is different from ExtensionArray.shift, which\\n        shifts the *position* of each element, padding the end with\\n        missing values.\\n\\n        Parameters\\n        ----------\\n        periods : int\\n            Number of periods to shift by.\\n        freq : pandas.DateOffset, pandas.Timedelta, or string\\n            Frequency increment to shift by.\\n        \"\"\"\\n        if freq is not None and freq != self.freq:\\n            if isinstance(freq, str):\\n                freq = frequencies.to_offset(freq)\\n            offset = periods * freq\\n            result = self + offset\\n            return result\\n\\n        if periods == 0:\\n            # immutable so OK\\n            return self.copy()\\n\\n        if self.freq is None:\\n            raise NullFrequencyError(\"Cannot shift with no freq\")\\n\\n        start = self[0] + periods * self.freq\\n        end = self[-1] + periods * self.freq\\n\\n        # Note: in the DatetimeTZ case, _generate_range will infer the\\n        #  appropriate timezone from `start` and `end`, so tz does not need\\n        #  to be passed explicitly.\\n        return self._generate_range(start=start, end=end, periods=None,\\n                                    freq=self.freq)',\n 'def _ensure_localized(self, arg, ambiguous=\\'raise\\', nonexistent=\\'raise\\',\\n                          from_utc=False):\\n        \"\"\"\\n        Ensure that we are re-localized.\\n\\n        This is for compat as we can then call this on all datetimelike\\n        arrays generally (ignored for Period/Timedelta)\\n\\n        Parameters\\n        ----------\\n        arg : Union[DatetimeLikeArray, DatetimeIndexOpsMixin, ndarray]\\n        ambiguous : str, bool, or bool-ndarray, default \\'raise\\'\\n        nonexistent : str, default \\'raise\\'\\n        from_utc : bool, default False\\n            If True, localize the i8 ndarray to UTC first before converting to\\n            the appropriate tz. If False, localize directly to the tz.\\n\\n        Returns\\n        -------\\n        localized array\\n        \"\"\"\\n\\n        # reconvert to local tz\\n        tz = getattr(self, \\'tz\\', None)\\n        if tz is not None:\\n            if not isinstance(arg, type(self)):\\n                arg = self._simple_new(arg)\\n            if from_utc:\\n                arg = arg.tz_localize(\\'UTC\\').tz_convert(self.tz)\\n            else:\\n                arg = arg.tz_localize(\\n                    self.tz, ambiguous=ambiguous, nonexistent=nonexistent\\n                )\\n        return arg',\n 'def min(self, axis=None, skipna=True, *args, **kwargs):\\n        \"\"\"\\n        Return the minimum value of the Array or minimum along\\n        an axis.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.min\\n        Index.min : Return the minimum value in an Index.\\n        Series.min : Return the minimum value in a Series.\\n        \"\"\"\\n        nv.validate_min(args, kwargs)\\n        nv.validate_minmax_axis(axis)\\n\\n        result = nanops.nanmin(self.asi8, skipna=skipna, mask=self.isna())\\n        if isna(result):\\n            # Period._from_ordinal does not handle np.nan gracefully\\n            return NaT\\n        return self._box_func(result)',\n 'def max(self, axis=None, skipna=True, *args, **kwargs):\\n        \"\"\"\\n        Return the maximum value of the Array or maximum along\\n        an axis.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.max\\n        Index.max : Return the maximum value in an Index.\\n        Series.max : Return the maximum value in a Series.\\n        \"\"\"\\n        # TODO: skipna is broken with max.\\n        # See https://github.com/pandas-dev/pandas/issues/24265\\n        nv.validate_max(args, kwargs)\\n        nv.validate_minmax_axis(axis)\\n\\n        mask = self.isna()\\n        if skipna:\\n            values = self[~mask].asi8\\n        elif mask.any():\\n            return NaT\\n        else:\\n            values = self.asi8\\n\\n        if not len(values):\\n            # short-circut for empty max / min\\n            return NaT\\n\\n        result = nanops.nanmax(values, skipna=skipna)\\n        # Don\\'t have to worry about NA `result`, since no NA went in.\\n        return self._box_func(result)',\n 'def _period_array_cmp(cls, op):\\n    \"\"\"\\n    Wrap comparison operations to convert Period-like to PeriodDtype\\n    \"\"\"\\n    opname = \\'__{name}__\\'.format(name=op.__name__)\\n    nat_result = opname == \\'__ne__\\'\\n\\n    def wrapper(self, other):\\n        op = getattr(self.asi8, opname)\\n\\n        if isinstance(other, (ABCDataFrame, ABCSeries, ABCIndexClass)):\\n            return NotImplemented\\n\\n        if is_list_like(other) and len(other) != len(self):\\n            raise ValueError(\"Lengths must match\")\\n\\n        if isinstance(other, Period):\\n            self._check_compatible_with(other)\\n\\n            result = op(other.ordinal)\\n        elif isinstance(other, cls):\\n            self._check_compatible_with(other)\\n\\n            result = op(other.asi8)\\n\\n            mask = self._isnan | other._isnan\\n            if mask.any():\\n                result[mask] = nat_result\\n\\n            return result\\n        elif other is NaT:\\n            result = np.empty(len(self.asi8), dtype=bool)\\n            result.fill(nat_result)\\n        else:\\n            other = Period(other, freq=self.freq)\\n            result = op(other.ordinal)\\n\\n        if self._hasnans:\\n            result[self._isnan] = nat_result\\n\\n        return result\\n\\n    return compat.set_function_name(wrapper, opname, cls)',\n 'def _raise_on_incompatible(left, right):\\n    \"\"\"\\n    Helper function to render a consistent error message when raising\\n    IncompatibleFrequency.\\n\\n    Parameters\\n    ----------\\n    left : PeriodArray\\n    right : DateOffset, Period, ndarray, or timedelta-like\\n\\n    Raises\\n    ------\\n    IncompatibleFrequency\\n    \"\"\"\\n    # GH#24283 error message format depends on whether right is scalar\\n    if isinstance(right, np.ndarray):\\n        other_freq = None\\n    elif isinstance(right, (ABCPeriodIndex, PeriodArray, Period, DateOffset)):\\n        other_freq = right.freqstr\\n    else:\\n        other_freq = _delta_to_tick(Timedelta(right)).freqstr\\n\\n    msg = DIFFERENT_FREQ.format(cls=type(left).__name__,\\n                                own_freq=left.freqstr,\\n                                other_freq=other_freq)\\n    raise IncompatibleFrequency(msg)',\n 'def period_array(\\n        data: Sequence[Optional[Period]],\\n        freq: Optional[Tick] = None,\\n        copy: bool = False,\\n) -> PeriodArray:\\n    \"\"\"\\n    Construct a new PeriodArray from a sequence of Period scalars.\\n\\n    Parameters\\n    ----------\\n    data : Sequence of Period objects\\n        A sequence of Period objects. These are required to all have\\n        the same ``freq.`` Missing values can be indicated by ``None``\\n        or ``pandas.NaT``.\\n    freq : str, Tick, or Offset\\n        The frequency of every element of the array. This can be specified\\n        to avoid inferring the `freq` from `data`.\\n    copy : bool, default False\\n        Whether to ensure a copy of the data is made.\\n\\n    Returns\\n    -------\\n    PeriodArray\\n\\n    See Also\\n    --------\\n    PeriodArray\\n    pandas.PeriodIndex\\n\\n    Examples\\n    --------\\n    >>> period_array([pd.Period(\\'2017\\', freq=\\'A\\'),\\n    ...               pd.Period(\\'2018\\', freq=\\'A\\')])\\n    <PeriodArray>\\n    [\\'2017\\', \\'2018\\']\\n    Length: 2, dtype: period[A-DEC]\\n\\n    >>> period_array([pd.Period(\\'2017\\', freq=\\'A\\'),\\n    ...               pd.Period(\\'2018\\', freq=\\'A\\'),\\n    ...               pd.NaT])\\n    <PeriodArray>\\n    [\\'2017\\', \\'2018\\', \\'NaT\\']\\n    Length: 3, dtype: period[A-DEC]\\n\\n    Integers that look like years are handled\\n\\n    >>> period_array([2000, 2001, 2002], freq=\\'D\\')\\n    [\\'2000-01-01\\', \\'2001-01-01\\', \\'2002-01-01\\']\\n    Length: 3, dtype: period[D]\\n\\n    Datetime-like strings may also be passed\\n\\n    >>> period_array([\\'2000-Q1\\', \\'2000-Q2\\', \\'2000-Q3\\', \\'2000-Q4\\'], freq=\\'Q\\')\\n    <PeriodArray>\\n    [\\'2000Q1\\', \\'2000Q2\\', \\'2000Q3\\', \\'2000Q4\\']\\n    Length: 4, dtype: period[Q-DEC]\\n    \"\"\"\\n    if is_datetime64_dtype(data):\\n        return PeriodArray._from_datetime64(data, freq)\\n    if isinstance(data, (ABCPeriodIndex, ABCSeries, PeriodArray)):\\n        return PeriodArray(data, freq)\\n\\n    # other iterable of some kind\\n    if not isinstance(data, (np.ndarray, list, tuple)):\\n        data = list(data)\\n\\n    data = np.asarray(data)\\n\\n    if freq:\\n        dtype = PeriodDtype(freq)\\n    else:\\n        dtype = None\\n\\n    if is_float_dtype(data) and len(data) > 0:\\n        raise TypeError(\"PeriodIndex does not allow \"\\n                        \"floating point in construction\")\\n\\n    data = ensure_object(data)\\n\\n    return PeriodArray._from_sequence(data, dtype=dtype)',\n 'def validate_dtype_freq(dtype, freq):\\n    \"\"\"\\n    If both a dtype and a freq are available, ensure they match.  If only\\n    dtype is available, extract the implied freq.\\n\\n    Parameters\\n    ----------\\n    dtype : dtype\\n    freq : DateOffset or None\\n\\n    Returns\\n    -------\\n    freq : DateOffset\\n\\n    Raises\\n    ------\\n    ValueError : non-period dtype\\n    IncompatibleFrequency : mismatch between dtype and freq\\n    \"\"\"\\n    if freq is not None:\\n        freq = frequencies.to_offset(freq)\\n\\n    if dtype is not None:\\n        dtype = pandas_dtype(dtype)\\n        if not is_period_dtype(dtype):\\n            raise ValueError(\\'dtype must be PeriodDtype\\')\\n        if freq is None:\\n            freq = dtype.freq\\n        elif freq != dtype.freq:\\n            raise IncompatibleFrequency(\\'specified freq and dtype \\'\\n                                        \\'are different\\')\\n    return freq',\n 'def dt64arr_to_periodarr(data, freq, tz=None):\\n    \"\"\"\\n    Convert an datetime-like array to values Period ordinals.\\n\\n    Parameters\\n    ----------\\n    data : Union[Series[datetime64[ns]], DatetimeIndex, ndarray[datetime64ns]]\\n    freq : Optional[Union[str, Tick]]\\n        Must match the `freq` on the `data` if `data` is a DatetimeIndex\\n        or Series.\\n    tz : Optional[tzinfo]\\n\\n    Returns\\n    -------\\n    ordinals : ndarray[int]\\n    freq : Tick\\n        The frequencey extracted from the Series or DatetimeIndex if that\\'s\\n        used.\\n\\n    \"\"\"\\n    if data.dtype != np.dtype(\\'M8[ns]\\'):\\n        raise ValueError(\\'Wrong dtype: {dtype}\\'.format(dtype=data.dtype))\\n\\n    if freq is None:\\n        if isinstance(data, ABCIndexClass):\\n            data, freq = data._values, data.freq\\n        elif isinstance(data, ABCSeries):\\n            data, freq = data._values, data.dt.freq\\n\\n    freq = Period._maybe_convert_freq(freq)\\n\\n    if isinstance(data, (ABCIndexClass, ABCSeries)):\\n        data = data._values\\n\\n    base, mult = libfrequencies.get_freq_code(freq)\\n    return libperiod.dt64arr_to_periodarr(data.view(\\'i8\\'), base, tz), freq',\n 'def _from_datetime64(cls, data, freq, tz=None):\\n        \"\"\"\\n        Construct a PeriodArray from a datetime64 array\\n\\n        Parameters\\n        ----------\\n        data : ndarray[datetime64[ns], datetime64[ns, tz]]\\n        freq : str or Tick\\n        tz : tzinfo, optional\\n\\n        Returns\\n        -------\\n        PeriodArray[freq]\\n        \"\"\"\\n        data, freq = dt64arr_to_periodarr(data, freq, tz)\\n        return cls(data, freq=freq)',\n 'def to_timestamp(self, freq=None, how=\\'start\\'):\\n        \"\"\"\\n        Cast to DatetimeArray/Index.\\n\\n        Parameters\\n        ----------\\n        freq : string or DateOffset, optional\\n            Target frequency. The default is \\'D\\' for week or longer,\\n            \\'S\\' otherwise\\n        how : {\\'s\\', \\'e\\', \\'start\\', \\'end\\'}\\n\\n        Returns\\n        -------\\n        DatetimeArray/Index\\n        \"\"\"\\n        from pandas.core.arrays import DatetimeArray\\n\\n        how = libperiod._validate_end_alias(how)\\n\\n        end = how == \\'E\\'\\n        if end:\\n            if freq == \\'B\\':\\n                # roll forward to ensure we land on B date\\n                adjust = Timedelta(1, \\'D\\') - Timedelta(1, \\'ns\\')\\n                return self.to_timestamp(how=\\'start\\') + adjust\\n            else:\\n                adjust = Timedelta(1, \\'ns\\')\\n                return (self + self.freq).to_timestamp(how=\\'start\\') - adjust\\n\\n        if freq is None:\\n            base, mult = libfrequencies.get_freq_code(self.freq)\\n            freq = libfrequencies.get_to_timestamp_base(base)\\n        else:\\n            freq = Period._maybe_convert_freq(freq)\\n\\n        base, mult = libfrequencies.get_freq_code(freq)\\n        new_data = self.asfreq(freq, how=how)\\n\\n        new_data = libperiod.periodarr_to_dt64arr(new_data.asi8, base)\\n        return DatetimeArray._from_sequence(new_data, freq=\\'infer\\')',\n 'def _time_shift(self, periods, freq=None):\\n        \"\"\"\\n        Shift each value by `periods`.\\n\\n        Note this is different from ExtensionArray.shift, which\\n        shifts the *position* of each element, padding the end with\\n        missing values.\\n\\n        Parameters\\n        ----------\\n        periods : int\\n            Number of periods to shift by.\\n        freq : pandas.DateOffset, pandas.Timedelta, or string\\n            Frequency increment to shift by.\\n        \"\"\"\\n        if freq is not None:\\n            raise TypeError(\"`freq` argument is not supported for \"\\n                            \"{cls}._time_shift\"\\n                            .format(cls=type(self).__name__))\\n        values = self.asi8 + periods * self.freq.n\\n        if self._hasnans:\\n            values[self._isnan] = iNaT\\n        return type(self)(values, freq=self.freq)',\n 'def asfreq(self, freq=None, how=\\'E\\'):\\n        \"\"\"\\n        Convert the Period Array/Index to the specified frequency `freq`.\\n\\n        Parameters\\n        ----------\\n        freq : str\\n            a frequency\\n        how : str {\\'E\\', \\'S\\'}\\n            \\'E\\', \\'END\\', or \\'FINISH\\' for end,\\n            \\'S\\', \\'START\\', or \\'BEGIN\\' for start.\\n            Whether the elements should be aligned to the end\\n            or start within pa period. January 31st (\\'END\\') vs.\\n            January 1st (\\'START\\') for example.\\n\\n        Returns\\n        -------\\n        new : Period Array/Index with the new frequency\\n\\n        Examples\\n        --------\\n        >>> pidx = pd.period_range(\\'2010-01-01\\', \\'2015-01-01\\', freq=\\'A\\')\\n        >>> pidx\\n        PeriodIndex([\\'2010\\', \\'2011\\', \\'2012\\', \\'2013\\', \\'2014\\', \\'2015\\'],\\n        dtype=\\'period[A-DEC]\\', freq=\\'A-DEC\\')\\n\\n        >>> pidx.asfreq(\\'M\\')\\n        PeriodIndex([\\'2010-12\\', \\'2011-12\\', \\'2012-12\\', \\'2013-12\\', \\'2014-12\\',\\n        \\'2015-12\\'], dtype=\\'period[M]\\', freq=\\'M\\')\\n\\n        >>> pidx.asfreq(\\'M\\', how=\\'S\\')\\n        PeriodIndex([\\'2010-01\\', \\'2011-01\\', \\'2012-01\\', \\'2013-01\\', \\'2014-01\\',\\n        \\'2015-01\\'], dtype=\\'period[M]\\', freq=\\'M\\')\\n        \"\"\"\\n        how = libperiod._validate_end_alias(how)\\n\\n        freq = Period._maybe_convert_freq(freq)\\n\\n        base1, mult1 = libfrequencies.get_freq_code(self.freq)\\n        base2, mult2 = libfrequencies.get_freq_code(freq)\\n\\n        asi8 = self.asi8\\n        # mult1 can\\'t be negative or 0\\n        end = how == \\'E\\'\\n        if end:\\n            ordinal = asi8 + mult1 - 1\\n        else:\\n            ordinal = asi8\\n\\n        new_data = period_asfreq_arr(ordinal, base1, base2, end)\\n\\n        if self._hasnans:\\n            new_data[self._isnan] = iNaT\\n\\n        return type(self)(new_data, freq=freq)',\n 'def _format_native_types(self, na_rep=\\'NaT\\', date_format=None, **kwargs):\\n        \"\"\"\\n        actually format my specific types\\n        \"\"\"\\n        values = self.astype(object)\\n\\n        if date_format:\\n            formatter = lambda dt: dt.strftime(date_format)\\n        else:\\n            formatter = lambda dt: \\'%s\\' % dt\\n\\n        if self._hasnans:\\n            mask = self._isnan\\n            values[mask] = na_rep\\n            imask = ~mask\\n            values[imask] = np.array([formatter(dt) for dt\\n                                      in values[imask]])\\n        else:\\n            values = np.array([formatter(dt) for dt in values])\\n        return values',\n 'def _add_timedeltalike_scalar(self, other):\\n        \"\"\"\\n        Parameters\\n        ----------\\n        other : timedelta, Tick, np.timedelta64\\n\\n        Returns\\n        -------\\n        result : ndarray[int64]\\n        \"\"\"\\n        assert isinstance(self.freq, Tick)  # checked by calling function\\n        assert isinstance(other, (timedelta, np.timedelta64, Tick))\\n\\n        if notna(other):\\n            # special handling for np.timedelta64(\"NaT\"), avoid calling\\n            #  _check_timedeltalike_freq_compat as that would raise TypeError\\n            other = self._check_timedeltalike_freq_compat(other)\\n\\n        # Note: when calling parent class\\'s _add_timedeltalike_scalar,\\n        #  it will call delta_to_nanoseconds(delta).  Because delta here\\n        #  is an integer, delta_to_nanoseconds will return it unchanged.\\n        ordinals = super()._add_timedeltalike_scalar(other)\\n        return ordinals',\n 'def _add_delta_tdi(self, other):\\n        \"\"\"\\n        Parameters\\n        ----------\\n        other : TimedeltaArray or ndarray[timedelta64]\\n\\n        Returns\\n        -------\\n        result : ndarray[int64]\\n        \"\"\"\\n        assert isinstance(self.freq, Tick)  # checked by calling function\\n\\n        delta = self._check_timedeltalike_freq_compat(other)\\n        return self._addsub_int_array(delta, operator.add).asi8',\n 'def _add_delta(self, other):\\n        \"\"\"\\n        Add a timedelta-like, Tick, or TimedeltaIndex-like object\\n        to self, yielding a new PeriodArray\\n\\n        Parameters\\n        ----------\\n        other : {timedelta, np.timedelta64, Tick,\\n                 TimedeltaIndex, ndarray[timedelta64]}\\n\\n        Returns\\n        -------\\n        result : PeriodArray\\n        \"\"\"\\n        if not isinstance(self.freq, Tick):\\n            # We cannot add timedelta-like to non-tick PeriodArray\\n            _raise_on_incompatible(self, other)\\n\\n        new_ordinals = super()._add_delta(other)\\n        return type(self)(new_ordinals, freq=self.freq)',\n 'def _check_timedeltalike_freq_compat(self, other):\\n        \"\"\"\\n        Arithmetic operations with timedelta-like scalars or array `other`\\n        are only valid if `other` is an integer multiple of `self.freq`.\\n        If the operation is valid, find that integer multiple.  Otherwise,\\n        raise because the operation is invalid.\\n\\n        Parameters\\n        ----------\\n        other : timedelta, np.timedelta64, Tick,\\n                ndarray[timedelta64], TimedeltaArray, TimedeltaIndex\\n\\n        Returns\\n        -------\\n        multiple : int or ndarray[int64]\\n\\n        Raises\\n        ------\\n        IncompatibleFrequency\\n        \"\"\"\\n        assert isinstance(self.freq, Tick)  # checked by calling function\\n        own_offset = frequencies.to_offset(self.freq.rule_code)\\n        base_nanos = delta_to_nanoseconds(own_offset)\\n\\n        if isinstance(other, (timedelta, np.timedelta64, Tick)):\\n            nanos = delta_to_nanoseconds(other)\\n\\n        elif isinstance(other, np.ndarray):\\n            # numpy timedelta64 array; all entries must be compatible\\n            assert other.dtype.kind == \\'m\\'\\n            if other.dtype != _TD_DTYPE:\\n                # i.e. non-nano unit\\n                # TODO: disallow unit-less timedelta64\\n                other = other.astype(_TD_DTYPE)\\n            nanos = other.view(\\'i8\\')\\n        else:\\n            # TimedeltaArray/Index\\n            nanos = other.asi8\\n\\n        if np.all(nanos % base_nanos == 0):\\n            # nanos being added is an integer multiple of the\\n            #  base-frequency to self.freq\\n            delta = nanos // base_nanos\\n            # delta is the integer (or integer-array) number of periods\\n            # by which will be added to self.\\n            return delta\\n\\n        _raise_on_incompatible(self, other)',\n 'def _isna_old(obj):\\n    \"\"\"Detect missing values. Treat None, NaN, INF, -INF as null.\\n\\n    Parameters\\n    ----------\\n    arr: ndarray or object value\\n\\n    Returns\\n    -------\\n    boolean ndarray or boolean\\n    \"\"\"\\n    if is_scalar(obj):\\n        return libmissing.checknull_old(obj)\\n    # hack (for now) because MI registers as ndarray\\n    elif isinstance(obj, ABCMultiIndex):\\n        raise NotImplementedError(\"isna is not defined for MultiIndex\")\\n    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass)):\\n        return _isna_ndarraylike_old(obj)\\n    elif isinstance(obj, ABCGeneric):\\n        return obj._constructor(obj._data.isna(func=_isna_old))\\n    elif isinstance(obj, list):\\n        return _isna_ndarraylike_old(np.asarray(obj, dtype=object))\\n    elif hasattr(obj, \\'__array__\\'):\\n        return _isna_ndarraylike_old(np.asarray(obj))\\n    else:\\n        return obj is None',\n 'def _use_inf_as_na(key):\\n    \"\"\"Option change callback for na/inf behaviour\\n    Choose which replacement for numpy.isnan / -numpy.isfinite is used.\\n\\n    Parameters\\n    ----------\\n    flag: bool\\n        True means treat None, NaN, INF, -INF as null (old way),\\n        False means None and NaN are null, but INF, -INF are not null\\n        (new way).\\n\\n    Notes\\n    -----\\n    This approach to setting global module values is discussed and\\n    approved here:\\n\\n    * http://stackoverflow.com/questions/4859217/\\n      programmatically-creating-variables-in-python/4859312#4859312\\n    \"\"\"\\n    from pandas._config import get_option\\n    flag = get_option(key)\\n    if flag:\\n        globals()[\\'_isna\\'] = _isna_old\\n    else:\\n        globals()[\\'_isna\\'] = _isna_new',\n 'def _isna_compat(arr, fill_value=np.nan):\\n    \"\"\"\\n    Parameters\\n    ----------\\n    arr: a numpy array\\n    fill_value: fill value, default to np.nan\\n\\n    Returns\\n    -------\\n    True if we can fill using this fill_value\\n    \"\"\"\\n    dtype = arr.dtype\\n    if isna(fill_value):\\n        return not (is_bool_dtype(dtype) or\\n                    is_integer_dtype(dtype))\\n    return True',\n 'def array_equivalent(left, right, strict_nan=False):\\n    \"\"\"\\n    True if two arrays, left and right, have equal non-NaN elements, and NaNs\\n    in corresponding locations.  False otherwise. It is assumed that left and\\n    right are NumPy arrays of the same dtype. The behavior of this function\\n    (particularly with respect to NaNs) is not defined if the dtypes are\\n    different.\\n\\n    Parameters\\n    ----------\\n    left, right : ndarrays\\n    strict_nan : bool, default False\\n        If True, consider NaN and None to be different.\\n\\n    Returns\\n    -------\\n    b : bool\\n        Returns True if the arrays are equivalent.\\n\\n    Examples\\n    --------\\n    >>> array_equivalent(\\n    ...     np.array([1, 2, np.nan]),\\n    ...     np.array([1, 2, np.nan]))\\n    True\\n    >>> array_equivalent(\\n    ...     np.array([1, np.nan, 2]),\\n    ...     np.array([1, 2, np.nan]))\\n    False\\n    \"\"\"\\n\\n    left, right = np.asarray(left), np.asarray(right)\\n\\n    # shape compat\\n    if left.shape != right.shape:\\n        return False\\n\\n    # Object arrays can contain None, NaN and NaT.\\n    # string dtypes must be come to this path for NumPy 1.7.1 compat\\n    if is_string_dtype(left) or is_string_dtype(right):\\n\\n        if not strict_nan:\\n            # isna considers NaN and None to be equivalent.\\n            return lib.array_equivalent_object(\\n                ensure_object(left.ravel()), ensure_object(right.ravel()))\\n\\n        for left_value, right_value in zip(left, right):\\n            if left_value is NaT and right_value is not NaT:\\n                return False\\n\\n            elif isinstance(left_value, float) and np.isnan(left_value):\\n                if (not isinstance(right_value, float) or\\n                        not np.isnan(right_value)):\\n                    return False\\n            else:\\n                if left_value != right_value:\\n                    return False\\n        return True\\n\\n    # NaNs can occur in float and complex arrays.\\n    if is_float_dtype(left) or is_complex_dtype(left):\\n\\n        # empty\\n        if not (np.prod(left.shape) and np.prod(right.shape)):\\n            return True\\n        return ((left == right) | (isna(left) & isna(right))).all()\\n\\n    # numpy will will not allow this type of datetimelike vs integer comparison\\n    elif is_datetimelike_v_numeric(left, right):\\n        return False\\n\\n    # M8/m8\\n    elif needs_i8_conversion(left) and needs_i8_conversion(right):\\n        if not is_dtype_equal(left.dtype, right.dtype):\\n            return False\\n\\n        left = left.view(\\'i8\\')\\n        right = right.view(\\'i8\\')\\n\\n    # if we have structured dtypes, compare first\\n    if (left.dtype.type is np.void or\\n            right.dtype.type is np.void):\\n        if left.dtype != right.dtype:\\n            return False\\n\\n    return np.array_equal(left, right)',\n 'def _infer_fill_value(val):\\n    \"\"\"\\n    infer the fill value for the nan/NaT from the provided\\n    scalar/ndarray/list-like if we are a NaT, return the correct dtyped\\n    element to provide proper block construction\\n    \"\"\"\\n\\n    if not is_list_like(val):\\n        val = [val]\\n    val = np.array(val, copy=False)\\n    if is_datetimelike(val):\\n        return np.array(\\'NaT\\', dtype=val.dtype)\\n    elif is_object_dtype(val.dtype):\\n        dtype = lib.infer_dtype(ensure_object(val), skipna=False)\\n        if dtype in [\\'datetime\\', \\'datetime64\\']:\\n            return np.array(\\'NaT\\', dtype=_NS_DTYPE)\\n        elif dtype in [\\'timedelta\\', \\'timedelta64\\']:\\n            return np.array(\\'NaT\\', dtype=_TD_DTYPE)\\n    return np.nan',\n 'def _maybe_fill(arr, fill_value=np.nan):\\n    \"\"\"\\n    if we have a compatible fill_value and arr dtype, then fill\\n    \"\"\"\\n    if _isna_compat(arr, fill_value):\\n        arr.fill(fill_value)\\n    return arr',\n 'def na_value_for_dtype(dtype, compat=True):\\n    \"\"\"\\n    Return a dtype compat na value\\n\\n    Parameters\\n    ----------\\n    dtype : string / dtype\\n    compat : boolean, default True\\n\\n    Returns\\n    -------\\n    np.dtype or a pandas dtype\\n\\n    Examples\\n    --------\\n    >>> na_value_for_dtype(np.dtype(\\'int64\\'))\\n    0\\n    >>> na_value_for_dtype(np.dtype(\\'int64\\'), compat=False)\\n    nan\\n    >>> na_value_for_dtype(np.dtype(\\'float64\\'))\\n    nan\\n    >>> na_value_for_dtype(np.dtype(\\'bool\\'))\\n    False\\n    >>> na_value_for_dtype(np.dtype(\\'datetime64[ns]\\'))\\n    NaT\\n    \"\"\"\\n    dtype = pandas_dtype(dtype)\\n\\n    if is_extension_array_dtype(dtype):\\n        return dtype.na_value\\n    if (is_datetime64_dtype(dtype) or is_datetime64tz_dtype(dtype) or\\n            is_timedelta64_dtype(dtype) or is_period_dtype(dtype)):\\n        return NaT\\n    elif is_float_dtype(dtype):\\n        return np.nan\\n    elif is_integer_dtype(dtype):\\n        if compat:\\n            return 0\\n        return np.nan\\n    elif is_bool_dtype(dtype):\\n        return False\\n    return np.nan',\n 'def remove_na_arraylike(arr):\\n    \"\"\"\\n    Return array-like containing only true/non-NaN values, possibly empty.\\n    \"\"\"\\n    if is_extension_array_dtype(arr):\\n        return arr[notna(arr)]\\n    else:\\n        return arr[notna(lib.values_from_object(arr))]',\n 'def table(ax, data, rowLabels=None, colLabels=None, **kwargs):\\n    \"\"\"\\n    Helper function to convert DataFrame and Series to matplotlib.table\\n\\n    Parameters\\n    ----------\\n    ax : Matplotlib axes object\\n    data : DataFrame or Series\\n        data for table contents\\n    kwargs : keywords, optional\\n        keyword arguments which passed to matplotlib.table.table.\\n        If `rowLabels` or `colLabels` is not specified, data index or column\\n        name will be used.\\n\\n    Returns\\n    -------\\n    matplotlib table object\\n    \"\"\"\\n    if isinstance(data, ABCSeries):\\n        data = data.to_frame()\\n    elif isinstance(data, ABCDataFrame):\\n        pass\\n    else:\\n        raise ValueError(\\'Input data must be DataFrame or Series\\')\\n\\n    if rowLabels is None:\\n        rowLabels = data.index\\n\\n    if colLabels is None:\\n        colLabels = data.columns\\n\\n    cellText = data.values\\n\\n    import matplotlib.table\\n    table = matplotlib.table.table(ax, cellText=cellText,\\n                                   rowLabels=rowLabels,\\n                                   colLabels=colLabels, **kwargs)\\n    return table',\n 'def _subplots(naxes=None, sharex=False, sharey=False, squeeze=True,\\n              subplot_kw=None, ax=None, layout=None, layout_type=\\'box\\',\\n              **fig_kw):\\n    \"\"\"Create a figure with a set of subplots already made.\\n\\n    This utility wrapper makes it convenient to create common layouts of\\n    subplots, including the enclosing figure object, in a single call.\\n\\n    Keyword arguments:\\n\\n    naxes : int\\n      Number of required axes. Exceeded axes are set invisible. Default is\\n      nrows * ncols.\\n\\n    sharex : bool\\n      If True, the X axis will be shared amongst all subplots.\\n\\n    sharey : bool\\n      If True, the Y axis will be shared amongst all subplots.\\n\\n    squeeze : bool\\n\\n      If True, extra dimensions are squeezed out from the returned axis object:\\n        - if only one subplot is constructed (nrows=ncols=1), the resulting\\n        single Axis object is returned as a scalar.\\n        - for Nx1 or 1xN subplots, the returned object is a 1-d numpy object\\n        array of Axis objects are returned as numpy 1-d arrays.\\n        - for NxM subplots with N>1 and M>1 are returned as a 2d array.\\n\\n      If False, no squeezing is done: the returned axis object is always\\n      a 2-d array containing Axis instances, even if it ends up being 1x1.\\n\\n    subplot_kw : dict\\n      Dict with keywords passed to the add_subplot() call used to create each\\n      subplots.\\n\\n    ax : Matplotlib axis object, optional\\n\\n    layout : tuple\\n      Number of rows and columns of the subplot grid.\\n      If not specified, calculated from naxes and layout_type\\n\\n    layout_type : {\\'box\\', \\'horziontal\\', \\'vertical\\'}, default \\'box\\'\\n      Specify how to layout the subplot grid.\\n\\n    fig_kw : Other keyword arguments to be passed to the figure() call.\\n        Note that all keywords not recognized above will be\\n        automatically included here.\\n\\n    Returns:\\n\\n    fig, ax : tuple\\n      - fig is the Matplotlib Figure object\\n      - ax can be either a single axis object or an array of axis objects if\\n      more than one subplot was created.  The dimensions of the resulting array\\n      can be controlled with the squeeze keyword, see above.\\n\\n    **Examples:**\\n\\n    x = np.linspace(0, 2*np.pi, 400)\\n    y = np.sin(x**2)\\n\\n    # Just a figure and one subplot\\n    f, ax = plt.subplots()\\n    ax.plot(x, y)\\n    ax.set_title(\\'Simple plot\\')\\n\\n    # Two subplots, unpack the output array immediately\\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\\n    ax1.plot(x, y)\\n    ax1.set_title(\\'Sharing Y axis\\')\\n    ax2.scatter(x, y)\\n\\n    # Four polar axes\\n    plt.subplots(2, 2, subplot_kw=dict(polar=True))\\n    \"\"\"\\n    import matplotlib.pyplot as plt\\n\\n    if subplot_kw is None:\\n        subplot_kw = {}\\n\\n    if ax is None:\\n        fig = plt.figure(**fig_kw)\\n    else:\\n        if is_list_like(ax):\\n            ax = _flatten(ax)\\n            if layout is not None:\\n                warnings.warn(\"When passing multiple axes, layout keyword is \"\\n                              \"ignored\", UserWarning)\\n            if sharex or sharey:\\n                warnings.warn(\"When passing multiple axes, sharex and sharey \"\\n                              \"are ignored. These settings must be specified \"\\n                              \"when creating axes\", UserWarning,\\n                              stacklevel=4)\\n            if len(ax) == naxes:\\n                fig = ax[0].get_figure()\\n                return fig, ax\\n            else:\\n                raise ValueError(\"The number of passed axes must be {0}, the \"\\n                                 \"same as the output plot\".format(naxes))\\n\\n        fig = ax.get_figure()\\n        # if ax is passed and a number of subplots is 1, return ax as it is\\n        if naxes == 1:\\n            if squeeze:\\n                return fig, ax\\n            else:\\n                return fig, _flatten(ax)\\n        else:\\n            warnings.warn(\"To output multiple subplots, the figure containing \"\\n                          \"the passed axes is being cleared\", UserWarning,\\n                          stacklevel=4)\\n            fig.clear()\\n\\n    nrows, ncols = _get_layout(naxes, layout=layout, layout_type=layout_type)\\n    nplots = nrows * ncols\\n\\n    # Create empty object array to hold all axes.  It\\'s easiest to make it 1-d\\n    # so we can just append subplots upon creation, and then\\n    axarr = np.empty(nplots, dtype=object)\\n\\n    # Create first subplot separately, so we can share it if requested\\n    ax0 = fig.add_subplot(nrows, ncols, 1, **subplot_kw)\\n\\n    if sharex:\\n        subplot_kw[\\'sharex\\'] = ax0\\n    if sharey:\\n        subplot_kw[\\'sharey\\'] = ax0\\n    axarr[0] = ax0\\n\\n    # Note off-by-one counting because add_subplot uses the MATLAB 1-based\\n    # convention.\\n    for i in range(1, nplots):\\n        kwds = subplot_kw.copy()\\n        # Set sharex and sharey to None for blank/dummy axes, these can\\n        # interfere with proper axis limits on the visible axes if\\n        # they share axes e.g. issue #7528\\n        if i >= naxes:\\n            kwds[\\'sharex\\'] = None\\n            kwds[\\'sharey\\'] = None\\n        ax = fig.add_subplot(nrows, ncols, i + 1, **kwds)\\n        axarr[i] = ax\\n\\n    if naxes != nplots:\\n        for ax in axarr[naxes:]:\\n            ax.set_visible(False)\\n\\n    _handle_shared_axes(axarr, nplots, naxes, nrows, ncols, sharex, sharey)\\n\\n    if squeeze:\\n        # Reshape the array to have the final desired dimension (nrow,ncol),\\n        # though discarding unneeded dimensions that equal 1.  If we only have\\n        # one subplot, just return it instead of a 1-element array.\\n        if nplots == 1:\\n            axes = axarr[0]\\n        else:\\n            axes = axarr.reshape(nrows, ncols).squeeze()\\n    else:\\n        # returned axis array will be always 2-d, even if nrows=ncols=1\\n        axes = axarr.reshape(nrows, ncols)\\n\\n    return fig, axes',\n 'def maybe_cythonize(extensions, *args, **kwargs):\\n    \"\"\"\\n    Render tempita templates before calling cythonize\\n    \"\"\"\\n    if len(sys.argv) > 1 and \\'clean\\' in sys.argv:\\n        # Avoid running cythonize on `python setup.py clean`\\n        # See https://github.com/cython/cython/issues/1495\\n        return extensions\\n    if not cython:\\n        # Avoid trying to look up numpy when installing from sdist\\n        # https://github.com/pandas-dev/pandas/issues/25193\\n        # TODO: See if this can be removed after pyproject.toml added.\\n        return extensions\\n\\n    numpy_incl = pkg_resources.resource_filename(\\'numpy\\', \\'core/include\\')\\n    # TODO: Is this really necessary here?\\n    for ext in extensions:\\n        if (hasattr(ext, \\'include_dirs\\') and\\n                numpy_incl not in ext.include_dirs):\\n            ext.include_dirs.append(numpy_incl)\\n\\n    build_ext.render_templates(_pxifiles)\\n    return cythonize(extensions, *args, **kwargs)',\n 'def _transform_fast(self, result, obj, func_nm):\\n        \"\"\"\\n        Fast transform path for aggregations\\n        \"\"\"\\n        # if there were groups with no observations (Categorical only?)\\n        # try casting data to original dtype\\n        cast = self._transform_should_cast(func_nm)\\n\\n        # for each col, reshape to to size of original frame\\n        # by take operation\\n        ids, _, ngroup = self.grouper.group_info\\n        output = []\\n        for i, _ in enumerate(result.columns):\\n            res = algorithms.take_1d(result.iloc[:, i].values, ids)\\n            if cast:\\n                res = self._try_cast(res, obj.iloc[:, i])\\n            output.append(res)\\n\\n        return DataFrame._from_arrays(output, columns=result.columns,\\n                                      index=obj.index)',\n 'def filter(self, func, dropna=True, *args, **kwargs):  # noqa\\n        \"\"\"\\n        Return a copy of a DataFrame excluding elements from groups that\\n        do not satisfy the boolean criterion specified by func.\\n\\n        Parameters\\n        ----------\\n        f : function\\n            Function to apply to each subframe. Should return True or False.\\n        dropna : Drop groups that do not pass the filter. True by default;\\n            if False, groups that evaluate False are filled with NaNs.\\n\\n        Returns\\n        -------\\n        filtered : DataFrame\\n\\n        Notes\\n        -----\\n        Each subframe is endowed the attribute \\'name\\' in case you need to know\\n        which group you are working on.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'A\\' : [\\'foo\\', \\'bar\\', \\'foo\\', \\'bar\\',\\n        ...                           \\'foo\\', \\'bar\\'],\\n        ...                    \\'B\\' : [1, 2, 3, 4, 5, 6],\\n        ...                    \\'C\\' : [2.0, 5., 8., 1., 2., 9.]})\\n        >>> grouped = df.groupby(\\'A\\')\\n        >>> grouped.filter(lambda x: x[\\'B\\'].mean() > 3.)\\n             A  B    C\\n        1  bar  2  5.0\\n        3  bar  4  1.0\\n        5  bar  6  9.0\\n        \"\"\"\\n\\n        indices = []\\n\\n        obj = self._selected_obj\\n        gen = self.grouper.get_iterator(obj, axis=self.axis)\\n\\n        for name, group in gen:\\n            object.__setattr__(group, \\'name\\', name)\\n\\n            res = func(group, *args, **kwargs)\\n\\n            try:\\n                res = res.squeeze()\\n            except AttributeError:  # allow e.g., scalars and frames to pass\\n                pass\\n\\n            # interpret the result of the filter\\n            if is_bool(res) or (is_scalar(res) and isna(res)):\\n                if res and notna(res):\\n                    indices.append(self._get_index(name))\\n            else:\\n                # non scalars aren\\'t allowed\\n                raise TypeError(\"filter function returned a %s, \"\\n                                \"but expected a scalar bool\" %\\n                                type(res).__name__)\\n\\n        return self._apply_filter(indices, dropna)',\n 'def _wrap_output(self, output, index, names=None):\\n        \"\"\" common agg/transform wrapping logic \"\"\"\\n        output = output[self._selection_name]\\n\\n        if names is not None:\\n            return DataFrame(output, index=index, columns=names)\\n        else:\\n            name = self._selection_name\\n            if name is None:\\n                name = self._selected_obj.name\\n            return Series(output, index=index, name=name)',\n 'def _transform_fast(self, func, func_nm):\\n        \"\"\"\\n        fast version of transform, only applicable to\\n        builtin/cythonizable functions\\n        \"\"\"\\n        if isinstance(func, str):\\n            func = getattr(self, func)\\n\\n        ids, _, ngroup = self.grouper.group_info\\n        cast = self._transform_should_cast(func_nm)\\n        out = algorithms.take_1d(func()._values, ids)\\n        if cast:\\n            out = self._try_cast(out, self.obj)\\n        return Series(out, index=self.obj.index, name=self.obj.name)',\n 'def filter(self, func, dropna=True, *args, **kwargs):  # noqa\\n        \"\"\"\\n        Return a copy of a Series excluding elements from groups that\\n        do not satisfy the boolean criterion specified by func.\\n\\n        Parameters\\n        ----------\\n        func : function\\n            To apply to each group. Should return True or False.\\n        dropna : Drop groups that do not pass the filter. True by default;\\n            if False, groups that evaluate False are filled with NaNs.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'A\\' : [\\'foo\\', \\'bar\\', \\'foo\\', \\'bar\\',\\n        ...                           \\'foo\\', \\'bar\\'],\\n        ...                    \\'B\\' : [1, 2, 3, 4, 5, 6],\\n        ...                    \\'C\\' : [2.0, 5., 8., 1., 2., 9.]})\\n        >>> grouped = df.groupby(\\'A\\')\\n        >>> df.groupby(\\'A\\').B.filter(lambda x: x.mean() > 3.)\\n        1    2\\n        3    4\\n        5    6\\n        Name: B, dtype: int64\\n\\n        Returns\\n        -------\\n        filtered : Series\\n        \"\"\"\\n        if isinstance(func, str):\\n            wrapper = lambda x: getattr(x, func)(*args, **kwargs)\\n        else:\\n            wrapper = lambda x: func(x, *args, **kwargs)\\n\\n        # Interpret np.nan as False.\\n        def true_and_notna(x, *args, **kwargs):\\n            b = wrapper(x, *args, **kwargs)\\n            return b and notna(b)\\n\\n        try:\\n            indices = [self._get_index(name) for name, group in self\\n                       if true_and_notna(group)]\\n        except ValueError:\\n            raise TypeError(\"the filter must return a boolean result\")\\n        except TypeError:\\n            raise TypeError(\"the filter must return a boolean result\")\\n\\n        filtered = self._apply_filter(indices, dropna)\\n        return filtered',\n 'def nunique(self, dropna=True):\\n        \"\"\"\\n        Return number of unique elements in the group.\\n        \"\"\"\\n        ids, _, _ = self.grouper.group_info\\n\\n        val = self.obj.get_values()\\n\\n        try:\\n            sorter = np.lexsort((val, ids))\\n        except TypeError:  # catches object dtypes\\n            msg = \\'val.dtype must be object, got {}\\'.format(val.dtype)\\n            assert val.dtype == object, msg\\n            val, _ = algorithms.factorize(val, sort=False)\\n            sorter = np.lexsort((val, ids))\\n            _isna = lambda a: a == -1\\n        else:\\n            _isna = isna\\n\\n        ids, val = ids[sorter], val[sorter]\\n\\n        # group boundaries are where group ids change\\n        # unique observations are where sorted values change\\n        idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]\\n        inc = np.r_[1, val[1:] != val[:-1]]\\n\\n        # 1st item of each group is a new unique observation\\n        mask = _isna(val)\\n        if dropna:\\n            inc[idx] = 1\\n            inc[mask] = 0\\n        else:\\n            inc[mask & np.r_[False, mask[:-1]]] = 0\\n            inc[idx] = 1\\n\\n        out = np.add.reduceat(inc, idx).astype(\\'int64\\', copy=False)\\n        if len(ids):\\n            # NaN/NaT group exists if the head of ids is -1,\\n            # so remove it from res and exclude its index from idx\\n            if ids[0] == -1:\\n                res = out[1:]\\n                idx = idx[np.flatnonzero(idx)]\\n            else:\\n                res = out\\n        else:\\n            res = out[1:]\\n        ri = self.grouper.result_index\\n\\n        # we might have duplications among the bins\\n        if len(res) != len(ri):\\n            res, out = np.zeros(len(ri), dtype=out.dtype), res\\n            res[ids[idx]] = out\\n\\n        return Series(res,\\n                      index=ri,\\n                      name=self._selection_name)',\n 'def count(self):\\n        \"\"\" Compute count of group, excluding missing values \"\"\"\\n        ids, _, ngroups = self.grouper.group_info\\n        val = self.obj.get_values()\\n\\n        mask = (ids != -1) & ~isna(val)\\n        ids = ensure_platform_int(ids)\\n        minlength = ngroups or 0\\n        out = np.bincount(ids[mask], minlength=minlength)\\n\\n        return Series(out,\\n                      index=self.grouper.result_index,\\n                      name=self._selection_name,\\n                      dtype=\\'int64\\')',\n 'def pct_change(self, periods=1, fill_method=\\'pad\\', limit=None, freq=None):\\n        \"\"\"Calcuate pct_change of each value to previous entry in group\"\"\"\\n        # TODO: Remove this conditional when #23918 is fixed\\n        if freq:\\n            return self.apply(lambda x: x.pct_change(periods=periods,\\n                                                     fill_method=fill_method,\\n                                                     limit=limit, freq=freq))\\n        filled = getattr(self, fill_method)(limit=limit)\\n        fill_grp = filled.groupby(self.grouper.labels)\\n        shifted = fill_grp.shift(periods=periods, freq=freq)\\n\\n        return (filled / shifted) - 1',\n 'def _gotitem(self, key, ndim, subset=None):\\n        \"\"\"\\n        sub-classes to define\\n        return a sliced object\\n\\n        Parameters\\n        ----------\\n        key : string / list of selections\\n        ndim : 1,2\\n            requested ndim of result\\n        subset : object, default None\\n            subset to act on\\n        \"\"\"\\n\\n        if ndim == 2:\\n            if subset is None:\\n                subset = self.obj\\n            return DataFrameGroupBy(subset, self.grouper, selection=key,\\n                                    grouper=self.grouper,\\n                                    exclusions=self.exclusions,\\n                                    as_index=self.as_index,\\n                                    observed=self.observed)\\n        elif ndim == 1:\\n            if subset is None:\\n                subset = self.obj[key]\\n            return SeriesGroupBy(subset, selection=key,\\n                                 grouper=self.grouper)\\n\\n        raise AssertionError(\"invalid ndim for _gotitem\")',\n 'def _reindex_output(self, result):\\n        \"\"\"\\n        If we have categorical groupers, then we want to make sure that\\n        we have a fully reindex-output to the levels. These may have not\\n        participated in the groupings (e.g. may have all been\\n        nan groups);\\n\\n        This can re-expand the output space\\n        \"\"\"\\n\\n        # we need to re-expand the output space to accomodate all values\\n        # whether observed or not in the cartesian product of our groupes\\n        groupings = self.grouper.groupings\\n        if groupings is None:\\n            return result\\n        elif len(groupings) == 1:\\n            return result\\n\\n        # if we only care about the observed values\\n        # we are done\\n        elif self.observed:\\n            return result\\n\\n        # reindexing only applies to a Categorical grouper\\n        elif not any(isinstance(ping.grouper, (Categorical, CategoricalIndex))\\n                     for ping in groupings):\\n            return result\\n\\n        levels_list = [ping.group_index for ping in groupings]\\n        index, _ = MultiIndex.from_product(\\n            levels_list, names=self.grouper.names).sortlevel()\\n\\n        if self.as_index:\\n            d = {self.obj._get_axis_name(self.axis): index, \\'copy\\': False}\\n            return result.reindex(**d)\\n\\n        # GH 13204\\n        # Here, the categorical in-axis groupers, which need to be fully\\n        # expanded, are columns in `result`. An idea is to do:\\n        # result = result.set_index(self.grouper.names)\\n        #                .reindex(index).reset_index()\\n        # but special care has to be taken because of possible not-in-axis\\n        # groupers.\\n        # So, we manually select and drop the in-axis grouper columns,\\n        # reindex `result`, and then reset the in-axis grouper columns.\\n\\n        # Select in-axis groupers\\n        in_axis_grps = ((i, ping.name) for (i, ping)\\n                        in enumerate(groupings) if ping.in_axis)\\n        g_nums, g_names = zip(*in_axis_grps)\\n\\n        result = result.drop(labels=list(g_names), axis=1)\\n\\n        # Set a temp index and reindex (possibly expanding)\\n        result = result.set_index(self.grouper.result_index\\n                                  ).reindex(index, copy=False)\\n\\n        # Reset in-axis grouper columns\\n        # (using level numbers `g_nums` because level names may not be unique)\\n        result = result.reset_index(level=g_nums)\\n\\n        return result.reset_index(drop=True)',\n 'def _fill(self, direction, limit=None):\\n        \"\"\"Overridden method to join grouped columns in output\"\"\"\\n        res = super()._fill(direction, limit=limit)\\n        output = OrderedDict(\\n            (grp.name, grp.grouper) for grp in self.grouper.groupings)\\n\\n        from pandas import concat\\n        return concat((self._wrap_transformed_output(output), res), axis=1)',\n 'def count(self):\\n        \"\"\" Compute count of group, excluding missing values \"\"\"\\n        from pandas.core.dtypes.missing import _isna_ndarraylike as _isna\\n\\n        data, _ = self._get_data_to_aggregate()\\n        ids, _, ngroups = self.grouper.group_info\\n        mask = ids != -1\\n\\n        val = ((mask & ~_isna(np.atleast_2d(blk.get_values())))\\n               for blk in data.blocks)\\n        loc = (blk.mgr_locs for blk in data.blocks)\\n\\n        counter = partial(\\n            lib.count_level_2d, labels=ids, max_bin=ngroups, axis=1)\\n        blk = map(make_block, map(counter, val), loc)\\n\\n        return self._wrap_agged_blocks(data.items, list(blk))',\n 'def nunique(self, dropna=True):\\n        \"\"\"\\n        Return DataFrame with number of distinct observations per group for\\n        each column.\\n\\n        .. versionadded:: 0.20.0\\n\\n        Parameters\\n        ----------\\n        dropna : boolean, default True\\n            Don\\'t include NaN in the counts.\\n\\n        Returns\\n        -------\\n        nunique: DataFrame\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'id\\': [\\'spam\\', \\'egg\\', \\'egg\\', \\'spam\\',\\n        ...                           \\'ham\\', \\'ham\\'],\\n        ...                    \\'value1\\': [1, 5, 5, 2, 5, 5],\\n        ...                    \\'value2\\': list(\\'abbaxy\\')})\\n        >>> df\\n             id  value1 value2\\n        0  spam       1      a\\n        1   egg       5      b\\n        2   egg       5      b\\n        3  spam       2      a\\n        4   ham       5      x\\n        5   ham       5      y\\n\\n        >>> df.groupby(\\'id\\').nunique()\\n            id  value1  value2\\n        id\\n        egg    1       1       1\\n        ham    1       1       2\\n        spam   1       2       1\\n\\n        Check for rows with the same id but conflicting values:\\n\\n        >>> df.groupby(\\'id\\').filter(lambda g: (g.nunique() > 1).any())\\n             id  value1 value2\\n        0  spam       1      a\\n        3  spam       2      a\\n        4   ham       5      x\\n        5   ham       5      y\\n        \"\"\"\\n\\n        obj = self._selected_obj\\n\\n        def groupby_series(obj, col=None):\\n            return SeriesGroupBy(obj,\\n                                 selection=col,\\n                                 grouper=self.grouper).nunique(dropna=dropna)\\n\\n        if isinstance(obj, Series):\\n            results = groupby_series(obj)\\n        else:\\n            from pandas.core.reshape.concat import concat\\n            results = [groupby_series(obj[col], col) for col in obj.columns]\\n            results = concat(results, axis=1)\\n            results.columns.names = obj.columns.names\\n\\n        if not self.as_index:\\n            results.index = ibase.default_index(len(results))\\n        return results',\n 'def extract_array(obj, extract_numpy=False):\\n    \"\"\"\\n    Extract the ndarray or ExtensionArray from a Series or Index.\\n\\n    For all other types, `obj` is just returned as is.\\n\\n    Parameters\\n    ----------\\n    obj : object\\n        For Series / Index, the underlying ExtensionArray is unboxed.\\n        For Numpy-backed ExtensionArrays, the ndarray is extracted.\\n\\n    extract_numpy : bool, default False\\n        Whether to extract the ndarray from a PandasArray\\n\\n    Returns\\n    -------\\n    arr : object\\n\\n    Examples\\n    --------\\n    >>> extract_array(pd.Series([\\'a\\', \\'b\\', \\'c\\'], dtype=\\'category\\'))\\n    [a, b, c]\\n    Categories (3, object): [a, b, c]\\n\\n    Other objects like lists, arrays, and DataFrames are just passed through.\\n\\n    >>> extract_array([1, 2, 3])\\n    [1, 2, 3]\\n\\n    For an ndarray-backed Series / Index a PandasArray is returned.\\n\\n    >>> extract_array(pd.Series([1, 2, 3]))\\n    <PandasArray>\\n    [1, 2, 3]\\n    Length: 3, dtype: int64\\n\\n    To extract all the way down to the ndarray, pass ``extract_numpy=True``.\\n\\n    >>> extract_array(pd.Series([1, 2, 3]), extract_numpy=True)\\n    array([1, 2, 3])\\n    \"\"\"\\n    if isinstance(obj, (ABCIndexClass, ABCSeries)):\\n        obj = obj.array\\n\\n    if extract_numpy and isinstance(obj, ABCPandasArray):\\n        obj = obj.to_numpy()\\n\\n    return obj',\n 'def flatten(l):\\n    \"\"\"\\n    Flatten an arbitrarily nested sequence.\\n\\n    Parameters\\n    ----------\\n    l : sequence\\n        The non string sequence to flatten\\n\\n    Notes\\n    -----\\n    This doesn\\'t consider strings sequences.\\n\\n    Returns\\n    -------\\n    flattened : generator\\n    \"\"\"\\n    for el in l:\\n        if _iterable_not_string(el):\\n            for s in flatten(el):\\n                yield s\\n        else:\\n            yield el',\n 'def is_bool_indexer(key: Any) -> bool:\\n    \"\"\"\\n    Check whether `key` is a valid boolean indexer.\\n\\n    Parameters\\n    ----------\\n    key : Any\\n        Only list-likes may be considered boolean indexers.\\n        All other types are not considered a boolean indexer.\\n        For array-like input, boolean ndarrays or ExtensionArrays\\n        with ``_is_boolean`` set are considered boolean indexers.\\n\\n    Returns\\n    -------\\n    bool\\n\\n    Raises\\n    ------\\n    ValueError\\n        When the array is an object-dtype ndarray or ExtensionArray\\n        and contains missing values.\\n    \"\"\"\\n    na_msg = \\'cannot index with vector containing NA / NaN values\\'\\n    if (isinstance(key, (ABCSeries, np.ndarray, ABCIndex)) or\\n            (is_array_like(key) and is_extension_array_dtype(key.dtype))):\\n        if key.dtype == np.object_:\\n            key = np.asarray(values_from_object(key))\\n\\n            if not lib.is_bool_array(key):\\n                if isna(key).any():\\n                    raise ValueError(na_msg)\\n                return False\\n            return True\\n        elif is_bool_dtype(key.dtype):\\n            # an ndarray with bool-dtype by definition has no missing values.\\n            # So we only need to check for NAs in ExtensionArrays\\n            if is_extension_array_dtype(key.dtype):\\n                if np.any(key.isna()):\\n                    raise ValueError(na_msg)\\n            return True\\n    elif isinstance(key, list):\\n        try:\\n            arr = np.asarray(key)\\n            return arr.dtype == np.bool_ and len(arr) == len(key)\\n        except TypeError:  # pragma: no cover\\n            return False\\n\\n    return False',\n 'def cast_scalar_indexer(val):\\n    \"\"\"\\n    To avoid numpy DeprecationWarnings, cast float to integer where valid.\\n\\n    Parameters\\n    ----------\\n    val : scalar\\n\\n    Returns\\n    -------\\n    outval : scalar\\n    \"\"\"\\n    # assumes lib.is_scalar(val)\\n    if lib.is_float(val) and val == int(val):\\n        return int(val)\\n    return val',\n 'def index_labels_to_array(labels, dtype=None):\\n    \"\"\"\\n    Transform label or iterable of labels to array, for use in Index.\\n\\n    Parameters\\n    ----------\\n    dtype : dtype\\n        If specified, use as dtype of the resulting array, otherwise infer.\\n\\n    Returns\\n    -------\\n    array\\n    \"\"\"\\n    if isinstance(labels, (str, tuple)):\\n        labels = [labels]\\n\\n    if not isinstance(labels, (list, np.ndarray)):\\n        try:\\n            labels = list(labels)\\n        except TypeError:  # non-iterable\\n            labels = [labels]\\n\\n    labels = asarray_tuplesafe(labels, dtype=dtype)\\n\\n    return labels',\n 'def is_null_slice(obj):\\n    \"\"\"\\n    We have a null slice.\\n    \"\"\"\\n    return (isinstance(obj, slice) and obj.start is None and\\n            obj.stop is None and obj.step is None)',\n 'def is_full_slice(obj, l):\\n    \"\"\"\\n    We have a full length slice.\\n    \"\"\"\\n    return (isinstance(obj, slice) and obj.start == 0 and obj.stop == l and\\n            obj.step is None)',\n 'def apply_if_callable(maybe_callable, obj, **kwargs):\\n    \"\"\"\\n    Evaluate possibly callable input using obj and kwargs if it is callable,\\n    otherwise return as it is.\\n\\n    Parameters\\n    ----------\\n    maybe_callable : possibly a callable\\n    obj : NDFrame\\n    **kwargs\\n    \"\"\"\\n\\n    if callable(maybe_callable):\\n        return maybe_callable(obj, **kwargs)\\n\\n    return maybe_callable',\n 'def standardize_mapping(into):\\n    \"\"\"\\n    Helper function to standardize a supplied mapping.\\n\\n    .. versionadded:: 0.21.0\\n\\n    Parameters\\n    ----------\\n    into : instance or subclass of collections.abc.Mapping\\n        Must be a class, an initialized collections.defaultdict,\\n        or an instance of a collections.abc.Mapping subclass.\\n\\n    Returns\\n    -------\\n    mapping : a collections.abc.Mapping subclass or other constructor\\n        a callable object that can accept an iterator to create\\n        the desired Mapping.\\n\\n    See Also\\n    --------\\n    DataFrame.to_dict\\n    Series.to_dict\\n    \"\"\"\\n    if not inspect.isclass(into):\\n        if isinstance(into, collections.defaultdict):\\n            return partial(\\n                collections.defaultdict, into.default_factory)\\n        into = type(into)\\n    if not issubclass(into, abc.Mapping):\\n        raise TypeError(\\'unsupported type: {into}\\'.format(into=into))\\n    elif into == collections.defaultdict:\\n        raise TypeError(\\n            \\'to_dict() only accepts initialized defaultdicts\\')\\n    return into',\n 'def random_state(state=None):\\n    \"\"\"\\n    Helper function for processing random_state arguments.\\n\\n    Parameters\\n    ----------\\n    state : int, np.random.RandomState, None.\\n        If receives an int, passes to np.random.RandomState() as seed.\\n        If receives an np.random.RandomState object, just returns object.\\n        If receives `None`, returns np.random.\\n        If receives anything else, raises an informative ValueError.\\n        Default None.\\n\\n    Returns\\n    -------\\n    np.random.RandomState\\n    \"\"\"\\n\\n    if is_integer(state):\\n        return np.random.RandomState(state)\\n    elif isinstance(state, np.random.RandomState):\\n        return state\\n    elif state is None:\\n        return np.random\\n    else:\\n        raise ValueError(\"random_state must be an integer, a numpy \"\\n                         \"RandomState, or None\")',\n 'def _pipe(obj, func, *args, **kwargs):\\n    \"\"\"\\n    Apply a function ``func`` to object ``obj`` either by passing obj as the\\n    first argument to the function or, in the case that the func is a tuple,\\n    interpret the first element of the tuple as a function and pass the obj to\\n    that function as a keyword argument whose key is the value of the second\\n    element of the tuple.\\n\\n    Parameters\\n    ----------\\n    func : callable or tuple of (callable, string)\\n        Function to apply to this object or, alternatively, a\\n        ``(callable, data_keyword)`` tuple where ``data_keyword`` is a\\n        string indicating the keyword of `callable`` that expects the\\n        object.\\n    args : iterable, optional\\n        positional arguments passed into ``func``.\\n    kwargs : dict, optional\\n        a dictionary of keyword arguments passed into ``func``.\\n\\n    Returns\\n    -------\\n    object : the return type of ``func``.\\n    \"\"\"\\n    if isinstance(func, tuple):\\n        func, target = func\\n        if target in kwargs:\\n            msg = \\'%s is both the pipe target and a keyword argument\\' % target\\n            raise ValueError(msg)\\n        kwargs[target] = obj\\n        return func(*args, **kwargs)\\n    else:\\n        return func(obj, *args, **kwargs)',\n 'def _get_rename_function(mapper):\\n    \"\"\"\\n    Returns a function that will map names/labels, dependent if mapper\\n    is a dict, Series or just a function.\\n    \"\"\"\\n    if isinstance(mapper, (abc.Mapping, ABCSeries)):\\n\\n        def f(x):\\n            if x in mapper:\\n                return mapper[x]\\n            else:\\n                return x\\n    else:\\n        f = mapper\\n\\n    return f',\n 'def _get_fill_value(dtype, fill_value=None, fill_value_typ=None):\\n    \"\"\" return the correct fill value for the dtype of the values \"\"\"\\n    if fill_value is not None:\\n        return fill_value\\n    if _na_ok_dtype(dtype):\\n        if fill_value_typ is None:\\n            return np.nan\\n        else:\\n            if fill_value_typ == \\'+inf\\':\\n                return np.inf\\n            else:\\n                return -np.inf\\n    else:\\n        if fill_value_typ is None:\\n            return tslibs.iNaT\\n        else:\\n            if fill_value_typ == \\'+inf\\':\\n                # need the max int here\\n                return _int64_max\\n            else:\\n                return tslibs.iNaT',\n 'def _get_values(values, skipna, fill_value=None, fill_value_typ=None,\\n                isfinite=False, copy=True, mask=None):\\n    \"\"\" utility to get the values view, mask, dtype\\n    if necessary copy and mask using the specified fill_value\\n    copy = True will force the copy\\n    \"\"\"\\n\\n    if is_datetime64tz_dtype(values):\\n        # com.values_from_object returns M8[ns] dtype instead of tz-aware,\\n        #  so this case must be handled separately from the rest\\n        dtype = values.dtype\\n        values = getattr(values, \"_values\", values)\\n    else:\\n        values = com.values_from_object(values)\\n        dtype = values.dtype\\n\\n    if mask is None:\\n        if isfinite:\\n            mask = _isfinite(values)\\n        else:\\n            mask = isna(values)\\n\\n    if is_datetime_or_timedelta_dtype(values) or is_datetime64tz_dtype(values):\\n        # changing timedelta64/datetime64 to int64 needs to happen after\\n        #  finding `mask` above\\n        values = getattr(values, \"asi8\", values)\\n        values = values.view(np.int64)\\n\\n    dtype_ok = _na_ok_dtype(dtype)\\n\\n    # get our fill value (in case we need to provide an alternative\\n    # dtype for it)\\n    fill_value = _get_fill_value(dtype, fill_value=fill_value,\\n                                 fill_value_typ=fill_value_typ)\\n\\n    if skipna:\\n        if copy:\\n            values = values.copy()\\n        if dtype_ok:\\n            np.putmask(values, mask, fill_value)\\n\\n        # promote if needed\\n        else:\\n            values, changed = maybe_upcast_putmask(values, mask, fill_value)\\n\\n    elif copy:\\n        values = values.copy()\\n\\n    # return a platform independent precision dtype\\n    dtype_max = dtype\\n    if is_integer_dtype(dtype) or is_bool_dtype(dtype):\\n        dtype_max = np.int64\\n    elif is_float_dtype(dtype):\\n        dtype_max = np.float64\\n\\n    return values, mask, dtype, dtype_max, fill_value',\n 'def _wrap_results(result, dtype, fill_value=None):\\n    \"\"\" wrap our results if needed \"\"\"\\n\\n    if is_datetime64_dtype(dtype) or is_datetime64tz_dtype(dtype):\\n        if fill_value is None:\\n            # GH#24293\\n            fill_value = iNaT\\n        if not isinstance(result, np.ndarray):\\n            tz = getattr(dtype, \\'tz\\', None)\\n            assert not isna(fill_value), \"Expected non-null fill_value\"\\n            if result == fill_value:\\n                result = np.nan\\n            result = tslibs.Timestamp(result, tz=tz)\\n        else:\\n            result = result.view(dtype)\\n    elif is_timedelta64_dtype(dtype):\\n        if not isinstance(result, np.ndarray):\\n            if result == fill_value:\\n                result = np.nan\\n\\n            # raise if we have a timedelta64[ns] which is too large\\n            if np.fabs(result) > _int64_max:\\n                raise ValueError(\"overflow in timedelta operation\")\\n\\n            result = tslibs.Timedelta(result, unit=\\'ns\\')\\n        else:\\n            result = result.astype(\\'i8\\').view(dtype)\\n\\n    return result',\n 'def _na_for_min_count(values, axis):\\n    \"\"\"Return the missing value for `values`\\n\\n    Parameters\\n    ----------\\n    values : ndarray\\n    axis : int or None\\n        axis for the reduction\\n\\n    Returns\\n    -------\\n    result : scalar or ndarray\\n        For 1-D values, returns a scalar of the correct missing type.\\n        For 2-D values, returns a 1-D array where each element is missing.\\n    \"\"\"\\n    # we either return np.nan or pd.NaT\\n    if is_numeric_dtype(values):\\n        values = values.astype(\\'float64\\')\\n    fill_value = na_value_for_dtype(values.dtype)\\n\\n    if values.ndim == 1:\\n        return fill_value\\n    else:\\n        result_shape = (values.shape[:axis] +\\n                        values.shape[axis + 1:])\\n        result = np.empty(result_shape, dtype=values.dtype)\\n        result.fill(fill_value)\\n        return result',\n 'def nanany(values, axis=None, skipna=True, mask=None):\\n    \"\"\"\\n    Check if any elements along an axis evaluate to True.\\n\\n    Parameters\\n    ----------\\n    values : ndarray\\n    axis : int, optional\\n    skipna : bool, default True\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    -------\\n    result : bool\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, 2])\\n    >>> nanops.nanany(s)\\n    True\\n\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([np.nan])\\n    >>> nanops.nanany(s)\\n    False\\n    \"\"\"\\n    values, mask, dtype, _, _ = _get_values(values, skipna, False, copy=skipna,\\n                                            mask=mask)\\n    return values.any(axis)',\n 'def nanall(values, axis=None, skipna=True, mask=None):\\n    \"\"\"\\n    Check if all elements along an axis evaluate to True.\\n\\n    Parameters\\n    ----------\\n    values : ndarray\\n    axis: int, optional\\n    skipna : bool, default True\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    -------\\n    result : bool\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, 2, np.nan])\\n    >>> nanops.nanall(s)\\n    True\\n\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, 0])\\n    >>> nanops.nanall(s)\\n    False\\n    \"\"\"\\n    values, mask, dtype, _, _ = _get_values(values, skipna, True, copy=skipna,\\n                                            mask=mask)\\n    return values.all(axis)',\n 'def nansum(values, axis=None, skipna=True, min_count=0, mask=None):\\n    \"\"\"\\n    Sum the elements along an axis ignoring NaNs\\n\\n    Parameters\\n    ----------\\n    values : ndarray[dtype]\\n    axis: int, optional\\n    skipna : bool, default True\\n    min_count: int, default 0\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    -------\\n    result : dtype\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, 2, np.nan])\\n    >>> nanops.nansum(s)\\n    3.0\\n    \"\"\"\\n    values, mask, dtype, dtype_max, _ = _get_values(values,\\n                                                    skipna, 0, mask=mask)\\n    dtype_sum = dtype_max\\n    if is_float_dtype(dtype):\\n        dtype_sum = dtype\\n    elif is_timedelta64_dtype(dtype):\\n        dtype_sum = np.float64\\n    the_sum = values.sum(axis, dtype=dtype_sum)\\n    the_sum = _maybe_null_out(the_sum, axis, mask, min_count=min_count)\\n\\n    return _wrap_results(the_sum, dtype)',\n 'def nanmean(values, axis=None, skipna=True, mask=None):\\n    \"\"\"\\n    Compute the mean of the element along an axis ignoring NaNs\\n\\n    Parameters\\n    ----------\\n    values : ndarray\\n    axis: int, optional\\n    skipna : bool, default True\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    -------\\n    result : float\\n        Unless input is a float array, in which case use the same\\n        precision as the input array.\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, 2, np.nan])\\n    >>> nanops.nanmean(s)\\n    1.5\\n    \"\"\"\\n    values, mask, dtype, dtype_max, _ = _get_values(\\n        values, skipna, 0, mask=mask)\\n    dtype_sum = dtype_max\\n    dtype_count = np.float64\\n    if (is_integer_dtype(dtype) or is_timedelta64_dtype(dtype) or\\n            is_datetime64_dtype(dtype) or is_datetime64tz_dtype(dtype)):\\n        dtype_sum = np.float64\\n    elif is_float_dtype(dtype):\\n        dtype_sum = dtype\\n        dtype_count = dtype\\n    count = _get_counts(mask, axis, dtype=dtype_count)\\n    the_sum = _ensure_numeric(values.sum(axis, dtype=dtype_sum))\\n\\n    if axis is not None and getattr(the_sum, \\'ndim\\', False):\\n        with np.errstate(all=\"ignore\"):\\n            # suppress division by zero warnings\\n            the_mean = the_sum / count\\n        ct_mask = count == 0\\n        if ct_mask.any():\\n            the_mean[ct_mask] = np.nan\\n    else:\\n        the_mean = the_sum / count if count > 0 else np.nan\\n\\n    return _wrap_results(the_mean, dtype)',\n 'def nanmedian(values, axis=None, skipna=True, mask=None):\\n    \"\"\"\\n    Parameters\\n    ----------\\n    values : ndarray\\n    axis: int, optional\\n    skipna : bool, default True\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    -------\\n    result : float\\n        Unless input is a float array, in which case use the same\\n        precision as the input array.\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, np.nan, 2, 2])\\n    >>> nanops.nanmedian(s)\\n    2.0\\n    \"\"\"\\n    def get_median(x):\\n        mask = notna(x)\\n        if not skipna and not mask.all():\\n            return np.nan\\n        return np.nanmedian(x[mask])\\n\\n    values, mask, dtype, dtype_max, _ = _get_values(values, skipna, mask=mask)\\n    if not is_float_dtype(values):\\n        values = values.astype(\\'f8\\')\\n        values[mask] = np.nan\\n\\n    if axis is None:\\n        values = values.ravel()\\n\\n    notempty = values.size\\n\\n    # an array from a frame\\n    if values.ndim > 1:\\n\\n        # there\\'s a non-empty array to apply over otherwise numpy raises\\n        if notempty:\\n            if not skipna:\\n                return _wrap_results(\\n                    np.apply_along_axis(get_median, axis, values), dtype)\\n\\n            # fastpath for the skipna case\\n            return _wrap_results(np.nanmedian(values, axis), dtype)\\n\\n        # must return the correct shape, but median is not defined for the\\n        # empty set so return nans of shape \"everything but the passed axis\"\\n        # since \"axis\" is where the reduction would occur if we had a nonempty\\n        # array\\n        shp = np.array(values.shape)\\n        dims = np.arange(values.ndim)\\n        ret = np.empty(shp[dims != axis])\\n        ret.fill(np.nan)\\n        return _wrap_results(ret, dtype)\\n\\n    # otherwise return a scalar value\\n    return _wrap_results(get_median(values) if notempty else np.nan, dtype)',\n 'def nanstd(values, axis=None, skipna=True, ddof=1, mask=None):\\n    \"\"\"\\n    Compute the standard deviation along given axis while ignoring NaNs\\n\\n    Parameters\\n    ----------\\n    values : ndarray\\n    axis: int, optional\\n    skipna : bool, default True\\n    ddof : int, default 1\\n        Delta Degrees of Freedom. The divisor used in calculations is N - ddof,\\n        where N represents the number of elements.\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    -------\\n    result : float\\n        Unless input is a float array, in which case use the same\\n        precision as the input array.\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, np.nan, 2, 3])\\n    >>> nanops.nanstd(s)\\n    1.0\\n    \"\"\"\\n    result = np.sqrt(nanvar(values, axis=axis, skipna=skipna, ddof=ddof,\\n                            mask=mask))\\n    return _wrap_results(result, values.dtype)',\n 'def nanvar(values, axis=None, skipna=True, ddof=1, mask=None):\\n    \"\"\"\\n    Compute the variance along given axis while ignoring NaNs\\n\\n    Parameters\\n    ----------\\n    values : ndarray\\n    axis: int, optional\\n    skipna : bool, default True\\n    ddof : int, default 1\\n        Delta Degrees of Freedom. The divisor used in calculations is N - ddof,\\n        where N represents the number of elements.\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    -------\\n    result : float\\n        Unless input is a float array, in which case use the same\\n        precision as the input array.\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, np.nan, 2, 3])\\n    >>> nanops.nanvar(s)\\n    1.0\\n    \"\"\"\\n    values = com.values_from_object(values)\\n    dtype = values.dtype\\n    if mask is None:\\n        mask = isna(values)\\n    if is_any_int_dtype(values):\\n        values = values.astype(\\'f8\\')\\n        values[mask] = np.nan\\n\\n    if is_float_dtype(values):\\n        count, d = _get_counts_nanvar(mask, axis, ddof, values.dtype)\\n    else:\\n        count, d = _get_counts_nanvar(mask, axis, ddof)\\n\\n    if skipna:\\n        values = values.copy()\\n        np.putmask(values, mask, 0)\\n\\n    # xref GH10242\\n    # Compute variance via two-pass algorithm, which is stable against\\n    # cancellation errors and relatively accurate for small numbers of\\n    # observations.\\n    #\\n    # See https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\\n    avg = _ensure_numeric(values.sum(axis=axis, dtype=np.float64)) / count\\n    if axis is not None:\\n        avg = np.expand_dims(avg, axis)\\n    sqr = _ensure_numeric((avg - values) ** 2)\\n    np.putmask(sqr, mask, 0)\\n    result = sqr.sum(axis=axis, dtype=np.float64) / d\\n\\n    # Return variance as np.float64 (the datatype used in the accumulator),\\n    # unless we were dealing with a float array, in which case use the same\\n    # precision as the original values array.\\n    if is_float_dtype(dtype):\\n        result = result.astype(dtype)\\n    return _wrap_results(result, values.dtype)',\n 'def nansem(values, axis=None, skipna=True, ddof=1, mask=None):\\n    \"\"\"\\n    Compute the standard error in the mean along given axis while ignoring NaNs\\n\\n    Parameters\\n    ----------\\n    values : ndarray\\n    axis: int, optional\\n    skipna : bool, default True\\n    ddof : int, default 1\\n        Delta Degrees of Freedom. The divisor used in calculations is N - ddof,\\n        where N represents the number of elements.\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    -------\\n    result : float64\\n        Unless input is a float array, in which case use the same\\n        precision as the input array.\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, np.nan, 2, 3])\\n    >>> nanops.nansem(s)\\n     0.5773502691896258\\n    \"\"\"\\n\\n    # This checks if non-numeric-like data is passed with numeric_only=False\\n    # and raises a TypeError otherwise\\n    nanvar(values, axis, skipna, ddof=ddof, mask=mask)\\n\\n    if mask is None:\\n        mask = isna(values)\\n    if not is_float_dtype(values.dtype):\\n        values = values.astype(\\'f8\\')\\n    count, _ = _get_counts_nanvar(mask, axis, ddof, values.dtype)\\n    var = nanvar(values, axis, skipna, ddof=ddof)\\n\\n    return np.sqrt(var) / np.sqrt(count)',\n 'def nanargmax(values, axis=None, skipna=True, mask=None):\\n    \"\"\"\\n    Parameters\\n    ----------\\n    values : ndarray\\n    axis: int, optional\\n    skipna : bool, default True\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    --------\\n    result : int\\n        The index of max value in specified axis or -1 in the NA case\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, 2, 3, np.nan, 4])\\n    >>> nanops.nanargmax(s)\\n    4\\n    \"\"\"\\n    values, mask, dtype, _, _ = _get_values(\\n        values, skipna, fill_value_typ=\\'-inf\\', mask=mask)\\n    result = values.argmax(axis)\\n    result = _maybe_arg_null_out(result, axis, mask, skipna)\\n    return result',\n 'def nanargmin(values, axis=None, skipna=True, mask=None):\\n    \"\"\"\\n    Parameters\\n    ----------\\n    values : ndarray\\n    axis: int, optional\\n    skipna : bool, default True\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    --------\\n    result : int\\n        The index of min value in specified axis or -1 in the NA case\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, 2, 3, np.nan, 4])\\n    >>> nanops.nanargmin(s)\\n    0\\n    \"\"\"\\n    values, mask, dtype, _, _ = _get_values(\\n        values, skipna, fill_value_typ=\\'+inf\\', mask=mask)\\n    result = values.argmin(axis)\\n    result = _maybe_arg_null_out(result, axis, mask, skipna)\\n    return result',\n 'def nanskew(values, axis=None, skipna=True, mask=None):\\n    \"\"\" Compute the sample skewness.\\n\\n    The statistic computed here is the adjusted Fisher-Pearson standardized\\n    moment coefficient G1. The algorithm computes this coefficient directly\\n    from the second and third central moment.\\n\\n    Parameters\\n    ----------\\n    values : ndarray\\n    axis: int, optional\\n    skipna : bool, default True\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    -------\\n    result : float64\\n        Unless input is a float array, in which case use the same\\n        precision as the input array.\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1,np.nan, 1, 2])\\n    >>> nanops.nanskew(s)\\n    1.7320508075688787\\n    \"\"\"\\n    values = com.values_from_object(values)\\n    if mask is None:\\n        mask = isna(values)\\n    if not is_float_dtype(values.dtype):\\n        values = values.astype(\\'f8\\')\\n        count = _get_counts(mask, axis)\\n    else:\\n        count = _get_counts(mask, axis, dtype=values.dtype)\\n\\n    if skipna:\\n        values = values.copy()\\n        np.putmask(values, mask, 0)\\n\\n    mean = values.sum(axis, dtype=np.float64) / count\\n    if axis is not None:\\n        mean = np.expand_dims(mean, axis)\\n\\n    adjusted = values - mean\\n    if skipna:\\n        np.putmask(adjusted, mask, 0)\\n    adjusted2 = adjusted ** 2\\n    adjusted3 = adjusted2 * adjusted\\n    m2 = adjusted2.sum(axis, dtype=np.float64)\\n    m3 = adjusted3.sum(axis, dtype=np.float64)\\n\\n    # floating point error\\n    #\\n    # #18044 in _libs/windows.pyx calc_skew follow this behavior\\n    # to fix the fperr to treat m2 <1e-14 as zero\\n    m2 = _zero_out_fperr(m2)\\n    m3 = _zero_out_fperr(m3)\\n\\n    with np.errstate(invalid=\\'ignore\\', divide=\\'ignore\\'):\\n        result = (count * (count - 1) ** 0.5 / (count - 2)) * (m3 / m2 ** 1.5)\\n\\n    dtype = values.dtype\\n    if is_float_dtype(dtype):\\n        result = result.astype(dtype)\\n\\n    if isinstance(result, np.ndarray):\\n        result = np.where(m2 == 0, 0, result)\\n        result[count < 3] = np.nan\\n        return result\\n    else:\\n        result = 0 if m2 == 0 else result\\n        if count < 3:\\n            return np.nan\\n        return result',\n 'def nankurt(values, axis=None, skipna=True, mask=None):\\n    \"\"\"\\n    Compute the sample excess kurtosis\\n\\n    The statistic computed here is the adjusted Fisher-Pearson standardized\\n    moment coefficient G2, computed directly from the second and fourth\\n    central moment.\\n\\n    Parameters\\n    ----------\\n    values : ndarray\\n    axis: int, optional\\n    skipna : bool, default True\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    -------\\n    result : float64\\n        Unless input is a float array, in which case use the same\\n        precision as the input array.\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1,np.nan, 1, 3, 2])\\n    >>> nanops.nankurt(s)\\n    -1.2892561983471076\\n    \"\"\"\\n    values = com.values_from_object(values)\\n    if mask is None:\\n        mask = isna(values)\\n    if not is_float_dtype(values.dtype):\\n        values = values.astype(\\'f8\\')\\n        count = _get_counts(mask, axis)\\n    else:\\n        count = _get_counts(mask, axis, dtype=values.dtype)\\n\\n    if skipna:\\n        values = values.copy()\\n        np.putmask(values, mask, 0)\\n\\n    mean = values.sum(axis, dtype=np.float64) / count\\n    if axis is not None:\\n        mean = np.expand_dims(mean, axis)\\n\\n    adjusted = values - mean\\n    if skipna:\\n        np.putmask(adjusted, mask, 0)\\n    adjusted2 = adjusted ** 2\\n    adjusted4 = adjusted2 ** 2\\n    m2 = adjusted2.sum(axis, dtype=np.float64)\\n    m4 = adjusted4.sum(axis, dtype=np.float64)\\n\\n    with np.errstate(invalid=\\'ignore\\', divide=\\'ignore\\'):\\n        adj = 3 * (count - 1) ** 2 / ((count - 2) * (count - 3))\\n        numer = count * (count + 1) * (count - 1) * m4\\n        denom = (count - 2) * (count - 3) * m2 ** 2\\n\\n    # floating point error\\n    #\\n    # #18044 in _libs/windows.pyx calc_kurt follow this behavior\\n    # to fix the fperr to treat denom <1e-14 as zero\\n    numer = _zero_out_fperr(numer)\\n    denom = _zero_out_fperr(denom)\\n\\n    if not isinstance(denom, np.ndarray):\\n        # if ``denom`` is a scalar, check these corner cases first before\\n        # doing division\\n        if count < 4:\\n            return np.nan\\n        if denom == 0:\\n            return 0\\n\\n    with np.errstate(invalid=\\'ignore\\', divide=\\'ignore\\'):\\n        result = numer / denom - adj\\n\\n    dtype = values.dtype\\n    if is_float_dtype(dtype):\\n        result = result.astype(dtype)\\n\\n    if isinstance(result, np.ndarray):\\n        result = np.where(denom == 0, 0, result)\\n        result[count < 4] = np.nan\\n\\n    return result',\n 'def nanprod(values, axis=None, skipna=True, min_count=0, mask=None):\\n    \"\"\"\\n    Parameters\\n    ----------\\n    values : ndarray[dtype]\\n    axis: int, optional\\n    skipna : bool, default True\\n    min_count: int, default 0\\n    mask : ndarray[bool], optional\\n        nan-mask if known\\n\\n    Returns\\n    -------\\n    result : dtype\\n\\n    Examples\\n    --------\\n    >>> import pandas.core.nanops as nanops\\n    >>> s = pd.Series([1, 2, 3, np.nan])\\n    >>> nanops.nanprod(s)\\n    6.0\\n\\n    Returns\\n    --------\\n    The product of all elements on a given axis. ( NaNs are treated as 1)\\n    \"\"\"\\n    if mask is None:\\n        mask = isna(values)\\n    if skipna and not is_any_int_dtype(values):\\n        values = values.copy()\\n        values[mask] = 1\\n    result = values.prod(axis)\\n    return _maybe_null_out(result, axis, mask, min_count=min_count)',\n 'def nancorr(a, b, method=\\'pearson\\', min_periods=None):\\n    \"\"\"\\n    a, b: ndarrays\\n    \"\"\"\\n    if len(a) != len(b):\\n        raise AssertionError(\\'Operands to nancorr must have same size\\')\\n\\n    if min_periods is None:\\n        min_periods = 1\\n\\n    valid = notna(a) & notna(b)\\n    if not valid.all():\\n        a = a[valid]\\n        b = b[valid]\\n\\n    if len(a) < min_periods:\\n        return np.nan\\n\\n    f = get_corr_func(method)\\n    return f(a, b)',\n 'def _nanpercentile_1d(values, mask, q, na_value, interpolation):\\n    \"\"\"\\n    Wraper for np.percentile that skips missing values, specialized to\\n    1-dimensional case.\\n\\n    Parameters\\n    ----------\\n    values : array over which to find quantiles\\n    mask : ndarray[bool]\\n        locations in values that should be considered missing\\n    q : scalar or array of quantile indices to find\\n    na_value : scalar\\n        value to return for empty or all-null values\\n    interpolation : str\\n\\n    Returns\\n    -------\\n    quantiles : scalar or array\\n    \"\"\"\\n    # mask is Union[ExtensionArray, ndarray]\\n    values = values[~mask]\\n\\n    if len(values) == 0:\\n        if lib.is_scalar(q):\\n            return na_value\\n        else:\\n            return np.array([na_value] * len(q),\\n                            dtype=values.dtype)\\n\\n    return np.percentile(values, q, interpolation=interpolation)',\n 'def nanpercentile(values, q, axis, na_value, mask, ndim, interpolation):\\n    \"\"\"\\n    Wraper for np.percentile that skips missing values.\\n\\n    Parameters\\n    ----------\\n    values : array over which to find quantiles\\n    q : scalar or array of quantile indices to find\\n    axis : {0, 1}\\n    na_value : scalar\\n        value to return for empty or all-null values\\n    mask : ndarray[bool]\\n        locations in values that should be considered missing\\n    ndim : {1, 2}\\n    interpolation : str\\n\\n    Returns\\n    -------\\n    quantiles : scalar or array\\n    \"\"\"\\n    if not lib.is_scalar(mask) and mask.any():\\n        if ndim == 1:\\n            return _nanpercentile_1d(values, mask, q, na_value,\\n                                     interpolation=interpolation)\\n        else:\\n            # for nonconsolidatable blocks mask is 1D, but values 2D\\n            if mask.ndim < values.ndim:\\n                mask = mask.reshape(values.shape)\\n            if axis == 0:\\n                values = values.T\\n                mask = mask.T\\n            result = [_nanpercentile_1d(val, m, q, na_value,\\n                                        interpolation=interpolation)\\n                      for (val, m) in zip(list(values), list(mask))]\\n            result = np.array(result, dtype=values.dtype, copy=False).T\\n            return result\\n    else:\\n        return np.percentile(values, q, axis=axis, interpolation=interpolation)',\n 'def write_th(self, s, header=False, indent=0, tags=None):\\n        \"\"\"\\n        Method for writting a formatted <th> cell.\\n\\n        If col_space is set on the formatter then that is used for\\n        the value of min-width.\\n\\n        Parameters\\n        ----------\\n        s : object\\n            The data to be written inside the cell.\\n        header : boolean, default False\\n            Set to True if the <th> is for use inside <thead>.  This will\\n            cause min-width to be set if there is one.\\n        indent : int, default 0\\n            The indentation level of the cell.\\n        tags : string, default None\\n            Tags to include in the cell.\\n\\n        Returns\\n        -------\\n        A written <th> cell.\\n        \"\"\"\\n        if header and self.fmt.col_space is not None:\\n            tags = (tags or \"\")\\n            tags += (\\'style=\"min-width: {colspace};\"\\'\\n                     .format(colspace=self.fmt.col_space))\\n\\n        return self._write_cell(s, kind=\\'th\\', indent=indent, tags=tags)',\n 'def read_clipboard(sep=r\\'\\\\s+\\', **kwargs):  # pragma: no cover\\n    r\"\"\"\\n    Read text from clipboard and pass to read_csv. See read_csv for the\\n    full argument list\\n\\n    Parameters\\n    ----------\\n    sep : str, default \\'\\\\s+\\'\\n        A string or regex delimiter. The default of \\'\\\\s+\\' denotes\\n        one or more whitespace characters.\\n\\n    Returns\\n    -------\\n    parsed : DataFrame\\n    \"\"\"\\n    encoding = kwargs.pop(\\'encoding\\', \\'utf-8\\')\\n\\n    # only utf-8 is valid for passed value because that\\'s what clipboard\\n    # supports\\n    if encoding is not None and encoding.lower().replace(\\'-\\', \\'\\') != \\'utf8\\':\\n        raise NotImplementedError(\\n            \\'reading from clipboard only supports utf-8 encoding\\')\\n\\n    from pandas.io.clipboard import clipboard_get\\n    from pandas.io.parsers import read_csv\\n    text = clipboard_get()\\n\\n    # Try to decode (if needed, as \"text\" might already be a string here).\\n    try:\\n        text = text.decode(kwargs.get(\\'encoding\\')\\n                           or get_option(\\'display.encoding\\'))\\n    except AttributeError:\\n        pass\\n\\n    # Excel copies into clipboard with \\\\t separation\\n    # inspect no more then the 10 first lines, if they\\n    # all contain an equal number (>0) of tabs, infer\\n    # that this came from excel and set \\'sep\\' accordingly\\n    lines = text[:10000].split(\\'\\\\n\\')[:-1][:10]\\n\\n    # Need to remove leading white space, since read_csv\\n    # accepts:\\n    #    a  b\\n    # 0  1  2\\n    # 1  3  4\\n\\n    counts = {x.lstrip().count(\\'\\\\t\\') for x in lines}\\n    if len(lines) > 1 and len(counts) == 1 and counts.pop() != 0:\\n        sep = \\'\\\\t\\'\\n\\n    # Edge case where sep is specified to be None, return to default\\n    if sep is None and kwargs.get(\\'delim_whitespace\\') is None:\\n        sep = r\\'\\\\s+\\'\\n\\n    # Regex separator currently only works with python engine.\\n    # Default to python if separator is multi-character (regex)\\n    if len(sep) > 1 and kwargs.get(\\'engine\\') is None:\\n        kwargs[\\'engine\\'] = \\'python\\'\\n    elif len(sep) > 1 and kwargs.get(\\'engine\\') == \\'c\\':\\n        warnings.warn(\\'read_clipboard with regex separator does not work\\'\\n                      \\' properly with c engine\\')\\n\\n    return read_csv(StringIO(text), sep=sep, **kwargs)',\n 'def to_clipboard(obj, excel=True, sep=None, **kwargs):  # pragma: no cover\\n    \"\"\"\\n    Attempt to write text representation of object to the system clipboard\\n    The clipboard can be then pasted into Excel for example.\\n\\n    Parameters\\n    ----------\\n    obj : the object to write to the clipboard\\n    excel : boolean, defaults to True\\n            if True, use the provided separator, writing in a csv\\n            format for allowing easy pasting into excel.\\n            if False, write a string representation of the object\\n            to the clipboard\\n    sep : optional, defaults to tab\\n    other keywords are passed to to_csv\\n\\n    Notes\\n    -----\\n    Requirements for your platform\\n      - Linux: xclip, or xsel (with gtk or PyQt4 modules)\\n      - Windows:\\n      - OS X:\\n    \"\"\"\\n    encoding = kwargs.pop(\\'encoding\\', \\'utf-8\\')\\n\\n    # testing if an invalid encoding is passed to clipboard\\n    if encoding is not None and encoding.lower().replace(\\'-\\', \\'\\') != \\'utf8\\':\\n        raise ValueError(\\'clipboard only supports utf-8 encoding\\')\\n\\n    from pandas.io.clipboard import clipboard_set\\n    if excel is None:\\n        excel = True\\n\\n    if excel:\\n        try:\\n            if sep is None:\\n                sep = \\'\\\\t\\'\\n            buf = StringIO()\\n\\n            # clipboard_set (pyperclip) expects unicode\\n            obj.to_csv(buf, sep=sep, encoding=\\'utf-8\\', **kwargs)\\n            text = buf.getvalue()\\n\\n            clipboard_set(text)\\n            return\\n        except TypeError:\\n            warnings.warn(\\'to_clipboard in excel mode requires a single \\'\\n                          \\'character separator.\\')\\n    elif sep is not None:\\n        warnings.warn(\\'to_clipboard with excel=False ignores the sep argument\\')\\n\\n    if isinstance(obj, ABCDataFrame):\\n        # str(df) has various unhelpful defaults, like truncation\\n        with option_context(\\'display.max_colwidth\\', 999999):\\n            objstr = obj.to_string(**kwargs)\\n    else:\\n        objstr = str(obj)\\n    clipboard_set(objstr)',\n 'def _get_skiprows(skiprows):\\n    \"\"\"Get an iterator given an integer, slice or container.\\n\\n    Parameters\\n    ----------\\n    skiprows : int, slice, container\\n        The iterator to use to skip rows; can also be a slice.\\n\\n    Raises\\n    ------\\n    TypeError\\n        * If `skiprows` is not a slice, integer, or Container\\n\\n    Returns\\n    -------\\n    it : iterable\\n        A proper iterator to use to skip rows of a DataFrame.\\n    \"\"\"\\n    if isinstance(skiprows, slice):\\n        return lrange(skiprows.start or 0, skiprows.stop, skiprows.step or 1)\\n    elif isinstance(skiprows, numbers.Integral) or is_list_like(skiprows):\\n        return skiprows\\n    elif skiprows is None:\\n        return 0\\n    raise TypeError(\\'%r is not a valid type for skipping rows\\' %\\n                    type(skiprows).__name__)',\n 'def _read(obj):\\n    \"\"\"Try to read from a url, file or string.\\n\\n    Parameters\\n    ----------\\n    obj : str, unicode, or file-like\\n\\n    Returns\\n    -------\\n    raw_text : str\\n    \"\"\"\\n    if _is_url(obj):\\n        with urlopen(obj) as url:\\n            text = url.read()\\n    elif hasattr(obj, \\'read\\'):\\n        text = obj.read()\\n    elif isinstance(obj, (str, bytes)):\\n        text = obj\\n        try:\\n            if os.path.isfile(text):\\n                with open(text, \\'rb\\') as f:\\n                    return f.read()\\n        except (TypeError, ValueError):\\n            pass\\n    else:\\n        raise TypeError(\"Cannot read object of type %r\" % type(obj).__name__)\\n    return text',\n 'def _build_xpath_expr(attrs):\\n    \"\"\"Build an xpath expression to simulate bs4\\'s ability to pass in kwargs to\\n    search for attributes when using the lxml parser.\\n\\n    Parameters\\n    ----------\\n    attrs : dict\\n        A dict of HTML attributes. These are NOT checked for validity.\\n\\n    Returns\\n    -------\\n    expr : unicode\\n        An XPath expression that checks for the given HTML attributes.\\n    \"\"\"\\n    # give class attribute as class_ because class is a python keyword\\n    if \\'class_\\' in attrs:\\n        attrs[\\'class\\'] = attrs.pop(\\'class_\\')\\n\\n    s = [\"@{key}={val!r}\".format(key=k, val=v) for k, v in attrs.items()]\\n    return \\'[{expr}]\\'.format(expr=\\' and \\'.join(s))',\n 'def _parser_dispatch(flavor):\\n    \"\"\"Choose the parser based on the input flavor.\\n\\n    Parameters\\n    ----------\\n    flavor : str\\n        The type of parser to use. This must be a valid backend.\\n\\n    Returns\\n    -------\\n    cls : _HtmlFrameParser subclass\\n        The parser class based on the requested input flavor.\\n\\n    Raises\\n    ------\\n    ValueError\\n        * If `flavor` is not a valid backend.\\n    ImportError\\n        * If you do not have the requested `flavor`\\n    \"\"\"\\n    valid_parsers = list(_valid_parsers.keys())\\n    if flavor not in valid_parsers:\\n        raise ValueError(\\'{invalid!r} is not a valid flavor, valid flavors \\'\\n                         \\'are {valid}\\'\\n                         .format(invalid=flavor, valid=valid_parsers))\\n\\n    if flavor in (\\'bs4\\', \\'html5lib\\'):\\n        if not _HAS_HTML5LIB:\\n            raise ImportError(\"html5lib not found, please install it\")\\n        if not _HAS_BS4:\\n            raise ImportError(\\n                \"BeautifulSoup4 (bs4) not found, please install it\")\\n        import bs4\\n        if LooseVersion(bs4.__version__) <= LooseVersion(\\'4.2.0\\'):\\n            raise ValueError(\"A minimum version of BeautifulSoup 4.2.1 \"\\n                             \"is required\")\\n\\n    else:\\n        if not _HAS_LXML:\\n            raise ImportError(\"lxml not found, please install it\")\\n    return _valid_parsers[flavor]',\n 'def read_html(io, match=\\'.+\\', flavor=None, header=None, index_col=None,\\n              skiprows=None, attrs=None, parse_dates=False,\\n              tupleize_cols=None, thousands=\\',\\', encoding=None,\\n              decimal=\\'.\\', converters=None, na_values=None,\\n              keep_default_na=True, displayed_only=True):\\n    r\"\"\"Read HTML tables into a ``list`` of ``DataFrame`` objects.\\n\\n    Parameters\\n    ----------\\n    io : str or file-like\\n        A URL, a file-like object, or a raw string containing HTML. Note that\\n        lxml only accepts the http, ftp and file url protocols. If you have a\\n        URL that starts with ``\\'https\\'`` you might try removing the ``\\'s\\'``.\\n\\n    match : str or compiled regular expression, optional\\n        The set of tables containing text matching this regex or string will be\\n        returned. Unless the HTML is extremely simple you will probably need to\\n        pass a non-empty string here. Defaults to \\'.+\\' (match any non-empty\\n        string). The default value will return all tables contained on a page.\\n        This value is converted to a regular expression so that there is\\n        consistent behavior between Beautiful Soup and lxml.\\n\\n    flavor : str or None, container of strings\\n        The parsing engine to use. \\'bs4\\' and \\'html5lib\\' are synonymous with\\n        each other, they are both there for backwards compatibility. The\\n        default of ``None`` tries to use ``lxml`` to parse and if that fails it\\n        falls back on ``bs4`` + ``html5lib``.\\n\\n    header : int or list-like or None, optional\\n        The row (or list of rows for a :class:`~pandas.MultiIndex`) to use to\\n        make the columns headers.\\n\\n    index_col : int or list-like or None, optional\\n        The column (or list of columns) to use to create the index.\\n\\n    skiprows : int or list-like or slice or None, optional\\n        0-based. Number of rows to skip after parsing the column integer. If a\\n        sequence of integers or a slice is given, will skip the rows indexed by\\n        that sequence.  Note that a single element sequence means \\'skip the nth\\n        row\\' whereas an integer means \\'skip n rows\\'.\\n\\n    attrs : dict or None, optional\\n        This is a dictionary of attributes that you can pass to use to identify\\n        the table in the HTML. These are not checked for validity before being\\n        passed to lxml or Beautiful Soup. However, these attributes must be\\n        valid HTML table attributes to work correctly. For example, ::\\n\\n            attrs = {\\'id\\': \\'table\\'}\\n\\n        is a valid attribute dictionary because the \\'id\\' HTML tag attribute is\\n        a valid HTML attribute for *any* HTML tag as per `this document\\n        <http://www.w3.org/TR/html-markup/global-attributes.html>`__. ::\\n\\n            attrs = {\\'asdf\\': \\'table\\'}\\n\\n        is *not* a valid attribute dictionary because \\'asdf\\' is not a valid\\n        HTML attribute even if it is a valid XML attribute.  Valid HTML 4.01\\n        table attributes can be found `here\\n        <http://www.w3.org/TR/REC-html40/struct/tables.html#h-11.2>`__. A\\n        working draft of the HTML 5 spec can be found `here\\n        <http://www.w3.org/TR/html-markup/table.html>`__. It contains the\\n        latest information on table attributes for the modern web.\\n\\n    parse_dates : bool, optional\\n        See :func:`~read_csv` for more details.\\n\\n    tupleize_cols : bool, optional\\n        If ``False`` try to parse multiple header rows into a\\n        :class:`~pandas.MultiIndex`, otherwise return raw tuples. Defaults to\\n        ``False``.\\n\\n        .. deprecated:: 0.21.0\\n           This argument will be removed and will always convert to MultiIndex\\n\\n    thousands : str, optional\\n        Separator to use to parse thousands. Defaults to ``\\',\\'``.\\n\\n    encoding : str or None, optional\\n        The encoding used to decode the web page. Defaults to ``None``.``None``\\n        preserves the previous encoding behavior, which depends on the\\n        underlying parser library (e.g., the parser library will try to use\\n        the encoding provided by the document).\\n\\n    decimal : str, default \\'.\\'\\n        Character to recognize as decimal point (e.g. use \\',\\' for European\\n        data).\\n\\n        .. versionadded:: 0.19.0\\n\\n    converters : dict, default None\\n        Dict of functions for converting values in certain columns. Keys can\\n        either be integers or column labels, values are functions that take one\\n        input argument, the cell (not column) content, and return the\\n        transformed content.\\n\\n        .. versionadded:: 0.19.0\\n\\n    na_values : iterable, default None\\n        Custom NA values\\n\\n        .. versionadded:: 0.19.0\\n\\n    keep_default_na : bool, default True\\n        If na_values are specified and keep_default_na is False the default NaN\\n        values are overridden, otherwise they\\'re appended to\\n\\n        .. versionadded:: 0.19.0\\n\\n    displayed_only : bool, default True\\n        Whether elements with \"display: none\" should be parsed\\n\\n        .. versionadded:: 0.23.0\\n\\n    Returns\\n    -------\\n    dfs : list of DataFrames\\n\\n    See Also\\n    --------\\n    read_csv\\n\\n    Notes\\n    -----\\n    Before using this function you should read the :ref:`gotchas about the\\n    HTML parsing libraries <io.html.gotchas>`.\\n\\n    Expect to do some cleanup after you call this function. For example, you\\n    might need to manually assign column names if the column names are\\n    converted to NaN when you pass the `header=0` argument. We try to assume as\\n    little as possible about the structure of the table and push the\\n    idiosyncrasies of the HTML contained in the table to the user.\\n\\n    This function searches for ``<table>`` elements and only for ``<tr>``\\n    and ``<th>`` rows and ``<td>`` elements within each ``<tr>`` or ``<th>``\\n    element in the table. ``<td>`` stands for \"table data\". This function\\n    attempts to properly handle ``colspan`` and ``rowspan`` attributes.\\n    If the function has a ``<thead>`` argument, it is used to construct\\n    the header, otherwise the function attempts to find the header within\\n    the body (by putting rows with only ``<th>`` elements into the header).\\n\\n        .. versionadded:: 0.21.0\\n\\n    Similar to :func:`~read_csv` the `header` argument is applied\\n    **after** `skiprows` is applied.\\n\\n    This function will *always* return a list of :class:`DataFrame` *or*\\n    it will fail, e.g., it will *not* return an empty list.\\n\\n    Examples\\n    --------\\n    See the :ref:`read_html documentation in the IO section of the docs\\n    <io.read_html>` for some examples of reading in HTML tables.\\n    \"\"\"\\n    _importers()\\n\\n    # Type check here. We don\\'t want to parse only to fail because of an\\n    # invalid value of an integer skiprows.\\n    if isinstance(skiprows, numbers.Integral) and skiprows < 0:\\n        raise ValueError(\\'cannot skip rows starting from the end of the \\'\\n                         \\'data (you passed a negative value)\\')\\n    _validate_header_arg(header)\\n    return _parse(flavor=flavor, io=io, match=match, header=header,\\n                  index_col=index_col, skiprows=skiprows,\\n                  parse_dates=parse_dates, tupleize_cols=tupleize_cols,\\n                  thousands=thousands, attrs=attrs, encoding=encoding,\\n                  decimal=decimal, converters=converters, na_values=na_values,\\n                  keep_default_na=keep_default_na,\\n                  displayed_only=displayed_only)',\n 'def parse_tables(self):\\n        \"\"\"\\n        Parse and return all tables from the DOM.\\n\\n        Returns\\n        -------\\n        list of parsed (header, body, footer) tuples from tables.\\n        \"\"\"\\n        tables = self._parse_tables(self._build_doc(), self.match, self.attrs)\\n        return (self._parse_thead_tbody_tfoot(table) for table in tables)',\n 'def _parse_thead_tbody_tfoot(self, table_html):\\n        \"\"\"\\n        Given a table, return parsed header, body, and foot.\\n\\n        Parameters\\n        ----------\\n        table_html : node-like\\n\\n        Returns\\n        -------\\n        tuple of (header, body, footer), each a list of list-of-text rows.\\n\\n        Notes\\n        -----\\n        Header and body are lists-of-lists. Top level list is a list of\\n        rows. Each row is a list of str text.\\n\\n        Logic: Use <thead>, <tbody>, <tfoot> elements to identify\\n               header, body, and footer, otherwise:\\n               - Put all rows into body\\n               - Move rows from top of body to header only if\\n                 all elements inside row are <th>\\n               - Move rows from bottom of body to footer only if\\n                 all elements inside row are <th>\\n        \"\"\"\\n\\n        header_rows = self._parse_thead_tr(table_html)\\n        body_rows = self._parse_tbody_tr(table_html)\\n        footer_rows = self._parse_tfoot_tr(table_html)\\n\\n        def row_is_all_th(row):\\n            return all(self._equals_tag(t, \\'th\\') for t in\\n                       self._parse_td(row))\\n\\n        if not header_rows:\\n            # The table has no <thead>. Move the top all-<th> rows from\\n            # body_rows to header_rows. (This is a common case because many\\n            # tables in the wild have no <thead> or <tfoot>\\n            while body_rows and row_is_all_th(body_rows[0]):\\n                header_rows.append(body_rows.pop(0))\\n\\n        header = self._expand_colspan_rowspan(header_rows)\\n        body = self._expand_colspan_rowspan(body_rows)\\n        footer = self._expand_colspan_rowspan(footer_rows)\\n\\n        return header, body, footer',\n 'def _expand_colspan_rowspan(self, rows):\\n        \"\"\"\\n        Given a list of <tr>s, return a list of text rows.\\n\\n        Parameters\\n        ----------\\n        rows : list of node-like\\n            List of <tr>s\\n\\n        Returns\\n        -------\\n        list of list\\n            Each returned row is a list of str text.\\n\\n        Notes\\n        -----\\n        Any cell with ``rowspan`` or ``colspan`` will have its contents copied\\n        to subsequent cells.\\n        \"\"\"\\n\\n        all_texts = []  # list of rows, each a list of str\\n        remainder = []  # list of (index, text, nrows)\\n\\n        for tr in rows:\\n            texts = []  # the output for this row\\n            next_remainder = []\\n\\n            index = 0\\n            tds = self._parse_td(tr)\\n            for td in tds:\\n                # Append texts from previous rows with rowspan>1 that come\\n                # before this <td>\\n                while remainder and remainder[0][0] <= index:\\n                    prev_i, prev_text, prev_rowspan = remainder.pop(0)\\n                    texts.append(prev_text)\\n                    if prev_rowspan > 1:\\n                        next_remainder.append((prev_i, prev_text,\\n                                               prev_rowspan - 1))\\n                    index += 1\\n\\n                # Append the text from this <td>, colspan times\\n                text = _remove_whitespace(self._text_getter(td))\\n                rowspan = int(self._attr_getter(td, \\'rowspan\\') or 1)\\n                colspan = int(self._attr_getter(td, \\'colspan\\') or 1)\\n\\n                for _ in range(colspan):\\n                    texts.append(text)\\n                    if rowspan > 1:\\n                        next_remainder.append((index, text, rowspan - 1))\\n                    index += 1\\n\\n            # Append texts from previous rows at the final position\\n            for prev_i, prev_text, prev_rowspan in remainder:\\n                texts.append(prev_text)\\n                if prev_rowspan > 1:\\n                    next_remainder.append((prev_i, prev_text,\\n                                           prev_rowspan - 1))\\n\\n            all_texts.append(texts)\\n            remainder = next_remainder\\n\\n        # Append rows that only appear because the previous row had non-1\\n        # rowspan\\n        while remainder:\\n            next_remainder = []\\n            texts = []\\n            for prev_i, prev_text, prev_rowspan in remainder:\\n                texts.append(prev_text)\\n                if prev_rowspan > 1:\\n                    next_remainder.append((prev_i, prev_text,\\n                                           prev_rowspan - 1))\\n            all_texts.append(texts)\\n            remainder = next_remainder\\n\\n        return all_texts',\n 'def _handle_hidden_tables(self, tbl_list, attr_name):\\n        \"\"\"\\n        Return list of tables, potentially removing hidden elements\\n\\n        Parameters\\n        ----------\\n        tbl_list : list of node-like\\n            Type of list elements will vary depending upon parser used\\n        attr_name : str\\n            Name of the accessor for retrieving HTML attributes\\n\\n        Returns\\n        -------\\n        list of node-like\\n            Return type matches `tbl_list`\\n        \"\"\"\\n        if not self.displayed_only:\\n            return tbl_list\\n\\n        return [x for x in tbl_list if \"display:none\" not in\\n                getattr(x, attr_name).get(\\'style\\', \\'\\').replace(\" \", \"\")]',\n 'def _build_doc(self):\\n        \"\"\"\\n        Raises\\n        ------\\n        ValueError\\n            * If a URL that lxml cannot parse is passed.\\n\\n        Exception\\n            * Any other ``Exception`` thrown. For example, trying to parse a\\n              URL that is syntactically correct on a machine with no internet\\n              connection will fail.\\n\\n        See Also\\n        --------\\n        pandas.io.html._HtmlFrameParser._build_doc\\n        \"\"\"\\n        from lxml.html import parse, fromstring, HTMLParser\\n        from lxml.etree import XMLSyntaxError\\n        parser = HTMLParser(recover=True, encoding=self.encoding)\\n\\n        try:\\n            if _is_url(self.io):\\n                with urlopen(self.io) as f:\\n                    r = parse(f, parser=parser)\\n            else:\\n                # try to parse the input in the simplest way\\n                r = parse(self.io, parser=parser)\\n            try:\\n                r = r.getroot()\\n            except AttributeError:\\n                pass\\n        except (UnicodeDecodeError, IOError) as e:\\n            # if the input is a blob of html goop\\n            if not _is_url(self.io):\\n                r = fromstring(self.io, parser=parser)\\n\\n                try:\\n                    r = r.getroot()\\n                except AttributeError:\\n                    pass\\n            else:\\n                raise e\\n        else:\\n            if not hasattr(r, \\'text_content\\'):\\n                raise XMLSyntaxError(\"no text parsed from document\", 0, 0, 0)\\n        return r',\n 'def get_dtype_kinds(l):\\n    \"\"\"\\n    Parameters\\n    ----------\\n    l : list of arrays\\n\\n    Returns\\n    -------\\n    a set of kinds that exist in this list of arrays\\n    \"\"\"\\n\\n    typs = set()\\n    for arr in l:\\n\\n        dtype = arr.dtype\\n        if is_categorical_dtype(dtype):\\n            typ = \\'category\\'\\n        elif is_sparse(arr):\\n            typ = \\'sparse\\'\\n        elif isinstance(arr, ABCRangeIndex):\\n            typ = \\'range\\'\\n        elif is_datetime64tz_dtype(arr):\\n            # if to_concat contains different tz,\\n            # the result must be object dtype\\n            typ = str(arr.dtype)\\n        elif is_datetime64_dtype(dtype):\\n            typ = \\'datetime\\'\\n        elif is_timedelta64_dtype(dtype):\\n            typ = \\'timedelta\\'\\n        elif is_object_dtype(dtype):\\n            typ = \\'object\\'\\n        elif is_bool_dtype(dtype):\\n            typ = \\'bool\\'\\n        elif is_extension_array_dtype(dtype):\\n            typ = str(arr.dtype)\\n        else:\\n            typ = dtype.kind\\n        typs.add(typ)\\n    return typs',\n 'def _get_series_result_type(result, objs=None):\\n    \"\"\"\\n    return appropriate class of Series concat\\n    input is either dict or array-like\\n    \"\"\"\\n    from pandas import SparseSeries, SparseDataFrame, DataFrame\\n\\n    # concat Series with axis 1\\n    if isinstance(result, dict):\\n        # concat Series with axis 1\\n        if all(isinstance(c, (SparseSeries, SparseDataFrame))\\n               for c in result.values()):\\n            return SparseDataFrame\\n        else:\\n            return DataFrame\\n\\n    # otherwise it is a SingleBlockManager (axis = 0)\\n    if result._block.is_sparse:\\n        return SparseSeries\\n    else:\\n        return objs[0]._constructor',\n 'def _get_frame_result_type(result, objs):\\n    \"\"\"\\n    return appropriate class of DataFrame-like concat\\n    if all blocks are sparse, return SparseDataFrame\\n    otherwise, return 1st obj\\n    \"\"\"\\n\\n    if (result.blocks and (\\n            any(isinstance(obj, ABCSparseDataFrame) for obj in objs))):\\n        from pandas.core.sparse.api import SparseDataFrame\\n        return SparseDataFrame\\n    else:\\n        return next(obj for obj in objs if not isinstance(obj,\\n                                                          ABCSparseDataFrame))',\n 'def _concat_compat(to_concat, axis=0):\\n    \"\"\"\\n    provide concatenation of an array of arrays each of which is a single\\n    \\'normalized\\' dtypes (in that for example, if it\\'s object, then it is a\\n    non-datetimelike and provide a combined dtype for the resulting array that\\n    preserves the overall dtype if possible)\\n\\n    Parameters\\n    ----------\\n    to_concat : array of arrays\\n    axis : axis to provide concatenation\\n\\n    Returns\\n    -------\\n    a single array, preserving the combined dtypes\\n    \"\"\"\\n\\n    # filter empty arrays\\n    # 1-d dtypes always are included here\\n    def is_nonempty(x):\\n        try:\\n            return x.shape[axis] > 0\\n        except Exception:\\n            return True\\n\\n    # If all arrays are empty, there\\'s nothing to convert, just short-cut to\\n    # the concatenation, #3121.\\n    #\\n    # Creating an empty array directly is tempting, but the winnings would be\\n    # marginal given that it would still require shape & dtype calculation and\\n    # np.concatenate which has them both implemented is compiled.\\n\\n    typs = get_dtype_kinds(to_concat)\\n    _contains_datetime = any(typ.startswith(\\'datetime\\') for typ in typs)\\n    _contains_period = any(typ.startswith(\\'period\\') for typ in typs)\\n\\n    if \\'category\\' in typs:\\n        # this must be priort to _concat_datetime,\\n        # to support Categorical + datetime-like\\n        return _concat_categorical(to_concat, axis=axis)\\n\\n    elif _contains_datetime or \\'timedelta\\' in typs or _contains_period:\\n        return _concat_datetime(to_concat, axis=axis, typs=typs)\\n\\n    # these are mandated to handle empties as well\\n    elif \\'sparse\\' in typs:\\n        return _concat_sparse(to_concat, axis=axis, typs=typs)\\n\\n    all_empty = all(not is_nonempty(x) for x in to_concat)\\n    if any(is_extension_array_dtype(x) for x in to_concat) and axis == 1:\\n        to_concat = [np.atleast_2d(x.astype(\\'object\\')) for x in to_concat]\\n\\n    if all_empty:\\n        # we have all empties, but may need to coerce the result dtype to\\n        # object if we have non-numeric type operands (numpy would otherwise\\n        # cast this to float)\\n        typs = get_dtype_kinds(to_concat)\\n        if len(typs) != 1:\\n\\n            if (not len(typs - {\\'i\\', \\'u\\', \\'f\\'}) or\\n                    not len(typs - {\\'bool\\', \\'i\\', \\'u\\'})):\\n                # let numpy coerce\\n                pass\\n            else:\\n                # coerce to object\\n                to_concat = [x.astype(\\'object\\') for x in to_concat]\\n\\n    return np.concatenate(to_concat, axis=axis)',\n 'def _concat_categorical(to_concat, axis=0):\\n    \"\"\"Concatenate an object/categorical array of arrays, each of which is a\\n    single dtype\\n\\n    Parameters\\n    ----------\\n    to_concat : array of arrays\\n    axis : int\\n        Axis to provide concatenation in the current implementation this is\\n        always 0, e.g. we only have 1D categoricals\\n\\n    Returns\\n    -------\\n    Categorical\\n        A single array, preserving the combined dtypes\\n    \"\"\"\\n\\n    # we could have object blocks and categoricals here\\n    # if we only have a single categoricals then combine everything\\n    # else its a non-compat categorical\\n    categoricals = [x for x in to_concat if is_categorical_dtype(x.dtype)]\\n\\n    # validate the categories\\n    if len(categoricals) != len(to_concat):\\n        pass\\n    else:\\n        # when all categories are identical\\n        first = to_concat[0]\\n        if all(first.is_dtype_equal(other) for other in to_concat[1:]):\\n            return union_categoricals(categoricals)\\n\\n    # extract the categoricals & coerce to object if needed\\n    to_concat = [x.get_values() if is_categorical_dtype(x.dtype)\\n                 else np.asarray(x).ravel() if not is_datetime64tz_dtype(x)\\n                 else np.asarray(x.astype(object)) for x in to_concat]\\n    result = _concat_compat(to_concat)\\n    if axis == 1:\\n        result = result.reshape(1, len(result))\\n    return result',\n 'def union_categoricals(to_union, sort_categories=False, ignore_order=False):\\n    \"\"\"\\n    Combine list-like of Categorical-like, unioning categories. All\\n    categories must have the same dtype.\\n\\n    .. versionadded:: 0.19.0\\n\\n    Parameters\\n    ----------\\n    to_union : list-like of Categorical, CategoricalIndex,\\n               or Series with dtype=\\'category\\'\\n    sort_categories : boolean, default False\\n        If true, resulting categories will be lexsorted, otherwise\\n        they will be ordered as they appear in the data.\\n    ignore_order : boolean, default False\\n        If true, the ordered attribute of the Categoricals will be ignored.\\n        Results in an unordered categorical.\\n\\n        .. versionadded:: 0.20.0\\n\\n    Returns\\n    -------\\n    result : Categorical\\n\\n    Raises\\n    ------\\n    TypeError\\n        - all inputs do not have the same dtype\\n        - all inputs do not have the same ordered property\\n        - all inputs are ordered and their categories are not identical\\n        - sort_categories=True and Categoricals are ordered\\n    ValueError\\n        Empty list of categoricals passed\\n\\n    Notes\\n    -----\\n\\n    To learn more about categories, see `link\\n    <http://pandas.pydata.org/pandas-docs/stable/categorical.html#unioning>`__\\n\\n    Examples\\n    --------\\n\\n    >>> from pandas.api.types import union_categoricals\\n\\n    If you want to combine categoricals that do not necessarily have\\n    the same categories, `union_categoricals` will combine a list-like\\n    of categoricals. The new categories will be the union of the\\n    categories being combined.\\n\\n    >>> a = pd.Categorical([\"b\", \"c\"])\\n    >>> b = pd.Categorical([\"a\", \"b\"])\\n    >>> union_categoricals([a, b])\\n    [b, c, a, b]\\n    Categories (3, object): [b, c, a]\\n\\n    By default, the resulting categories will be ordered as they appear\\n    in the `categories` of the data. If you want the categories to be\\n    lexsorted, use `sort_categories=True` argument.\\n\\n    >>> union_categoricals([a, b], sort_categories=True)\\n    [b, c, a, b]\\n    Categories (3, object): [a, b, c]\\n\\n    `union_categoricals` also works with the case of combining two\\n    categoricals of the same categories and order information (e.g. what\\n    you could also `append` for).\\n\\n    >>> a = pd.Categorical([\"a\", \"b\"], ordered=True)\\n    >>> b = pd.Categorical([\"a\", \"b\", \"a\"], ordered=True)\\n    >>> union_categoricals([a, b])\\n    [a, b, a, b, a]\\n    Categories (2, object): [a < b]\\n\\n    Raises `TypeError` because the categories are ordered and not identical.\\n\\n    >>> a = pd.Categorical([\"a\", \"b\"], ordered=True)\\n    >>> b = pd.Categorical([\"a\", \"b\", \"c\"], ordered=True)\\n    >>> union_categoricals([a, b])\\n    TypeError: to union ordered Categoricals, all categories must be the same\\n\\n    New in version 0.20.0\\n\\n    Ordered categoricals with different categories or orderings can be\\n    combined by using the `ignore_ordered=True` argument.\\n\\n    >>> a = pd.Categorical([\"a\", \"b\", \"c\"], ordered=True)\\n    >>> b = pd.Categorical([\"c\", \"b\", \"a\"], ordered=True)\\n    >>> union_categoricals([a, b], ignore_order=True)\\n    [a, b, c, c, b, a]\\n    Categories (3, object): [a, b, c]\\n\\n    `union_categoricals` also works with a `CategoricalIndex`, or `Series`\\n    containing categorical data, but note that the resulting array will\\n    always be a plain `Categorical`\\n\\n    >>> a = pd.Series([\"b\", \"c\"], dtype=\\'category\\')\\n    >>> b = pd.Series([\"a\", \"b\"], dtype=\\'category\\')\\n    >>> union_categoricals([a, b])\\n    [b, c, a, b]\\n    Categories (3, object): [b, c, a]\\n    \"\"\"\\n    from pandas import Index, Categorical, CategoricalIndex, Series\\n    from pandas.core.arrays.categorical import _recode_for_categories\\n\\n    if len(to_union) == 0:\\n        raise ValueError(\\'No Categoricals to union\\')\\n\\n    def _maybe_unwrap(x):\\n        if isinstance(x, (CategoricalIndex, Series)):\\n            return x.values\\n        elif isinstance(x, Categorical):\\n            return x\\n        else:\\n            raise TypeError(\"all components to combine must be Categorical\")\\n\\n    to_union = [_maybe_unwrap(x) for x in to_union]\\n    first = to_union[0]\\n\\n    if not all(is_dtype_equal(other.categories.dtype, first.categories.dtype)\\n               for other in to_union[1:]):\\n        raise TypeError(\"dtype of categories must be the same\")\\n\\n    ordered = False\\n    if all(first.is_dtype_equal(other) for other in to_union[1:]):\\n        # identical categories - fastpath\\n        categories = first.categories\\n        ordered = first.ordered\\n\\n        if all(first.categories.equals(other.categories)\\n               for other in to_union[1:]):\\n            new_codes = np.concatenate([c.codes for c in to_union])\\n        else:\\n            codes = [first.codes] + [_recode_for_categories(other.codes,\\n                                                            other.categories,\\n                                                            first.categories)\\n                                     for other in to_union[1:]]\\n            new_codes = np.concatenate(codes)\\n\\n        if sort_categories and not ignore_order and ordered:\\n            raise TypeError(\"Cannot use sort_categories=True with \"\\n                            \"ordered Categoricals\")\\n\\n        if sort_categories and not categories.is_monotonic_increasing:\\n            categories = categories.sort_values()\\n            indexer = categories.get_indexer(first.categories)\\n\\n            from pandas.core.algorithms import take_1d\\n            new_codes = take_1d(indexer, new_codes, fill_value=-1)\\n    elif ignore_order or all(not c.ordered for c in to_union):\\n        # different categories - union and recode\\n        cats = first.categories.append([c.categories for c in to_union[1:]])\\n        categories = Index(cats.unique())\\n        if sort_categories:\\n            categories = categories.sort_values()\\n\\n        new_codes = [_recode_for_categories(c.codes, c.categories, categories)\\n                     for c in to_union]\\n        new_codes = np.concatenate(new_codes)\\n    else:\\n        # ordered - to show a proper error message\\n        if all(c.ordered for c in to_union):\\n            msg = (\"to union ordered Categoricals, \"\\n                   \"all categories must be the same\")\\n            raise TypeError(msg)\\n        else:\\n            raise TypeError(\\'Categorical.ordered must be the same\\')\\n\\n    if ignore_order:\\n        ordered = False\\n\\n    return Categorical(new_codes, categories=categories, ordered=ordered,\\n                       fastpath=True)',\n 'def _concat_datetime(to_concat, axis=0, typs=None):\\n    \"\"\"\\n    provide concatenation of an datetimelike array of arrays each of which is a\\n    single M8[ns], datetimet64[ns, tz] or m8[ns] dtype\\n\\n    Parameters\\n    ----------\\n    to_concat : array of arrays\\n    axis : axis to provide concatenation\\n    typs : set of to_concat dtypes\\n\\n    Returns\\n    -------\\n    a single array, preserving the combined dtypes\\n    \"\"\"\\n\\n    if typs is None:\\n        typs = get_dtype_kinds(to_concat)\\n\\n    # multiple types, need to coerce to object\\n    if len(typs) != 1:\\n        return _concatenate_2d([_convert_datetimelike_to_object(x)\\n                                for x in to_concat],\\n                               axis=axis)\\n\\n    # must be single dtype\\n    if any(typ.startswith(\\'datetime\\') for typ in typs):\\n\\n        if \\'datetime\\' in typs:\\n            to_concat = [x.astype(np.int64, copy=False) for x in to_concat]\\n            return _concatenate_2d(to_concat, axis=axis).view(_NS_DTYPE)\\n        else:\\n            # when to_concat has different tz, len(typs) > 1.\\n            # thus no need to care\\n            return _concat_datetimetz(to_concat)\\n\\n    elif \\'timedelta\\' in typs:\\n        return _concatenate_2d([x.view(np.int64) for x in to_concat],\\n                               axis=axis).view(_TD_DTYPE)\\n\\n    elif any(typ.startswith(\\'period\\') for typ in typs):\\n        assert len(typs) == 1\\n        cls = to_concat[0]\\n        new_values = cls._concat_same_type(to_concat)\\n        return new_values',\n 'def _concat_datetimetz(to_concat, name=None):\\n    \"\"\"\\n    concat DatetimeIndex with the same tz\\n    all inputs must be DatetimeIndex\\n    it is used in DatetimeIndex.append also\\n    \"\"\"\\n    # Right now, internals will pass a List[DatetimeArray] here\\n    # for reductions like quantile. I would like to disentangle\\n    # all this before we get here.\\n    sample = to_concat[0]\\n\\n    if isinstance(sample, ABCIndexClass):\\n        return sample._concat_same_dtype(to_concat, name=name)\\n    elif isinstance(sample, ABCDatetimeArray):\\n        return sample._concat_same_type(to_concat)',\n 'def _concat_index_asobject(to_concat, name=None):\\n    \"\"\"\\n    concat all inputs as object. DatetimeIndex, TimedeltaIndex and\\n    PeriodIndex are converted to object dtype before concatenation\\n    \"\"\"\\n    from pandas import Index\\n    from pandas.core.arrays import ExtensionArray\\n\\n    klasses = (ABCDatetimeIndex, ABCTimedeltaIndex, ABCPeriodIndex,\\n               ExtensionArray)\\n    to_concat = [x.astype(object) if isinstance(x, klasses) else x\\n                 for x in to_concat]\\n\\n    self = to_concat[0]\\n    attribs = self._get_attributes_dict()\\n    attribs[\\'name\\'] = name\\n\\n    to_concat = [x._values if isinstance(x, Index) else x\\n                 for x in to_concat]\\n\\n    return self._shallow_copy_with_infer(np.concatenate(to_concat), **attribs)',\n 'def _concat_sparse(to_concat, axis=0, typs=None):\\n    \"\"\"\\n    provide concatenation of an sparse/dense array of arrays each of which is a\\n    single dtype\\n\\n    Parameters\\n    ----------\\n    to_concat : array of arrays\\n    axis : axis to provide concatenation\\n    typs : set of to_concat dtypes\\n\\n    Returns\\n    -------\\n    a single array, preserving the combined dtypes\\n    \"\"\"\\n\\n    from pandas.core.arrays import SparseArray\\n\\n    fill_values = [x.fill_value for x in to_concat\\n                   if isinstance(x, SparseArray)]\\n    fill_value = fill_values[0]\\n\\n    # TODO: Fix join unit generation so we aren\\'t passed this.\\n    to_concat = [x if isinstance(x, SparseArray)\\n                 else SparseArray(x.squeeze(), fill_value=fill_value)\\n                 for x in to_concat]\\n\\n    return SparseArray._concat_same_type(to_concat)',\n 'def _concat_rangeindex_same_dtype(indexes):\\n    \"\"\"\\n    Concatenates multiple RangeIndex instances. All members of \"indexes\" must\\n    be of type RangeIndex; result will be RangeIndex if possible, Int64Index\\n    otherwise. E.g.:\\n    indexes = [RangeIndex(3), RangeIndex(3, 6)] -> RangeIndex(6)\\n    indexes = [RangeIndex(3), RangeIndex(4, 6)] -> Int64Index([0,1,2,4,5])\\n    \"\"\"\\n    from pandas import Int64Index, RangeIndex\\n\\n    start = step = next = None\\n\\n    # Filter the empty indexes\\n    non_empty_indexes = [obj for obj in indexes if len(obj)]\\n\\n    for obj in non_empty_indexes:\\n\\n        if start is None:\\n            # This is set by the first non-empty index\\n            start = obj._start\\n            if step is None and len(obj) > 1:\\n                step = obj._step\\n        elif step is None:\\n            # First non-empty index had only one element\\n            if obj._start == start:\\n                return _concat_index_same_dtype(indexes, klass=Int64Index)\\n            step = obj._start - start\\n\\n        non_consecutive = ((step != obj._step and len(obj) > 1) or\\n                           (next is not None and obj._start != next))\\n        if non_consecutive:\\n            return _concat_index_same_dtype(indexes, klass=Int64Index)\\n\\n        if step is not None:\\n            next = obj[-1] + step\\n\\n    if non_empty_indexes:\\n        # Get the stop value from \"next\" or alternatively\\n        # from the last non-empty index\\n        stop = non_empty_indexes[-1]._stop if next is None else next\\n        return RangeIndex(start, stop, step)\\n\\n    # Here all \"indexes\" had 0 length, i.e. were empty.\\n    # In this case return an empty range index.\\n    return RangeIndex(0, 0)',\n 'def rewrite_exception(old_name, new_name):\\n    \"\"\"Rewrite the message of an exception.\"\"\"\\n    try:\\n        yield\\n    except Exception as e:\\n        msg = e.args[0]\\n        msg = msg.replace(old_name, new_name)\\n        args = (msg,)\\n        if len(e.args) > 1:\\n            args = args + e.args[1:]\\n        e.args = args\\n        raise',\n 'def _get_level_lengths(index, hidden_elements=None):\\n    \"\"\"\\n    Given an index, find the level length for each element.\\n\\n    Optional argument is a list of index positions which\\n    should not be visible.\\n\\n    Result is a dictionary of (level, inital_position): span\\n    \"\"\"\\n    sentinel = object()\\n    levels = index.format(sparsify=sentinel, adjoin=False, names=False)\\n\\n    if hidden_elements is None:\\n        hidden_elements = []\\n\\n    lengths = {}\\n    if index.nlevels == 1:\\n        for i, value in enumerate(levels):\\n            if(i not in hidden_elements):\\n                lengths[(0, i)] = 1\\n        return lengths\\n\\n    for i, lvl in enumerate(levels):\\n        for j, row in enumerate(lvl):\\n            if not get_option(\\'display.multi_sparse\\'):\\n                lengths[(i, j)] = 1\\n            elif (row != sentinel) and (j not in hidden_elements):\\n                last_label = j\\n                lengths[(i, last_label)] = 1\\n            elif (row != sentinel):\\n                # even if its hidden, keep track of it in case\\n                # length >1 and later elements are visible\\n                last_label = j\\n                lengths[(i, last_label)] = 0\\n            elif(j not in hidden_elements):\\n                lengths[(i, last_label)] += 1\\n\\n    non_zero_lengths = {\\n        element: length for element, length in lengths.items() if length >= 1}\\n\\n    return non_zero_lengths',\n 'def _translate(self):\\n        \"\"\"\\n        Convert the DataFrame in `self.data` and the attrs from `_build_styles`\\n        into a dictionary of {head, body, uuid, cellstyle}.\\n        \"\"\"\\n        table_styles = self.table_styles or []\\n        caption = self.caption\\n        ctx = self.ctx\\n        precision = self.precision\\n        hidden_index = self.hidden_index\\n        hidden_columns = self.hidden_columns\\n        uuid = self.uuid or str(uuid1()).replace(\"-\", \"_\")\\n        ROW_HEADING_CLASS = \"row_heading\"\\n        COL_HEADING_CLASS = \"col_heading\"\\n        INDEX_NAME_CLASS = \"index_name\"\\n\\n        DATA_CLASS = \"data\"\\n        BLANK_CLASS = \"blank\"\\n        BLANK_VALUE = \"\"\\n\\n        def format_attr(pair):\\n            return \"{key}={value}\".format(**pair)\\n\\n        # for sparsifying a MultiIndex\\n        idx_lengths = _get_level_lengths(self.index)\\n        col_lengths = _get_level_lengths(self.columns, hidden_columns)\\n\\n        cell_context = dict()\\n\\n        n_rlvls = self.data.index.nlevels\\n        n_clvls = self.data.columns.nlevels\\n        rlabels = self.data.index.tolist()\\n        clabels = self.data.columns.tolist()\\n\\n        if n_rlvls == 1:\\n            rlabels = [[x] for x in rlabels]\\n        if n_clvls == 1:\\n            clabels = [[x] for x in clabels]\\n        clabels = list(zip(*clabels))\\n\\n        cellstyle = []\\n        head = []\\n\\n        for r in range(n_clvls):\\n            # Blank for Index columns...\\n            row_es = [{\"type\": \"th\",\\n                       \"value\": BLANK_VALUE,\\n                       \"display_value\": BLANK_VALUE,\\n                       \"is_visible\": not hidden_index,\\n                       \"class\": \" \".join([BLANK_CLASS])}] * (n_rlvls - 1)\\n\\n            # ... except maybe the last for columns.names\\n            name = self.data.columns.names[r]\\n            cs = [BLANK_CLASS if name is None else INDEX_NAME_CLASS,\\n                  \"level{lvl}\".format(lvl=r)]\\n            name = BLANK_VALUE if name is None else name\\n            row_es.append({\"type\": \"th\",\\n                           \"value\": name,\\n                           \"display_value\": name,\\n                           \"class\": \" \".join(cs),\\n                           \"is_visible\": not hidden_index})\\n\\n            if clabels:\\n                for c, value in enumerate(clabels[r]):\\n                    cs = [COL_HEADING_CLASS, \"level{lvl}\".format(lvl=r),\\n                          \"col{col}\".format(col=c)]\\n                    cs.extend(cell_context.get(\\n                        \"col_headings\", {}).get(r, {}).get(c, []))\\n                    es = {\\n                        \"type\": \"th\",\\n                        \"value\": value,\\n                        \"display_value\": value,\\n                        \"class\": \" \".join(cs),\\n                        \"is_visible\": _is_visible(c, r, col_lengths),\\n                    }\\n                    colspan = col_lengths.get((r, c), 0)\\n                    if colspan > 1:\\n                        es[\"attributes\"] = [\\n                            format_attr({\"key\": \"colspan\", \"value\": colspan})\\n                        ]\\n                    row_es.append(es)\\n                head.append(row_es)\\n\\n        if (self.data.index.names and\\n                com._any_not_none(*self.data.index.names) and\\n                not hidden_index):\\n            index_header_row = []\\n\\n            for c, name in enumerate(self.data.index.names):\\n                cs = [INDEX_NAME_CLASS,\\n                      \"level{lvl}\".format(lvl=c)]\\n                name = \\'\\' if name is None else name\\n                index_header_row.append({\"type\": \"th\", \"value\": name,\\n                                         \"class\": \" \".join(cs)})\\n\\n            index_header_row.extend(\\n                [{\"type\": \"th\",\\n                  \"value\": BLANK_VALUE,\\n                  \"class\": \" \".join([BLANK_CLASS])\\n                  }] * (len(clabels[0]) - len(hidden_columns)))\\n\\n            head.append(index_header_row)\\n\\n        body = []\\n        for r, idx in enumerate(self.data.index):\\n            row_es = []\\n            for c, value in enumerate(rlabels[r]):\\n                rid = [ROW_HEADING_CLASS, \"level{lvl}\".format(lvl=c),\\n                       \"row{row}\".format(row=r)]\\n                es = {\\n                    \"type\": \"th\",\\n                    \"is_visible\": (_is_visible(r, c, idx_lengths) and\\n                                   not hidden_index),\\n                    \"value\": value,\\n                    \"display_value\": value,\\n                    \"id\": \"_\".join(rid[1:]),\\n                    \"class\": \" \".join(rid)\\n                }\\n                rowspan = idx_lengths.get((c, r), 0)\\n                if rowspan > 1:\\n                    es[\"attributes\"] = [\\n                        format_attr({\"key\": \"rowspan\", \"value\": rowspan})\\n                    ]\\n                row_es.append(es)\\n\\n            for c, col in enumerate(self.data.columns):\\n                cs = [DATA_CLASS, \"row{row}\".format(row=r),\\n                      \"col{col}\".format(col=c)]\\n                cs.extend(cell_context.get(\"data\", {}).get(r, {}).get(c, []))\\n                formatter = self._display_funcs[(r, c)]\\n                value = self.data.iloc[r, c]\\n                row_dict = {\"type\": \"td\",\\n                            \"value\": value,\\n                            \"class\": \" \".join(cs),\\n                            \"display_value\": formatter(value),\\n                            \"is_visible\": (c not in hidden_columns)}\\n                # only add an id if the cell has a style\\n                if (self.cell_ids or\\n                        not(len(ctx[r, c]) == 1 and ctx[r, c][0] == \\'\\')):\\n                    row_dict[\"id\"] = \"_\".join(cs[1:])\\n                row_es.append(row_dict)\\n                props = []\\n                for x in ctx[r, c]:\\n                    # have to handle empty styles like [\\'\\']\\n                    if x.count(\":\"):\\n                        props.append(x.split(\":\"))\\n                    else:\\n                        props.append([\\'\\', \\'\\'])\\n                cellstyle.append({\\'props\\': props,\\n                                  \\'selector\\': \"row{row}_col{col}\"\\n                                  .format(row=r, col=c)})\\n            body.append(row_es)\\n\\n        table_attr = self.table_attributes\\n        use_mathjax = get_option(\"display.html.use_mathjax\")\\n        if not use_mathjax:\\n            table_attr = table_attr or \\'\\'\\n            if \\'class=\"\\' in table_attr:\\n                table_attr = table_attr.replace(\\'class=\"\\',\\n                                                \\'class=\"tex2jax_ignore \\')\\n            else:\\n                table_attr += \\' class=\"tex2jax_ignore\"\\'\\n\\n        return dict(head=head, cellstyle=cellstyle, body=body, uuid=uuid,\\n                    precision=precision, table_styles=table_styles,\\n                    caption=caption, table_attributes=table_attr)',\n 'def format(self, formatter, subset=None):\\n        \"\"\"\\n        Format the text display value of cells.\\n\\n        .. versionadded:: 0.18.0\\n\\n        Parameters\\n        ----------\\n        formatter : str, callable, or dict\\n        subset : IndexSlice\\n            An argument to ``DataFrame.loc`` that restricts which elements\\n            ``formatter`` is applied to.\\n\\n        Returns\\n        -------\\n        self : Styler\\n\\n        Notes\\n        -----\\n\\n        ``formatter`` is either an ``a`` or a dict ``{column name: a}`` where\\n        ``a`` is one of\\n\\n        - str: this will be wrapped in: ``a.format(x)``\\n        - callable: called with the value of an individual cell\\n\\n        The default display value for numeric values is the \"general\" (``g``)\\n        format with ``pd.options.display.precision`` precision.\\n\\n        Examples\\n        --------\\n\\n        >>> df = pd.DataFrame(np.random.randn(4, 2), columns=[\\'a\\', \\'b\\'])\\n        >>> df.style.format(\"{:.2%}\")\\n        >>> df[\\'c\\'] = [\\'a\\', \\'b\\', \\'c\\', \\'d\\']\\n        >>> df.style.format({\\'c\\': str.upper})\\n        \"\"\"\\n        if subset is None:\\n            row_locs = range(len(self.data))\\n            col_locs = range(len(self.data.columns))\\n        else:\\n            subset = _non_reducing_slice(subset)\\n            if len(subset) == 1:\\n                subset = subset, self.data.columns\\n\\n            sub_df = self.data.loc[subset]\\n            row_locs = self.data.index.get_indexer_for(sub_df.index)\\n            col_locs = self.data.columns.get_indexer_for(sub_df.columns)\\n\\n        if is_dict_like(formatter):\\n            for col, col_formatter in formatter.items():\\n                # formatter must be callable, so \\'{}\\' are converted to lambdas\\n                col_formatter = _maybe_wrap_formatter(col_formatter)\\n                col_num = self.data.columns.get_indexer_for([col])[0]\\n\\n                for row_num in row_locs:\\n                    self._display_funcs[(row_num, col_num)] = col_formatter\\n        else:\\n            # single scalar to format all cells with\\n            locs = product(*(row_locs, col_locs))\\n            for i, j in locs:\\n                formatter = _maybe_wrap_formatter(formatter)\\n                self._display_funcs[(i, j)] = formatter\\n        return self',\n 'def render(self, **kwargs):\\n        \"\"\"\\n        Render the built up styles to HTML.\\n\\n        Parameters\\n        ----------\\n        **kwargs\\n            Any additional keyword arguments are passed\\n            through to ``self.template.render``.\\n            This is useful when you need to provide\\n            additional variables for a custom template.\\n\\n            .. versionadded:: 0.20\\n\\n        Returns\\n        -------\\n        rendered : str\\n            The rendered HTML.\\n\\n        Notes\\n        -----\\n        ``Styler`` objects have defined the ``_repr_html_`` method\\n        which automatically calls ``self.render()`` when it\\'s the\\n        last item in a Notebook cell. When calling ``Styler.render()``\\n        directly, wrap the result in ``IPython.display.HTML`` to view\\n        the rendered HTML in the notebook.\\n\\n        Pandas uses the following keys in render. Arguments passed\\n        in ``**kwargs`` take precedence, so think carefully if you want\\n        to override them:\\n\\n        * head\\n        * cellstyle\\n        * body\\n        * uuid\\n        * precision\\n        * table_styles\\n        * caption\\n        * table_attributes\\n        \"\"\"\\n        self._compute()\\n        # TODO: namespace all the pandas keys\\n        d = self._translate()\\n        # filter out empty styles, every cell will have a class\\n        # but the list of props may just be [[\\'\\', \\'\\']].\\n        # so we have the neested anys below\\n        trimmed = [x for x in d[\\'cellstyle\\']\\n                   if any(any(y) for y in x[\\'props\\'])]\\n        d[\\'cellstyle\\'] = trimmed\\n        d.update(kwargs)\\n        return self.template.render(**d)',\n 'def _update_ctx(self, attrs):\\n        \"\"\"\\n        Update the state of the Styler.\\n\\n        Collects a mapping of {index_label: [\\'<property>: <value>\\']}.\\n\\n        attrs : Series or DataFrame\\n        should contain strings of \\'<property>: <value>;<prop2>: <val2>\\'\\n        Whitespace shouldn\\'t matter and the final trailing \\';\\' shouldn\\'t\\n        matter.\\n        \"\"\"\\n        for row_label, v in attrs.iterrows():\\n            for col_label, col in v.iteritems():\\n                i = self.index.get_indexer([row_label])[0]\\n                j = self.columns.get_indexer([col_label])[0]\\n                for pair in col.rstrip(\";\").split(\";\"):\\n                    self.ctx[(i, j)].append(pair)',\n 'def _compute(self):\\n        \"\"\"\\n        Execute the style functions built up in `self._todo`.\\n\\n        Relies on the conventions that all style functions go through\\n        .apply or .applymap. The append styles to apply as tuples of\\n\\n        (application method, *args, **kwargs)\\n        \"\"\"\\n        r = self\\n        for func, args, kwargs in self._todo:\\n            r = func(self)(*args, **kwargs)\\n        return r',\n 'def apply(self, func, axis=0, subset=None, **kwargs):\\n        \"\"\"\\n        Apply a function column-wise, row-wise, or table-wise,\\n        updating the HTML representation with the result.\\n\\n        Parameters\\n        ----------\\n        func : function\\n            ``func`` should take a Series or DataFrame (depending\\n            on ``axis``), and return an object with the same shape.\\n            Must return a DataFrame with identical index and\\n            column labels when ``axis=None``\\n        axis : {0 or \\'index\\', 1 or \\'columns\\', None}, default 0\\n            apply to each column (``axis=0`` or ``\\'index\\'``), to each row\\n            (``axis=1`` or ``\\'columns\\'``), or to the entire DataFrame at once\\n            with ``axis=None``.\\n        subset : IndexSlice\\n            a valid indexer to limit ``data`` to *before* applying the\\n            function. Consider using a pandas.IndexSlice\\n        kwargs : dict\\n            pass along to ``func``\\n\\n        Returns\\n        -------\\n        self : Styler\\n\\n        Notes\\n        -----\\n        The output shape of ``func`` should match the input, i.e. if\\n        ``x`` is the input row, column, or table (depending on ``axis``),\\n        then ``func(x).shape == x.shape`` should be true.\\n\\n        This is similar to ``DataFrame.apply``, except that ``axis=None``\\n        applies the function to the entire DataFrame at once,\\n        rather than column-wise or row-wise.\\n\\n        Examples\\n        --------\\n        >>> def highlight_max(x):\\n        ...     return [\\'background-color: yellow\\' if v == x.max() else \\'\\'\\n                        for v in x]\\n        ...\\n        >>> df = pd.DataFrame(np.random.randn(5, 2))\\n        >>> df.style.apply(highlight_max)\\n        \"\"\"\\n        self._todo.append((lambda instance: getattr(instance, \\'_apply\\'),\\n                           (func, axis, subset), kwargs))\\n        return self',\n 'def applymap(self, func, subset=None, **kwargs):\\n        \"\"\"\\n        Apply a function elementwise, updating the HTML\\n        representation with the result.\\n\\n        Parameters\\n        ----------\\n        func : function\\n            ``func`` should take a scalar and return a scalar\\n        subset : IndexSlice\\n            a valid indexer to limit ``data`` to *before* applying the\\n            function. Consider using a pandas.IndexSlice\\n        kwargs : dict\\n            pass along to ``func``\\n\\n        Returns\\n        -------\\n        self : Styler\\n\\n        See Also\\n        --------\\n        Styler.where\\n        \"\"\"\\n        self._todo.append((lambda instance: getattr(instance, \\'_applymap\\'),\\n                           (func, subset), kwargs))\\n        return self',\n 'def where(self, cond, value, other=None, subset=None, **kwargs):\\n        \"\"\"\\n        Apply a function elementwise, updating the HTML\\n        representation with a style which is selected in\\n        accordance with the return value of a function.\\n\\n        .. versionadded:: 0.21.0\\n\\n        Parameters\\n        ----------\\n        cond : callable\\n            ``cond`` should take a scalar and return a boolean\\n        value : str\\n            applied when ``cond`` returns true\\n        other : str\\n            applied when ``cond`` returns false\\n        subset : IndexSlice\\n            a valid indexer to limit ``data`` to *before* applying the\\n            function. Consider using a pandas.IndexSlice\\n        kwargs : dict\\n            pass along to ``cond``\\n\\n        Returns\\n        -------\\n        self : Styler\\n\\n        See Also\\n        --------\\n        Styler.applymap\\n        \"\"\"\\n\\n        if other is None:\\n            other = \\'\\'\\n\\n        return self.applymap(lambda val: value if cond(val) else other,\\n                             subset=subset, **kwargs)',\n 'def hide_columns(self, subset):\\n        \"\"\"\\n        Hide columns from rendering.\\n\\n        .. versionadded:: 0.23.0\\n\\n        Parameters\\n        ----------\\n        subset : IndexSlice\\n            An argument to ``DataFrame.loc`` that identifies which columns\\n            are hidden.\\n\\n        Returns\\n        -------\\n        self : Styler\\n        \"\"\"\\n        subset = _non_reducing_slice(subset)\\n        hidden_df = self.data.loc[subset]\\n        self.hidden_columns = self.columns.get_indexer_for(hidden_df.columns)\\n        return self',\n 'def highlight_null(self, null_color=\\'red\\'):\\n        \"\"\"\\n        Shade the background ``null_color`` for missing values.\\n\\n        Parameters\\n        ----------\\n        null_color : str\\n\\n        Returns\\n        -------\\n        self : Styler\\n        \"\"\"\\n        self.applymap(self._highlight_null, null_color=null_color)\\n        return self',\n 'def background_gradient(self, cmap=\\'PuBu\\', low=0, high=0, axis=0,\\n                            subset=None, text_color_threshold=0.408):\\n        \"\"\"\\n        Color the background in a gradient according to\\n        the data in each column (optionally row).\\n\\n        Requires matplotlib.\\n\\n        Parameters\\n        ----------\\n        cmap : str or colormap\\n            matplotlib colormap\\n        low, high : float\\n            compress the range by these values.\\n        axis : {0 or \\'index\\', 1 or \\'columns\\', None}, default 0\\n            apply to each column (``axis=0`` or ``\\'index\\'``), to each row\\n            (``axis=1`` or ``\\'columns\\'``), or to the entire DataFrame at once\\n            with ``axis=None``.\\n        subset : IndexSlice\\n            a valid slice for ``data`` to limit the style application to.\\n        text_color_threshold : float or int\\n            luminance threshold for determining text color. Facilitates text\\n            visibility across varying background colors. From 0 to 1.\\n            0 = all text is dark colored, 1 = all text is light colored.\\n\\n            .. versionadded:: 0.24.0\\n\\n        Returns\\n        -------\\n        self : Styler\\n\\n        Raises\\n        ------\\n        ValueError\\n            If ``text_color_threshold`` is not a value from 0 to 1.\\n\\n        Notes\\n        -----\\n        Set ``text_color_threshold`` or tune ``low`` and ``high`` to keep the\\n        text legible by not using the entire range of the color map. The range\\n        of the data is extended by ``low * (x.max() - x.min())`` and ``high *\\n        (x.max() - x.min())`` before normalizing.\\n        \"\"\"\\n        subset = _maybe_numeric_slice(self.data, subset)\\n        subset = _non_reducing_slice(subset)\\n        self.apply(self._background_gradient, cmap=cmap, subset=subset,\\n                   axis=axis, low=low, high=high,\\n                   text_color_threshold=text_color_threshold)\\n        return self',\n 'def _background_gradient(s, cmap=\\'PuBu\\', low=0, high=0,\\n                             text_color_threshold=0.408):\\n        \"\"\"\\n        Color background in a range according to the data.\\n        \"\"\"\\n        if (not isinstance(text_color_threshold, (float, int)) or\\n                not 0 <= text_color_threshold <= 1):\\n            msg = \"`text_color_threshold` must be a value from 0 to 1.\"\\n            raise ValueError(msg)\\n\\n        with _mpl(Styler.background_gradient) as (plt, colors):\\n            smin = s.values.min()\\n            smax = s.values.max()\\n            rng = smax - smin\\n            # extend lower / upper bounds, compresses color range\\n            norm = colors.Normalize(smin - (rng * low), smax + (rng * high))\\n            # matplotlib colors.Normalize modifies inplace?\\n            # https://github.com/matplotlib/matplotlib/issues/5427\\n            rgbas = plt.cm.get_cmap(cmap)(norm(s.values))\\n\\n            def relative_luminance(rgba):\\n                \"\"\"\\n                Calculate relative luminance of a color.\\n\\n                The calculation adheres to the W3C standards\\n                (https://www.w3.org/WAI/GL/wiki/Relative_luminance)\\n\\n                Parameters\\n                ----------\\n                color : rgb or rgba tuple\\n\\n                Returns\\n                -------\\n                float\\n                    The relative luminance as a value from 0 to 1\\n                \"\"\"\\n                r, g, b = (\\n                    x / 12.92 if x <= 0.03928 else ((x + 0.055) / 1.055 ** 2.4)\\n                    for x in rgba[:3]\\n                )\\n                return 0.2126 * r + 0.7152 * g + 0.0722 * b\\n\\n            def css(rgba):\\n                dark = relative_luminance(rgba) < text_color_threshold\\n                text_color = \\'#f1f1f1\\' if dark else \\'#000000\\'\\n                return \\'background-color: {b};color: {c};\\'.format(\\n                    b=colors.rgb2hex(rgba), c=text_color\\n                )\\n\\n            if s.ndim == 1:\\n                return [css(rgba) for rgba in rgbas]\\n            else:\\n                return pd.DataFrame(\\n                    [[css(rgba) for rgba in row] for row in rgbas],\\n                    index=s.index, columns=s.columns\\n                )',\n 'def set_properties(self, subset=None, **kwargs):\\n        \"\"\"\\n        Convenience method for setting one or more non-data dependent\\n        properties or each cell.\\n\\n        Parameters\\n        ----------\\n        subset : IndexSlice\\n            a valid slice for ``data`` to limit the style application to\\n        kwargs : dict\\n            property: value pairs to be set for each cell\\n\\n        Returns\\n        -------\\n        self : Styler\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame(np.random.randn(10, 4))\\n        >>> df.style.set_properties(color=\"white\", align=\"right\")\\n        >>> df.style.set_properties(**{\\'background-color\\': \\'yellow\\'})\\n        \"\"\"\\n        values = \\';\\'.join(\\'{p}: {v}\\'.format(p=p, v=v)\\n                          for p, v in kwargs.items())\\n        f = lambda x: values\\n        return self.applymap(f, subset=subset)',\n 'def _bar(s, align, colors, width=100, vmin=None, vmax=None):\\n        \"\"\"\\n        Draw bar chart in dataframe cells.\\n        \"\"\"\\n        # Get input value range.\\n        smin = s.min() if vmin is None else vmin\\n        if isinstance(smin, ABCSeries):\\n            smin = smin.min()\\n        smax = s.max() if vmax is None else vmax\\n        if isinstance(smax, ABCSeries):\\n            smax = smax.max()\\n        if align == \\'mid\\':\\n            smin = min(0, smin)\\n            smax = max(0, smax)\\n        elif align == \\'zero\\':\\n            # For \"zero\" mode, we want the range to be symmetrical around zero.\\n            smax = max(abs(smin), abs(smax))\\n            smin = -smax\\n        # Transform to percent-range of linear-gradient\\n        normed = width * (s.values - smin) / (smax - smin + 1e-12)\\n        zero = -width * smin / (smax - smin + 1e-12)\\n\\n        def css_bar(start, end, color):\\n            \"\"\"\\n            Generate CSS code to draw a bar from start to end.\\n            \"\"\"\\n            css = \\'width: 10em; height: 80%;\\'\\n            if end > start:\\n                css += \\'background: linear-gradient(90deg,\\'\\n                if start > 0:\\n                    css += \\' transparent {s:.1f}%, {c} {s:.1f}%, \\'.format(\\n                        s=start, c=color\\n                    )\\n                css += \\'{c} {e:.1f}%, transparent {e:.1f}%)\\'.format(\\n                    e=min(end, width), c=color,\\n                )\\n            return css\\n\\n        def css(x):\\n            if pd.isna(x):\\n                return \\'\\'\\n\\n            # avoid deprecated indexing `colors[x > zero]`\\n            color = colors[1] if x > zero else colors[0]\\n\\n            if align == \\'left\\':\\n                return css_bar(0, x, color)\\n            else:\\n                return css_bar(min(x, zero), max(x, zero), color)\\n\\n        if s.ndim == 1:\\n            return [css(x) for x in normed]\\n        else:\\n            return pd.DataFrame(\\n                [[css(x) for x in row] for row in normed],\\n                index=s.index, columns=s.columns\\n            )',\n 'def bar(self, subset=None, axis=0, color=\\'#d65f5f\\', width=100,\\n            align=\\'left\\', vmin=None, vmax=None):\\n        \"\"\"\\n        Draw bar chart in the cell backgrounds.\\n\\n        Parameters\\n        ----------\\n        subset : IndexSlice, optional\\n            A valid slice for `data` to limit the style application to.\\n        axis : {0 or \\'index\\', 1 or \\'columns\\', None}, default 0\\n            apply to each column (``axis=0`` or ``\\'index\\'``), to each row\\n            (``axis=1`` or ``\\'columns\\'``), or to the entire DataFrame at once\\n            with ``axis=None``.\\n        color : str or 2-tuple/list\\n            If a str is passed, the color is the same for both\\n            negative and positive numbers. If 2-tuple/list is used, the\\n            first element is the color_negative and the second is the\\n            color_positive (eg: [\\'#d65f5f\\', \\'#5fba7d\\']).\\n        width : float, default 100\\n            A number between 0 or 100. The largest value will cover `width`\\n            percent of the cell\\'s width.\\n        align : {\\'left\\', \\'zero\\',\\' mid\\'}, default \\'left\\'\\n            How to align the bars with the cells.\\n\\n            - \\'left\\' : the min value starts at the left of the cell.\\n            - \\'zero\\' : a value of zero is located at the center of the cell.\\n            - \\'mid\\' : the center of the cell is at (max-min)/2, or\\n              if values are all negative (positive) the zero is aligned\\n              at the right (left) of the cell.\\n\\n              .. versionadded:: 0.20.0\\n\\n        vmin : float, optional\\n            Minimum bar value, defining the left hand limit\\n            of the bar drawing range, lower values are clipped to `vmin`.\\n            When None (default): the minimum value of the data will be used.\\n\\n            .. versionadded:: 0.24.0\\n\\n        vmax : float, optional\\n            Maximum bar value, defining the right hand limit\\n            of the bar drawing range, higher values are clipped to `vmax`.\\n            When None (default): the maximum value of the data will be used.\\n\\n            .. versionadded:: 0.24.0\\n\\n        Returns\\n        -------\\n        self : Styler\\n        \"\"\"\\n        if align not in (\\'left\\', \\'zero\\', \\'mid\\'):\\n            raise ValueError(\"`align` must be one of {\\'left\\', \\'zero\\',\\' mid\\'}\")\\n\\n        if not (is_list_like(color)):\\n            color = [color, color]\\n        elif len(color) == 1:\\n            color = [color[0], color[0]]\\n        elif len(color) > 2:\\n            raise ValueError(\"`color` must be string or a list-like\"\\n                             \" of length 2: [`color_neg`, `color_pos`]\"\\n                             \" (eg: color=[\\'#d65f5f\\', \\'#5fba7d\\'])\")\\n\\n        subset = _maybe_numeric_slice(self.data, subset)\\n        subset = _non_reducing_slice(subset)\\n        self.apply(self._bar, subset=subset, axis=axis,\\n                   align=align, colors=color, width=width,\\n                   vmin=vmin, vmax=vmax)\\n\\n        return self',\n 'def highlight_max(self, subset=None, color=\\'yellow\\', axis=0):\\n        \"\"\"\\n        Highlight the maximum by shading the background.\\n\\n        Parameters\\n        ----------\\n        subset : IndexSlice, default None\\n            a valid slice for ``data`` to limit the style application to.\\n        color : str, default \\'yellow\\'\\n        axis : {0 or \\'index\\', 1 or \\'columns\\', None}, default 0\\n            apply to each column (``axis=0`` or ``\\'index\\'``), to each row\\n            (``axis=1`` or ``\\'columns\\'``), or to the entire DataFrame at once\\n            with ``axis=None``.\\n\\n        Returns\\n        -------\\n        self : Styler\\n        \"\"\"\\n        return self._highlight_handler(subset=subset, color=color, axis=axis,\\n                                       max_=True)',\n 'def highlight_min(self, subset=None, color=\\'yellow\\', axis=0):\\n        \"\"\"\\n        Highlight the minimum by shading the background.\\n\\n        Parameters\\n        ----------\\n        subset : IndexSlice, default None\\n            a valid slice for ``data`` to limit the style application to.\\n        color : str, default \\'yellow\\'\\n        axis : {0 or \\'index\\', 1 or \\'columns\\', None}, default 0\\n            apply to each column (``axis=0`` or ``\\'index\\'``), to each row\\n            (``axis=1`` or ``\\'columns\\'``), or to the entire DataFrame at once\\n            with ``axis=None``.\\n\\n        Returns\\n        -------\\n        self : Styler\\n        \"\"\"\\n        return self._highlight_handler(subset=subset, color=color, axis=axis,\\n                                       max_=False)',\n 'def _highlight_extrema(data, color=\\'yellow\\', max_=True):\\n        \"\"\"\\n        Highlight the min or max in a Series or DataFrame.\\n        \"\"\"\\n        attr = \\'background-color: {0}\\'.format(color)\\n        if data.ndim == 1:  # Series from .apply\\n            if max_:\\n                extrema = data == data.max()\\n            else:\\n                extrema = data == data.min()\\n            return [attr if v else \\'\\' for v in extrema]\\n        else:  # DataFrame from .tee\\n            if max_:\\n                extrema = data == data.max().max()\\n            else:\\n                extrema = data == data.min().min()\\n            return pd.DataFrame(np.where(extrema, attr, \\'\\'),\\n                                index=data.index, columns=data.columns)',\n 'def from_custom_template(cls, searchpath, name):\\n        \"\"\"\\n        Factory function for creating a subclass of ``Styler``\\n        with a custom template and Jinja environment.\\n\\n        Parameters\\n        ----------\\n        searchpath : str or list\\n            Path or paths of directories containing the templates\\n        name : str\\n            Name of your custom template to use for rendering\\n\\n        Returns\\n        -------\\n        MyStyler : subclass of Styler\\n            Has the correct ``env`` and ``template`` class attributes set.\\n        \"\"\"\\n        loader = ChoiceLoader([\\n            FileSystemLoader(searchpath),\\n            cls.loader,\\n        ])\\n\\n        class MyStyler(cls):\\n            env = Environment(loader=loader)\\n            template = env.get_template(name)\\n\\n        return MyStyler',\n 'def register(self, dtype):\\n        \"\"\"\\n        Parameters\\n        ----------\\n        dtype : ExtensionDtype\\n        \"\"\"\\n        if not issubclass(dtype, (PandasExtensionDtype, ExtensionDtype)):\\n            raise ValueError(\"can only register pandas extension dtypes\")\\n\\n        self.dtypes.append(dtype)',\n 'def find(self, dtype):\\n        \"\"\"\\n        Parameters\\n        ----------\\n        dtype : PandasExtensionDtype or string\\n\\n        Returns\\n        -------\\n        return the first matching dtype, otherwise return None\\n        \"\"\"\\n        if not isinstance(dtype, str):\\n            dtype_type = dtype\\n            if not isinstance(dtype, type):\\n                dtype_type = type(dtype)\\n            if issubclass(dtype_type, ExtensionDtype):\\n                return dtype\\n\\n            return None\\n\\n        for dtype_type in self.dtypes:\\n            try:\\n                return dtype_type.construct_from_string(dtype)\\n            except TypeError:\\n                pass\\n\\n        return None',\n 'def np_datetime64_compat(s, *args, **kwargs):\\n    \"\"\"\\n    provide compat for construction of strings to numpy datetime64\\'s with\\n    tz-changes in 1.11 that make \\'2015-01-01 09:00:00Z\\' show a deprecation\\n    warning, when need to pass \\'2015-01-01 09:00:00\\'\\n    \"\"\"\\n    s = tz_replacer(s)\\n    return np.datetime64(s, *args, **kwargs)',\n 'def np_array_datetime64_compat(arr, *args, **kwargs):\\n    \"\"\"\\n    provide compat for construction of an array of strings to a\\n    np.array(..., dtype=np.datetime64(..))\\n    tz-changes in 1.11 that make \\'2015-01-01 09:00:00Z\\' show a deprecation\\n    warning, when need to pass \\'2015-01-01 09:00:00\\'\\n    \"\"\"\\n    # is_list_like\\n    if (hasattr(arr, \\'__iter__\\') and not isinstance(arr, (str, bytes))):\\n        arr = [tz_replacer(s) for s in arr]\\n    else:\\n        arr = tz_replacer(arr)\\n\\n    return np.array(arr, *args, **kwargs)',\n 'def _assert_safe_casting(cls, data, subarr):\\n        \"\"\"\\n        Ensure incoming data can be represented as ints.\\n        \"\"\"\\n        if not issubclass(data.dtype.type, np.signedinteger):\\n            if not np.array_equal(data, subarr):\\n                raise TypeError(\\'Unsafe NumPy casting, you must \\'\\n                                \\'explicitly cast\\')',\n 'def get_value(self, series, key):\\n        \"\"\" we always want to get an index value, never a value \"\"\"\\n        if not is_scalar(key):\\n            raise InvalidIndexError\\n\\n        k = com.values_from_object(key)\\n        loc = self.get_loc(k)\\n        new_values = com.values_from_object(series)[loc]\\n\\n        return new_values',\n 'def equals(self, other):\\n        \"\"\"\\n        Determines if two Index objects contain the same elements.\\n        \"\"\"\\n        if self is other:\\n            return True\\n\\n        if not isinstance(other, Index):\\n            return False\\n\\n        # need to compare nans locations and make sure that they are the same\\n        # since nans don\\'t compare equal this is a bit tricky\\n        try:\\n            if not isinstance(other, Float64Index):\\n                other = self._constructor(other)\\n            if (not is_dtype_equal(self.dtype, other.dtype) or\\n                    self.shape != other.shape):\\n                return False\\n            left, right = self._ndarray_values, other._ndarray_values\\n            return ((left == right) | (self._isnan & other._isnan)).all()\\n        except (TypeError, ValueError):\\n            return False',\n 'def _ensure_decoded(s):\\n    \"\"\" if we have bytes, decode them to unicode \"\"\"\\n    if isinstance(s, np.bytes_):\\n        s = s.decode(\\'UTF-8\\')\\n    return s',\n 'def _ensure_term(where, scope_level):\\n    \"\"\"\\n    ensure that the where is a Term or a list of Term\\n    this makes sure that we are capturing the scope of variables\\n    that are passed\\n    create the terms here with a frame_level=2 (we are 2 levels down)\\n    \"\"\"\\n\\n    # only consider list/tuple here as an ndarray is automatically a coordinate\\n    # list\\n    level = scope_level + 1\\n    if isinstance(where, (list, tuple)):\\n        wlist = []\\n        for w in filter(lambda x: x is not None, where):\\n            if not maybe_expression(w):\\n                wlist.append(w)\\n            else:\\n                wlist.append(Term(w, scope_level=level))\\n        where = wlist\\n    elif maybe_expression(where):\\n        where = Term(where, scope_level=level)\\n    return where',\n 'def to_hdf(path_or_buf, key, value, mode=None, complevel=None, complib=None,\\n           append=None, **kwargs):\\n    \"\"\" store this object, close it if we opened it \"\"\"\\n\\n    if append:\\n        f = lambda store: store.append(key, value, **kwargs)\\n    else:\\n        f = lambda store: store.put(key, value, **kwargs)\\n\\n    path_or_buf = _stringify_path(path_or_buf)\\n    if isinstance(path_or_buf, str):\\n        with HDFStore(path_or_buf, mode=mode, complevel=complevel,\\n                      complib=complib) as store:\\n            f(store)\\n    else:\\n        f(path_or_buf)',\n 'def read_hdf(path_or_buf, key=None, mode=\\'r\\', **kwargs):\\n    \"\"\"\\n    Read from the store, close it if we opened it.\\n\\n    Retrieve pandas object stored in file, optionally based on where\\n    criteria\\n\\n    Parameters\\n    ----------\\n    path_or_buf : string, buffer or path object\\n        Path to the file to open, or an open :class:`pandas.HDFStore` object.\\n        Supports any object implementing the ``__fspath__`` protocol.\\n        This includes :class:`pathlib.Path` and py._path.local.LocalPath\\n        objects.\\n\\n        .. versionadded:: 0.19.0 support for pathlib, py.path.\\n        .. versionadded:: 0.21.0 support for __fspath__ protocol.\\n\\n    key : object, optional\\n        The group identifier in the store. Can be omitted if the HDF file\\n        contains a single pandas object.\\n    mode : {\\'r\\', \\'r+\\', \\'a\\'}, optional\\n        Mode to use when opening the file. Ignored if path_or_buf is a\\n        :class:`pandas.HDFStore`. Default is \\'r\\'.\\n    where : list, optional\\n        A list of Term (or convertible) objects.\\n    start : int, optional\\n        Row number to start selection.\\n    stop  : int, optional\\n        Row number to stop selection.\\n    columns : list, optional\\n        A list of columns names to return.\\n    iterator : bool, optional\\n        Return an iterator object.\\n    chunksize : int, optional\\n        Number of rows to include in an iteration when using an iterator.\\n    errors : str, default \\'strict\\'\\n        Specifies how encoding and decoding errors are to be handled.\\n        See the errors argument for :func:`open` for a full list\\n        of options.\\n    **kwargs\\n        Additional keyword arguments passed to HDFStore.\\n\\n    Returns\\n    -------\\n    item : object\\n        The selected object. Return type depends on the object stored.\\n\\n    See Also\\n    --------\\n    DataFrame.to_hdf : Write a HDF file from a DataFrame.\\n    HDFStore : Low-level access to HDF files.\\n\\n    Examples\\n    --------\\n    >>> df = pd.DataFrame([[1, 1.0, \\'a\\']], columns=[\\'x\\', \\'y\\', \\'z\\'])\\n    >>> df.to_hdf(\\'./store.h5\\', \\'data\\')\\n    >>> reread = pd.read_hdf(\\'./store.h5\\')\\n    \"\"\"\\n\\n    if mode not in [\\'r\\', \\'r+\\', \\'a\\']:\\n        raise ValueError(\\'mode {0} is not allowed while performing a read. \\'\\n                         \\'Allowed modes are r, r+ and a.\\'.format(mode))\\n    # grab the scope\\n    if \\'where\\' in kwargs:\\n        kwargs[\\'where\\'] = _ensure_term(kwargs[\\'where\\'], scope_level=1)\\n\\n    if isinstance(path_or_buf, HDFStore):\\n        if not path_or_buf.is_open:\\n            raise IOError(\\'The HDFStore must be open for reading.\\')\\n\\n        store = path_or_buf\\n        auto_close = False\\n    else:\\n        path_or_buf = _stringify_path(path_or_buf)\\n        if not isinstance(path_or_buf, str):\\n            raise NotImplementedError(\\'Support for generic buffers has not \\'\\n                                      \\'been implemented.\\')\\n        try:\\n            exists = os.path.exists(path_or_buf)\\n\\n        # if filepath is too long\\n        except (TypeError, ValueError):\\n            exists = False\\n\\n        if not exists:\\n            raise FileNotFoundError(\\n                \\'File {path} does not exist\\'.format(path=path_or_buf))\\n\\n        store = HDFStore(path_or_buf, mode=mode, **kwargs)\\n        # can\\'t auto open/close if we are using an iterator\\n        # so delegate to the iterator\\n        auto_close = True\\n\\n    try:\\n        if key is None:\\n            groups = store.groups()\\n            if len(groups) == 0:\\n                raise ValueError(\\'No dataset in HDF5 file.\\')\\n            candidate_only_group = groups[0]\\n\\n            # For the HDF file to have only one dataset, all other groups\\n            # should then be metadata groups for that candidate group. (This\\n            # assumes that the groups() method enumerates parent groups\\n            # before their children.)\\n            for group_to_check in groups[1:]:\\n                if not _is_metadata_of(group_to_check, candidate_only_group):\\n                    raise ValueError(\\'key must be provided when HDF5 file \\'\\n                                     \\'contains multiple datasets.\\')\\n            key = candidate_only_group._v_pathname\\n        return store.select(key, auto_close=auto_close, **kwargs)\\n    except (ValueError, TypeError, KeyError):\\n        # if there is an error, close the store\\n        try:\\n            store.close()\\n        except AttributeError:\\n            pass\\n\\n        raise',\n 'def _is_metadata_of(group, parent_group):\\n    \"\"\"Check if a given group is a metadata group for a given parent_group.\"\"\"\\n    if group._v_depth <= parent_group._v_depth:\\n        return False\\n\\n    current = group\\n    while current._v_depth > 1:\\n        parent = current._v_parent\\n        if parent == parent_group and current._v_name == \\'meta\\':\\n            return True\\n        current = current._v_parent\\n    return False',\n 'def _get_info(info, name):\\n    \"\"\" get/create the info for this name \"\"\"\\n    try:\\n        idx = info[name]\\n    except KeyError:\\n        idx = info[name] = dict()\\n    return idx',\n 'def _get_tz(tz):\\n    \"\"\" for a tz-aware type, return an encoded zone \"\"\"\\n    zone = timezones.get_timezone(tz)\\n    if zone is None:\\n        zone = tz.utcoffset().total_seconds()\\n    return zone',\n 'def _set_tz(values, tz, preserve_UTC=False, coerce=False):\\n    \"\"\"\\n    coerce the values to a DatetimeIndex if tz is set\\n    preserve the input shape if possible\\n\\n    Parameters\\n    ----------\\n    values : ndarray\\n    tz : string/pickled tz object\\n    preserve_UTC : boolean,\\n        preserve the UTC of the result\\n    coerce : if we do not have a passed timezone, coerce to M8[ns] ndarray\\n    \"\"\"\\n    if tz is not None:\\n        name = getattr(values, \\'name\\', None)\\n        values = values.ravel()\\n        tz = timezones.get_timezone(_ensure_decoded(tz))\\n        values = DatetimeIndex(values, name=name)\\n        if values.tz is None:\\n            values = values.tz_localize(\\'UTC\\').tz_convert(tz)\\n        if preserve_UTC:\\n            if tz == \\'UTC\\':\\n                values = list(values)\\n    elif coerce:\\n        values = np.asarray(values, dtype=\\'M8[ns]\\')\\n\\n    return values',\n 'def _convert_string_array(data, encoding, errors, itemsize=None):\\n    \"\"\"\\n    we take a string-like that is object dtype and coerce to a fixed size\\n    string type\\n\\n    Parameters\\n    ----------\\n    data : a numpy array of object dtype\\n    encoding : None or string-encoding\\n    errors : handler for encoding errors\\n    itemsize : integer, optional, defaults to the max length of the strings\\n\\n    Returns\\n    -------\\n    data in a fixed-length string dtype, encoded to bytes if needed\\n    \"\"\"\\n\\n    # encode if needed\\n    if encoding is not None and len(data):\\n        data = Series(data.ravel()).str.encode(\\n            encoding, errors).values.reshape(data.shape)\\n\\n    # create the sized dtype\\n    if itemsize is None:\\n        ensured = ensure_object(data.ravel())\\n        itemsize = max(1, libwriters.max_len_string_array(ensured))\\n\\n    data = np.asarray(data, dtype=\"S{size}\".format(size=itemsize))\\n    return data',\n 'def _unconvert_string_array(data, nan_rep=None, encoding=None,\\n                            errors=\\'strict\\'):\\n    \"\"\"\\n    inverse of _convert_string_array\\n\\n    Parameters\\n    ----------\\n    data : fixed length string dtyped array\\n    nan_rep : the storage repr of NaN, optional\\n    encoding : the encoding of the data, optional\\n    errors : handler for encoding errors, default \\'strict\\'\\n\\n    Returns\\n    -------\\n    an object array of the decoded data\\n\\n    \"\"\"\\n    shape = data.shape\\n    data = np.asarray(data.ravel(), dtype=object)\\n\\n    # guard against a None encoding (because of a legacy\\n    # where the passed encoding is actually None)\\n    encoding = _ensure_encoding(encoding)\\n    if encoding is not None and len(data):\\n\\n        itemsize = libwriters.max_len_string_array(ensure_object(data))\\n        dtype = \"U{0}\".format(itemsize)\\n\\n        if isinstance(data[0], bytes):\\n            data = Series(data).str.decode(encoding, errors=errors).values\\n        else:\\n            data = data.astype(dtype, copy=False).astype(object, copy=False)\\n\\n    if nan_rep is None:\\n        nan_rep = \\'nan\\'\\n\\n    data = libwriters.string_array_replace_from_nan_rep(data, nan_rep)\\n    return data.reshape(shape)',\n 'def open(self, mode=\\'a\\', **kwargs):\\n        \"\"\"\\n        Open the file in the specified mode\\n\\n        Parameters\\n        ----------\\n        mode : {\\'a\\', \\'w\\', \\'r\\', \\'r+\\'}, default \\'a\\'\\n            See HDFStore docstring or tables.open_file for info about modes\\n        \"\"\"\\n        tables = _tables()\\n\\n        if self._mode != mode:\\n\\n            # if we are changing a write mode to read, ok\\n            if self._mode in [\\'a\\', \\'w\\'] and mode in [\\'r\\', \\'r+\\']:\\n                pass\\n            elif mode in [\\'w\\']:\\n\\n                # this would truncate, raise here\\n                if self.is_open:\\n                    raise PossibleDataLossError(\\n                        \"Re-opening the file [{0}] with mode [{1}] \"\\n                        \"will delete the current file!\"\\n                        .format(self._path, self._mode)\\n                    )\\n\\n            self._mode = mode\\n\\n        # close and reopen the handle\\n        if self.is_open:\\n            self.close()\\n\\n        if self._complevel and self._complevel > 0:\\n            self._filters = _tables().Filters(self._complevel, self._complib,\\n                                              fletcher32=self._fletcher32)\\n\\n        try:\\n            self._handle = tables.open_file(self._path, self._mode, **kwargs)\\n        except (IOError) as e:  # pragma: no cover\\n            if \\'can not be written\\' in str(e):\\n                print(\\n                    \\'Opening {path} in read-only mode\\'.format(path=self._path))\\n                self._handle = tables.open_file(self._path, \\'r\\', **kwargs)\\n            else:\\n                raise\\n\\n        except (ValueError) as e:\\n\\n            # trap PyTables >= 3.1 FILE_OPEN_POLICY exception\\n            # to provide an updated message\\n            if \\'FILE_OPEN_POLICY\\' in str(e):\\n                e = ValueError(\\n                    \"PyTables [{version}] no longer supports opening multiple \"\\n                    \"files\\\\n\"\\n                    \"even in read-only mode on this HDF5 version \"\\n                    \"[{hdf_version}]. You can accept this\\\\n\"\\n                    \"and not open the same file multiple times at once,\\\\n\"\\n                    \"upgrade the HDF5 version, or downgrade to PyTables 3.0.0 \"\\n                    \"which allows\\\\n\"\\n                    \"files to be opened multiple times at once\\\\n\"\\n                    .format(version=tables.__version__,\\n                            hdf_version=tables.get_hdf5_version()))\\n\\n            raise e\\n\\n        except (Exception) as e:\\n\\n            # trying to read from a non-existent file causes an error which\\n            # is not part of IOError, make it one\\n            if self._mode == \\'r\\' and \\'Unable to open/create file\\' in str(e):\\n                raise IOError(str(e))\\n            raise',\n 'def flush(self, fsync=False):\\n        \"\"\"\\n        Force all buffered modifications to be written to disk.\\n\\n        Parameters\\n        ----------\\n        fsync : bool (default False)\\n          call ``os.fsync()`` on the file handle to force writing to disk.\\n\\n        Notes\\n        -----\\n        Without ``fsync=True``, flushing may not guarantee that the OS writes\\n        to disk. With fsync, the operation will block until the OS claims the\\n        file has been written; however, other caching layers may still\\n        interfere.\\n        \"\"\"\\n        if self._handle is not None:\\n            self._handle.flush()\\n            if fsync:\\n                try:\\n                    os.fsync(self._handle.fileno())\\n                except OSError:\\n                    pass',\n 'def get(self, key):\\n        \"\"\"\\n        Retrieve pandas object stored in file\\n\\n        Parameters\\n        ----------\\n        key : object\\n\\n        Returns\\n        -------\\n        obj : same type as object stored in file\\n        \"\"\"\\n        group = self.get_node(key)\\n        if group is None:\\n            raise KeyError(\\'No object named {key} in the file\\'.format(key=key))\\n        return self._read_group(group)',\n 'def select(self, key, where=None, start=None, stop=None, columns=None,\\n               iterator=False, chunksize=None, auto_close=False, **kwargs):\\n        \"\"\"\\n        Retrieve pandas object stored in file, optionally based on where\\n        criteria\\n\\n        Parameters\\n        ----------\\n        key : object\\n        where : list of Term (or convertible) objects, optional\\n        start : integer (defaults to None), row number to start selection\\n        stop  : integer (defaults to None), row number to stop selection\\n        columns : a list of columns that if not None, will limit the return\\n            columns\\n        iterator : boolean, return an iterator, default False\\n        chunksize : nrows to include in iteration, return an iterator\\n        auto_close : boolean, should automatically close the store when\\n            finished, default is False\\n\\n        Returns\\n        -------\\n        The selected object\\n        \"\"\"\\n        group = self.get_node(key)\\n        if group is None:\\n            raise KeyError(\\'No object named {key} in the file\\'.format(key=key))\\n\\n        # create the storer and axes\\n        where = _ensure_term(where, scope_level=1)\\n        s = self._create_storer(group)\\n        s.infer_axes()\\n\\n        # function to call on iteration\\n        def func(_start, _stop, _where):\\n            return s.read(start=_start, stop=_stop,\\n                          where=_where,\\n                          columns=columns)\\n\\n        # create the iterator\\n        it = TableIterator(self, s, func, where=where, nrows=s.nrows,\\n                           start=start, stop=stop, iterator=iterator,\\n                           chunksize=chunksize, auto_close=auto_close)\\n\\n        return it.get_result()',\n 'def select_as_coordinates(\\n            self, key, where=None, start=None, stop=None, **kwargs):\\n        \"\"\"\\n        return the selection as an Index\\n\\n        Parameters\\n        ----------\\n        key : object\\n        where : list of Term (or convertible) objects, optional\\n        start : integer (defaults to None), row number to start selection\\n        stop  : integer (defaults to None), row number to stop selection\\n        \"\"\"\\n        where = _ensure_term(where, scope_level=1)\\n        return self.get_storer(key).read_coordinates(where=where, start=start,\\n                                                     stop=stop, **kwargs)',\n 'def select_column(self, key, column, **kwargs):\\n        \"\"\"\\n        return a single column from the table. This is generally only useful to\\n        select an indexable\\n\\n        Parameters\\n        ----------\\n        key : object\\n        column: the column of interest\\n\\n        Exceptions\\n        ----------\\n        raises KeyError if the column is not found (or key is not a valid\\n            store)\\n        raises ValueError if the column can not be extracted individually (it\\n            is part of a data block)\\n\\n        \"\"\"\\n        return self.get_storer(key).read_column(column=column, **kwargs)',\n 'def select_as_multiple(self, keys, where=None, selector=None, columns=None,\\n                           start=None, stop=None, iterator=False,\\n                           chunksize=None, auto_close=False, **kwargs):\\n        \"\"\" Retrieve pandas objects from multiple tables\\n\\n        Parameters\\n        ----------\\n        keys : a list of the tables\\n        selector : the table to apply the where criteria (defaults to keys[0]\\n            if not supplied)\\n        columns : the columns I want back\\n        start : integer (defaults to None), row number to start selection\\n        stop  : integer (defaults to None), row number to stop selection\\n        iterator : boolean, return an iterator, default False\\n        chunksize : nrows to include in iteration, return an iterator\\n\\n        Exceptions\\n        ----------\\n        raises KeyError if keys or selector is not found or keys is empty\\n        raises TypeError if keys is not a list or tuple\\n        raises ValueError if the tables are not ALL THE SAME DIMENSIONS\\n        \"\"\"\\n\\n        # default to single select\\n        where = _ensure_term(where, scope_level=1)\\n        if isinstance(keys, (list, tuple)) and len(keys) == 1:\\n            keys = keys[0]\\n        if isinstance(keys, str):\\n            return self.select(key=keys, where=where, columns=columns,\\n                               start=start, stop=stop, iterator=iterator,\\n                               chunksize=chunksize, **kwargs)\\n\\n        if not isinstance(keys, (list, tuple)):\\n            raise TypeError(\"keys must be a list/tuple\")\\n\\n        if not len(keys):\\n            raise ValueError(\"keys must have a non-zero length\")\\n\\n        if selector is None:\\n            selector = keys[0]\\n\\n        # collect the tables\\n        tbls = [self.get_storer(k) for k in keys]\\n        s = self.get_storer(selector)\\n\\n        # validate rows\\n        nrows = None\\n        for t, k in itertools.chain([(s, selector)], zip(tbls, keys)):\\n            if t is None:\\n                raise KeyError(\"Invalid table [{key}]\".format(key=k))\\n            if not t.is_table:\\n                raise TypeError(\\n                    \"object [{obj}] is not a table, and cannot be used in all \"\\n                    \"select as multiple\".format(obj=t.pathname)\\n                )\\n\\n            if nrows is None:\\n                nrows = t.nrows\\n            elif t.nrows != nrows:\\n                raise ValueError(\\n                    \"all tables must have exactly the same nrows!\")\\n\\n        # axis is the concentation axes\\n        axis = list({t.non_index_axes[0][0] for t in tbls})[0]\\n\\n        def func(_start, _stop, _where):\\n\\n            # retrieve the objs, _where is always passed as a set of\\n            # coordinates here\\n            objs = [t.read(where=_where, columns=columns, start=_start,\\n                           stop=_stop, **kwargs) for t in tbls]\\n\\n            # concat and return\\n            return concat(objs, axis=axis,\\n                          verify_integrity=False)._consolidate()\\n\\n        # create the iterator\\n        it = TableIterator(self, s, func, where=where, nrows=nrows,\\n                           start=start, stop=stop, iterator=iterator,\\n                           chunksize=chunksize, auto_close=auto_close)\\n\\n        return it.get_result(coordinates=True)',\n 'def put(self, key, value, format=None, append=False, **kwargs):\\n        \"\"\"\\n        Store object in HDFStore\\n\\n        Parameters\\n        ----------\\n        key      : object\\n        value    : {Series, DataFrame}\\n        format   : \\'fixed(f)|table(t)\\', default is \\'fixed\\'\\n            fixed(f) : Fixed format\\n                       Fast writing/reading. Not-appendable, nor searchable\\n            table(t) : Table format\\n                       Write as a PyTables Table structure which may perform\\n                       worse but allow more flexible operations like searching\\n                       / selecting subsets of the data\\n        append   : boolean, default False\\n            This will force Table format, append the input data to the\\n            existing.\\n        data_columns : list of columns to create as data columns, or True to\\n            use all columns. See\\n            `here <http://pandas.pydata.org/pandas-docs/stable/io.html#query-via-data-columns>`__ # noqa\\n        encoding : default None, provide an encoding for strings\\n        dropna   : boolean, default False, do not write an ALL nan row to\\n            the store settable by the option \\'io.hdf.dropna_table\\'\\n        \"\"\"\\n        if format is None:\\n            format = get_option(\"io.hdf.default_format\") or \\'fixed\\'\\n        kwargs = self._validate_format(format, kwargs)\\n        self._write_to_group(key, value, append=append, **kwargs)',\n 'def remove(self, key, where=None, start=None, stop=None):\\n        \"\"\"\\n        Remove pandas object partially by specifying the where condition\\n\\n        Parameters\\n        ----------\\n        key : string\\n            Node to remove or delete rows from\\n        where : list of Term (or convertible) objects, optional\\n        start : integer (defaults to None), row number to start selection\\n        stop  : integer (defaults to None), row number to stop selection\\n\\n        Returns\\n        -------\\n        number of rows removed (or None if not a Table)\\n\\n        Exceptions\\n        ----------\\n        raises KeyError if key is not a valid store\\n\\n        \"\"\"\\n        where = _ensure_term(where, scope_level=1)\\n        try:\\n            s = self.get_storer(key)\\n        except KeyError:\\n            # the key is not a valid store, re-raising KeyError\\n            raise\\n        except Exception:\\n\\n            if where is not None:\\n                raise ValueError(\\n                    \"trying to remove a node with a non-None where clause!\")\\n\\n            # we are actually trying to remove a node (with children)\\n            s = self.get_node(key)\\n            if s is not None:\\n                s._f_remove(recursive=True)\\n                return None\\n\\n        # remove the node\\n        if com._all_none(where, start, stop):\\n            s.group._f_remove(recursive=True)\\n\\n        # delete from the table\\n        else:\\n            if not s.is_table:\\n                raise ValueError(\\n                    \\'can only remove with where on objects written as tables\\')\\n            return s.delete(where=where, start=start, stop=stop)',\n 'def append(self, key, value, format=None, append=True, columns=None,\\n               dropna=None, **kwargs):\\n        \"\"\"\\n        Append to Table in file. Node must already exist and be Table\\n        format.\\n\\n        Parameters\\n        ----------\\n        key : object\\n        value : {Series, DataFrame}\\n        format : \\'table\\' is the default\\n            table(t) : table format\\n                       Write as a PyTables Table structure which may perform\\n                       worse but allow more flexible operations like searching\\n                       / selecting subsets of the data\\n        append       : boolean, default True, append the input data to the\\n            existing\\n        data_columns :  list of columns, or True, default None\\n            List of columns to create as indexed data columns for on-disk\\n            queries, or True to use all columns. By default only the axes\\n            of the object are indexed. See `here\\n            <http://pandas.pydata.org/pandas-docs/stable/io.html#query-via-data-columns>`__.\\n        min_itemsize : dict of columns that specify minimum string sizes\\n        nan_rep      : string to use as string nan represenation\\n        chunksize    : size to chunk the writing\\n        expectedrows : expected TOTAL row size of this table\\n        encoding     : default None, provide an encoding for strings\\n        dropna       : boolean, default False, do not write an ALL nan row to\\n            the store settable by the option \\'io.hdf.dropna_table\\'\\n\\n        Notes\\n        -----\\n        Does *not* check if data being appended overlaps with existing\\n        data in the table, so be careful\\n        \"\"\"\\n        if columns is not None:\\n            raise TypeError(\"columns is not a supported keyword in append, \"\\n                            \"try data_columns\")\\n\\n        if dropna is None:\\n            dropna = get_option(\"io.hdf.dropna_table\")\\n        if format is None:\\n            format = get_option(\"io.hdf.default_format\") or \\'table\\'\\n        kwargs = self._validate_format(format, kwargs)\\n        self._write_to_group(key, value, append=append, dropna=dropna,\\n                             **kwargs)',\n 'def append_to_multiple(self, d, value, selector, data_columns=None,\\n                           axes=None, dropna=False, **kwargs):\\n        \"\"\"\\n        Append to multiple tables\\n\\n        Parameters\\n        ----------\\n        d : a dict of table_name to table_columns, None is acceptable as the\\n            values of one node (this will get all the remaining columns)\\n        value : a pandas object\\n        selector : a string that designates the indexable table; all of its\\n            columns will be designed as data_columns, unless data_columns is\\n            passed, in which case these are used\\n        data_columns : list of columns to create as data columns, or True to\\n            use all columns\\n        dropna : if evaluates to True, drop rows from all tables if any single\\n                 row in each table has all NaN. Default False.\\n\\n        Notes\\n        -----\\n        axes parameter is currently not accepted\\n\\n        \"\"\"\\n        if axes is not None:\\n            raise TypeError(\"axes is currently not accepted as a parameter to\"\\n                            \" append_to_multiple; you can create the \"\\n                            \"tables independently instead\")\\n\\n        if not isinstance(d, dict):\\n            raise ValueError(\\n                \"append_to_multiple must have a dictionary specified as the \"\\n                \"way to split the value\"\\n            )\\n\\n        if selector not in d:\\n            raise ValueError(\\n                \"append_to_multiple requires a selector that is in passed dict\"\\n            )\\n\\n        # figure out the splitting axis (the non_index_axis)\\n        axis = list(set(range(value.ndim)) - set(_AXES_MAP[type(value)]))[0]\\n\\n        # figure out how to split the value\\n        remain_key = None\\n        remain_values = []\\n        for k, v in d.items():\\n            if v is None:\\n                if remain_key is not None:\\n                    raise ValueError(\\n                        \"append_to_multiple can only have one value in d that \"\\n                        \"is None\"\\n                    )\\n                remain_key = k\\n            else:\\n                remain_values.extend(v)\\n        if remain_key is not None:\\n            ordered = value.axes[axis]\\n            ordd = ordered.difference(Index(remain_values))\\n            ordd = sorted(ordered.get_indexer(ordd))\\n            d[remain_key] = ordered.take(ordd)\\n\\n        # data_columns\\n        if data_columns is None:\\n            data_columns = d[selector]\\n\\n        # ensure rows are synchronized across the tables\\n        if dropna:\\n            idxs = (value[cols].dropna(how=\\'all\\').index for cols in d.values())\\n            valid_index = next(idxs)\\n            for index in idxs:\\n                valid_index = valid_index.intersection(index)\\n            value = value.loc[valid_index]\\n\\n        # append\\n        for k, v in d.items():\\n            dc = data_columns if k == selector else None\\n\\n            # compute the val\\n            val = value.reindex(v, axis=axis)\\n\\n            self.append(k, val, data_columns=dc, **kwargs)',\n 'def create_table_index(self, key, **kwargs):\\n        \"\"\" Create a pytables index on the table\\n        Parameters\\n        ----------\\n        key : object (the node to index)\\n\\n        Exceptions\\n        ----------\\n        raises if the node is not a table\\n\\n        \"\"\"\\n\\n        # version requirements\\n        _tables()\\n        s = self.get_storer(key)\\n        if s is None:\\n            return\\n\\n        if not s.is_table:\\n            raise TypeError(\\n                \"cannot create table index on a Fixed format store\")\\n        s.create_index(**kwargs)',\n 'def groups(self):\\n        \"\"\"return a list of all the top-level nodes (that are not themselves a\\n        pandas storage object)\\n        \"\"\"\\n        _tables()\\n        self._check_if_open()\\n        return [\\n            g for g in self._handle.walk_groups()\\n            if (not isinstance(g, _table_mod.link.Link) and\\n                (getattr(g._v_attrs, \\'pandas_type\\', None) or\\n                 getattr(g, \\'table\\', None) or\\n                (isinstance(g, _table_mod.table.Table) and\\n                 g._v_name != \\'table\\')))\\n        ]',\n 'def walk(self, where=\"/\"):\\n        \"\"\" Walk the pytables group hierarchy for pandas objects\\n\\n        This generator will yield the group path, subgroups and pandas object\\n        names for each group.\\n        Any non-pandas PyTables objects that are not a group will be ignored.\\n\\n        The `where` group itself is listed first (preorder), then each of its\\n        child groups (following an alphanumerical order) is also traversed,\\n        following the same procedure.\\n\\n        .. versionadded:: 0.24.0\\n\\n        Parameters\\n        ----------\\n        where : str, optional\\n            Group where to start walking.\\n            If not supplied, the root group is used.\\n\\n        Yields\\n        ------\\n        path : str\\n            Full path to a group (without trailing \\'/\\')\\n        groups : list of str\\n            names of the groups contained in `path`\\n        leaves : list of str\\n            names of the pandas objects contained in `path`\\n        \"\"\"\\n        _tables()\\n        self._check_if_open()\\n        for g in self._handle.walk_groups(where):\\n            if getattr(g._v_attrs, \\'pandas_type\\', None) is not None:\\n                continue\\n\\n            groups = []\\n            leaves = []\\n            for child in g._v_children.values():\\n                pandas_type = getattr(child._v_attrs, \\'pandas_type\\', None)\\n                if pandas_type is None:\\n                    if isinstance(child, _table_mod.group.Group):\\n                        groups.append(child._v_name)\\n                else:\\n                    leaves.append(child._v_name)\\n\\n            yield (g._v_pathname.rstrip(\\'/\\'), groups, leaves)',\n 'def get_node(self, key):\\n        \"\"\" return the node with the key or None if it does not exist \"\"\"\\n        self._check_if_open()\\n        try:\\n            if not key.startswith(\\'/\\'):\\n                key = \\'/\\' + key\\n            return self._handle.get_node(self.root, key)\\n        except _table_mod.exceptions.NoSuchNodeError:\\n            return None',\n 'def get_storer(self, key):\\n        \"\"\" return the storer object for a key, raise if not in the file \"\"\"\\n        group = self.get_node(key)\\n        if group is None:\\n            raise KeyError(\\'No object named {key} in the file\\'.format(key=key))\\n\\n        s = self._create_storer(group)\\n        s.infer_axes()\\n        return s',\n 'def copy(self, file, mode=\\'w\\', propindexes=True, keys=None, complib=None,\\n             complevel=None, fletcher32=False, overwrite=True):\\n        \"\"\" copy the existing store to a new file, upgrading in place\\n\\n            Parameters\\n            ----------\\n            propindexes: restore indexes in copied file (defaults to True)\\n            keys       : list of keys to include in the copy (defaults to all)\\n            overwrite  : overwrite (remove and replace) existing nodes in the\\n                new store (default is True)\\n            mode, complib, complevel, fletcher32 same as in HDFStore.__init__\\n\\n            Returns\\n            -------\\n            open file handle of the new store\\n\\n        \"\"\"\\n        new_store = HDFStore(\\n            file,\\n            mode=mode,\\n            complib=complib,\\n            complevel=complevel,\\n            fletcher32=fletcher32)\\n        if keys is None:\\n            keys = list(self.keys())\\n        if not isinstance(keys, (tuple, list)):\\n            keys = [keys]\\n        for k in keys:\\n            s = self.get_storer(k)\\n            if s is not None:\\n\\n                if k in new_store:\\n                    if overwrite:\\n                        new_store.remove(k)\\n\\n                data = self.select(k)\\n                if s.is_table:\\n\\n                    index = False\\n                    if propindexes:\\n                        index = [a.name for a in s.axes if a.is_indexed]\\n                    new_store.append(\\n                        k, data, index=index,\\n                        data_columns=getattr(s, \\'data_columns\\', None),\\n                        encoding=s.encoding\\n                    )\\n                else:\\n                    new_store.put(k, data, encoding=s.encoding)\\n\\n        return new_store',\n 'def info(self):\\n        \"\"\"\\n        Print detailed information on the store.\\n\\n        .. versionadded:: 0.21.0\\n        \"\"\"\\n        output = \\'{type}\\\\nFile path: {path}\\\\n\\'.format(\\n            type=type(self), path=pprint_thing(self._path))\\n        if self.is_open:\\n            lkeys = sorted(list(self.keys()))\\n            if len(lkeys):\\n                keys = []\\n                values = []\\n\\n                for k in lkeys:\\n                    try:\\n                        s = self.get_storer(k)\\n                        if s is not None:\\n                            keys.append(pprint_thing(s.pathname or k))\\n                            values.append(\\n                                pprint_thing(s or \\'invalid_HDFStore node\\'))\\n                    except Exception as detail:\\n                        keys.append(k)\\n                        values.append(\\n                            \"[invalid_HDFStore node: {detail}]\".format(\\n                                detail=pprint_thing(detail)))\\n\\n                output += adjoin(12, keys, values)\\n            else:\\n                output += \\'Empty\\'\\n        else:\\n            output += \"File is CLOSED\"\\n\\n        return output',\n 'def _validate_format(self, format, kwargs):\\n        \"\"\" validate / deprecate formats; return the new kwargs \"\"\"\\n        kwargs = kwargs.copy()\\n\\n        # validate\\n        try:\\n            kwargs[\\'format\\'] = _FORMAT_MAP[format.lower()]\\n        except KeyError:\\n            raise TypeError(\"invalid HDFStore format specified [{0}]\"\\n                            .format(format))\\n\\n        return kwargs',\n 'def _create_storer(self, group, format=None, value=None, append=False,\\n                       **kwargs):\\n        \"\"\" return a suitable class to operate \"\"\"\\n\\n        def error(t):\\n            raise TypeError(\\n                \"cannot properly create the storer for: [{t}] [group->\"\\n                \"{group},value->{value},format->{format},append->{append},\"\\n                \"kwargs->{kwargs}]\".format(t=t, group=group,\\n                                           value=type(value), format=format,\\n                                           append=append, kwargs=kwargs))\\n\\n        pt = _ensure_decoded(getattr(group._v_attrs, \\'pandas_type\\', None))\\n        tt = _ensure_decoded(getattr(group._v_attrs, \\'table_type\\', None))\\n\\n        # infer the pt from the passed value\\n        if pt is None:\\n            if value is None:\\n\\n                _tables()\\n                if (getattr(group, \\'table\\', None) or\\n                        isinstance(group, _table_mod.table.Table)):\\n                    pt = \\'frame_table\\'\\n                    tt = \\'generic_table\\'\\n                else:\\n                    raise TypeError(\\n                        \"cannot create a storer if the object is not existing \"\\n                        \"nor a value are passed\")\\n            else:\\n\\n                try:\\n                    pt = _TYPE_MAP[type(value)]\\n                except KeyError:\\n                    error(\\'_TYPE_MAP\\')\\n\\n                # we are actually a table\\n                if format == \\'table\\':\\n                    pt += \\'_table\\'\\n\\n        # a storer node\\n        if \\'table\\' not in pt:\\n            try:\\n                return globals()[_STORER_MAP[pt]](self, group, **kwargs)\\n            except KeyError:\\n                error(\\'_STORER_MAP\\')\\n\\n        # existing node (and must be a table)\\n        if tt is None:\\n\\n            # if we are a writer, determine the tt\\n            if value is not None:\\n\\n                if pt == \\'series_table\\':\\n                    index = getattr(value, \\'index\\', None)\\n                    if index is not None:\\n                        if index.nlevels == 1:\\n                            tt = \\'appendable_series\\'\\n                        elif index.nlevels > 1:\\n                            tt = \\'appendable_multiseries\\'\\n                elif pt == \\'frame_table\\':\\n                    index = getattr(value, \\'index\\', None)\\n                    if index is not None:\\n                        if index.nlevels == 1:\\n                            tt = \\'appendable_frame\\'\\n                        elif index.nlevels > 1:\\n                            tt = \\'appendable_multiframe\\'\\n                elif pt == \\'wide_table\\':\\n                    tt = \\'appendable_panel\\'\\n                elif pt == \\'ndim_table\\':\\n                    tt = \\'appendable_ndim\\'\\n\\n            else:\\n\\n                # distiguish between a frame/table\\n                tt = \\'legacy_panel\\'\\n                try:\\n                    fields = group.table._v_attrs.fields\\n                    if len(fields) == 1 and fields[0] == \\'value\\':\\n                        tt = \\'legacy_frame\\'\\n                except IndexError:\\n                    pass\\n\\n        try:\\n            return globals()[_TABLE_MAP[tt]](self, group, **kwargs)\\n        except KeyError:\\n            error(\\'_TABLE_MAP\\')',\n 'def set_name(self, name, kind_attr=None):\\n        \"\"\" set the name of this indexer \"\"\"\\n        self.name = name\\n        self.kind_attr = kind_attr or \"{name}_kind\".format(name=name)\\n        if self.cname is None:\\n            self.cname = name\\n\\n        return self',\n 'def set_pos(self, pos):\\n        \"\"\" set the position of this column in the Table \"\"\"\\n        self.pos = pos\\n        if pos is not None and self.typ is not None:\\n            self.typ._v_pos = pos\\n        return self',\n 'def is_indexed(self):\\n        \"\"\" return whether I am an indexed column \"\"\"\\n        try:\\n            return getattr(self.table.cols, self.cname).is_indexed\\n        except AttributeError:\\n            False',\n 'def infer(self, handler):\\n        \"\"\"infer this column from the table: create and return a new object\"\"\"\\n        table = handler.table\\n        new_self = self.copy()\\n        new_self.set_table(table)\\n        new_self.get_attr()\\n        new_self.read_metadata(handler)\\n        return new_self',\n 'def convert(self, values, nan_rep, encoding, errors):\\n        \"\"\" set the values from this selection: take = take ownership \"\"\"\\n\\n        # values is a recarray\\n        if values.dtype.fields is not None:\\n            values = values[self.cname]\\n\\n        values = _maybe_convert(values, self.kind, encoding, errors)\\n\\n        kwargs = dict()\\n        if self.freq is not None:\\n            kwargs[\\'freq\\'] = _ensure_decoded(self.freq)\\n        if self.index_name is not None:\\n            kwargs[\\'name\\'] = _ensure_decoded(self.index_name)\\n        # making an Index instance could throw a number of different errors\\n        try:\\n            self.values = Index(values, **kwargs)\\n        except Exception:  # noqa: E722\\n\\n            # if the output freq is different that what we recorded,\\n            # it should be None (see also \\'doc example part 2\\')\\n            if \\'freq\\' in kwargs:\\n                kwargs[\\'freq\\'] = None\\n            self.values = Index(values, **kwargs)\\n\\n        self.values = _set_tz(self.values, self.tz)\\n\\n        return self',\n 'def maybe_set_size(self, min_itemsize=None):\\n        \"\"\" maybe set a string col itemsize:\\n               min_itemsize can be an integer or a dict with this columns name\\n               with an integer size \"\"\"\\n        if _ensure_decoded(self.kind) == \\'string\\':\\n\\n            if isinstance(min_itemsize, dict):\\n                min_itemsize = min_itemsize.get(self.name)\\n\\n            if min_itemsize is not None and self.typ.itemsize < min_itemsize:\\n                self.typ = _tables(\\n                ).StringCol(itemsize=min_itemsize, pos=self.pos)',\n 'def validate_col(self, itemsize=None):\\n        \"\"\" validate this column: return the compared against itemsize \"\"\"\\n\\n        # validate this column for string truncation (or reset to the max size)\\n        if _ensure_decoded(self.kind) == \\'string\\':\\n            c = self.col\\n            if c is not None:\\n                if itemsize is None:\\n                    itemsize = self.itemsize\\n                if c.itemsize < itemsize:\\n                    raise ValueError(\\n                        \"Trying to store a string with len [{itemsize}] in \"\\n                        \"[{cname}] column but\\\\nthis column has a limit of \"\\n                        \"[{c_itemsize}]!\\\\nConsider using min_itemsize to \"\\n                        \"preset the sizes on these columns\".format(\\n                            itemsize=itemsize, cname=self.cname,\\n                            c_itemsize=c.itemsize))\\n                return c.itemsize\\n\\n        return None',\n 'def update_info(self, info):\\n        \"\"\" set/update the info for this indexable with the key/value\\n            if there is a conflict raise/warn as needed \"\"\"\\n\\n        for key in self._info_fields:\\n\\n            value = getattr(self, key, None)\\n            idx = _get_info(info, self.name)\\n\\n            existing_value = idx.get(key)\\n            if key in idx and value is not None and existing_value != value:\\n\\n                # frequency/name just warn\\n                if key in [\\'freq\\', \\'index_name\\']:\\n                    ws = attribute_conflict_doc % (key, existing_value, value)\\n                    warnings.warn(ws, AttributeConflictWarning, stacklevel=6)\\n\\n                    # reset\\n                    idx[key] = None\\n                    setattr(self, key, None)\\n\\n                else:\\n                    raise ValueError(\\n                        \"invalid info for [{name}] for [{key}], \"\\n                        \"existing_value [{existing_value}] conflicts with \"\\n                        \"new value [{value}]\".format(\\n                            name=self.name, key=key,\\n                            existing_value=existing_value, value=value))\\n            else:\\n                if value is not None or existing_value is not None:\\n                    idx[key] = value\\n\\n        return self',\n 'def set_info(self, info):\\n        \"\"\" set my state from the passed info \"\"\"\\n        idx = info.get(self.name)\\n        if idx is not None:\\n            self.__dict__.update(idx)',\n 'def validate_metadata(self, handler):\\n        \"\"\" validate that kind=category does not change the categories \"\"\"\\n        if self.meta == \\'category\\':\\n            new_metadata = self.metadata\\n            cur_metadata = handler.read_metadata(self.cname)\\n            if (new_metadata is not None and cur_metadata is not None and\\n                    not array_equivalent(new_metadata, cur_metadata)):\\n                raise ValueError(\"cannot append a categorical with \"\\n                                 \"different categories to the existing\")',\n 'def write_metadata(self, handler):\\n        \"\"\" set the meta data \"\"\"\\n        if self.metadata is not None:\\n            handler.write_metadata(self.cname, self.metadata)',\n 'def convert(self, values, nan_rep, encoding, errors):\\n        \"\"\" set the values from this selection: take = take ownership \"\"\"\\n\\n        self.values = Int64Index(np.arange(self.table.nrows))\\n        return self',\n 'def create_for_block(\\n            cls, i=None, name=None, cname=None, version=None, **kwargs):\\n        \"\"\" return a new datacol with the block i \"\"\"\\n\\n        if cname is None:\\n            cname = name or \\'values_block_{idx}\\'.format(idx=i)\\n        if name is None:\\n            name = cname\\n\\n        # prior to 0.10.1, we named values blocks like: values_block_0 an the\\n        # name values_0\\n        try:\\n            if version[0] == 0 and version[1] <= 10 and version[2] == 0:\\n                m = re.search(r\"values_block_(\\\\d+)\", name)\\n                if m:\\n                    name = \"values_{group}\".format(group=m.groups()[0])\\n        except IndexError:\\n            pass\\n\\n        return cls(name=name, cname=cname, **kwargs)',\n 'def set_metadata(self, metadata):\\n        \"\"\" record the metadata \"\"\"\\n        if metadata is not None:\\n            metadata = np.array(metadata, copy=False).ravel()\\n        self.metadata = metadata',\n 'def set_atom(self, block, block_items, existing_col, min_itemsize,\\n                 nan_rep, info, encoding=None, errors=\\'strict\\'):\\n        \"\"\" create and setup my atom from the block b \"\"\"\\n\\n        self.values = list(block_items)\\n\\n        # short-cut certain block types\\n        if block.is_categorical:\\n            return self.set_atom_categorical(block, items=block_items,\\n                                             info=info)\\n        elif block.is_datetimetz:\\n            return self.set_atom_datetime64tz(block, info=info)\\n        elif block.is_datetime:\\n            return self.set_atom_datetime64(block)\\n        elif block.is_timedelta:\\n            return self.set_atom_timedelta64(block)\\n        elif block.is_complex:\\n            return self.set_atom_complex(block)\\n\\n        dtype = block.dtype.name\\n        inferred_type = lib.infer_dtype(block.values, skipna=False)\\n\\n        if inferred_type == \\'date\\':\\n            raise TypeError(\\n                \"[date] is not implemented as a table column\")\\n        elif inferred_type == \\'datetime\\':\\n            # after 8260\\n            # this only would be hit for a mutli-timezone dtype\\n            # which is an error\\n\\n            raise TypeError(\\n                \"too many timezones in this block, create separate \"\\n                \"data columns\"\\n            )\\n        elif inferred_type == \\'unicode\\':\\n            raise TypeError(\\n                \"[unicode] is not implemented as a table column\")\\n\\n        # this is basically a catchall; if say a datetime64 has nans then will\\n        # end up here ###\\n        elif inferred_type == \\'string\\' or dtype == \\'object\\':\\n            self.set_atom_string(\\n                block, block_items,\\n                existing_col,\\n                min_itemsize,\\n                nan_rep,\\n                encoding,\\n                errors)\\n\\n        # set as a data block\\n        else:\\n            self.set_atom_data(block)',\n 'def get_atom_coltype(self, kind=None):\\n        \"\"\" return the PyTables column class for this column \"\"\"\\n        if kind is None:\\n            kind = self.kind\\n        if self.kind.startswith(\\'uint\\'):\\n            col_name = \"UInt{name}Col\".format(name=kind[4:])\\n        else:\\n            col_name = \"{name}Col\".format(name=kind.capitalize())\\n\\n        return getattr(_tables(), col_name)',\n 'def validate_attr(self, append):\\n        \"\"\"validate that we have the same order as the existing & same dtype\"\"\"\\n        if append:\\n            existing_fields = getattr(self.attrs, self.kind_attr, None)\\n            if (existing_fields is not None and\\n                    existing_fields != list(self.values)):\\n                raise ValueError(\"appended items do not match existing items\"\\n                                 \" in table!\")\\n\\n            existing_dtype = getattr(self.attrs, self.dtype_attr, None)\\n            if (existing_dtype is not None and\\n                    existing_dtype != self.dtype):\\n                raise ValueError(\"appended items dtype do not match existing \"\\n                                 \"items dtype in table!\")',\n 'def convert(self, values, nan_rep, encoding, errors):\\n        \"\"\"set the data from this selection (and convert to the correct dtype\\n        if we can)\\n        \"\"\"\\n\\n        # values is a recarray\\n        if values.dtype.fields is not None:\\n            values = values[self.cname]\\n\\n        self.set_data(values)\\n\\n        # use the meta if needed\\n        meta = _ensure_decoded(self.meta)\\n\\n        # convert to the correct dtype\\n        if self.dtype is not None:\\n            dtype = _ensure_decoded(self.dtype)\\n\\n            # reverse converts\\n            if dtype == \\'datetime64\\':\\n\\n                # recreate with tz if indicated\\n                self.data = _set_tz(self.data, self.tz, coerce=True)\\n\\n            elif dtype == \\'timedelta64\\':\\n                self.data = np.asarray(self.data, dtype=\\'m8[ns]\\')\\n            elif dtype == \\'date\\':\\n                try:\\n                    self.data = np.asarray(\\n                        [date.fromordinal(v) for v in self.data], dtype=object)\\n                except ValueError:\\n                    self.data = np.asarray(\\n                        [date.fromtimestamp(v) for v in self.data],\\n                        dtype=object)\\n            elif dtype == \\'datetime\\':\\n                self.data = np.asarray(\\n                    [datetime.fromtimestamp(v) for v in self.data],\\n                    dtype=object)\\n\\n            elif meta == \\'category\\':\\n\\n                # we have a categorical\\n                categories = self.metadata\\n                codes = self.data.ravel()\\n\\n                # if we have stored a NaN in the categories\\n                # then strip it; in theory we could have BOTH\\n                # -1s in the codes and nulls :<\\n                if categories is None:\\n                    # Handle case of NaN-only categorical columns in which case\\n                    # the categories are an empty array; when this is stored,\\n                    # pytables cannot write a zero-len array, so on readback\\n                    # the categories would be None and `read_hdf()` would fail.\\n                    categories = Index([], dtype=np.float64)\\n                else:\\n                    mask = isna(categories)\\n                    if mask.any():\\n                        categories = categories[~mask]\\n                        codes[codes != -1] -= mask.astype(int).cumsum().values\\n\\n                self.data = Categorical.from_codes(codes,\\n                                                   categories=categories,\\n                                                   ordered=self.ordered)\\n\\n            else:\\n\\n                try:\\n                    self.data = self.data.astype(dtype, copy=False)\\n                except TypeError:\\n                    self.data = self.data.astype(\\'O\\', copy=False)\\n\\n        # convert nans / decode\\n        if _ensure_decoded(self.kind) == \\'string\\':\\n            self.data = _unconvert_string_array(\\n                self.data, nan_rep=nan_rep, encoding=encoding, errors=errors)\\n\\n        return self',\n 'def get_attr(self):\\n        \"\"\" get the data for this column \"\"\"\\n        self.values = getattr(self.attrs, self.kind_attr, None)\\n        self.dtype = getattr(self.attrs, self.dtype_attr, None)\\n        self.meta = getattr(self.attrs, self.meta_attr, None)\\n        self.set_kind()',\n 'def set_attr(self):\\n        \"\"\" set the data for this column \"\"\"\\n        setattr(self.attrs, self.kind_attr, self.values)\\n        setattr(self.attrs, self.meta_attr, self.meta)\\n        if self.dtype is not None:\\n            setattr(self.attrs, self.dtype_attr, self.dtype)',\n 'def set_version(self):\\n        \"\"\" compute and set our version \"\"\"\\n        version = _ensure_decoded(\\n            getattr(self.group._v_attrs, \\'pandas_version\\', None))\\n        try:\\n            self.version = tuple(int(x) for x in version.split(\\'.\\'))\\n            if len(self.version) == 2:\\n                self.version = self.version + (0,)\\n        except AttributeError:\\n            self.version = (0, 0, 0)',\n 'def set_object_info(self):\\n        \"\"\" set my pandas type & version \"\"\"\\n        self.attrs.pandas_type = str(self.pandas_kind)\\n        self.attrs.pandas_version = str(_version)\\n        self.set_version()',\n 'def infer_axes(self):\\n        \"\"\" infer the axes of my storer\\n              return a boolean indicating if we have a valid storer or not \"\"\"\\n\\n        s = self.storable\\n        if s is None:\\n            return False\\n        self.get_attrs()\\n        return True',\n 'def delete(self, where=None, start=None, stop=None, **kwargs):\\n        \"\"\"\\n        support fully deleting the node in its entirety (only) - where\\n        specification must be None\\n        \"\"\"\\n        if com._all_none(where, start, stop):\\n            self._handle.remove_node(self.group, recursive=True)\\n            return None\\n\\n        raise TypeError(\"cannot delete on an abstract storer\")',\n 'def validate_read(self, kwargs):\\n        \"\"\"\\n        remove table keywords from kwargs and return\\n        raise if any keywords are passed which are not-None\\n        \"\"\"\\n        kwargs = copy.copy(kwargs)\\n\\n        columns = kwargs.pop(\\'columns\\', None)\\n        if columns is not None:\\n            raise TypeError(\"cannot pass a column specification when reading \"\\n                            \"a Fixed format store. this store must be \"\\n                            \"selected in its entirety\")\\n        where = kwargs.pop(\\'where\\', None)\\n        if where is not None:\\n            raise TypeError(\"cannot pass a where specification when reading \"\\n                            \"from a Fixed format store. this store must be \"\\n                            \"selected in its entirety\")\\n        return kwargs',\n 'def set_attrs(self):\\n        \"\"\" set our object attributes \"\"\"\\n        self.attrs.encoding = self.encoding\\n        self.attrs.errors = self.errors',\n 'def get_attrs(self):\\n        \"\"\" retrieve our attributes \"\"\"\\n        self.encoding = _ensure_encoding(getattr(self.attrs, \\'encoding\\', None))\\n        self.errors = _ensure_decoded(getattr(self.attrs, \\'errors\\', \\'strict\\'))\\n        for n in self.attributes:\\n            setattr(self, n, _ensure_decoded(getattr(self.attrs, n, None)))',\n 'def read_array(self, key, start=None, stop=None):\\n        \"\"\" read an array for the specified node (off of group \"\"\"\\n        import tables\\n        node = getattr(self.group, key)\\n        attrs = node._v_attrs\\n\\n        transposed = getattr(attrs, \\'transposed\\', False)\\n\\n        if isinstance(node, tables.VLArray):\\n            ret = node[0][start:stop]\\n        else:\\n            dtype = getattr(attrs, \\'value_type\\', None)\\n            shape = getattr(attrs, \\'shape\\', None)\\n\\n            if shape is not None:\\n                # length 0 axis\\n                ret = np.empty(shape, dtype=dtype)\\n            else:\\n                ret = node[start:stop]\\n\\n            if dtype == \\'datetime64\\':\\n\\n                # reconstruct a timezone if indicated\\n                ret = _set_tz(ret, getattr(attrs, \\'tz\\', None), coerce=True)\\n\\n            elif dtype == \\'timedelta64\\':\\n                ret = np.asarray(ret, dtype=\\'m8[ns]\\')\\n\\n        if transposed:\\n            return ret.T\\n        else:\\n            return ret',\n 'def write_array_empty(self, key, value):\\n        \"\"\" write a 0-len array \"\"\"\\n\\n        # ugly hack for length 0 axes\\n        arr = np.empty((1,) * value.ndim)\\n        self._handle.create_array(self.group, key, arr)\\n        getattr(self.group, key)._v_attrs.value_type = str(value.dtype)\\n        getattr(self.group, key)._v_attrs.shape = value.shape',\n 'def validate_read(self, kwargs):\\n        \"\"\"\\n        we don\\'t support start, stop kwds in Sparse\\n        \"\"\"\\n        kwargs = super().validate_read(kwargs)\\n        if \\'start\\' in kwargs or \\'stop\\' in kwargs:\\n            raise NotImplementedError(\"start and/or stop are not supported \"\\n                                      \"in fixed Sparse reading\")\\n        return kwargs',\n 'def write(self, obj, **kwargs):\\n        \"\"\" write it as a collection of individual sparse series \"\"\"\\n        super().write(obj, **kwargs)\\n        for name, ss in obj.items():\\n            key = \\'sparse_series_{name}\\'.format(name=name)\\n            if key not in self.group._v_children:\\n                node = self._handle.create_group(self.group, key)\\n            else:\\n                node = getattr(self.group, key)\\n            s = SparseSeriesFixed(self.parent, node)\\n            s.write(ss)\\n        self.attrs.default_fill_value = obj.default_fill_value\\n        self.attrs.default_kind = obj.default_kind\\n        self.write_index(\\'columns\\', obj.columns)',\n 'def validate(self, other):\\n        \"\"\" validate against an existing table \"\"\"\\n        if other is None:\\n            return\\n\\n        if other.table_type != self.table_type:\\n            raise TypeError(\\n                \"incompatible table_type with existing \"\\n                \"[{other} - {self}]\".format(\\n                    other=other.table_type, self=self.table_type))\\n\\n        for c in [\\'index_axes\\', \\'non_index_axes\\', \\'values_axes\\']:\\n            sv = getattr(self, c, None)\\n            ov = getattr(other, c, None)\\n            if sv != ov:\\n\\n                # show the error for the specific axes\\n                for i, sax in enumerate(sv):\\n                    oax = ov[i]\\n                    if sax != oax:\\n                        raise ValueError(\\n                            \"invalid combinate of [{c}] on appending data \"\\n                            \"[{sax}] vs current table [{oax}]\".format(\\n                                c=c, sax=sax, oax=oax))\\n\\n                # should never get here\\n                raise Exception(\\n                    \"invalid combinate of [{c}] on appending data [{sv}] vs \"\\n                    \"current table [{ov}]\".format(c=c, sv=sv, ov=ov))',\n 'def validate_metadata(self, existing):\\n        \"\"\" create / validate metadata \"\"\"\\n        self.metadata = [\\n            c.name for c in self.values_axes if c.metadata is not None]',\n 'def validate_multiindex(self, obj):\\n        \"\"\"validate that we can store the multi-index; reset and return the\\n        new object\\n        \"\"\"\\n        levels = [l if l is not None else \"level_{0}\".format(i)\\n                  for i, l in enumerate(obj.index.names)]\\n        try:\\n            return obj.reset_index(), levels\\n        except ValueError:\\n            raise ValueError(\"duplicate names/columns in the multi-index when \"\\n                             \"storing as a table\")',\n 'def nrows_expected(self):\\n        \"\"\" based on our axes, compute the expected nrows \"\"\"\\n        return np.prod([i.cvalues.shape[0] for i in self.index_axes])',\n 'def data_orientation(self):\\n        \"\"\"return a tuple of my permutated axes, non_indexable at the front\"\"\"\\n        return tuple(itertools.chain([int(a[0]) for a in self.non_index_axes],\\n                                     [int(a.axis) for a in self.index_axes]))',\n 'def queryables(self):\\n        \"\"\" return a dict of the kinds allowable columns for this object \"\"\"\\n\\n        # compute the values_axes queryables\\n        return dict(\\n            [(a.cname, a) for a in self.index_axes] +\\n            [(self.storage_obj_type._AXIS_NAMES[axis], None)\\n             for axis, values in self.non_index_axes] +\\n            [(v.cname, v) for v in self.values_axes\\n             if v.name in set(self.data_columns)]\\n        )',\n 'def _get_metadata_path(self, key):\\n        \"\"\" return the metadata pathname for this key \"\"\"\\n        return \"{group}/meta/{key}/meta\".format(group=self.group._v_pathname,\\n                                                key=key)',\n 'def write_metadata(self, key, values):\\n        \"\"\"\\n        write out a meta data array to the key as a fixed-format Series\\n\\n        Parameters\\n        ----------\\n        key : string\\n        values : ndarray\\n\\n        \"\"\"\\n        values = Series(values)\\n        self.parent.put(self._get_metadata_path(key), values, format=\\'table\\',\\n                        encoding=self.encoding, errors=self.errors,\\n                        nan_rep=self.nan_rep)',\n 'def read_metadata(self, key):\\n        \"\"\" return the meta data array for this key \"\"\"\\n        if getattr(getattr(self.group, \\'meta\\', None), key, None) is not None:\\n            return self.parent.select(self._get_metadata_path(key))\\n        return None',\n 'def set_attrs(self):\\n        \"\"\" set our table type & indexables \"\"\"\\n        self.attrs.table_type = str(self.table_type)\\n        self.attrs.index_cols = self.index_cols()\\n        self.attrs.values_cols = self.values_cols()\\n        self.attrs.non_index_axes = self.non_index_axes\\n        self.attrs.data_columns = self.data_columns\\n        self.attrs.nan_rep = self.nan_rep\\n        self.attrs.encoding = self.encoding\\n        self.attrs.errors = self.errors\\n        self.attrs.levels = self.levels\\n        self.attrs.metadata = self.metadata\\n        self.set_info()',\n 'def get_attrs(self):\\n        \"\"\" retrieve our attributes \"\"\"\\n        self.non_index_axes = getattr(\\n            self.attrs, \\'non_index_axes\\', None) or []\\n        self.data_columns = getattr(\\n            self.attrs, \\'data_columns\\', None) or []\\n        self.info = getattr(\\n            self.attrs, \\'info\\', None) or dict()\\n        self.nan_rep = getattr(self.attrs, \\'nan_rep\\', None)\\n        self.encoding = _ensure_encoding(\\n            getattr(self.attrs, \\'encoding\\', None))\\n        self.errors = _ensure_decoded(getattr(self.attrs, \\'errors\\', \\'strict\\'))\\n        self.levels = getattr(\\n            self.attrs, \\'levels\\', None) or []\\n        self.index_axes = [\\n            a.infer(self) for a in self.indexables if a.is_an_indexable\\n        ]\\n        self.values_axes = [\\n            a.infer(self) for a in self.indexables if not a.is_an_indexable\\n        ]\\n        self.metadata = getattr(\\n            self.attrs, \\'metadata\\', None) or []',\n 'def validate_version(self, where=None):\\n        \"\"\" are we trying to operate on an old version? \"\"\"\\n        if where is not None:\\n            if (self.version[0] <= 0 and self.version[1] <= 10 and\\n                    self.version[2] < 1):\\n                ws = incompatibility_doc % \\'.\\'.join(\\n                    [str(x) for x in self.version])\\n                warnings.warn(ws, IncompatibilityWarning)',\n 'def validate_min_itemsize(self, min_itemsize):\\n        \"\"\"validate the min_itemisze doesn\\'t contain items that are not in the\\n        axes this needs data_columns to be defined\\n        \"\"\"\\n        if min_itemsize is None:\\n            return\\n        if not isinstance(min_itemsize, dict):\\n            return\\n\\n        q = self.queryables()\\n        for k, v in min_itemsize.items():\\n\\n            # ok, apply generally\\n            if k == \\'values\\':\\n                continue\\n            if k not in q:\\n                raise ValueError(\\n                    \"min_itemsize has the key [{key}] which is not an axis or \"\\n                    \"data_column\".format(key=k))',\n 'def indexables(self):\\n        \"\"\" create/cache the indexables if they don\\'t exist \"\"\"\\n        if self._indexables is None:\\n\\n            self._indexables = []\\n\\n            # index columns\\n            self._indexables.extend([\\n                IndexCol(name=name, axis=axis, pos=i)\\n                for i, (axis, name) in enumerate(self.attrs.index_cols)\\n            ])\\n\\n            # values columns\\n            dc = set(self.data_columns)\\n            base_pos = len(self._indexables)\\n\\n            def f(i, c):\\n                klass = DataCol\\n                if c in dc:\\n                    klass = DataIndexableCol\\n                return klass.create_for_block(i=i, name=c, pos=base_pos + i,\\n                                              version=self.version)\\n\\n            self._indexables.extend(\\n                [f(i, c) for i, c in enumerate(self.attrs.values_cols)])\\n\\n        return self._indexables',\n 'def create_index(self, columns=None, optlevel=None, kind=None):\\n        \"\"\"\\n        Create a pytables index on the specified columns\\n          note: cannot index Time64Col() or ComplexCol currently;\\n          PyTables must be >= 3.0\\n\\n        Parameters\\n        ----------\\n        columns : False (don\\'t create an index), True (create all columns\\n            index), None or list_like (the indexers to index)\\n        optlevel: optimization level (defaults to 6)\\n        kind    : kind of index (defaults to \\'medium\\')\\n\\n        Exceptions\\n        ----------\\n        raises if the node is not a table\\n\\n        \"\"\"\\n\\n        if not self.infer_axes():\\n            return\\n        if columns is False:\\n            return\\n\\n        # index all indexables and data_columns\\n        if columns is None or columns is True:\\n            columns = [a.cname for a in self.axes if a.is_data_indexable]\\n        if not isinstance(columns, (tuple, list)):\\n            columns = [columns]\\n\\n        kw = dict()\\n        if optlevel is not None:\\n            kw[\\'optlevel\\'] = optlevel\\n        if kind is not None:\\n            kw[\\'kind\\'] = kind\\n\\n        table = self.table\\n        for c in columns:\\n            v = getattr(table.cols, c, None)\\n            if v is not None:\\n\\n                # remove the index if the kind/optlevel have changed\\n                if v.is_indexed:\\n                    index = v.index\\n                    cur_optlevel = index.optlevel\\n                    cur_kind = index.kind\\n\\n                    if kind is not None and cur_kind != kind:\\n                        v.remove_index()\\n                    else:\\n                        kw[\\'kind\\'] = cur_kind\\n\\n                    if optlevel is not None and cur_optlevel != optlevel:\\n                        v.remove_index()\\n                    else:\\n                        kw[\\'optlevel\\'] = cur_optlevel\\n\\n                # create the index\\n                if not v.is_indexed:\\n                    if v.type.startswith(\\'complex\\'):\\n                        raise TypeError(\\n                            \\'Columns containing complex values can be stored \\'\\n                            \\'but cannot\\'\\n                            \\' be indexed when using table format. Either use \\'\\n                            \\'fixed format, set index=False, or do not include \\'\\n                            \\'the columns containing complex values to \\'\\n                            \\'data_columns when initializing the table.\\')\\n                    v.create_index(**kw)',\n 'def read_axes(self, where, **kwargs):\\n        \"\"\"create and return the axes sniffed from the table: return boolean\\n        for success\\n        \"\"\"\\n\\n        # validate the version\\n        self.validate_version(where)\\n\\n        # infer the data kind\\n        if not self.infer_axes():\\n            return False\\n\\n        # create the selection\\n        self.selection = Selection(self, where=where, **kwargs)\\n        values = self.selection.select()\\n\\n        # convert the data\\n        for a in self.axes:\\n            a.set_info(self.info)\\n            a.convert(values, nan_rep=self.nan_rep, encoding=self.encoding,\\n                      errors=self.errors)\\n\\n        return True',\n 'def validate_data_columns(self, data_columns, min_itemsize):\\n        \"\"\"take the input data_columns and min_itemize and create a data\\n        columns spec\\n        \"\"\"\\n\\n        if not len(self.non_index_axes):\\n            return []\\n\\n        axis, axis_labels = self.non_index_axes[0]\\n        info = self.info.get(axis, dict())\\n        if info.get(\\'type\\') == \\'MultiIndex\\' and data_columns:\\n            raise ValueError(\"cannot use a multi-index on axis [{0}] with \"\\n                             \"data_columns {1}\".format(axis, data_columns))\\n\\n        # evaluate the passed data_columns, True == use all columns\\n        # take only valide axis labels\\n        if data_columns is True:\\n            data_columns = list(axis_labels)\\n        elif data_columns is None:\\n            data_columns = []\\n\\n        # if min_itemsize is a dict, add the keys (exclude \\'values\\')\\n        if isinstance(min_itemsize, dict):\\n\\n            existing_data_columns = set(data_columns)\\n            data_columns.extend([\\n                k for k in min_itemsize.keys()\\n                if k != \\'values\\' and k not in existing_data_columns\\n            ])\\n\\n        # return valid columns in the order of our axis\\n        return [c for c in data_columns if c in axis_labels]',\n 'def create_axes(self, axes, obj, validate=True, nan_rep=None,\\n                    data_columns=None, min_itemsize=None, **kwargs):\\n        \"\"\" create and return the axes\\n        leagcy tables create an indexable column, indexable index,\\n        non-indexable fields\\n\\n            Parameters:\\n            -----------\\n            axes: a list of the axes in order to create (names or numbers of\\n                the axes)\\n            obj : the object to create axes on\\n            validate: validate the obj against an existing object already\\n                written\\n            min_itemsize: a dict of the min size for a column in bytes\\n            nan_rep : a values to use for string column nan_rep\\n            encoding : the encoding for string values\\n            data_columns : a list of columns that we want to create separate to\\n                allow indexing (or True will force all columns)\\n\\n        \"\"\"\\n\\n        # set the default axes if needed\\n        if axes is None:\\n            try:\\n                axes = _AXES_MAP[type(obj)]\\n            except KeyError:\\n                raise TypeError(\\n                    \"cannot properly create the storer for: [group->{group},\"\\n                    \"value->{value}]\".format(\\n                        group=self.group._v_name, value=type(obj)))\\n\\n        # map axes to numbers\\n        axes = [obj._get_axis_number(a) for a in axes]\\n\\n        # do we have an existing table (if so, use its axes & data_columns)\\n        if self.infer_axes():\\n            existing_table = self.copy()\\n            existing_table.infer_axes()\\n            axes = [a.axis for a in existing_table.index_axes]\\n            data_columns = existing_table.data_columns\\n            nan_rep = existing_table.nan_rep\\n            self.encoding = existing_table.encoding\\n            self.errors = existing_table.errors\\n            self.info = copy.copy(existing_table.info)\\n        else:\\n            existing_table = None\\n\\n        # currently support on ndim-1 axes\\n        if len(axes) != self.ndim - 1:\\n            raise ValueError(\\n                \"currently only support ndim-1 indexers in an AppendableTable\")\\n\\n        # create according to the new data\\n        self.non_index_axes = []\\n        self.data_columns = []\\n\\n        # nan_representation\\n        if nan_rep is None:\\n            nan_rep = \\'nan\\'\\n\\n        self.nan_rep = nan_rep\\n\\n        # create axes to index and non_index\\n        index_axes_map = dict()\\n        for i, a in enumerate(obj.axes):\\n\\n            if i in axes:\\n                name = obj._AXIS_NAMES[i]\\n                index_axes_map[i] = _convert_index(\\n                    a, self.encoding, self.errors, self.format_type\\n                ).set_name(name).set_axis(i)\\n            else:\\n\\n                # we might be able to change the axes on the appending data if\\n                # necessary\\n                append_axis = list(a)\\n                if existing_table is not None:\\n                    indexer = len(self.non_index_axes)\\n                    exist_axis = existing_table.non_index_axes[indexer][1]\\n                    if not array_equivalent(np.array(append_axis),\\n                                            np.array(exist_axis)):\\n\\n                        # ahah! -> reindex\\n                        if array_equivalent(np.array(sorted(append_axis)),\\n                                            np.array(sorted(exist_axis))):\\n                            append_axis = exist_axis\\n\\n                # the non_index_axes info\\n                info = _get_info(self.info, i)\\n                info[\\'names\\'] = list(a.names)\\n                info[\\'type\\'] = a.__class__.__name__\\n\\n                self.non_index_axes.append((i, append_axis))\\n\\n        # set axis positions (based on the axes)\\n        self.index_axes = [\\n            index_axes_map[a].set_pos(j).update_info(self.info)\\n            for j, a in enumerate(axes)\\n        ]\\n        j = len(self.index_axes)\\n\\n        # check for column conflicts\\n        for a in self.axes:\\n            a.maybe_set_size(min_itemsize=min_itemsize)\\n\\n        # reindex by our non_index_axes & compute data_columns\\n        for a in self.non_index_axes:\\n            obj = _reindex_axis(obj, a[0], a[1])\\n\\n        def get_blk_items(mgr, blocks):\\n            return [mgr.items.take(blk.mgr_locs) for blk in blocks]\\n\\n        # figure out data_columns and get out blocks\\n        block_obj = self.get_object(obj)._consolidate()\\n        blocks = block_obj._data.blocks\\n        blk_items = get_blk_items(block_obj._data, blocks)\\n        if len(self.non_index_axes):\\n            axis, axis_labels = self.non_index_axes[0]\\n            data_columns = self.validate_data_columns(\\n                data_columns, min_itemsize)\\n            if len(data_columns):\\n                mgr = block_obj.reindex(\\n                    Index(axis_labels).difference(Index(data_columns)),\\n                    axis=axis\\n                )._data\\n\\n                blocks = list(mgr.blocks)\\n                blk_items = get_blk_items(mgr, blocks)\\n                for c in data_columns:\\n                    mgr = block_obj.reindex([c], axis=axis)._data\\n                    blocks.extend(mgr.blocks)\\n                    blk_items.extend(get_blk_items(mgr, mgr.blocks))\\n\\n        # reorder the blocks in the same order as the existing_table if we can\\n        if existing_table is not None:\\n            by_items = {tuple(b_items.tolist()): (b, b_items)\\n                        for b, b_items in zip(blocks, blk_items)}\\n            new_blocks = []\\n            new_blk_items = []\\n            for ea in existing_table.values_axes:\\n                items = tuple(ea.values)\\n                try:\\n                    b, b_items = by_items.pop(items)\\n                    new_blocks.append(b)\\n                    new_blk_items.append(b_items)\\n                except (IndexError, KeyError):\\n                    raise ValueError(\\n                        \"cannot match existing table structure for [{items}] \"\\n                        \"on appending data\".format(\\n                            items=(\\',\\'.join(pprint_thing(item) for\\n                                            item in items))))\\n            blocks = new_blocks\\n            blk_items = new_blk_items\\n\\n        # add my values\\n        self.values_axes = []\\n        for i, (b, b_items) in enumerate(zip(blocks, blk_items)):\\n\\n            # shape of the data column are the indexable axes\\n            klass = DataCol\\n            name = None\\n\\n            # we have a data_column\\n            if (data_columns and len(b_items) == 1 and\\n                    b_items[0] in data_columns):\\n                klass = DataIndexableCol\\n                name = b_items[0]\\n                self.data_columns.append(name)\\n\\n            # make sure that we match up the existing columns\\n            # if we have an existing table\\n            if existing_table is not None and validate:\\n                try:\\n                    existing_col = existing_table.values_axes[i]\\n                except (IndexError, KeyError):\\n                    raise ValueError(\\n                        \"Incompatible appended table [{blocks}]\"\\n                        \"with existing table [{table}]\".format(\\n                            blocks=blocks,\\n                            table=existing_table.values_axes))\\n            else:\\n                existing_col = None\\n\\n            try:\\n                col = klass.create_for_block(\\n                    i=i, name=name, version=self.version)\\n                col.set_atom(block=b, block_items=b_items,\\n                             existing_col=existing_col,\\n                             min_itemsize=min_itemsize,\\n                             nan_rep=nan_rep,\\n                             encoding=self.encoding,\\n                             errors=self.errors,\\n                             info=self.info)\\n                col.set_pos(j)\\n\\n                self.values_axes.append(col)\\n            except (NotImplementedError, ValueError, TypeError) as e:\\n                raise e\\n            except Exception as detail:\\n                raise Exception(\\n                    \"cannot find the correct atom type -> \"\\n                    \"[dtype->{name},items->{items}] {detail!s}\".format(\\n                        name=b.dtype.name, items=b_items, detail=detail))\\n            j += 1\\n\\n        # validate our min_itemsize\\n        self.validate_min_itemsize(min_itemsize)\\n\\n        # validate our metadata\\n        self.validate_metadata(existing_table)\\n\\n        # validate the axes if we have an existing table\\n        if validate:\\n            self.validate(existing_table)',\n 'def process_axes(self, obj, columns=None):\\n        \"\"\" process axes filters \"\"\"\\n\\n        # make a copy to avoid side effects\\n        if columns is not None:\\n            columns = list(columns)\\n\\n        # make sure to include levels if we have them\\n        if columns is not None and self.is_multi_index:\\n            for n in self.levels:\\n                if n not in columns:\\n                    columns.insert(0, n)\\n\\n        # reorder by any non_index_axes & limit to the select columns\\n        for axis, labels in self.non_index_axes:\\n            obj = _reindex_axis(obj, axis, labels, columns)\\n\\n        # apply the selection filters (but keep in the same order)\\n        if self.selection.filter is not None:\\n            for field, op, filt in self.selection.filter.format():\\n\\n                def process_filter(field, filt):\\n\\n                    for axis_name in obj._AXIS_NAMES.values():\\n                        axis_number = obj._get_axis_number(axis_name)\\n                        axis_values = obj._get_axis(axis_name)\\n\\n                        # see if the field is the name of an axis\\n                        if field == axis_name:\\n\\n                            # if we have a multi-index, then need to include\\n                            # the levels\\n                            if self.is_multi_index:\\n                                filt = filt.union(Index(self.levels))\\n\\n                            takers = op(axis_values, filt)\\n                            return obj.loc._getitem_axis(takers,\\n                                                         axis=axis_number)\\n\\n                        # this might be the name of a file IN an axis\\n                        elif field in axis_values:\\n\\n                            # we need to filter on this dimension\\n                            values = ensure_index(getattr(obj, field).values)\\n                            filt = ensure_index(filt)\\n\\n                            # hack until we support reversed dim flags\\n                            if isinstance(obj, DataFrame):\\n                                axis_number = 1 - axis_number\\n                            takers = op(values, filt)\\n                            return obj.loc._getitem_axis(takers,\\n                                                         axis=axis_number)\\n\\n                    raise ValueError(\"cannot find the field [{field}] for \"\\n                                     \"filtering!\".format(field=field))\\n\\n                obj = process_filter(field, filt)\\n\\n        return obj',\n 'def create_description(self, complib=None, complevel=None,\\n                           fletcher32=False, expectedrows=None):\\n        \"\"\" create the description of the table from the axes & values \"\"\"\\n\\n        # provided expected rows if its passed\\n        if expectedrows is None:\\n            expectedrows = max(self.nrows_expected, 10000)\\n\\n        d = dict(name=\\'table\\', expectedrows=expectedrows)\\n\\n        # description from the axes & values\\n        d[\\'description\\'] = {a.cname: a.typ for a in self.axes}\\n\\n        if complib:\\n            if complevel is None:\\n                complevel = self._complevel or 9\\n            filters = _tables().Filters(\\n                complevel=complevel, complib=complib,\\n                fletcher32=fletcher32 or self._fletcher32)\\n            d[\\'filters\\'] = filters\\n        elif self._filters is not None:\\n            d[\\'filters\\'] = self._filters\\n\\n        return d',\n 'def read_coordinates(self, where=None, start=None, stop=None, **kwargs):\\n        \"\"\"select coordinates (row numbers) from a table; return the\\n        coordinates object\\n        \"\"\"\\n\\n        # validate the version\\n        self.validate_version(where)\\n\\n        # infer the data kind\\n        if not self.infer_axes():\\n            return False\\n\\n        # create the selection\\n        self.selection = Selection(\\n            self, where=where, start=start, stop=stop, **kwargs)\\n        coords = self.selection.select_coords()\\n        if self.selection.filter is not None:\\n            for field, op, filt in self.selection.filter.format():\\n                data = self.read_column(\\n                    field, start=coords.min(), stop=coords.max() + 1)\\n                coords = coords[\\n                    op(data.iloc[coords - coords.min()], filt).values]\\n\\n        return Index(coords)',\n 'def read_column(self, column, where=None, start=None, stop=None):\\n        \"\"\"return a single column from the table, generally only indexables\\n        are interesting\\n        \"\"\"\\n\\n        # validate the version\\n        self.validate_version()\\n\\n        # infer the data kind\\n        if not self.infer_axes():\\n            return False\\n\\n        if where is not None:\\n            raise TypeError(\"read_column does not currently accept a where \"\\n                            \"clause\")\\n\\n        # find the axes\\n        for a in self.axes:\\n            if column == a.name:\\n\\n                if not a.is_data_indexable:\\n                    raise ValueError(\\n                        \"column [{column}] can not be extracted individually; \"\\n                        \"it is not data indexable\".format(column=column))\\n\\n                # column must be an indexable or a data column\\n                c = getattr(self.table.cols, column)\\n                a.set_info(self.info)\\n                return Series(_set_tz(a.convert(c[start:stop],\\n                                                nan_rep=self.nan_rep,\\n                                                encoding=self.encoding,\\n                                                errors=self.errors\\n                                                ).take_data(),\\n                                      a.tz, True), name=column)\\n\\n        raise KeyError(\\n            \"column [{column}] not found in the table\".format(column=column))',\n 'def read(self, where=None, columns=None, **kwargs):\\n        \"\"\"we have n indexable columns, with an arbitrary number of data\\n        axes\\n        \"\"\"\\n\\n        if not self.read_axes(where=where, **kwargs):\\n            return None\\n\\n        raise NotImplementedError(\"Panel is removed in pandas 0.25.0\")',\n 'def write_data(self, chunksize, dropna=False):\\n        \"\"\" we form the data into a 2-d including indexes,values,mask\\n            write chunk-by-chunk \"\"\"\\n\\n        names = self.dtype.names\\n        nrows = self.nrows_expected\\n\\n        # if dropna==True, then drop ALL nan rows\\n        masks = []\\n        if dropna:\\n\\n            for a in self.values_axes:\\n\\n                # figure the mask: only do if we can successfully process this\\n                # column, otherwise ignore the mask\\n                mask = isna(a.data).all(axis=0)\\n                if isinstance(mask, np.ndarray):\\n                    masks.append(mask.astype(\\'u1\\', copy=False))\\n\\n        # consolidate masks\\n        if len(masks):\\n            mask = masks[0]\\n            for m in masks[1:]:\\n                mask = mask & m\\n            mask = mask.ravel()\\n        else:\\n            mask = None\\n\\n        # broadcast the indexes if needed\\n        indexes = [a.cvalues for a in self.index_axes]\\n        nindexes = len(indexes)\\n        bindexes = []\\n        for i, idx in enumerate(indexes):\\n\\n            # broadcast to all other indexes except myself\\n            if i > 0 and i < nindexes:\\n                repeater = np.prod(\\n                    [indexes[bi].shape[0] for bi in range(0, i)])\\n                idx = np.tile(idx, repeater)\\n\\n            if i < nindexes - 1:\\n                repeater = np.prod([indexes[bi].shape[0]\\n                                    for bi in range(i + 1, nindexes)])\\n                idx = np.repeat(idx, repeater)\\n\\n            bindexes.append(idx)\\n\\n        # transpose the values so first dimension is last\\n        # reshape the values if needed\\n        values = [a.take_data() for a in self.values_axes]\\n        values = [v.transpose(np.roll(np.arange(v.ndim), v.ndim - 1))\\n                  for v in values]\\n        bvalues = []\\n        for i, v in enumerate(values):\\n            new_shape = (nrows,) + self.dtype[names[nindexes + i]].shape\\n            bvalues.append(values[i].reshape(new_shape))\\n\\n        # write the chunks\\n        if chunksize is None:\\n            chunksize = 100000\\n\\n        rows = np.empty(min(chunksize, nrows), dtype=self.dtype)\\n        chunks = int(nrows / chunksize) + 1\\n        for i in range(chunks):\\n            start_i = i * chunksize\\n            end_i = min((i + 1) * chunksize, nrows)\\n            if start_i >= end_i:\\n                break\\n\\n            self.write_data_chunk(\\n                rows,\\n                indexes=[a[start_i:end_i] for a in bindexes],\\n                mask=mask[start_i:end_i] if mask is not None else None,\\n                values=[v[start_i:end_i] for v in bvalues])',\n 'def write_data_chunk(self, rows, indexes, mask, values):\\n        \"\"\"\\n        Parameters\\n        ----------\\n        rows : an empty memory space where we are putting the chunk\\n        indexes : an array of the indexes\\n        mask : an array of the masks\\n        values : an array of the values\\n        \"\"\"\\n\\n        # 0 len\\n        for v in values:\\n            if not np.prod(v.shape):\\n                return\\n\\n        try:\\n            nrows = indexes[0].shape[0]\\n            if nrows != len(rows):\\n                rows = np.empty(nrows, dtype=self.dtype)\\n            names = self.dtype.names\\n            nindexes = len(indexes)\\n\\n            # indexes\\n            for i, idx in enumerate(indexes):\\n                rows[names[i]] = idx\\n\\n            # values\\n            for i, v in enumerate(values):\\n                rows[names[i + nindexes]] = v\\n\\n            # mask\\n            if mask is not None:\\n                m = ~mask.ravel().astype(bool, copy=False)\\n                if not m.all():\\n                    rows = rows[m]\\n\\n        except Exception as detail:\\n            raise Exception(\\n                \"cannot create row-data -> {detail}\".format(detail=detail))\\n\\n        try:\\n            if len(rows):\\n                self.table.append(rows)\\n                self.table.flush()\\n        except Exception as detail:\\n            raise TypeError(\\n                \"tables cannot write this data -> {detail}\".format(\\n                    detail=detail))',\n 'def write(self, obj, data_columns=None, **kwargs):\\n        \"\"\" we are going to write this as a frame table \"\"\"\\n        if not isinstance(obj, DataFrame):\\n            name = obj.name or \\'values\\'\\n            obj = DataFrame({name: obj}, index=obj.index)\\n            obj.columns = [name]\\n        return super().write(obj=obj, data_columns=obj.columns.tolist(),\\n                             **kwargs)',\n 'def write(self, obj, **kwargs):\\n        \"\"\" we are going to write this as a frame table \"\"\"\\n        name = obj.name or \\'values\\'\\n        obj, self.levels = self.validate_multiindex(obj)\\n        cols = list(self.levels)\\n        cols.append(name)\\n        obj.columns = cols\\n        return super().write(obj=obj, **kwargs)',\n 'def get_attrs(self):\\n        \"\"\" retrieve our attributes \"\"\"\\n        self.non_index_axes = []\\n        self.nan_rep = None\\n        self.levels = []\\n\\n        self.index_axes = [a.infer(self)\\n                           for a in self.indexables if a.is_an_indexable]\\n        self.values_axes = [a.infer(self)\\n                            for a in self.indexables if not a.is_an_indexable]\\n        self.data_columns = [a.name for a in self.values_axes]',\n 'def indexables(self):\\n        \"\"\" create the indexables from the table description \"\"\"\\n        if self._indexables is None:\\n\\n            d = self.description\\n\\n            # the index columns is just a simple index\\n            self._indexables = [GenericIndexCol(name=\\'index\\', axis=0)]\\n\\n            for i, n in enumerate(d._v_names):\\n\\n                dc = GenericDataIndexableCol(\\n                    name=n, pos=i, values=[n], version=self.version)\\n                self._indexables.append(dc)\\n\\n        return self._indexables',\n 'def generate(self, where):\\n        \"\"\" where can be a : dict,list,tuple,string \"\"\"\\n        if where is None:\\n            return None\\n\\n        q = self.table.queryables()\\n        try:\\n            return Expr(where, queryables=q, encoding=self.table.encoding)\\n        except NameError:\\n            # raise a nice message, suggesting that the user should use\\n            # data_columns\\n            raise ValueError(\\n                \"The passed where expression: {0}\\\\n\"\\n                \"            contains an invalid variable reference\\\\n\"\\n                \"            all of the variable references must be a \"\\n                \"reference to\\\\n\"\\n                \"            an axis (e.g. \\'index\\' or \\'columns\\'), or a \"\\n                \"data_column\\\\n\"\\n                \"            The currently defined references are: {1}\\\\n\"\\n                .format(where, \\',\\'.join(q.keys()))\\n            )',\n 'def select(self):\\n        \"\"\"\\n        generate the selection\\n        \"\"\"\\n        if self.condition is not None:\\n            return self.table.table.read_where(self.condition.format(),\\n                                               start=self.start,\\n                                               stop=self.stop)\\n        elif self.coordinates is not None:\\n            return self.table.table.read_coordinates(self.coordinates)\\n        return self.table.table.read(start=self.start, stop=self.stop)',\n 'def select_coords(self):\\n        \"\"\"\\n        generate the selection\\n        \"\"\"\\n        start, stop = self.start, self.stop\\n        nrows = self.table.nrows\\n        if start is None:\\n            start = 0\\n        elif start < 0:\\n            start += nrows\\n        if self.stop is None:\\n            stop = nrows\\n        elif stop < 0:\\n            stop += nrows\\n\\n        if self.condition is not None:\\n            return self.table.table.get_where_list(self.condition.format(),\\n                                                   start=start, stop=stop,\\n                                                   sort=True)\\n        elif self.coordinates is not None:\\n            return self.coordinates\\n\\n        return np.arange(start, stop)',\n 'def astype(self, dtype, copy=True):\\n        \"\"\"\\n        Cast to a NumPy array with \\'dtype\\'.\\n\\n        Parameters\\n        ----------\\n        dtype : str or dtype\\n            Typecode or data-type to which the array is cast.\\n        copy : bool, default True\\n            Whether to copy the data, even if not necessary. If False,\\n            a copy is made only if the old dtype does not match the\\n            new dtype.\\n\\n        Returns\\n        -------\\n        array : ndarray\\n            NumPy ndarray with \\'dtype\\' for its dtype.\\n        \"\"\"\\n        return np.array(self, dtype=dtype, copy=copy)',\n 'def argsort(self, ascending=True, kind=\\'quicksort\\', *args, **kwargs):\\n        \"\"\"\\n        Return the indices that would sort this array.\\n\\n        Parameters\\n        ----------\\n        ascending : bool, default True\\n            Whether the indices should result in an ascending\\n            or descending sort.\\n        kind : {\\'quicksort\\', \\'mergesort\\', \\'heapsort\\'}, optional\\n            Sorting algorithm.\\n        *args, **kwargs:\\n            passed through to :func:`numpy.argsort`.\\n\\n        Returns\\n        -------\\n        index_array : ndarray\\n            Array of indices that sort ``self``.\\n\\n        See Also\\n        --------\\n        numpy.argsort : Sorting implementation used internally.\\n        \"\"\"\\n        # Implementor note: You have two places to override the behavior of\\n        # argsort.\\n        # 1. _values_for_argsort : construct the values passed to np.argsort\\n        # 2. argsort : total control over sorting.\\n        ascending = nv.validate_argsort_with_ascending(ascending, args, kwargs)\\n        values = self._values_for_argsort()\\n        result = np.argsort(values, kind=kind, **kwargs)\\n        if not ascending:\\n            result = result[::-1]\\n        return result',\n 'def fillna(self, value=None, method=None, limit=None):\\n        \"\"\"\\n        Fill NA/NaN values using the specified method.\\n\\n        Parameters\\n        ----------\\n        value : scalar, array-like\\n            If a scalar value is passed it is used to fill all missing values.\\n            Alternatively, an array-like \\'value\\' can be given. It\\'s expected\\n            that the array-like have the same length as \\'self\\'.\\n        method : {\\'backfill\\', \\'bfill\\', \\'pad\\', \\'ffill\\', None}, default None\\n            Method to use for filling holes in reindexed Series\\n            pad / ffill: propagate last valid observation forward to next valid\\n            backfill / bfill: use NEXT valid observation to fill gap\\n        limit : int, default None\\n            If method is specified, this is the maximum number of consecutive\\n            NaN values to forward/backward fill. In other words, if there is\\n            a gap with more than this number of consecutive NaNs, it will only\\n            be partially filled. If method is not specified, this is the\\n            maximum number of entries along the entire axis where NaNs will be\\n            filled.\\n\\n        Returns\\n        -------\\n        filled : ExtensionArray with NA/NaN filled\\n        \"\"\"\\n        from pandas.api.types import is_array_like\\n        from pandas.util._validators import validate_fillna_kwargs\\n        from pandas.core.missing import pad_1d, backfill_1d\\n\\n        value, method = validate_fillna_kwargs(value, method)\\n\\n        mask = self.isna()\\n\\n        if is_array_like(value):\\n            if len(value) != len(self):\\n                raise ValueError(\"Length of \\'value\\' does not match. Got ({}) \"\\n                                 \" expected {}\".format(len(value), len(self)))\\n            value = value[mask]\\n\\n        if mask.any():\\n            if method is not None:\\n                func = pad_1d if method == \\'pad\\' else backfill_1d\\n                new_values = func(self.astype(object), limit=limit,\\n                                  mask=mask)\\n                new_values = self._from_sequence(new_values, dtype=self.dtype)\\n            else:\\n                # fill with value\\n                new_values = self.copy()\\n                new_values[mask] = value\\n        else:\\n            new_values = self.copy()\\n        return new_values',\n 'def shift(\\n            self,\\n            periods: int = 1,\\n            fill_value: object = None,\\n    ) -> ABCExtensionArray:\\n        \"\"\"\\n        Shift values by desired number.\\n\\n        Newly introduced missing values are filled with\\n        ``self.dtype.na_value``.\\n\\n        .. versionadded:: 0.24.0\\n\\n        Parameters\\n        ----------\\n        periods : int, default 1\\n            The number of periods to shift. Negative values are allowed\\n            for shifting backwards.\\n\\n        fill_value : object, optional\\n            The scalar value to use for newly introduced missing values.\\n            The default is ``self.dtype.na_value``\\n\\n            .. versionadded:: 0.24.0\\n\\n        Returns\\n        -------\\n        shifted : ExtensionArray\\n\\n        Notes\\n        -----\\n        If ``self`` is empty or ``periods`` is 0, a copy of ``self`` is\\n        returned.\\n\\n        If ``periods > len(self)``, then an array of size\\n        len(self) is returned, with all values filled with\\n        ``self.dtype.na_value``.\\n        \"\"\"\\n        # Note: this implementation assumes that `self.dtype.na_value` can be\\n        # stored in an instance of your ExtensionArray with `self.dtype`.\\n        if not len(self) or periods == 0:\\n            return self.copy()\\n\\n        if isna(fill_value):\\n            fill_value = self.dtype.na_value\\n\\n        empty = self._from_sequence(\\n            [fill_value] * min(abs(periods), len(self)),\\n            dtype=self.dtype\\n        )\\n        if periods > 0:\\n            a = empty\\n            b = self[:-periods]\\n        else:\\n            a = self[abs(periods):]\\n            b = empty\\n        return self._concat_same_type([a, b])',\n 'def unique(self):\\n        \"\"\"\\n        Compute the ExtensionArray of unique values.\\n\\n        Returns\\n        -------\\n        uniques : ExtensionArray\\n        \"\"\"\\n        from pandas import unique\\n\\n        uniques = unique(self.astype(object))\\n        return self._from_sequence(uniques, dtype=self.dtype)',\n 'def searchsorted(self, value, side=\"left\", sorter=None):\\n        \"\"\"\\n        Find indices where elements should be inserted to maintain order.\\n\\n        .. versionadded:: 0.24.0\\n\\n        Find the indices into a sorted array `self` (a) such that, if the\\n        corresponding elements in `value` were inserted before the indices,\\n        the order of `self` would be preserved.\\n\\n        Assuming that `self` is sorted:\\n\\n        ======  ================================\\n        `side`  returned index `i` satisfies\\n        ======  ================================\\n        left    ``self[i-1] < value <= self[i]``\\n        right   ``self[i-1] <= value < self[i]``\\n        ======  ================================\\n\\n        Parameters\\n        ----------\\n        value : array_like\\n            Values to insert into `self`.\\n        side : {\\'left\\', \\'right\\'}, optional\\n            If \\'left\\', the index of the first suitable location found is given.\\n            If \\'right\\', return the last such index.  If there is no suitable\\n            index, return either 0 or N (where N is the length of `self`).\\n        sorter : 1-D array_like, optional\\n            Optional array of integer indices that sort array a into ascending\\n            order. They are typically the result of argsort.\\n\\n        Returns\\n        -------\\n        array of ints\\n            Array of insertion points with the same shape as `value`.\\n\\n        See Also\\n        --------\\n        numpy.searchsorted : Similar method from NumPy.\\n        \"\"\"\\n        # Note: the base tests provided by pandas only test the basics.\\n        # We do not test\\n        # 1. Values outside the range of the `data_for_sorting` fixture\\n        # 2. Values between the values in the `data_for_sorting` fixture\\n        # 3. Missing values.\\n        arr = self.astype(object)\\n        return arr.searchsorted(value, side=side, sorter=sorter)',\n 'def _values_for_factorize(self) -> Tuple[np.ndarray, Any]:\\n        \"\"\"\\n        Return an array and missing value suitable for factorization.\\n\\n        Returns\\n        -------\\n        values : ndarray\\n\\n            An array suitable for factorization. This should maintain order\\n            and be a supported dtype (Float64, Int64, UInt64, String, Object).\\n            By default, the extension array is cast to object dtype.\\n        na_value : object\\n            The value in `values` to consider missing. This will be treated\\n            as NA in the factorization routines, so it will be coded as\\n            `na_sentinal` and not included in `uniques`. By default,\\n            ``np.nan`` is used.\\n\\n        Notes\\n        -----\\n        The values returned by this method are also used in\\n        :func:`pandas.util.hash_pandas_object`.\\n        \"\"\"\\n        return self.astype(object), np.nan',\n 'def factorize(\\n            self,\\n            na_sentinel: int = -1,\\n    ) -> Tuple[np.ndarray, ABCExtensionArray]:\\n        \"\"\"\\n        Encode the extension array as an enumerated type.\\n\\n        Parameters\\n        ----------\\n        na_sentinel : int, default -1\\n            Value to use in the `labels` array to indicate missing values.\\n\\n        Returns\\n        -------\\n        labels : ndarray\\n            An integer NumPy array that\\'s an indexer into the original\\n            ExtensionArray.\\n        uniques : ExtensionArray\\n            An ExtensionArray containing the unique values of `self`.\\n\\n            .. note::\\n\\n               uniques will *not* contain an entry for the NA value of\\n               the ExtensionArray if there are any missing values present\\n               in `self`.\\n\\n        See Also\\n        --------\\n        pandas.factorize : Top-level factorize method that dispatches here.\\n\\n        Notes\\n        -----\\n        :meth:`pandas.factorize` offers a `sort` keyword as well.\\n        \"\"\"\\n        # Impelmentor note: There are two ways to override the behavior of\\n        # pandas.factorize\\n        # 1. _values_for_factorize and _from_factorize.\\n        #    Specify the values passed to pandas\\' internal factorization\\n        #    routines, and how to convert from those values back to the\\n        #    original ExtensionArray.\\n        # 2. ExtensionArray.factorize.\\n        #    Complete control over factorization.\\n        from pandas.core.algorithms import _factorize_array\\n\\n        arr, na_value = self._values_for_factorize()\\n\\n        labels, uniques = _factorize_array(arr, na_sentinel=na_sentinel,\\n                                           na_value=na_value)\\n\\n        uniques = self._from_factorized(uniques, self)\\n        return labels, uniques',\n 'def take(\\n            self,\\n            indices: Sequence[int],\\n            allow_fill: bool = False,\\n            fill_value: Any = None\\n    ) -> ABCExtensionArray:\\n        \"\"\"\\n        Take elements from an array.\\n\\n        Parameters\\n        ----------\\n        indices : sequence of integers\\n            Indices to be taken.\\n        allow_fill : bool, default False\\n            How to handle negative values in `indices`.\\n\\n            * False: negative values in `indices` indicate positional indices\\n              from the right (the default). This is similar to\\n              :func:`numpy.take`.\\n\\n            * True: negative values in `indices` indicate\\n              missing values. These values are set to `fill_value`. Any other\\n              other negative values raise a ``ValueError``.\\n\\n        fill_value : any, optional\\n            Fill value to use for NA-indices when `allow_fill` is True.\\n            This may be ``None``, in which case the default NA value for\\n            the type, ``self.dtype.na_value``, is used.\\n\\n            For many ExtensionArrays, there will be two representations of\\n            `fill_value`: a user-facing \"boxed\" scalar, and a low-level\\n            physical NA value. `fill_value` should be the user-facing version,\\n            and the implementation should handle translating that to the\\n            physical version for processing the take if necessary.\\n\\n        Returns\\n        -------\\n        ExtensionArray\\n\\n        Raises\\n        ------\\n        IndexError\\n            When the indices are out of bounds for the array.\\n        ValueError\\n            When `indices` contains negative values other than ``-1``\\n            and `allow_fill` is True.\\n\\n        Notes\\n        -----\\n        ExtensionArray.take is called by ``Series.__getitem__``, ``.loc``,\\n        ``iloc``, when `indices` is a sequence of values. Additionally,\\n        it\\'s called by :meth:`Series.reindex`, or any other method\\n        that causes realignment, with a `fill_value`.\\n\\n        See Also\\n        --------\\n        numpy.take\\n        pandas.api.extensions.take\\n\\n        Examples\\n        --------\\n        Here\\'s an example implementation, which relies on casting the\\n        extension array to object dtype. This uses the helper method\\n        :func:`pandas.api.extensions.take`.\\n\\n        .. code-block:: python\\n\\n           def take(self, indices, allow_fill=False, fill_value=None):\\n               from pandas.core.algorithms import take\\n\\n               # If the ExtensionArray is backed by an ndarray, then\\n               # just pass that here instead of coercing to object.\\n               data = self.astype(object)\\n\\n               if allow_fill and fill_value is None:\\n                   fill_value = self.dtype.na_value\\n\\n               # fill value should always be translated from the scalar\\n               # type for the array, to the physical storage type for\\n               # the data, before passing to take.\\n\\n               result = take(data, indices, fill_value=fill_value,\\n                             allow_fill=allow_fill)\\n               return self._from_sequence(result, dtype=self.dtype)\\n        \"\"\"\\n        # Implementer note: The `fill_value` parameter should be a user-facing\\n        # value, an instance of self.dtype.type. When passed `fill_value=None`,\\n        # the default of `self.dtype.na_value` should be used.\\n        # This may differ from the physical storage type your ExtensionArray\\n        # uses. In this case, your implementation is responsible for casting\\n        # the user-facing type to the storage type, before using\\n        # pandas.api.extensions.take\\n        raise AbstractMethodError(self)',\n 'def _formatter(\\n            self,\\n            boxed: bool = False,\\n    ) -> Callable[[Any], Optional[str]]:\\n        \"\"\"Formatting function for scalar values.\\n\\n        This is used in the default \\'__repr__\\'. The returned formatting\\n        function receives instances of your scalar type.\\n\\n        Parameters\\n        ----------\\n        boxed: bool, default False\\n            An indicated for whether or not your array is being printed\\n            within a Series, DataFrame, or Index (True), or just by\\n            itself (False). This may be useful if you want scalar values\\n            to appear differently within a Series versus on its own (e.g.\\n            quoted or not).\\n\\n        Returns\\n        -------\\n        Callable[[Any], str]\\n            A callable that gets instances of the scalar type and\\n            returns a string. By default, :func:`repr` is used\\n            when ``boxed=False`` and :func:`str` is used when\\n            ``boxed=True``.\\n        \"\"\"\\n        if boxed:\\n            return str\\n        return repr',\n 'def _reduce(self, name, skipna=True, **kwargs):\\n        \"\"\"\\n        Return a scalar result of performing the reduction operation.\\n\\n        Parameters\\n        ----------\\n        name : str\\n            Name of the function, supported values are:\\n            { any, all, min, max, sum, mean, median, prod,\\n            std, var, sem, kurt, skew }.\\n        skipna : bool, default True\\n            If True, skip NaN values.\\n        **kwargs\\n            Additional keyword arguments passed to the reduction function.\\n            Currently, `ddof` is the only supported kwarg.\\n\\n        Returns\\n        -------\\n        scalar\\n\\n        Raises\\n        ------\\n        TypeError : subclass does not define reductions\\n        \"\"\"\\n        raise TypeError(\"cannot perform {name} with type {dtype}\".format(\\n            name=name, dtype=self.dtype))',\n 'def _create_method(cls, op, coerce_to_dtype=True):\\n        \"\"\"\\n        A class method that returns a method that will correspond to an\\n        operator for an ExtensionArray subclass, by dispatching to the\\n        relevant operator defined on the individual elements of the\\n        ExtensionArray.\\n\\n        Parameters\\n        ----------\\n        op : function\\n            An operator that takes arguments op(a, b)\\n        coerce_to_dtype :  bool, default True\\n            boolean indicating whether to attempt to convert\\n            the result to the underlying ExtensionArray dtype.\\n            If it\\'s not possible to create a new ExtensionArray with the\\n            values, an ndarray is returned instead.\\n\\n        Returns\\n        -------\\n        Callable[[Any, Any], Union[ndarray, ExtensionArray]]\\n            A method that can be bound to a class. When used, the method\\n            receives the two arguments, one of which is the instance of\\n            this class, and should return an ExtensionArray or an ndarray.\\n\\n            Returning an ndarray may be necessary when the result of the\\n            `op` cannot be stored in the ExtensionArray. The dtype of the\\n            ndarray uses NumPy\\'s normal inference rules.\\n\\n        Example\\n        -------\\n        Given an ExtensionArray subclass called MyExtensionArray, use\\n\\n        >>> __add__ = cls._create_method(operator.add)\\n\\n        in the class definition of MyExtensionArray to create the operator\\n        for addition, that will be based on the operator implementation\\n        of the underlying elements of the ExtensionArray\\n        \"\"\"\\n\\n        def _binop(self, other):\\n            def convert_values(param):\\n                if isinstance(param, ExtensionArray) or is_list_like(param):\\n                    ovalues = param\\n                else:  # Assume its an object\\n                    ovalues = [param] * len(self)\\n                return ovalues\\n\\n            if isinstance(other, (ABCSeries, ABCIndexClass)):\\n                # rely on pandas to unbox and dispatch to us\\n                return NotImplemented\\n\\n            lvalues = self\\n            rvalues = convert_values(other)\\n\\n            # If the operator is not defined for the underlying objects,\\n            # a TypeError should be raised\\n            res = [op(a, b) for (a, b) in zip(lvalues, rvalues)]\\n\\n            def _maybe_convert(arr):\\n                if coerce_to_dtype:\\n                    # https://github.com/pandas-dev/pandas/issues/22850\\n                    # We catch all regular exceptions here, and fall back\\n                    # to an ndarray.\\n                    try:\\n                        res = self._from_sequence(arr)\\n                    except Exception:\\n                        res = np.asarray(arr)\\n                else:\\n                    res = np.asarray(arr)\\n                return res\\n\\n            if op.__name__ in {\\'divmod\\', \\'rdivmod\\'}:\\n                a, b = zip(*res)\\n                res = _maybe_convert(a), _maybe_convert(b)\\n            else:\\n                res = _maybe_convert(res)\\n            return res\\n\\n        op_name = ops._get_op_name(op, True)\\n        return set_function_name(_binop, op_name, cls)',\n 'def ea_passthrough(array_method):\\n    \"\"\"\\n    Make an alias for a method of the underlying ExtensionArray.\\n\\n    Parameters\\n    ----------\\n    array_method : method on an Array class\\n\\n    Returns\\n    -------\\n    method\\n    \"\"\"\\n\\n    def method(self, *args, **kwargs):\\n        return array_method(self._data, *args, **kwargs)\\n\\n    method.__name__ = array_method.__name__\\n    method.__doc__ = array_method.__doc__\\n    return method',\n 'def _create_comparison_method(cls, op):\\n        \"\"\"\\n        Create a comparison method that dispatches to ``cls.values``.\\n        \"\"\"\\n        def wrapper(self, other):\\n            if isinstance(other, ABCSeries):\\n                # the arrays defer to Series for comparison ops but the indexes\\n                #  don\\'t, so we have to unwrap here.\\n                other = other._values\\n\\n            result = op(self._data, maybe_unwrap_index(other))\\n            return result\\n\\n        wrapper.__doc__ = op.__doc__\\n        wrapper.__name__ = \\'__{}__\\'.format(op.__name__)\\n        return wrapper',\n 'def equals(self, other):\\n        \"\"\"\\n        Determines if two Index objects contain the same elements.\\n        \"\"\"\\n        if self.is_(other):\\n            return True\\n\\n        if not isinstance(other, ABCIndexClass):\\n            return False\\n        elif not isinstance(other, type(self)):\\n            try:\\n                other = type(self)(other)\\n            except Exception:\\n                return False\\n\\n        if not is_dtype_equal(self.dtype, other.dtype):\\n            # have different timezone\\n            return False\\n\\n        elif is_period_dtype(self):\\n            if not is_period_dtype(other):\\n                return False\\n            if self.freq != other.freq:\\n                return False\\n\\n        return np.array_equal(self.asi8, other.asi8)',\n 'def _join_i8_wrapper(joinf, dtype, with_indexers=True):\\n        \"\"\"\\n        Create the join wrapper methods.\\n        \"\"\"\\n        from pandas.core.arrays.datetimelike import DatetimeLikeArrayMixin\\n\\n        @staticmethod\\n        def wrapper(left, right):\\n            if isinstance(left, (np.ndarray, ABCIndex, ABCSeries,\\n                                 DatetimeLikeArrayMixin)):\\n                left = left.view(\\'i8\\')\\n            if isinstance(right, (np.ndarray, ABCIndex, ABCSeries,\\n                                  DatetimeLikeArrayMixin)):\\n                right = right.view(\\'i8\\')\\n            results = joinf(left, right)\\n            if with_indexers:\\n                join_index, left_indexer, right_indexer = results\\n                join_index = join_index.view(dtype)\\n                return join_index, left_indexer, right_indexer\\n            return results\\n\\n        return wrapper',\n 'def sort_values(self, return_indexer=False, ascending=True):\\n        \"\"\"\\n        Return sorted copy of Index.\\n        \"\"\"\\n        if return_indexer:\\n            _as = self.argsort()\\n            if not ascending:\\n                _as = _as[::-1]\\n            sorted_index = self.take(_as)\\n            return sorted_index, _as\\n        else:\\n            sorted_values = np.sort(self._ndarray_values)\\n            attribs = self._get_attributes_dict()\\n            freq = attribs[\\'freq\\']\\n\\n            if freq is not None and not is_period_dtype(self):\\n                if freq.n > 0 and not ascending:\\n                    freq = freq * -1\\n                elif freq.n < 0 and ascending:\\n                    freq = freq * -1\\n            attribs[\\'freq\\'] = freq\\n\\n            if not ascending:\\n                sorted_values = sorted_values[::-1]\\n\\n            return self._simple_new(sorted_values, **attribs)',\n 'def min(self, axis=None, skipna=True, *args, **kwargs):\\n        \"\"\"\\n        Return the minimum value of the Index or minimum along\\n        an axis.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.min\\n        Series.min : Return the minimum value in a Series.\\n        \"\"\"\\n        nv.validate_min(args, kwargs)\\n        nv.validate_minmax_axis(axis)\\n\\n        if not len(self):\\n            return self._na_value\\n\\n        i8 = self.asi8\\n        try:\\n            # quick check\\n            if len(i8) and self.is_monotonic:\\n                if i8[0] != iNaT:\\n                    return self._box_func(i8[0])\\n\\n            if self.hasnans:\\n                if skipna:\\n                    min_stamp = self[~self._isnan].asi8.min()\\n                else:\\n                    return self._na_value\\n            else:\\n                min_stamp = i8.min()\\n            return self._box_func(min_stamp)\\n        except ValueError:\\n            return self._na_value',\n 'def argmin(self, axis=None, skipna=True, *args, **kwargs):\\n        \"\"\"\\n        Returns the indices of the minimum values along an axis.\\n\\n        See `numpy.ndarray.argmin` for more information on the\\n        `axis` parameter.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argmin\\n        \"\"\"\\n        nv.validate_argmin(args, kwargs)\\n        nv.validate_minmax_axis(axis)\\n\\n        i8 = self.asi8\\n        if self.hasnans:\\n            mask = self._isnan\\n            if mask.all() or not skipna:\\n                return -1\\n            i8 = i8.copy()\\n            i8[mask] = np.iinfo(\\'int64\\').max\\n        return i8.argmin()',\n 'def max(self, axis=None, skipna=True, *args, **kwargs):\\n        \"\"\"\\n        Return the maximum value of the Index or maximum along\\n        an axis.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.max\\n        Series.max : Return the maximum value in a Series.\\n        \"\"\"\\n        nv.validate_max(args, kwargs)\\n        nv.validate_minmax_axis(axis)\\n\\n        if not len(self):\\n            return self._na_value\\n\\n        i8 = self.asi8\\n        try:\\n            # quick check\\n            if len(i8) and self.is_monotonic:\\n                if i8[-1] != iNaT:\\n                    return self._box_func(i8[-1])\\n\\n            if self.hasnans:\\n                if skipna:\\n                    max_stamp = self[~self._isnan].asi8.max()\\n                else:\\n                    return self._na_value\\n            else:\\n                max_stamp = i8.max()\\n            return self._box_func(max_stamp)\\n        except ValueError:\\n            return self._na_value',\n 'def argmax(self, axis=None, skipna=True, *args, **kwargs):\\n        \"\"\"\\n        Returns the indices of the maximum values along an axis.\\n\\n        See `numpy.ndarray.argmax` for more information on the\\n        `axis` parameter.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.argmax\\n        \"\"\"\\n        nv.validate_argmax(args, kwargs)\\n        nv.validate_minmax_axis(axis)\\n\\n        i8 = self.asi8\\n        if self.hasnans:\\n            mask = self._isnan\\n            if mask.all() or not skipna:\\n                return -1\\n            i8 = i8.copy()\\n            i8[mask] = 0\\n        return i8.argmax()',\n 'def _format_attrs(self):\\n        \"\"\"\\n        Return a list of tuples of the (attr,formatted_value).\\n        \"\"\"\\n        attrs = super()._format_attrs()\\n        for attrib in self._attributes:\\n            if attrib == \\'freq\\':\\n                freq = self.freqstr\\n                if freq is not None:\\n                    freq = \"\\'%s\\'\" % freq\\n                attrs.append((\\'freq\\', freq))\\n        return attrs',\n 'def _convert_scalar_indexer(self, key, kind=None):\\n        \"\"\"\\n        We don\\'t allow integer or float indexing on datetime-like when using\\n        loc.\\n\\n        Parameters\\n        ----------\\n        key : label of the slice bound\\n        kind : {\\'ix\\', \\'loc\\', \\'getitem\\', \\'iloc\\'} or None\\n        \"\"\"\\n\\n        assert kind in [\\'ix\\', \\'loc\\', \\'getitem\\', \\'iloc\\', None]\\n\\n        # we don\\'t allow integer/float indexing for loc\\n        # we don\\'t allow float indexing for ix/getitem\\n        if is_scalar(key):\\n            is_int = is_integer(key)\\n            is_flt = is_float(key)\\n            if kind in [\\'loc\\'] and (is_int or is_flt):\\n                self._invalid_indexer(\\'index\\', key)\\n            elif kind in [\\'ix\\', \\'getitem\\'] and is_flt:\\n                self._invalid_indexer(\\'index\\', key)\\n\\n        return super()._convert_scalar_indexer(key, kind=kind)',\n 'def _add_datetimelike_methods(cls):\\n        \"\"\"\\n        Add in the datetimelike methods (as we may have to override the\\n        superclass).\\n        \"\"\"\\n\\n        def __add__(self, other):\\n            # dispatch to ExtensionArray implementation\\n            result = self._data.__add__(maybe_unwrap_index(other))\\n            return wrap_arithmetic_op(self, other, result)\\n\\n        cls.__add__ = __add__\\n\\n        def __radd__(self, other):\\n            # alias for __add__\\n            return self.__add__(other)\\n        cls.__radd__ = __radd__\\n\\n        def __sub__(self, other):\\n            # dispatch to ExtensionArray implementation\\n            result = self._data.__sub__(maybe_unwrap_index(other))\\n            return wrap_arithmetic_op(self, other, result)\\n\\n        cls.__sub__ = __sub__\\n\\n        def __rsub__(self, other):\\n            result = self._data.__rsub__(maybe_unwrap_index(other))\\n            return wrap_arithmetic_op(self, other, result)\\n\\n        cls.__rsub__ = __rsub__',\n 'def isin(self, values):\\n        \"\"\"\\n        Compute boolean array of whether each index value is found in the\\n        passed set of values.\\n\\n        Parameters\\n        ----------\\n        values : set or sequence of values\\n\\n        Returns\\n        -------\\n        is_contained : ndarray (boolean dtype)\\n        \"\"\"\\n        if not isinstance(values, type(self)):\\n            try:\\n                values = type(self)(values)\\n            except ValueError:\\n                return self.astype(object).isin(values)\\n\\n        return algorithms.isin(self.asi8, values.asi8)',\n 'def _summary(self, name=None):\\n        \"\"\"\\n        Return a summarized representation.\\n\\n        Parameters\\n        ----------\\n        name : str\\n            name to use in the summary representation\\n\\n        Returns\\n        -------\\n        String with a summarized representation of the index\\n        \"\"\"\\n        formatter = self._formatter_func\\n        if len(self) > 0:\\n            index_summary = \\', %s to %s\\' % (formatter(self[0]),\\n                                            formatter(self[-1]))\\n        else:\\n            index_summary = \\'\\'\\n\\n        if name is None:\\n            name = type(self).__name__\\n        result = \\'%s: %s entries%s\\' % (printing.pprint_thing(name),\\n                                       len(self), index_summary)\\n        if self.freq:\\n            result += \\'\\\\nFreq: %s\\' % self.freqstr\\n\\n        # display as values, not quoted\\n        result = result.replace(\"\\'\", \"\")\\n        return result',\n 'def _concat_same_dtype(self, to_concat, name):\\n        \"\"\"\\n        Concatenate to_concat which has the same class.\\n        \"\"\"\\n        attribs = self._get_attributes_dict()\\n        attribs[\\'name\\'] = name\\n        # do not pass tz to set because tzlocal cannot be hashed\\n        if len({str(x.dtype) for x in to_concat}) != 1:\\n            raise ValueError(\\'to_concat must have the same tz\\')\\n\\n        new_data = type(self._values)._concat_same_type(to_concat).asi8\\n\\n        # GH 3232: If the concat result is evenly spaced, we can retain the\\n        # original frequency\\n        is_diff_evenly_spaced = len(unique_deltas(new_data)) == 1\\n        if not is_period_dtype(self) and not is_diff_evenly_spaced:\\n            # reset freq\\n            attribs[\\'freq\\'] = None\\n\\n        return self._simple_new(new_data, **attribs)',\n 'def shift(self, periods, freq=None):\\n        \"\"\"\\n        Shift index by desired number of time frequency increments.\\n\\n        This method is for shifting the values of datetime-like indexes\\n        by a specified time increment a given number of times.\\n\\n        Parameters\\n        ----------\\n        periods : int\\n            Number of periods (or increments) to shift by,\\n            can be positive or negative.\\n\\n            .. versionchanged:: 0.24.0\\n\\n        freq : pandas.DateOffset, pandas.Timedelta or string, optional\\n            Frequency increment to shift by.\\n            If None, the index is shifted by its own `freq` attribute.\\n            Offset aliases are valid strings, e.g., \\'D\\', \\'W\\', \\'M\\' etc.\\n\\n        Returns\\n        -------\\n        pandas.DatetimeIndex\\n            Shifted index.\\n\\n        See Also\\n        --------\\n        Index.shift : Shift values of Index.\\n        PeriodIndex.shift : Shift values of PeriodIndex.\\n        \"\"\"\\n        result = self._data._time_shift(periods, freq=freq)\\n        return type(self)(result, name=self.name)',\n 'def _single_replace(self, to_replace, method, inplace, limit):\\n    \"\"\"\\n    Replaces values in a Series using the fill method specified when no\\n    replacement value is given in the replace method\\n    \"\"\"\\n    if self.ndim != 1:\\n        raise TypeError(\\'cannot replace {0} with method {1} on a {2}\\'\\n                        .format(to_replace, method, type(self).__name__))\\n\\n    orig_dtype = self.dtype\\n    result = self if inplace else self.copy()\\n    fill_f = missing.get_fill_func(method)\\n\\n    mask = missing.mask_missing(result.values, to_replace)\\n    values = fill_f(result.values, limit=limit, mask=mask)\\n\\n    if values.dtype == orig_dtype and inplace:\\n        return\\n\\n    result = pd.Series(values, index=self.index,\\n                       dtype=self.dtype).__finalize__(self)\\n\\n    if inplace:\\n        self._update_inplace(result._data)\\n        return\\n\\n    return result',\n 'def _doc_parms(cls):\\n    \"\"\"Return a tuple of the doc parms.\"\"\"\\n    axis_descr = \"{%s}\" % \\', \\'.join(\"{0} ({1})\".format(a, i)\\n                                    for i, a in enumerate(cls._AXIS_ORDERS))\\n    name = (cls._constructor_sliced.__name__\\n            if cls._AXIS_LEN > 1 else \\'scalar\\')\\n    name2 = cls.__name__\\n    return axis_descr, name, name2',\n 'def _init_mgr(self, mgr, axes=None, dtype=None, copy=False):\\n        \"\"\" passed a manager and a axes dict \"\"\"\\n        for a, axe in axes.items():\\n            if axe is not None:\\n                mgr = mgr.reindex_axis(axe,\\n                                       axis=self._get_block_manager_axis(a),\\n                                       copy=False)\\n\\n        # make a copy if explicitly requested\\n        if copy:\\n            mgr = mgr.copy()\\n        if dtype is not None:\\n            # avoid further copies if we can\\n            if len(mgr.blocks) > 1 or mgr.blocks[0].values.dtype != dtype:\\n                mgr = mgr.astype(dtype=dtype)\\n        return mgr',\n 'def _validate_dtype(self, dtype):\\n        \"\"\" validate the passed dtype \"\"\"\\n\\n        if dtype is not None:\\n            dtype = pandas_dtype(dtype)\\n\\n            # a compound dtype\\n            if dtype.kind == \\'V\\':\\n                raise NotImplementedError(\"compound dtypes are not implemented\"\\n                                          \" in the {0} constructor\"\\n                                          .format(self.__class__.__name__))\\n\\n        return dtype',\n 'def _setup_axes(cls, axes, info_axis=None, stat_axis=None, aliases=None,\\n                    slicers=None, axes_are_reversed=False, build_axes=True,\\n                    ns=None, docs=None):\\n        \"\"\"Provide axes setup for the major PandasObjects.\\n\\n        Parameters\\n        ----------\\n        axes : the names of the axes in order (lowest to highest)\\n        info_axis_num : the axis of the selector dimension (int)\\n        stat_axis_num : the number of axis for the default stats (int)\\n        aliases : other names for a single axis (dict)\\n        slicers : how axes slice to others (dict)\\n        axes_are_reversed : boolean whether to treat passed axes as\\n            reversed (DataFrame)\\n        build_axes : setup the axis properties (default True)\\n        \"\"\"\\n\\n        cls._AXIS_ORDERS = axes\\n        cls._AXIS_NUMBERS = {a: i for i, a in enumerate(axes)}\\n        cls._AXIS_LEN = len(axes)\\n        cls._AXIS_ALIASES = aliases or dict()\\n        cls._AXIS_IALIASES = {v: k for k, v in cls._AXIS_ALIASES.items()}\\n        cls._AXIS_NAMES = dict(enumerate(axes))\\n        cls._AXIS_SLICEMAP = slicers or None\\n        cls._AXIS_REVERSED = axes_are_reversed\\n\\n        # typ\\n        setattr(cls, \\'_typ\\', cls.__name__.lower())\\n\\n        # indexing support\\n        cls._ix = None\\n\\n        if info_axis is not None:\\n            cls._info_axis_number = info_axis\\n            cls._info_axis_name = axes[info_axis]\\n\\n        if stat_axis is not None:\\n            cls._stat_axis_number = stat_axis\\n            cls._stat_axis_name = axes[stat_axis]\\n\\n        # setup the actual axis\\n        if build_axes:\\n\\n            def set_axis(a, i):\\n                setattr(cls, a, properties.AxisProperty(i, docs.get(a, a)))\\n                cls._internal_names_set.add(a)\\n\\n            if axes_are_reversed:\\n                m = cls._AXIS_LEN - 1\\n                for i, a in cls._AXIS_NAMES.items():\\n                    set_axis(a, m - i)\\n            else:\\n                for i, a in cls._AXIS_NAMES.items():\\n                    set_axis(a, i)\\n\\n        assert not isinstance(ns, dict)',\n 'def _construct_axes_dict(self, axes=None, **kwargs):\\n        \"\"\"Return an axes dictionary for myself.\"\"\"\\n        d = {a: self._get_axis(a) for a in (axes or self._AXIS_ORDERS)}\\n        d.update(kwargs)\\n        return d',\n 'def _construct_axes_dict_from(self, axes, **kwargs):\\n        \"\"\"Return an axes dictionary for the passed axes.\"\"\"\\n        d = {a: ax for a, ax in zip(self._AXIS_ORDERS, axes)}\\n        d.update(kwargs)\\n        return d',\n 'def _construct_axes_dict_for_slice(self, axes=None, **kwargs):\\n        \"\"\"Return an axes dictionary for myself.\"\"\"\\n        d = {self._AXIS_SLICEMAP[a]: self._get_axis(a)\\n             for a in (axes or self._AXIS_ORDERS)}\\n        d.update(kwargs)\\n        return d',\n 'def _construct_axes_from_arguments(\\n            self, args, kwargs, require_all=False, sentinel=None):\\n        \"\"\"Construct and returns axes if supplied in args/kwargs.\\n\\n        If require_all, raise if all axis arguments are not supplied\\n        return a tuple of (axes, kwargs).\\n\\n        sentinel specifies the default parameter when an axis is not\\n        supplied; useful to distinguish when a user explicitly passes None\\n        in scenarios where None has special meaning.\\n        \"\"\"\\n\\n        # construct the args\\n        args = list(args)\\n        for a in self._AXIS_ORDERS:\\n\\n            # if we have an alias for this axis\\n            alias = self._AXIS_IALIASES.get(a)\\n            if alias is not None:\\n                if a in kwargs:\\n                    if alias in kwargs:\\n                        raise TypeError(\"arguments are mutually exclusive \"\\n                                        \"for [%s,%s]\" % (a, alias))\\n                    continue\\n                if alias in kwargs:\\n                    kwargs[a] = kwargs.pop(alias)\\n                    continue\\n\\n            # look for a argument by position\\n            if a not in kwargs:\\n                try:\\n                    kwargs[a] = args.pop(0)\\n                except IndexError:\\n                    if require_all:\\n                        raise TypeError(\"not enough/duplicate arguments \"\\n                                        \"specified!\")\\n\\n        axes = {a: kwargs.pop(a, sentinel) for a in self._AXIS_ORDERS}\\n        return axes, kwargs',\n 'def _get_block_manager_axis(cls, axis):\\n        \"\"\"Map the axis to the block_manager axis.\"\"\"\\n        axis = cls._get_axis_number(axis)\\n        if cls._AXIS_REVERSED:\\n            m = cls._AXIS_LEN - 1\\n            return m - axis\\n        return axis',\n 'def _get_space_character_free_column_resolvers(self):\\n        \"\"\"Return the space character free column resolvers of a dataframe.\\n\\n        Column names with spaces are \\'cleaned up\\' so that they can be referred\\n        to by backtick quoting.\\n        Used in :meth:`DataFrame.eval`.\\n        \"\"\"\\n        from pandas.core.computation.common import _remove_spaces_column_name\\n\\n        return {_remove_spaces_column_name(k): v for k, v\\n                in self.iteritems()}',\n 'def shape(self):\\n        \"\"\"\\n        Return a tuple of axis dimensions\\n        \"\"\"\\n        return tuple(len(self._get_axis(a)) for a in self._AXIS_ORDERS)',\n 'def transpose(self, *args, **kwargs):\\n        \"\"\"\\n        Permute the dimensions of the %(klass)s\\n\\n        Parameters\\n        ----------\\n        args : %(args_transpose)s\\n        copy : boolean, default False\\n            Make a copy of the underlying data. Mixed-dtype data will\\n            always result in a copy\\n        **kwargs\\n            Additional keyword arguments will be passed to the function.\\n\\n        Returns\\n        -------\\n        y : same as input\\n\\n        Examples\\n        --------\\n        >>> p.transpose(2, 0, 1)\\n        >>> p.transpose(2, 0, 1, copy=True)\\n        \"\"\"\\n\\n        # construct the args\\n        axes, kwargs = self._construct_axes_from_arguments(args, kwargs,\\n                                                           require_all=True)\\n        axes_names = tuple(self._get_axis_name(axes[a])\\n                           for a in self._AXIS_ORDERS)\\n        axes_numbers = tuple(self._get_axis_number(axes[a])\\n                             for a in self._AXIS_ORDERS)\\n\\n        # we must have unique axes\\n        if len(axes) != len(set(axes)):\\n            raise ValueError(\\'Must specify %s unique axes\\' % self._AXIS_LEN)\\n\\n        new_axes = self._construct_axes_dict_from(self, [self._get_axis(x)\\n                                                         for x in axes_names])\\n        new_values = self.values.transpose(axes_numbers)\\n        if kwargs.pop(\\'copy\\', None) or (len(args) and args[-1]):\\n            new_values = new_values.copy()\\n\\n        nv.validate_transpose_for_generic(self, kwargs)\\n        return self._constructor(new_values, **new_axes).__finalize__(self)',\n 'def swapaxes(self, axis1, axis2, copy=True):\\n        \"\"\"\\n        Interchange axes and swap values axes appropriately.\\n\\n        Returns\\n        -------\\n        y : same as input\\n        \"\"\"\\n        i = self._get_axis_number(axis1)\\n        j = self._get_axis_number(axis2)\\n\\n        if i == j:\\n            if copy:\\n                return self.copy()\\n            return self\\n\\n        mapping = {i: j, j: i}\\n\\n        new_axes = (self._get_axis(mapping.get(k, k))\\n                    for k in range(self._AXIS_LEN))\\n        new_values = self.values.swapaxes(i, j)\\n        if copy:\\n            new_values = new_values.copy()\\n\\n        return self._constructor(new_values, *new_axes).__finalize__(self)',\n 'def droplevel(self, level, axis=0):\\n        \"\"\"\\n        Return DataFrame with requested index / column level(s) removed.\\n\\n        .. versionadded:: 0.24.0\\n\\n        Parameters\\n        ----------\\n        level : int, str, or list-like\\n            If a string is given, must be the name of a level\\n            If list-like, elements must be names or positional indexes\\n            of levels.\\n\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n\\n        Returns\\n        -------\\n        DataFrame.droplevel()\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame([\\n        ...     [1, 2, 3, 4],\\n        ...     [5, 6, 7, 8],\\n        ...     [9, 10, 11, 12]\\n        ... ]).set_index([0, 1]).rename_axis([\\'a\\', \\'b\\'])\\n\\n        >>> df.columns = pd.MultiIndex.from_tuples([\\n        ...    (\\'c\\', \\'e\\'), (\\'d\\', \\'f\\')\\n        ... ], names=[\\'level_1\\', \\'level_2\\'])\\n\\n        >>> df\\n        level_1   c   d\\n        level_2   e   f\\n        a b\\n        1 2      3   4\\n        5 6      7   8\\n        9 10    11  12\\n\\n        >>> df.droplevel(\\'a\\')\\n        level_1   c   d\\n        level_2   e   f\\n        b\\n        2        3   4\\n        6        7   8\\n        10      11  12\\n\\n        >>> df.droplevel(\\'level2\\', axis=1)\\n        level_1   c   d\\n        a b\\n        1 2      3   4\\n        5 6      7   8\\n        9 10    11  12\\n        \"\"\"\\n        labels = self._get_axis(axis)\\n        new_labels = labels.droplevel(level)\\n        result = self.set_axis(new_labels, axis=axis, inplace=False)\\n        return result',\n 'def pop(self, item):\\n        \"\"\"\\n        Return item and drop from frame. Raise KeyError if not found.\\n\\n        Parameters\\n        ----------\\n        item : str\\n            Label of column to be popped.\\n\\n        Returns\\n        -------\\n        Series\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame([(\\'falcon\\', \\'bird\\', 389.0),\\n        ...                    (\\'parrot\\', \\'bird\\', 24.0),\\n        ...                    (\\'lion\\', \\'mammal\\', 80.5),\\n        ...                    (\\'monkey\\',\\'mammal\\', np.nan)],\\n        ...                   columns=(\\'name\\', \\'class\\', \\'max_speed\\'))\\n        >>> df\\n             name   class  max_speed\\n        0  falcon    bird      389.0\\n        1  parrot    bird       24.0\\n        2    lion  mammal       80.5\\n        3  monkey  mammal        NaN\\n\\n        >>> df.pop(\\'class\\')\\n        0      bird\\n        1      bird\\n        2    mammal\\n        3    mammal\\n        Name: class, dtype: object\\n\\n        >>> df\\n             name  max_speed\\n        0  falcon      389.0\\n        1  parrot       24.0\\n        2    lion       80.5\\n        3  monkey        NaN\\n        \"\"\"\\n        result = self[item]\\n        del self[item]\\n        try:\\n            result._reset_cacher()\\n        except AttributeError:\\n            pass\\n\\n        return result',\n 'def squeeze(self, axis=None):\\n        \"\"\"\\n        Squeeze 1 dimensional axis objects into scalars.\\n\\n        Series or DataFrames with a single element are squeezed to a scalar.\\n        DataFrames with a single column or a single row are squeezed to a\\n        Series. Otherwise the object is unchanged.\\n\\n        This method is most useful when you don\\'t know if your\\n        object is a Series or DataFrame, but you do know it has just a single\\n        column. In that case you can safely call `squeeze` to ensure you have a\\n        Series.\\n\\n        Parameters\\n        ----------\\n        axis : {0 or \\'index\\', 1 or \\'columns\\', None}, default None\\n            A specific axis to squeeze. By default, all length-1 axes are\\n            squeezed.\\n\\n            .. versionadded:: 0.20.0\\n\\n        Returns\\n        -------\\n        DataFrame, Series, or scalar\\n            The projection after squeezing `axis` or all the axes.\\n\\n        See Also\\n        --------\\n        Series.iloc : Integer-location based indexing for selecting scalars.\\n        DataFrame.iloc : Integer-location based indexing for selecting Series.\\n        Series.to_frame : Inverse of DataFrame.squeeze for a\\n            single-column DataFrame.\\n\\n        Examples\\n        --------\\n        >>> primes = pd.Series([2, 3, 5, 7])\\n\\n        Slicing might produce a Series with a single value:\\n\\n        >>> even_primes = primes[primes % 2 == 0]\\n        >>> even_primes\\n        0    2\\n        dtype: int64\\n\\n        >>> even_primes.squeeze()\\n        2\\n\\n        Squeezing objects with more than one value in every axis does nothing:\\n\\n        >>> odd_primes = primes[primes % 2 == 1]\\n        >>> odd_primes\\n        1    3\\n        2    5\\n        3    7\\n        dtype: int64\\n\\n        >>> odd_primes.squeeze()\\n        1    3\\n        2    5\\n        3    7\\n        dtype: int64\\n\\n        Squeezing is even more effective when used with DataFrames.\\n\\n        >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=[\\'a\\', \\'b\\'])\\n        >>> df\\n           a  b\\n        0  1  2\\n        1  3  4\\n\\n        Slicing a single column will produce a DataFrame with the columns\\n        having only one value:\\n\\n        >>> df_a = df[[\\'a\\']]\\n        >>> df_a\\n           a\\n        0  1\\n        1  3\\n\\n        So the columns can be squeezed down, resulting in a Series:\\n\\n        >>> df_a.squeeze(\\'columns\\')\\n        0    1\\n        1    3\\n        Name: a, dtype: int64\\n\\n        Slicing a single row from a single column will produce a single\\n        scalar DataFrame:\\n\\n        >>> df_0a = df.loc[df.index < 1, [\\'a\\']]\\n        >>> df_0a\\n           a\\n        0  1\\n\\n        Squeezing the rows produces a single scalar Series:\\n\\n        >>> df_0a.squeeze(\\'rows\\')\\n        a    1\\n        Name: 0, dtype: int64\\n\\n        Squeezing all axes wil project directly into a scalar:\\n\\n        >>> df_0a.squeeze()\\n        1\\n        \"\"\"\\n        axis = (self._AXIS_NAMES if axis is None else\\n                (self._get_axis_number(axis),))\\n        try:\\n            return self.iloc[\\n                tuple(0 if i in axis and len(a) == 1 else slice(None)\\n                      for i, a in enumerate(self.axes))]\\n        except Exception:\\n            return self',\n 'def swaplevel(self, i=-2, j=-1, axis=0):\\n        \"\"\"\\n        Swap levels i and j in a MultiIndex on a particular axis\\n\\n        Parameters\\n        ----------\\n        i, j : int, str (can be mixed)\\n            Level of index to be swapped. Can pass level name as string.\\n\\n        Returns\\n        -------\\n        swapped : same type as caller (new object)\\n\\n        .. versionchanged:: 0.18.1\\n\\n           The indexes ``i`` and ``j`` are now optional, and default to\\n           the two innermost levels of the index.\\n        \"\"\"\\n        axis = self._get_axis_number(axis)\\n        result = self.copy()\\n        labels = result._data.axes[axis]\\n        result._data.set_axis(axis, labels.swaplevel(i, j))\\n        return result',\n 'def rename(self, *args, **kwargs):\\n        \"\"\"\\n        Alter axes input function or functions. Function / dict values must be\\n        unique (1-to-1). Labels not contained in a dict / Series will be left\\n        as-is. Extra labels listed don\\'t throw an error. Alternatively, change\\n        ``Series.name`` with a scalar value (Series only).\\n\\n        Parameters\\n        ----------\\n        %(axes)s : scalar, list-like, dict-like or function, optional\\n            Scalar or list-like will alter the ``Series.name`` attribute,\\n            and raise on DataFrame or Panel.\\n            dict-like or functions are transformations to apply to\\n            that axis\\' values\\n        copy : bool, default True\\n            Also copy underlying data.\\n        inplace : bool, default False\\n            Whether to return a new %(klass)s. If True then value of copy is\\n            ignored.\\n        level : int or level name, default None\\n            In case of a MultiIndex, only rename labels in the specified\\n            level.\\n        errors : {\\'ignore\\', \\'raise\\'}, default \\'ignore\\'\\n            If \\'raise\\', raise a `KeyError` when a dict-like `mapper`, `index`,\\n            or `columns` contains labels that are not present in the Index\\n            being transformed.\\n            If \\'ignore\\', existing keys will be renamed and extra keys will be\\n            ignored.\\n\\n        Returns\\n        -------\\n        renamed : %(klass)s (new object)\\n\\n        Raises\\n        ------\\n        KeyError\\n            If any of the labels is not found in the selected axis and\\n            \"errors=\\'raise\\'\".\\n\\n        See Also\\n        --------\\n        NDFrame.rename_axis\\n\\n        Examples\\n        --------\\n\\n        >>> s = pd.Series([1, 2, 3])\\n        >>> s\\n        0    1\\n        1    2\\n        2    3\\n        dtype: int64\\n        >>> s.rename(\"my_name\") # scalar, changes Series.name\\n        0    1\\n        1    2\\n        2    3\\n        Name: my_name, dtype: int64\\n        >>> s.rename(lambda x: x ** 2)  # function, changes labels\\n        0    1\\n        1    2\\n        4    3\\n        dtype: int64\\n        >>> s.rename({1: 3, 2: 5})  # mapping, changes labels\\n        0    1\\n        3    2\\n        5    3\\n        dtype: int64\\n\\n        Since ``DataFrame`` doesn\\'t have a ``.name`` attribute,\\n        only mapping-type arguments are allowed.\\n\\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\\n        >>> df.rename(2)\\n        Traceback (most recent call last):\\n        ...\\n        TypeError: \\'int\\' object is not callable\\n\\n        ``DataFrame.rename`` supports two calling conventions\\n\\n        * ``(index=index_mapper, columns=columns_mapper, ...)``\\n        * ``(mapper, axis={\\'index\\', \\'columns\\'}, ...)``\\n\\n        We *highly* recommend using keyword arguments to clarify your\\n        intent.\\n\\n        >>> df.rename(index=str, columns={\"A\": \"a\", \"B\": \"c\"})\\n           a  c\\n        0  1  4\\n        1  2  5\\n        2  3  6\\n\\n        >>> df.rename(index=str, columns={\"A\": \"a\", \"C\": \"c\"})\\n           a  B\\n        0  1  4\\n        1  2  5\\n        2  3  6\\n\\n        Using axis-style parameters\\n\\n        >>> df.rename(str.lower, axis=\\'columns\\')\\n           a  b\\n        0  1  4\\n        1  2  5\\n        2  3  6\\n\\n        >>> df.rename({1: 2, 2: 4}, axis=\\'index\\')\\n           A  B\\n        0  1  4\\n        2  2  5\\n        4  3  6\\n\\n        See the :ref:`user guide <basics.rename>` for more.\\n        \"\"\"\\n        axes, kwargs = self._construct_axes_from_arguments(args, kwargs)\\n        copy = kwargs.pop(\\'copy\\', True)\\n        inplace = kwargs.pop(\\'inplace\\', False)\\n        level = kwargs.pop(\\'level\\', None)\\n        axis = kwargs.pop(\\'axis\\', None)\\n        errors = kwargs.pop(\\'errors\\', \\'ignore\\')\\n        if axis is not None:\\n            # Validate the axis\\n            self._get_axis_number(axis)\\n\\n        if kwargs:\\n            raise TypeError(\\'rename() got an unexpected keyword \\'\\n                            \\'argument \"{0}\"\\'.format(list(kwargs.keys())[0]))\\n\\n        if com.count_not_none(*axes.values()) == 0:\\n            raise TypeError(\\'must pass an index to rename\\')\\n\\n        self._consolidate_inplace()\\n        result = self if inplace else self.copy(deep=copy)\\n\\n        # start in the axis order to eliminate too many copies\\n        for axis in lrange(self._AXIS_LEN):\\n            v = axes.get(self._AXIS_NAMES[axis])\\n            if v is None:\\n                continue\\n            f = com._get_rename_function(v)\\n            baxis = self._get_block_manager_axis(axis)\\n            if level is not None:\\n                level = self.axes[axis]._get_level_number(level)\\n\\n            # GH 13473\\n            if not callable(v):\\n                indexer = self.axes[axis].get_indexer_for(v)\\n                if errors == \\'raise\\' and len(indexer[indexer == -1]):\\n                    missing_labels = [label for index, label in enumerate(v)\\n                                      if indexer[index] == -1]\\n                    raise KeyError(\\'{} not found in axis\\'\\n                                   .format(missing_labels))\\n\\n            result._data = result._data.rename_axis(f, axis=baxis, copy=copy,\\n                                                    level=level)\\n            result._clear_item_cache()\\n\\n        if inplace:\\n            self._update_inplace(result._data)\\n        else:\\n            return result.__finalize__(self)',\n 'def rename_axis(self, mapper=sentinel, **kwargs):\\n        \"\"\"\\n        Set the name of the axis for the index or columns.\\n\\n        Parameters\\n        ----------\\n        mapper : scalar, list-like, optional\\n            Value to set the axis name attribute.\\n        index, columns : scalar, list-like, dict-like or function, optional\\n            A scalar, list-like, dict-like or functions transformations to\\n            apply to that axis\\' values.\\n\\n            Use either ``mapper`` and ``axis`` to\\n            specify the axis to target with ``mapper``, or ``index``\\n            and/or ``columns``.\\n\\n            .. versionchanged:: 0.24.0\\n\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n            The axis to rename.\\n        copy : bool, default True\\n            Also copy underlying data.\\n        inplace : bool, default False\\n            Modifies the object directly, instead of creating a new Series\\n            or DataFrame.\\n\\n        Returns\\n        -------\\n        Series, DataFrame, or None\\n            The same type as the caller or None if `inplace` is True.\\n\\n        See Also\\n        --------\\n        Series.rename : Alter Series index labels or name.\\n        DataFrame.rename : Alter DataFrame index labels or name.\\n        Index.rename : Set new names on index.\\n\\n        Notes\\n        -----\\n        Prior to version 0.21.0, ``rename_axis`` could also be used to change\\n        the axis *labels* by passing a mapping or scalar. This behavior is\\n        deprecated and will be removed in a future version. Use ``rename``\\n        instead.\\n\\n        ``DataFrame.rename_axis`` supports two calling conventions\\n\\n        * ``(index=index_mapper, columns=columns_mapper, ...)``\\n        * ``(mapper, axis={\\'index\\', \\'columns\\'}, ...)``\\n\\n        The first calling convention will only modify the names of\\n        the index and/or the names of the Index object that is the columns.\\n        In this case, the parameter ``copy`` is ignored.\\n\\n        The second calling convention will modify the names of the\\n        the corresponding index if mapper is a list or a scalar.\\n        However, if mapper is dict-like or a function, it will use the\\n        deprecated behavior of modifying the axis *labels*.\\n\\n        We *highly* recommend using keyword arguments to clarify your\\n        intent.\\n\\n        Examples\\n        --------\\n        **Series**\\n\\n        >>> s = pd.Series([\"dog\", \"cat\", \"monkey\"])\\n        >>> s\\n        0       dog\\n        1       cat\\n        2    monkey\\n        dtype: object\\n        >>> s.rename_axis(\"animal\")\\n        animal\\n        0    dog\\n        1    cat\\n        2    monkey\\n        dtype: object\\n\\n        **DataFrame**\\n\\n        >>> df = pd.DataFrame({\"num_legs\": [4, 4, 2],\\n        ...                    \"num_arms\": [0, 0, 2]},\\n        ...                   [\"dog\", \"cat\", \"monkey\"])\\n        >>> df\\n                num_legs  num_arms\\n        dog            4         0\\n        cat            4         0\\n        monkey         2         2\\n        >>> df = df.rename_axis(\"animal\")\\n        >>> df\\n                num_legs  num_arms\\n        animal\\n        dog            4         0\\n        cat            4         0\\n        monkey         2         2\\n        >>> df = df.rename_axis(\"limbs\", axis=\"columns\")\\n        >>> df\\n        limbs   num_legs  num_arms\\n        animal\\n        dog            4         0\\n        cat            4         0\\n        monkey         2         2\\n\\n        **MultiIndex**\\n\\n        >>> df.index = pd.MultiIndex.from_product([[\\'mammal\\'],\\n        ...                                        [\\'dog\\', \\'cat\\', \\'monkey\\']],\\n        ...                                       names=[\\'type\\', \\'name\\'])\\n        >>> df\\n        limbs          num_legs  num_arms\\n        type   name\\n        mammal dog            4         0\\n               cat            4         0\\n               monkey         2         2\\n\\n        >>> df.rename_axis(index={\\'type\\': \\'class\\'})\\n        limbs          num_legs  num_arms\\n        class  name\\n        mammal dog            4         0\\n               cat            4         0\\n               monkey         2         2\\n\\n        >>> df.rename_axis(columns=str.upper)\\n        LIMBS          num_legs  num_arms\\n        type   name\\n        mammal dog            4         0\\n               cat            4         0\\n               monkey         2         2\\n        \"\"\"\\n        axes, kwargs = self._construct_axes_from_arguments(\\n            (), kwargs, sentinel=sentinel)\\n        copy = kwargs.pop(\\'copy\\', True)\\n        inplace = kwargs.pop(\\'inplace\\', False)\\n        axis = kwargs.pop(\\'axis\\', 0)\\n        if axis is not None:\\n            axis = self._get_axis_number(axis)\\n\\n        if kwargs:\\n            raise TypeError(\\'rename_axis() got an unexpected keyword \\'\\n                            \\'argument \"{0}\"\\'.format(list(kwargs.keys())[0]))\\n\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n\\n        if (mapper is not sentinel):\\n            # Use v0.23 behavior if a scalar or list\\n            non_mapper = is_scalar(mapper) or (is_list_like(mapper) and not\\n                                               is_dict_like(mapper))\\n            if non_mapper:\\n                return self._set_axis_name(mapper, axis=axis, inplace=inplace)\\n            else:\\n                # Deprecated (v0.21) behavior is if mapper is specified,\\n                # and not a list or scalar, then call rename\\n                msg = (\"Using \\'rename_axis\\' to alter labels is deprecated. \"\\n                       \"Use \\'.rename\\' instead\")\\n                warnings.warn(msg, FutureWarning, stacklevel=3)\\n                axis = self._get_axis_name(axis)\\n                d = {\\'copy\\': copy, \\'inplace\\': inplace}\\n                d[axis] = mapper\\n                return self.rename(**d)\\n        else:\\n            # Use new behavior.  Means that index and/or columns\\n            # is specified\\n            result = self if inplace else self.copy(deep=copy)\\n\\n            for axis in lrange(self._AXIS_LEN):\\n                v = axes.get(self._AXIS_NAMES[axis])\\n                if v is sentinel:\\n                    continue\\n                non_mapper = is_scalar(v) or (is_list_like(v) and not\\n                                              is_dict_like(v))\\n                if non_mapper:\\n                    newnames = v\\n                else:\\n                    f = com._get_rename_function(v)\\n                    curnames = self._get_axis(axis).names\\n                    newnames = [f(name) for name in curnames]\\n                result._set_axis_name(newnames, axis=axis,\\n                                      inplace=True)\\n            if not inplace:\\n                return result',\n 'def _set_axis_name(self, name, axis=0, inplace=False):\\n        \"\"\"\\n        Set the name(s) of the axis.\\n\\n        Parameters\\n        ----------\\n        name : str or list of str\\n            Name(s) to set.\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n            The axis to set the label. The value 0 or \\'index\\' specifies index,\\n            and the value 1 or \\'columns\\' specifies columns.\\n        inplace : bool, default False\\n            If `True`, do operation inplace and return None.\\n\\n            .. versionadded:: 0.21.0\\n\\n        Returns\\n        -------\\n        Series, DataFrame, or None\\n            The same type as the caller or `None` if `inplace` is `True`.\\n\\n        See Also\\n        --------\\n        DataFrame.rename : Alter the axis labels of :class:`DataFrame`.\\n        Series.rename : Alter the index labels or set the index name\\n            of :class:`Series`.\\n        Index.rename : Set the name of :class:`Index` or :class:`MultiIndex`.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\"num_legs\": [4, 4, 2]},\\n        ...                   [\"dog\", \"cat\", \"monkey\"])\\n        >>> df\\n                num_legs\\n        dog            4\\n        cat            4\\n        monkey         2\\n        >>> df._set_axis_name(\"animal\")\\n                num_legs\\n        animal\\n        dog            4\\n        cat            4\\n        monkey         2\\n        >>> df.index = pd.MultiIndex.from_product(\\n        ...                [[\"mammal\"], [\\'dog\\', \\'cat\\', \\'monkey\\']])\\n        >>> df._set_axis_name([\"type\", \"name\"])\\n                       legs\\n        type   name\\n        mammal dog        4\\n               cat        4\\n               monkey     2\\n        \"\"\"\\n        axis = self._get_axis_number(axis)\\n        idx = self._get_axis(axis).set_names(name)\\n\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        renamed = self if inplace else self.copy()\\n        renamed.set_axis(idx, axis=axis, inplace=True)\\n        if not inplace:\\n            return renamed',\n 'def equals(self, other):\\n        \"\"\"\\n        Test whether two objects contain the same elements.\\n\\n        This function allows two Series or DataFrames to be compared against\\n        each other to see if they have the same shape and elements. NaNs in\\n        the same location are considered equal. The column headers do not\\n        need to have the same type, but the elements within the columns must\\n        be the same dtype.\\n\\n        Parameters\\n        ----------\\n        other : Series or DataFrame\\n            The other Series or DataFrame to be compared with the first.\\n\\n        Returns\\n        -------\\n        bool\\n            True if all elements are the same in both objects, False\\n            otherwise.\\n\\n        See Also\\n        --------\\n        Series.eq : Compare two Series objects of the same length\\n            and return a Series where each element is True if the element\\n            in each Series is equal, False otherwise.\\n        DataFrame.eq : Compare two DataFrame objects of the same shape and\\n            return a DataFrame where each element is True if the respective\\n            element in each DataFrame is equal, False otherwise.\\n        assert_series_equal : Return True if left and right Series are equal,\\n            False otherwise.\\n        assert_frame_equal : Return True if left and right DataFrames are\\n            equal, False otherwise.\\n        numpy.array_equal : Return True if two arrays have the same shape\\n            and elements, False otherwise.\\n\\n        Notes\\n        -----\\n        This function requires that the elements have the same dtype as their\\n        respective elements in the other Series or DataFrame. However, the\\n        column labels do not need to have the same type, as long as they are\\n        still considered equal.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({1: [10], 2: [20]})\\n        >>> df\\n            1   2\\n        0  10  20\\n\\n        DataFrames df and exactly_equal have the same types and values for\\n        their elements and column labels, which will return True.\\n\\n        >>> exactly_equal = pd.DataFrame({1: [10], 2: [20]})\\n        >>> exactly_equal\\n            1   2\\n        0  10  20\\n        >>> df.equals(exactly_equal)\\n        True\\n\\n        DataFrames df and different_column_type have the same element\\n        types and values, but have different types for the column labels,\\n        which will still return True.\\n\\n        >>> different_column_type = pd.DataFrame({1.0: [10], 2.0: [20]})\\n        >>> different_column_type\\n           1.0  2.0\\n        0   10   20\\n        >>> df.equals(different_column_type)\\n        True\\n\\n        DataFrames df and different_data_type have different types for the\\n        same values for their elements, and will return False even though\\n        their column labels are the same values and types.\\n\\n        >>> different_data_type = pd.DataFrame({1: [10.0], 2: [20.0]})\\n        >>> different_data_type\\n              1     2\\n        0  10.0  20.0\\n        >>> df.equals(different_data_type)\\n        False\\n        \"\"\"\\n        if not isinstance(other, self._constructor):\\n            return False\\n        return self._data.equals(other._data)',\n 'def bool(self):\\n        \"\"\"\\n        Return the bool of a single element PandasObject.\\n\\n        This must be a boolean scalar value, either True or False.  Raise a\\n        ValueError if the PandasObject does not have exactly 1 element, or that\\n        element is not boolean\\n        \"\"\"\\n        v = self.squeeze()\\n        if isinstance(v, (bool, np.bool_)):\\n            return bool(v)\\n        elif is_scalar(v):\\n            raise ValueError(\"bool cannot act on a non-boolean single element \"\\n                             \"{0}\".format(self.__class__.__name__))\\n\\n        self.__nonzero__()',\n 'def _is_level_reference(self, key, axis=0):\\n        \"\"\"\\n        Test whether a key is a level reference for a given axis.\\n\\n        To be considered a level reference, `key` must be a string that:\\n          - (axis=0): Matches the name of an index level and does NOT match\\n            a column label.\\n          - (axis=1): Matches the name of a column level and does NOT match\\n            an index label.\\n\\n        Parameters\\n        ----------\\n        key : str\\n            Potential level name for the given axis\\n        axis : int, default 0\\n            Axis that levels are associated with (0 for index, 1 for columns)\\n\\n        Returns\\n        -------\\n        is_level : bool\\n        \"\"\"\\n        axis = self._get_axis_number(axis)\\n\\n        if self.ndim > 2:\\n            raise NotImplementedError(\\n                \"_is_level_reference is not implemented for {type}\"\\n                .format(type=type(self)))\\n\\n        return (key is not None and\\n                is_hashable(key) and\\n                key in self.axes[axis].names and\\n                not self._is_label_reference(key, axis=axis))',\n 'def _is_label_reference(self, key, axis=0):\\n        \"\"\"\\n        Test whether a key is a label reference for a given axis.\\n\\n        To be considered a label reference, `key` must be a string that:\\n          - (axis=0): Matches a column label\\n          - (axis=1): Matches an index label\\n\\n        Parameters\\n        ----------\\n        key: str\\n            Potential label name\\n        axis: int, default 0\\n            Axis perpendicular to the axis that labels are associated with\\n            (0 means search for column labels, 1 means search for index labels)\\n\\n        Returns\\n        -------\\n        is_label: bool\\n        \"\"\"\\n        if self.ndim > 2:\\n            raise NotImplementedError(\\n                \"_is_label_reference is not implemented for {type}\"\\n                .format(type=type(self)))\\n\\n        axis = self._get_axis_number(axis)\\n        other_axes = (ax for ax in range(self._AXIS_LEN) if ax != axis)\\n\\n        return (key is not None and\\n                is_hashable(key) and\\n                any(key in self.axes[ax] for ax in other_axes))',\n 'def _is_label_or_level_reference(self, key, axis=0):\\n        \"\"\"\\n        Test whether a key is a label or level reference for a given axis.\\n\\n        To be considered either a label or a level reference, `key` must be a\\n        string that:\\n          - (axis=0): Matches a column label or an index level\\n          - (axis=1): Matches an index label or a column level\\n\\n        Parameters\\n        ----------\\n        key: str\\n            Potential label or level name\\n        axis: int, default 0\\n            Axis that levels are associated with (0 for index, 1 for columns)\\n\\n        Returns\\n        -------\\n        is_label_or_level: bool\\n        \"\"\"\\n\\n        if self.ndim > 2:\\n            raise NotImplementedError(\\n                \"_is_label_or_level_reference is not implemented for {type}\"\\n                .format(type=type(self)))\\n\\n        return (self._is_level_reference(key, axis=axis) or\\n                self._is_label_reference(key, axis=axis))',\n 'def _check_label_or_level_ambiguity(self, key, axis=0):\\n        \"\"\"\\n        Check whether `key` is ambiguous.\\n\\n        By ambiguous, we mean that it matches both a level of the input\\n        `axis` and a label of the other axis.\\n\\n        Parameters\\n        ----------\\n        key: str or object\\n            label or level name\\n        axis: int, default 0\\n            Axis that levels are associated with (0 for index, 1 for columns)\\n\\n        Raises\\n        ------\\n        ValueError: `key` is ambiguous\\n        \"\"\"\\n        if self.ndim > 2:\\n            raise NotImplementedError(\\n                \"_check_label_or_level_ambiguity is not implemented for {type}\"\\n                .format(type=type(self)))\\n\\n        axis = self._get_axis_number(axis)\\n        other_axes = (ax for ax in range(self._AXIS_LEN) if ax != axis)\\n\\n        if (key is not None and\\n                is_hashable(key) and\\n                key in self.axes[axis].names and\\n                any(key in self.axes[ax] for ax in other_axes)):\\n\\n            # Build an informative and grammatical warning\\n            level_article, level_type = ((\\'an\\', \\'index\\')\\n                                         if axis == 0 else\\n                                         (\\'a\\', \\'column\\'))\\n\\n            label_article, label_type = ((\\'a\\', \\'column\\')\\n                                         if axis == 0 else\\n                                         (\\'an\\', \\'index\\'))\\n\\n            msg = (\"\\'{key}\\' is both {level_article} {level_type} level and \"\\n                   \"{label_article} {label_type} label, which is ambiguous.\"\\n                   ).format(key=key,\\n                            level_article=level_article,\\n                            level_type=level_type,\\n                            label_article=label_article,\\n                            label_type=label_type)\\n            raise ValueError(msg)',\n 'def _get_label_or_level_values(self, key, axis=0):\\n        \"\"\"\\n        Return a 1-D array of values associated with `key`, a label or level\\n        from the given `axis`.\\n\\n        Retrieval logic:\\n          - (axis=0): Return column values if `key` matches a column label.\\n            Otherwise return index level values if `key` matches an index\\n            level.\\n          - (axis=1): Return row values if `key` matches an index label.\\n            Otherwise return column level values if \\'key\\' matches a column\\n            level\\n\\n        Parameters\\n        ----------\\n        key: str\\n            Label or level name.\\n        axis: int, default 0\\n            Axis that levels are associated with (0 for index, 1 for columns)\\n\\n        Returns\\n        -------\\n        values: np.ndarray\\n\\n        Raises\\n        ------\\n        KeyError\\n            if `key` matches neither a label nor a level\\n        ValueError\\n            if `key` matches multiple labels\\n        FutureWarning\\n            if `key` is ambiguous. This will become an ambiguity error in a\\n            future version\\n        \"\"\"\\n        if self.ndim > 2:\\n            raise NotImplementedError(\\n                \"_get_label_or_level_values is not implemented for {type}\"\\n                .format(type=type(self)))\\n\\n        axis = self._get_axis_number(axis)\\n        other_axes = [ax for ax in range(self._AXIS_LEN) if ax != axis]\\n\\n        if self._is_label_reference(key, axis=axis):\\n            self._check_label_or_level_ambiguity(key, axis=axis)\\n            values = self.xs(key, axis=other_axes[0])._values\\n        elif self._is_level_reference(key, axis=axis):\\n            values = self.axes[axis].get_level_values(key)._values\\n        else:\\n            raise KeyError(key)\\n\\n        # Check for duplicates\\n        if values.ndim > 1:\\n\\n            if other_axes and isinstance(\\n                    self._get_axis(other_axes[0]), MultiIndex):\\n                multi_message = (\\'\\\\n\\'\\n                                 \\'For a multi-index, the label must be a \\'\\n                                 \\'tuple with elements corresponding to \\'\\n                                 \\'each level.\\')\\n            else:\\n                multi_message = \\'\\'\\n\\n            label_axis_name = \\'column\\' if axis == 0 else \\'index\\'\\n            raise ValueError((\"The {label_axis_name} label \\'{key}\\' \"\\n                              \"is not unique.{multi_message}\")\\n                             .format(key=key,\\n                                     label_axis_name=label_axis_name,\\n                                     multi_message=multi_message))\\n\\n        return values',\n 'def _drop_labels_or_levels(self, keys, axis=0):\\n        \"\"\"\\n        Drop labels and/or levels for the given `axis`.\\n\\n        For each key in `keys`:\\n          - (axis=0): If key matches a column label then drop the column.\\n            Otherwise if key matches an index level then drop the level.\\n          - (axis=1): If key matches an index label then drop the row.\\n            Otherwise if key matches a column level then drop the level.\\n\\n        Parameters\\n        ----------\\n        keys: str or list of str\\n            labels or levels to drop\\n        axis: int, default 0\\n            Axis that levels are associated with (0 for index, 1 for columns)\\n\\n        Returns\\n        -------\\n        dropped: DataFrame\\n\\n        Raises\\n        ------\\n        ValueError\\n            if any `keys` match neither a label nor a level\\n        \"\"\"\\n        if self.ndim > 2:\\n            raise NotImplementedError(\\n                \"_drop_labels_or_levels is not implemented for {type}\"\\n                .format(type=type(self)))\\n\\n        axis = self._get_axis_number(axis)\\n\\n        # Validate keys\\n        keys = com.maybe_make_list(keys)\\n        invalid_keys = [k for k in keys if not\\n                        self._is_label_or_level_reference(k, axis=axis)]\\n\\n        if invalid_keys:\\n            raise ValueError((\"The following keys are not valid labels or \"\\n                              \"levels for axis {axis}: {invalid_keys}\")\\n                             .format(axis=axis,\\n                                     invalid_keys=invalid_keys))\\n\\n        # Compute levels and labels to drop\\n        levels_to_drop = [k for k in keys\\n                          if self._is_level_reference(k, axis=axis)]\\n\\n        labels_to_drop = [k for k in keys\\n                          if not self._is_level_reference(k, axis=axis)]\\n\\n        # Perform copy upfront and then use inplace operations below.\\n        # This ensures that we always perform exactly one copy.\\n        # ``copy`` and/or ``inplace`` options could be added in the future.\\n        dropped = self.copy()\\n\\n        if axis == 0:\\n            # Handle dropping index levels\\n            if levels_to_drop:\\n                dropped.reset_index(levels_to_drop, drop=True, inplace=True)\\n\\n            # Handle dropping columns labels\\n            if labels_to_drop:\\n                dropped.drop(labels_to_drop, axis=1, inplace=True)\\n        else:\\n            # Handle dropping column levels\\n            if levels_to_drop:\\n                if isinstance(dropped.columns, MultiIndex):\\n                    # Drop the specified levels from the MultiIndex\\n                    dropped.columns = dropped.columns.droplevel(levels_to_drop)\\n                else:\\n                    # Drop the last level of Index by replacing with\\n                    # a RangeIndex\\n                    dropped.columns = RangeIndex(dropped.columns.size)\\n\\n            # Handle dropping index labels\\n            if labels_to_drop:\\n                dropped.drop(labels_to_drop, axis=0, inplace=True)\\n\\n        return dropped',\n 'def empty(self):\\n        \"\"\"\\n        Indicator whether DataFrame is empty.\\n\\n        True if DataFrame is entirely empty (no items), meaning any of the\\n        axes are of length 0.\\n\\n        Returns\\n        -------\\n        bool\\n            If DataFrame is empty, return True, if not return False.\\n\\n        See Also\\n        --------\\n        Series.dropna\\n        DataFrame.dropna\\n\\n        Notes\\n        -----\\n        If DataFrame contains only NaNs, it is still not considered empty. See\\n        the example below.\\n\\n        Examples\\n        --------\\n        An example of an actual empty DataFrame. Notice the index is empty:\\n\\n        >>> df_empty = pd.DataFrame({\\'A\\' : []})\\n        >>> df_empty\\n        Empty DataFrame\\n        Columns: [A]\\n        Index: []\\n        >>> df_empty.empty\\n        True\\n\\n        If we only have NaNs in our DataFrame, it is not considered empty! We\\n        will need to drop the NaNs to make the DataFrame empty:\\n\\n        >>> df = pd.DataFrame({\\'A\\' : [np.nan]})\\n        >>> df\\n            A\\n        0 NaN\\n        >>> df.empty\\n        False\\n        >>> df.dropna().empty\\n        True\\n        \"\"\"\\n        return any(len(self._get_axis(a)) == 0 for a in self._AXIS_ORDERS)',\n 'def _repr_data_resource_(self):\\n        \"\"\"\\n        Not a real Jupyter special repr method, but we use the same\\n        naming convention.\\n        \"\"\"\\n        if config.get_option(\"display.html.table_schema\"):\\n            data = self.head(config.get_option(\\'display.max_rows\\'))\\n            payload = json.loads(data.to_json(orient=\\'table\\'),\\n                                 object_pairs_hook=collections.OrderedDict)\\n            return payload',\n 'def to_json(self, path_or_buf=None, orient=None, date_format=None,\\n                double_precision=10, force_ascii=True, date_unit=\\'ms\\',\\n                default_handler=None, lines=False, compression=\\'infer\\',\\n                index=True):\\n        \"\"\"\\n        Convert the object to a JSON string.\\n\\n        Note NaN\\'s and None will be converted to null and datetime objects\\n        will be converted to UNIX timestamps.\\n\\n        Parameters\\n        ----------\\n        path_or_buf : string or file handle, optional\\n            File path or object. If not specified, the result is returned as\\n            a string.\\n        orient : string\\n            Indication of expected JSON string format.\\n\\n            * Series\\n\\n              - default is \\'index\\'\\n              - allowed values are: {\\'split\\',\\'records\\',\\'index\\',\\'table\\'}\\n\\n            * DataFrame\\n\\n              - default is \\'columns\\'\\n              - allowed values are:\\n                {\\'split\\',\\'records\\',\\'index\\',\\'columns\\',\\'values\\',\\'table\\'}\\n\\n            * The format of the JSON string\\n\\n              - \\'split\\' : dict like {\\'index\\' -> [index],\\n                \\'columns\\' -> [columns], \\'data\\' -> [values]}\\n              - \\'records\\' : list like\\n                [{column -> value}, ... , {column -> value}]\\n              - \\'index\\' : dict like {index -> {column -> value}}\\n              - \\'columns\\' : dict like {column -> {index -> value}}\\n              - \\'values\\' : just the values array\\n              - \\'table\\' : dict like {\\'schema\\': {schema}, \\'data\\': {data}}\\n                describing the data, and the data component is\\n                like ``orient=\\'records\\'``.\\n\\n                .. versionchanged:: 0.20.0\\n\\n        date_format : {None, \\'epoch\\', \\'iso\\'}\\n            Type of date conversion. \\'epoch\\' = epoch milliseconds,\\n            \\'iso\\' = ISO8601. The default depends on the `orient`. For\\n            ``orient=\\'table\\'``, the default is \\'iso\\'. For all other orients,\\n            the default is \\'epoch\\'.\\n        double_precision : int, default 10\\n            The number of decimal places to use when encoding\\n            floating point values.\\n        force_ascii : bool, default True\\n            Force encoded string to be ASCII.\\n        date_unit : string, default \\'ms\\' (milliseconds)\\n            The time unit to encode to, governs timestamp and ISO8601\\n            precision.  One of \\'s\\', \\'ms\\', \\'us\\', \\'ns\\' for second, millisecond,\\n            microsecond, and nanosecond respectively.\\n        default_handler : callable, default None\\n            Handler to call if object cannot otherwise be converted to a\\n            suitable format for JSON. Should receive a single argument which is\\n            the object to convert and return a serialisable object.\\n        lines : bool, default False\\n            If \\'orient\\' is \\'records\\' write out line delimited json format. Will\\n            throw ValueError if incorrect \\'orient\\' since others are not list\\n            like.\\n\\n            .. versionadded:: 0.19.0\\n\\n        compression : {\\'infer\\', \\'gzip\\', \\'bz2\\', \\'zip\\', \\'xz\\', None}\\n\\n            A string representing the compression to use in the output file,\\n            only used when the first argument is a filename. By default, the\\n            compression is inferred from the filename.\\n\\n            .. versionadded:: 0.21.0\\n            .. versionchanged:: 0.24.0\\n               \\'infer\\' option added and set to default\\n        index : bool, default True\\n            Whether to include the index values in the JSON string. Not\\n            including the index (``index=False``) is only supported when\\n            orient is \\'split\\' or \\'table\\'.\\n\\n            .. versionadded:: 0.23.0\\n\\n        See Also\\n        --------\\n        read_json\\n\\n        Examples\\n        --------\\n\\n        >>> df = pd.DataFrame([[\\'a\\', \\'b\\'], [\\'c\\', \\'d\\']],\\n        ...                   index=[\\'row 1\\', \\'row 2\\'],\\n        ...                   columns=[\\'col 1\\', \\'col 2\\'])\\n        >>> df.to_json(orient=\\'split\\')\\n        \\'{\"columns\":[\"col 1\",\"col 2\"],\\n          \"index\":[\"row 1\",\"row 2\"],\\n          \"data\":[[\"a\",\"b\"],[\"c\",\"d\"]]}\\'\\n\\n        Encoding/decoding a Dataframe using ``\\'records\\'`` formatted JSON.\\n        Note that index labels are not preserved with this encoding.\\n\\n        >>> df.to_json(orient=\\'records\\')\\n        \\'[{\"col 1\":\"a\",\"col 2\":\"b\"},{\"col 1\":\"c\",\"col 2\":\"d\"}]\\'\\n\\n        Encoding/decoding a Dataframe using ``\\'index\\'`` formatted JSON:\\n\\n        >>> df.to_json(orient=\\'index\\')\\n        \\'{\"row 1\":{\"col 1\":\"a\",\"col 2\":\"b\"},\"row 2\":{\"col 1\":\"c\",\"col 2\":\"d\"}}\\'\\n\\n        Encoding/decoding a Dataframe using ``\\'columns\\'`` formatted JSON:\\n\\n        >>> df.to_json(orient=\\'columns\\')\\n        \\'{\"col 1\":{\"row 1\":\"a\",\"row 2\":\"c\"},\"col 2\":{\"row 1\":\"b\",\"row 2\":\"d\"}}\\'\\n\\n        Encoding/decoding a Dataframe using ``\\'values\\'`` formatted JSON:\\n\\n        >>> df.to_json(orient=\\'values\\')\\n        \\'[[\"a\",\"b\"],[\"c\",\"d\"]]\\'\\n\\n        Encoding with Table Schema\\n\\n        >>> df.to_json(orient=\\'table\\')\\n        \\'{\"schema\": {\"fields\": [{\"name\": \"index\", \"type\": \"string\"},\\n                                {\"name\": \"col 1\", \"type\": \"string\"},\\n                                {\"name\": \"col 2\", \"type\": \"string\"}],\\n                     \"primaryKey\": \"index\",\\n                     \"pandas_version\": \"0.20.0\"},\\n          \"data\": [{\"index\": \"row 1\", \"col 1\": \"a\", \"col 2\": \"b\"},\\n                   {\"index\": \"row 2\", \"col 1\": \"c\", \"col 2\": \"d\"}]}\\'\\n        \"\"\"\\n\\n        from pandas.io import json\\n        if date_format is None and orient == \\'table\\':\\n            date_format = \\'iso\\'\\n        elif date_format is None:\\n            date_format = \\'epoch\\'\\n        return json.to_json(path_or_buf=path_or_buf, obj=self, orient=orient,\\n                            date_format=date_format,\\n                            double_precision=double_precision,\\n                            force_ascii=force_ascii, date_unit=date_unit,\\n                            default_handler=default_handler,\\n                            lines=lines, compression=compression,\\n                            index=index)',\n 'def to_hdf(self, path_or_buf, key, **kwargs):\\n        \"\"\"\\n        Write the contained data to an HDF5 file using HDFStore.\\n\\n        Hierarchical Data Format (HDF) is self-describing, allowing an\\n        application to interpret the structure and contents of a file with\\n        no outside information. One HDF file can hold a mix of related objects\\n        which can be accessed as a group or as individual objects.\\n\\n        In order to add another DataFrame or Series to an existing HDF file\\n        please use append mode and a different a key.\\n\\n        For more information see the :ref:`user guide <io.hdf5>`.\\n\\n        Parameters\\n        ----------\\n        path_or_buf : str or pandas.HDFStore\\n            File path or HDFStore object.\\n        key : str\\n            Identifier for the group in the store.\\n        mode : {\\'a\\', \\'w\\', \\'r+\\'}, default \\'a\\'\\n            Mode to open file:\\n\\n            - \\'w\\': write, a new file is created (an existing file with\\n              the same name would be deleted).\\n            - \\'a\\': append, an existing file is opened for reading and\\n              writing, and if the file does not exist it is created.\\n            - \\'r+\\': similar to \\'a\\', but the file must already exist.\\n        format : {\\'fixed\\', \\'table\\'}, default \\'fixed\\'\\n            Possible values:\\n\\n            - \\'fixed\\': Fixed format. Fast writing/reading. Not-appendable,\\n              nor searchable.\\n            - \\'table\\': Table format. Write as a PyTables Table structure\\n              which may perform worse but allow more flexible operations\\n              like searching / selecting subsets of the data.\\n        append : bool, default False\\n            For Table formats, append the input data to the existing.\\n        data_columns :  list of columns or True, optional\\n            List of columns to create as indexed data columns for on-disk\\n            queries, or True to use all columns. By default only the axes\\n            of the object are indexed. See :ref:`io.hdf5-query-data-columns`.\\n            Applicable only to format=\\'table\\'.\\n        complevel : {0-9}, optional\\n            Specifies a compression level for data.\\n            A value of 0 disables compression.\\n        complib : {\\'zlib\\', \\'lzo\\', \\'bzip2\\', \\'blosc\\'}, default \\'zlib\\'\\n            Specifies the compression library to be used.\\n            As of v0.20.2 these additional compressors for Blosc are supported\\n            (default if no compressor specified: \\'blosc:blosclz\\'):\\n            {\\'blosc:blosclz\\', \\'blosc:lz4\\', \\'blosc:lz4hc\\', \\'blosc:snappy\\',\\n            \\'blosc:zlib\\', \\'blosc:zstd\\'}.\\n            Specifying a compression library which is not available issues\\n            a ValueError.\\n        fletcher32 : bool, default False\\n            If applying compression use the fletcher32 checksum.\\n        dropna : bool, default False\\n            If true, ALL nan rows will not be written to store.\\n        errors : str, default \\'strict\\'\\n            Specifies how encoding and decoding errors are to be handled.\\n            See the errors argument for :func:`open` for a full list\\n            of options.\\n\\n        See Also\\n        --------\\n        DataFrame.read_hdf : Read from HDF file.\\n        DataFrame.to_parquet : Write a DataFrame to the binary parquet format.\\n        DataFrame.to_sql : Write to a sql table.\\n        DataFrame.to_feather : Write out feather-format for DataFrames.\\n        DataFrame.to_csv : Write out to a csv file.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'A\\': [1, 2, 3], \\'B\\': [4, 5, 6]},\\n        ...                   index=[\\'a\\', \\'b\\', \\'c\\'])\\n        >>> df.to_hdf(\\'data.h5\\', key=\\'df\\', mode=\\'w\\')\\n\\n        We can add another object to the same file:\\n\\n        >>> s = pd.Series([1, 2, 3, 4])\\n        >>> s.to_hdf(\\'data.h5\\', key=\\'s\\')\\n\\n        Reading from HDF file:\\n\\n        >>> pd.read_hdf(\\'data.h5\\', \\'df\\')\\n        A  B\\n        a  1  4\\n        b  2  5\\n        c  3  6\\n        >>> pd.read_hdf(\\'data.h5\\', \\'s\\')\\n        0    1\\n        1    2\\n        2    3\\n        3    4\\n        dtype: int64\\n\\n        Deleting file with data:\\n\\n        >>> import os\\n        >>> os.remove(\\'data.h5\\')\\n        \"\"\"\\n        from pandas.io import pytables\\n        return pytables.to_hdf(path_or_buf, key, self, **kwargs)',\n 'def to_msgpack(self, path_or_buf=None, encoding=\\'utf-8\\', **kwargs):\\n        \"\"\"\\n        Serialize object to input file path using msgpack format.\\n\\n        THIS IS AN EXPERIMENTAL LIBRARY and the storage format\\n        may not be stable until a future release.\\n\\n        Parameters\\n        ----------\\n        path : string File path, buffer-like, or None\\n            if None, return generated string\\n        append : bool whether to append to an existing msgpack\\n            (default is False)\\n        compress : type of compressor (zlib or blosc), default to None (no\\n            compression)\\n        \"\"\"\\n\\n        from pandas.io import packers\\n        return packers.to_msgpack(path_or_buf, self, encoding=encoding,\\n                                  **kwargs)',\n 'def to_sql(self, name, con, schema=None, if_exists=\\'fail\\', index=True,\\n               index_label=None, chunksize=None, dtype=None, method=None):\\n        \"\"\"\\n        Write records stored in a DataFrame to a SQL database.\\n\\n        Databases supported by SQLAlchemy [1]_ are supported. Tables can be\\n        newly created, appended to, or overwritten.\\n\\n        Parameters\\n        ----------\\n        name : string\\n            Name of SQL table.\\n        con : sqlalchemy.engine.Engine or sqlite3.Connection\\n            Using SQLAlchemy makes it possible to use any DB supported by that\\n            library. Legacy support is provided for sqlite3.Connection objects.\\n        schema : string, optional\\n            Specify the schema (if database flavor supports this). If None, use\\n            default schema.\\n        if_exists : {\\'fail\\', \\'replace\\', \\'append\\'}, default \\'fail\\'\\n            How to behave if the table already exists.\\n\\n            * fail: Raise a ValueError.\\n            * replace: Drop the table before inserting new values.\\n            * append: Insert new values to the existing table.\\n\\n        index : bool, default True\\n            Write DataFrame index as a column. Uses `index_label` as the column\\n            name in the table.\\n        index_label : string or sequence, default None\\n            Column label for index column(s). If None is given (default) and\\n            `index` is True, then the index names are used.\\n            A sequence should be given if the DataFrame uses MultiIndex.\\n        chunksize : int, optional\\n            Rows will be written in batches of this size at a time. By default,\\n            all rows will be written at once.\\n        dtype : dict, optional\\n            Specifying the datatype for columns. The keys should be the column\\n            names and the values should be the SQLAlchemy types or strings for\\n            the sqlite3 legacy mode.\\n        method : {None, \\'multi\\', callable}, default None\\n            Controls the SQL insertion clause used:\\n\\n            * None : Uses standard SQL ``INSERT`` clause (one per row).\\n            * \\'multi\\': Pass multiple values in a single ``INSERT`` clause.\\n            * callable with signature ``(pd_table, conn, keys, data_iter)``.\\n\\n            Details and a sample callable implementation can be found in the\\n            section :ref:`insert method <io.sql.method>`.\\n\\n            .. versionadded:: 0.24.0\\n\\n        Raises\\n        ------\\n        ValueError\\n            When the table already exists and `if_exists` is \\'fail\\' (the\\n            default).\\n\\n        See Also\\n        --------\\n        read_sql : Read a DataFrame from a table.\\n\\n        Notes\\n        -----\\n        Timezone aware datetime columns will be written as\\n        ``Timestamp with timezone`` type with SQLAlchemy if supported by the\\n        database. Otherwise, the datetimes will be stored as timezone unaware\\n        timestamps local to the original timezone.\\n\\n        .. versionadded:: 0.24.0\\n\\n        References\\n        ----------\\n        .. [1] http://docs.sqlalchemy.org\\n        .. [2] https://www.python.org/dev/peps/pep-0249/\\n\\n        Examples\\n        --------\\n\\n        Create an in-memory SQLite database.\\n\\n        >>> from sqlalchemy import create_engine\\n        >>> engine = create_engine(\\'sqlite://\\', echo=False)\\n\\n        Create a table from scratch with 3 rows.\\n\\n        >>> df = pd.DataFrame({\\'name\\' : [\\'User 1\\', \\'User 2\\', \\'User 3\\']})\\n        >>> df\\n             name\\n        0  User 1\\n        1  User 2\\n        2  User 3\\n\\n        >>> df.to_sql(\\'users\\', con=engine)\\n        >>> engine.execute(\"SELECT * FROM users\").fetchall()\\n        [(0, \\'User 1\\'), (1, \\'User 2\\'), (2, \\'User 3\\')]\\n\\n        >>> df1 = pd.DataFrame({\\'name\\' : [\\'User 4\\', \\'User 5\\']})\\n        >>> df1.to_sql(\\'users\\', con=engine, if_exists=\\'append\\')\\n        >>> engine.execute(\"SELECT * FROM users\").fetchall()\\n        [(0, \\'User 1\\'), (1, \\'User 2\\'), (2, \\'User 3\\'),\\n         (0, \\'User 4\\'), (1, \\'User 5\\')]\\n\\n        Overwrite the table with just ``df1``.\\n\\n        >>> df1.to_sql(\\'users\\', con=engine, if_exists=\\'replace\\',\\n        ...            index_label=\\'id\\')\\n        >>> engine.execute(\"SELECT * FROM users\").fetchall()\\n        [(0, \\'User 4\\'), (1, \\'User 5\\')]\\n\\n        Specify the dtype (especially useful for integers with missing values).\\n        Notice that while pandas is forced to store the data as floating point,\\n        the database supports nullable integers. When fetching the data with\\n        Python, we get back integer scalars.\\n\\n        >>> df = pd.DataFrame({\"A\": [1, None, 2]})\\n        >>> df\\n             A\\n        0  1.0\\n        1  NaN\\n        2  2.0\\n\\n        >>> from sqlalchemy.types import Integer\\n        >>> df.to_sql(\\'integers\\', con=engine, index=False,\\n        ...           dtype={\"A\": Integer()})\\n\\n        >>> engine.execute(\"SELECT * FROM integers\").fetchall()\\n        [(1,), (None,), (2,)]\\n        \"\"\"\\n        from pandas.io import sql\\n        sql.to_sql(self, name, con, schema=schema, if_exists=if_exists,\\n                   index=index, index_label=index_label, chunksize=chunksize,\\n                   dtype=dtype, method=method)',\n 'def to_pickle(self, path, compression=\\'infer\\',\\n                  protocol=pickle.HIGHEST_PROTOCOL):\\n        \"\"\"\\n        Pickle (serialize) object to file.\\n\\n        Parameters\\n        ----------\\n        path : str\\n            File path where the pickled object will be stored.\\n        compression : {\\'infer\\', \\'gzip\\', \\'bz2\\', \\'zip\\', \\'xz\\', None}, \\\\\\n        default \\'infer\\'\\n            A string representing the compression to use in the output file. By\\n            default, infers from the file extension in specified path.\\n\\n            .. versionadded:: 0.20.0\\n        protocol : int\\n            Int which indicates which protocol should be used by the pickler,\\n            default HIGHEST_PROTOCOL (see [1]_ paragraph 12.1.2). The possible\\n            values are 0, 1, 2, 3, 4. A negative value for the protocol\\n            parameter is equivalent to setting its value to HIGHEST_PROTOCOL.\\n\\n            .. [1] https://docs.python.org/3/library/pickle.html\\n            .. versionadded:: 0.21.0\\n\\n        See Also\\n        --------\\n        read_pickle : Load pickled pandas object (or any object) from file.\\n        DataFrame.to_hdf : Write DataFrame to an HDF5 file.\\n        DataFrame.to_sql : Write DataFrame to a SQL database.\\n        DataFrame.to_parquet : Write a DataFrame to the binary parquet format.\\n\\n        Examples\\n        --------\\n        >>> original_df = pd.DataFrame({\"foo\": range(5), \"bar\": range(5, 10)})\\n        >>> original_df\\n           foo  bar\\n        0    0    5\\n        1    1    6\\n        2    2    7\\n        3    3    8\\n        4    4    9\\n        >>> original_df.to_pickle(\"./dummy.pkl\")\\n\\n        >>> unpickled_df = pd.read_pickle(\"./dummy.pkl\")\\n        >>> unpickled_df\\n           foo  bar\\n        0    0    5\\n        1    1    6\\n        2    2    7\\n        3    3    8\\n        4    4    9\\n\\n        >>> import os\\n        >>> os.remove(\"./dummy.pkl\")\\n        \"\"\"\\n        from pandas.io.pickle import to_pickle\\n        return to_pickle(self, path, compression=compression,\\n                         protocol=protocol)',\n 'def to_clipboard(self, excel=True, sep=None, **kwargs):\\n        r\"\"\"\\n        Copy object to the system clipboard.\\n\\n        Write a text representation of object to the system clipboard.\\n        This can be pasted into Excel, for example.\\n\\n        Parameters\\n        ----------\\n        excel : bool, default True\\n            - True, use the provided separator, writing in a csv format for\\n              allowing easy pasting into excel.\\n            - False, write a string representation of the object to the\\n              clipboard.\\n\\n        sep : str, default ``\\'\\\\t\\'``\\n            Field delimiter.\\n        **kwargs\\n            These parameters will be passed to DataFrame.to_csv.\\n\\n        See Also\\n        --------\\n        DataFrame.to_csv : Write a DataFrame to a comma-separated values\\n            (csv) file.\\n        read_clipboard : Read text from clipboard and pass to read_table.\\n\\n        Notes\\n        -----\\n        Requirements for your platform.\\n\\n          - Linux : `xclip`, or `xsel` (with `gtk` or `PyQt4` modules)\\n          - Windows : none\\n          - OS X : none\\n\\n        Examples\\n        --------\\n        Copy the contents of a DataFrame to the clipboard.\\n\\n        >>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=[\\'A\\', \\'B\\', \\'C\\'])\\n        >>> df.to_clipboard(sep=\\',\\')\\n        ... # Wrote the following to the system clipboard:\\n        ... # ,A,B,C\\n        ... # 0,1,2,3\\n        ... # 1,4,5,6\\n\\n        We can omit the the index by passing the keyword `index` and setting\\n        it to false.\\n\\n        >>> df.to_clipboard(sep=\\',\\', index=False)\\n        ... # Wrote the following to the system clipboard:\\n        ... # A,B,C\\n        ... # 1,2,3\\n        ... # 4,5,6\\n        \"\"\"\\n        from pandas.io import clipboards\\n        clipboards.to_clipboard(self, excel=excel, sep=sep, **kwargs)',\n 'def to_xarray(self):\\n        \"\"\"\\n        Return an xarray object from the pandas object.\\n\\n        Returns\\n        -------\\n        xarray.DataArray or xarray.Dataset\\n            Data in the pandas structure converted to Dataset if the object is\\n            a DataFrame, or a DataArray if the object is a Series.\\n\\n        See Also\\n        --------\\n        DataFrame.to_hdf : Write DataFrame to an HDF5 file.\\n        DataFrame.to_parquet : Write a DataFrame to the binary parquet format.\\n\\n        Notes\\n        -----\\n        See the `xarray docs <http://xarray.pydata.org/en/stable/>`__\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame([(\\'falcon\\', \\'bird\\',  389.0, 2),\\n        ...                    (\\'parrot\\', \\'bird\\', 24.0, 2),\\n        ...                    (\\'lion\\',   \\'mammal\\', 80.5, 4),\\n        ...                    (\\'monkey\\', \\'mammal\\', np.nan, 4)],\\n        ...                    columns=[\\'name\\', \\'class\\', \\'max_speed\\',\\n        ...                             \\'num_legs\\'])\\n        >>> df\\n             name   class  max_speed  num_legs\\n        0  falcon    bird      389.0         2\\n        1  parrot    bird       24.0         2\\n        2    lion  mammal       80.5         4\\n        3  monkey  mammal        NaN         4\\n\\n        >>> df.to_xarray()\\n        <xarray.Dataset>\\n        Dimensions:    (index: 4)\\n        Coordinates:\\n          * index      (index) int64 0 1 2 3\\n        Data variables:\\n            name       (index) object \\'falcon\\' \\'parrot\\' \\'lion\\' \\'monkey\\'\\n            class      (index) object \\'bird\\' \\'bird\\' \\'mammal\\' \\'mammal\\'\\n            max_speed  (index) float64 389.0 24.0 80.5 nan\\n            num_legs   (index) int64 2 2 4 4\\n\\n        >>> df[\\'max_speed\\'].to_xarray()\\n        <xarray.DataArray \\'max_speed\\' (index: 4)>\\n        array([389. ,  24. ,  80.5,   nan])\\n        Coordinates:\\n          * index    (index) int64 0 1 2 3\\n\\n        >>> dates = pd.to_datetime([\\'2018-01-01\\', \\'2018-01-01\\',\\n        ...                         \\'2018-01-02\\', \\'2018-01-02\\'])\\n        >>> df_multiindex = pd.DataFrame({\\'date\\': dates,\\n        ...                    \\'animal\\': [\\'falcon\\', \\'parrot\\', \\'falcon\\',\\n        ...                               \\'parrot\\'],\\n        ...                    \\'speed\\': [350, 18, 361, 15]}).set_index([\\'date\\',\\n        ...                                                    \\'animal\\'])\\n        >>> df_multiindex\\n                           speed\\n        date       animal\\n        2018-01-01 falcon    350\\n                   parrot     18\\n        2018-01-02 falcon    361\\n                   parrot     15\\n\\n        >>> df_multiindex.to_xarray()\\n        <xarray.Dataset>\\n        Dimensions:  (animal: 2, date: 2)\\n        Coordinates:\\n          * date     (date) datetime64[ns] 2018-01-01 2018-01-02\\n          * animal   (animal) object \\'falcon\\' \\'parrot\\'\\n        Data variables:\\n            speed    (date, animal) int64 350 18 361 15\\n        \"\"\"\\n\\n        try:\\n            import xarray\\n        except ImportError:\\n            # Give a nice error message\\n            raise ImportError(\"the xarray library is not installed\\\\n\"\\n                              \"you can install via conda\\\\n\"\\n                              \"conda install xarray\\\\n\"\\n                              \"or via pip\\\\n\"\\n                              \"pip install xarray\\\\n\")\\n\\n        if self.ndim == 1:\\n            return xarray.DataArray.from_series(self)\\n        elif self.ndim == 2:\\n            return xarray.Dataset.from_dataframe(self)\\n\\n        # > 2 dims\\n        coords = [(a, self._get_axis(a)) for a in self._AXIS_ORDERS]\\n        return xarray.DataArray(self,\\n                                coords=coords,\\n                                )',\n 'def to_latex(self, buf=None, columns=None, col_space=None, header=True,\\n                 index=True, na_rep=\\'NaN\\', formatters=None, float_format=None,\\n                 sparsify=None, index_names=True, bold_rows=False,\\n                 column_format=None, longtable=None, escape=None,\\n                 encoding=None, decimal=\\'.\\', multicolumn=None,\\n                 multicolumn_format=None, multirow=None):\\n        r\"\"\"\\n        Render an object to a LaTeX tabular environment table.\\n\\n        Render an object to a tabular environment table. You can splice\\n        this into a LaTeX document. Requires \\\\usepackage{booktabs}.\\n\\n        .. versionchanged:: 0.20.2\\n           Added to Series\\n\\n        Parameters\\n        ----------\\n        buf : file descriptor or None\\n            Buffer to write to. If None, the output is returned as a string.\\n        columns : list of label, optional\\n            The subset of columns to write. Writes all columns by default.\\n        col_space : int, optional\\n            The minimum width of each column.\\n        header : bool or list of str, default True\\n            Write out the column names. If a list of strings is given,\\n            it is assumed to be aliases for the column names.\\n        index : bool, default True\\n            Write row names (index).\\n        na_rep : str, default \\'NaN\\'\\n            Missing data representation.\\n        formatters : list of functions or dict of {str: function}, optional\\n            Formatter functions to apply to columns\\' elements by position or\\n            name. The result of each function must be a unicode string.\\n            List must be of length equal to the number of columns.\\n        float_format : str, optional\\n            Format string for floating point numbers.\\n        sparsify : bool, optional\\n            Set to False for a DataFrame with a hierarchical index to print\\n            every multiindex key at each row. By default, the value will be\\n            read from the config module.\\n        index_names : bool, default True\\n            Prints the names of the indexes.\\n        bold_rows : bool, default False\\n            Make the row labels bold in the output.\\n        column_format : str, optional\\n            The columns format as specified in `LaTeX table format\\n            <https://en.wikibooks.org/wiki/LaTeX/Tables>`__ e.g. \\'rcl\\' for 3\\n            columns. By default, \\'l\\' will be used for all columns except\\n            columns of numbers, which default to \\'r\\'.\\n        longtable : bool, optional\\n            By default, the value will be read from the pandas config\\n            module. Use a longtable environment instead of tabular. Requires\\n            adding a \\\\usepackage{longtable} to your LaTeX preamble.\\n        escape : bool, optional\\n            By default, the value will be read from the pandas config\\n            module. When set to False prevents from escaping latex special\\n            characters in column names.\\n        encoding : str, optional\\n            A string representing the encoding to use in the output file,\\n            defaults to \\'utf-8\\'.\\n        decimal : str, default \\'.\\'\\n            Character recognized as decimal separator, e.g. \\',\\' in Europe.\\n\\n            .. versionadded:: 0.18.0\\n        multicolumn : bool, default True\\n            Use \\\\multicolumn to enhance MultiIndex columns.\\n            The default will be read from the config module.\\n\\n            .. versionadded:: 0.20.0\\n        multicolumn_format : str, default \\'l\\'\\n            The alignment for multicolumns, similar to `column_format`\\n            The default will be read from the config module.\\n\\n            .. versionadded:: 0.20.0\\n        multirow : bool, default False\\n            Use \\\\multirow to enhance MultiIndex rows. Requires adding a\\n            \\\\usepackage{multirow} to your LaTeX preamble. Will print\\n            centered labels (instead of top-aligned) across the contained\\n            rows, separating groups via clines. The default will be read\\n            from the pandas config module.\\n\\n            .. versionadded:: 0.20.0\\n\\n        Returns\\n        -------\\n        str or None\\n            If buf is None, returns the resulting LateX format as a\\n            string. Otherwise returns None.\\n\\n        See Also\\n        --------\\n        DataFrame.to_string : Render a DataFrame to a console-friendly\\n            tabular output.\\n        DataFrame.to_html : Render a DataFrame as an HTML table.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'name\\': [\\'Raphael\\', \\'Donatello\\'],\\n        ...                    \\'mask\\': [\\'red\\', \\'purple\\'],\\n        ...                    \\'weapon\\': [\\'sai\\', \\'bo staff\\']})\\n        >>> df.to_latex(index=False) # doctest: +NORMALIZE_WHITESPACE\\n        \\'\\\\\\\\begin{tabular}{lll}\\\\n\\\\\\\\toprule\\\\n      name &    mask &    weapon\\n        \\\\\\\\\\\\\\\\\\\\n\\\\\\\\midrule\\\\n   Raphael &     red &       sai \\\\\\\\\\\\\\\\\\\\n Donatello &\\n         purple &  bo staff \\\\\\\\\\\\\\\\\\\\n\\\\\\\\bottomrule\\\\n\\\\\\\\end{tabular}\\\\n\\'\\n        \"\"\"\\n        # Get defaults from the pandas config\\n        if self.ndim == 1:\\n            self = self.to_frame()\\n        if longtable is None:\\n            longtable = config.get_option(\"display.latex.longtable\")\\n        if escape is None:\\n            escape = config.get_option(\"display.latex.escape\")\\n        if multicolumn is None:\\n            multicolumn = config.get_option(\"display.latex.multicolumn\")\\n        if multicolumn_format is None:\\n            multicolumn_format = config.get_option(\\n                \"display.latex.multicolumn_format\")\\n        if multirow is None:\\n            multirow = config.get_option(\"display.latex.multirow\")\\n\\n        formatter = DataFrameFormatter(self, buf=buf, columns=columns,\\n                                       col_space=col_space, na_rep=na_rep,\\n                                       header=header, index=index,\\n                                       formatters=formatters,\\n                                       float_format=float_format,\\n                                       bold_rows=bold_rows,\\n                                       sparsify=sparsify,\\n                                       index_names=index_names,\\n                                       escape=escape, decimal=decimal)\\n        formatter.to_latex(column_format=column_format, longtable=longtable,\\n                           encoding=encoding, multicolumn=multicolumn,\\n                           multicolumn_format=multicolumn_format,\\n                           multirow=multirow)\\n\\n        if buf is None:\\n            return formatter.buf.getvalue()',\n 'def to_csv(self, path_or_buf=None, sep=\",\", na_rep=\\'\\', float_format=None,\\n               columns=None, header=True, index=True, index_label=None,\\n               mode=\\'w\\', encoding=None, compression=\\'infer\\', quoting=None,\\n               quotechar=\\'\"\\', line_terminator=None, chunksize=None,\\n               tupleize_cols=None, date_format=None, doublequote=True,\\n               escapechar=None, decimal=\\'.\\'):\\n        r\"\"\"\\n        Write object to a comma-separated values (csv) file.\\n\\n        .. versionchanged:: 0.24.0\\n            The order of arguments for Series was changed.\\n\\n        Parameters\\n        ----------\\n        path_or_buf : str or file handle, default None\\n            File path or object, if None is provided the result is returned as\\n            a string.  If a file object is passed it should be opened with\\n            `newline=\\'\\'`, disabling universal newlines.\\n\\n            .. versionchanged:: 0.24.0\\n\\n               Was previously named \"path\" for Series.\\n\\n        sep : str, default \\',\\'\\n            String of length 1. Field delimiter for the output file.\\n        na_rep : str, default \\'\\'\\n            Missing data representation.\\n        float_format : str, default None\\n            Format string for floating point numbers.\\n        columns : sequence, optional\\n            Columns to write.\\n        header : bool or list of str, default True\\n            Write out the column names. If a list of strings is given it is\\n            assumed to be aliases for the column names.\\n\\n            .. versionchanged:: 0.24.0\\n\\n               Previously defaulted to False for Series.\\n\\n        index : bool, default True\\n            Write row names (index).\\n        index_label : str or sequence, or False, default None\\n            Column label for index column(s) if desired. If None is given, and\\n            `header` and `index` are True, then the index names are used. A\\n            sequence should be given if the object uses MultiIndex. If\\n            False do not print fields for index names. Use index_label=False\\n            for easier importing in R.\\n        mode : str\\n            Python write mode, default \\'w\\'.\\n        encoding : str, optional\\n            A string representing the encoding to use in the output file,\\n            defaults to \\'utf-8\\'.\\n        compression : str, default \\'infer\\'\\n            Compression mode among the following possible values: {\\'infer\\',\\n            \\'gzip\\', \\'bz2\\', \\'zip\\', \\'xz\\', None}. If \\'infer\\' and `path_or_buf`\\n            is path-like, then detect compression from the following\\n            extensions: \\'.gz\\', \\'.bz2\\', \\'.zip\\' or \\'.xz\\'. (otherwise no\\n            compression).\\n\\n            .. versionchanged:: 0.24.0\\n\\n               \\'infer\\' option added and set to default.\\n\\n        quoting : optional constant from csv module\\n            Defaults to csv.QUOTE_MINIMAL. If you have set a `float_format`\\n            then floats are converted to strings and thus csv.QUOTE_NONNUMERIC\\n            will treat them as non-numeric.\\n        quotechar : str, default \\'\\\\\"\\'\\n            String of length 1. Character used to quote fields.\\n        line_terminator : str, optional\\n            The newline character or character sequence to use in the output\\n            file. Defaults to `os.linesep`, which depends on the OS in which\\n            this method is called (\\'\\\\n\\' for linux, \\'\\\\r\\\\n\\' for Windows, i.e.).\\n\\n            .. versionchanged:: 0.24.0\\n        chunksize : int or None\\n            Rows to write at a time.\\n        tupleize_cols : bool, default False\\n            Write MultiIndex columns as a list of tuples (if True) or in\\n            the new, expanded format, where each MultiIndex column is a row\\n            in the CSV (if False).\\n\\n            .. deprecated:: 0.21.0\\n               This argument will be removed and will always write each row\\n               of the multi-index as a separate row in the CSV file.\\n        date_format : str, default None\\n            Format string for datetime objects.\\n        doublequote : bool, default True\\n            Control quoting of `quotechar` inside a field.\\n        escapechar : str, default None\\n            String of length 1. Character used to escape `sep` and `quotechar`\\n            when appropriate.\\n        decimal : str, default \\'.\\'\\n            Character recognized as decimal separator. E.g. use \\',\\' for\\n            European data.\\n\\n        Returns\\n        -------\\n        None or str\\n            If path_or_buf is None, returns the resulting csv format as a\\n            string. Otherwise returns None.\\n\\n        See Also\\n        --------\\n        read_csv : Load a CSV file into a DataFrame.\\n        to_excel : Write DataFrame to an Excel file.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'name\\': [\\'Raphael\\', \\'Donatello\\'],\\n        ...                    \\'mask\\': [\\'red\\', \\'purple\\'],\\n        ...                    \\'weapon\\': [\\'sai\\', \\'bo staff\\']})\\n        >>> df.to_csv(index=False)\\n        \\'name,mask,weapon\\\\nRaphael,red,sai\\\\nDonatello,purple,bo staff\\\\n\\'\\n        \"\"\"\\n\\n        df = self if isinstance(self, ABCDataFrame) else self.to_frame()\\n\\n        if tupleize_cols is not None:\\n            warnings.warn(\"The \\'tupleize_cols\\' parameter is deprecated and \"\\n                          \"will be removed in a future version\",\\n                          FutureWarning, stacklevel=2)\\n        else:\\n            tupleize_cols = False\\n\\n        from pandas.io.formats.csvs import CSVFormatter\\n        formatter = CSVFormatter(df, path_or_buf,\\n                                 line_terminator=line_terminator, sep=sep,\\n                                 encoding=encoding,\\n                                 compression=compression, quoting=quoting,\\n                                 na_rep=na_rep, float_format=float_format,\\n                                 cols=columns, header=header, index=index,\\n                                 index_label=index_label, mode=mode,\\n                                 chunksize=chunksize, quotechar=quotechar,\\n                                 tupleize_cols=tupleize_cols,\\n                                 date_format=date_format,\\n                                 doublequote=doublequote,\\n                                 escapechar=escapechar, decimal=decimal)\\n        formatter.save()\\n\\n        if path_or_buf is None:\\n            return formatter.path_or_buf.getvalue()',\n 'def _create_indexer(cls, name, indexer):\\n        \"\"\"Create an indexer like _name in the class.\"\"\"\\n        if getattr(cls, name, None) is None:\\n            _indexer = functools.partial(indexer, name)\\n            setattr(cls, name, property(_indexer, doc=indexer.__doc__))',\n 'def get(self, key, default=None):\\n        \"\"\"\\n        Get item from object for given key (DataFrame column, Panel slice,\\n        etc.). Returns default value if not found.\\n\\n        Parameters\\n        ----------\\n        key : object\\n\\n        Returns\\n        -------\\n        value : same type as items contained in object\\n        \"\"\"\\n        try:\\n            return self[key]\\n        except (KeyError, ValueError, IndexError):\\n            return default',\n 'def _get_item_cache(self, item):\\n        \"\"\"Return the cached item, item represents a label indexer.\"\"\"\\n        cache = self._item_cache\\n        res = cache.get(item)\\n        if res is None:\\n            values = self._data.get(item)\\n            res = self._box_item_values(item, values)\\n            cache[item] = res\\n            res._set_as_cached(item, self)\\n\\n            # for a chain\\n            res._is_copy = self._is_copy\\n        return res',\n 'def _set_as_cached(self, item, cacher):\\n        \"\"\"Set the _cacher attribute on the calling object with a weakref to\\n        cacher.\\n        \"\"\"\\n        self._cacher = (item, weakref.ref(cacher))',\n 'def _iget_item_cache(self, item):\\n        \"\"\"Return the cached item, item represents a positional indexer.\"\"\"\\n        ax = self._info_axis\\n        if ax.is_unique:\\n            lower = self._get_item_cache(ax[item])\\n        else:\\n            lower = self._take(item, axis=self._info_axis_number)\\n        return lower',\n 'def _maybe_update_cacher(self, clear=False, verify_is_copy=True):\\n        \"\"\"\\n        See if we need to update our parent cacher if clear, then clear our\\n        cache.\\n\\n        Parameters\\n        ----------\\n        clear : boolean, default False\\n            clear the item cache\\n        verify_is_copy : boolean, default True\\n            provide is_copy checks\\n\\n        \"\"\"\\n\\n        cacher = getattr(self, \\'_cacher\\', None)\\n        if cacher is not None:\\n            ref = cacher[1]()\\n\\n            # we are trying to reference a dead referant, hence\\n            # a copy\\n            if ref is None:\\n                del self._cacher\\n            else:\\n                try:\\n                    ref._maybe_cache_changed(cacher[0], self)\\n                except Exception:\\n                    pass\\n\\n        if verify_is_copy:\\n            self._check_setitem_copy(stacklevel=5, t=\\'referant\\')\\n\\n        if clear:\\n            self._clear_item_cache()',\n 'def _slice(self, slobj, axis=0, kind=None):\\n        \"\"\"\\n        Construct a slice of this container.\\n\\n        kind parameter is maintained for compatibility with Series slicing.\\n        \"\"\"\\n        axis = self._get_block_manager_axis(axis)\\n        result = self._constructor(self._data.get_slice(slobj, axis=axis))\\n        result = result.__finalize__(self)\\n\\n        # this could be a view\\n        # but only in a single-dtyped view slicable case\\n        is_copy = axis != 0 or result._is_view\\n        result._set_is_copy(self, copy=is_copy)\\n        return result',\n 'def _check_is_chained_assignment_possible(self):\\n        \"\"\"\\n        Check if we are a view, have a cacher, and are of mixed type.\\n        If so, then force a setitem_copy check.\\n\\n        Should be called just near setting a value\\n\\n        Will return a boolean if it we are a view and are cached, but a\\n        single-dtype meaning that the cacher should be updated following\\n        setting.\\n        \"\"\"\\n        if self._is_view and self._is_cached:\\n            ref = self._get_cacher()\\n            if ref is not None and ref._is_mixed_type:\\n                self._check_setitem_copy(stacklevel=4, t=\\'referant\\',\\n                                         force=True)\\n            return True\\n        elif self._is_copy:\\n            self._check_setitem_copy(stacklevel=4, t=\\'referant\\')\\n        return False',\n 'def _take(self, indices, axis=0, is_copy=True):\\n        \"\"\"\\n        Return the elements in the given *positional* indices along an axis.\\n\\n        This means that we are not indexing according to actual values in\\n        the index attribute of the object. We are indexing according to the\\n        actual position of the element in the object.\\n\\n        This is the internal version of ``.take()`` and will contain a wider\\n        selection of parameters useful for internal use but not as suitable\\n        for public usage.\\n\\n        Parameters\\n        ----------\\n        indices : array-like\\n            An array of ints indicating which positions to take.\\n        axis : int, default 0\\n            The axis on which to select elements. \"0\" means that we are\\n            selecting rows, \"1\" means that we are selecting columns, etc.\\n        is_copy : bool, default True\\n            Whether to return a copy of the original object or not.\\n\\n        Returns\\n        -------\\n        taken : same type as caller\\n            An array-like containing the elements taken from the object.\\n\\n        See Also\\n        --------\\n        numpy.ndarray.take\\n        numpy.take\\n        \"\"\"\\n        self._consolidate_inplace()\\n\\n        new_data = self._data.take(indices,\\n                                   axis=self._get_block_manager_axis(axis),\\n                                   verify=True)\\n        result = self._constructor(new_data).__finalize__(self)\\n\\n        # Maybe set copy if we didn\\'t actually change the index.\\n        if is_copy:\\n            if not result._get_axis(axis).equals(self._get_axis(axis)):\\n                result._set_is_copy(self)\\n\\n        return result',\n 'def take(self, indices, axis=0, convert=None, is_copy=True, **kwargs):\\n        \"\"\"\\n        Return the elements in the given *positional* indices along an axis.\\n\\n        This means that we are not indexing according to actual values in\\n        the index attribute of the object. We are indexing according to the\\n        actual position of the element in the object.\\n\\n        Parameters\\n        ----------\\n        indices : array-like\\n            An array of ints indicating which positions to take.\\n        axis : {0 or \\'index\\', 1 or \\'columns\\', None}, default 0\\n            The axis on which to select elements. ``0`` means that we are\\n            selecting rows, ``1`` means that we are selecting columns.\\n        convert : bool, default True\\n            Whether to convert negative indices into positive ones.\\n            For example, ``-1`` would map to the ``len(axis) - 1``.\\n            The conversions are similar to the behavior of indexing a\\n            regular Python list.\\n\\n            .. deprecated:: 0.21.0\\n               In the future, negative indices will always be converted.\\n\\n        is_copy : bool, default True\\n            Whether to return a copy of the original object or not.\\n        **kwargs\\n            For compatibility with :meth:`numpy.take`. Has no effect on the\\n            output.\\n\\n        Returns\\n        -------\\n        taken : same type as caller\\n            An array-like containing the elements taken from the object.\\n\\n        See Also\\n        --------\\n        DataFrame.loc : Select a subset of a DataFrame by labels.\\n        DataFrame.iloc : Select a subset of a DataFrame by positions.\\n        numpy.take : Take elements from an array along an axis.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame([(\\'falcon\\', \\'bird\\',    389.0),\\n        ...                    (\\'parrot\\', \\'bird\\',     24.0),\\n        ...                    (\\'lion\\',   \\'mammal\\',   80.5),\\n        ...                    (\\'monkey\\', \\'mammal\\', np.nan)],\\n        ...                    columns=[\\'name\\', \\'class\\', \\'max_speed\\'],\\n        ...                    index=[0, 2, 3, 1])\\n        >>> df\\n             name   class  max_speed\\n        0  falcon    bird      389.0\\n        2  parrot    bird       24.0\\n        3    lion  mammal       80.5\\n        1  monkey  mammal        NaN\\n\\n        Take elements at positions 0 and 3 along the axis 0 (default).\\n\\n        Note how the actual indices selected (0 and 1) do not correspond to\\n        our selected indices 0 and 3. That\\'s because we are selecting the 0th\\n        and 3rd rows, not rows whose indices equal 0 and 3.\\n\\n        >>> df.take([0, 3])\\n             name   class  max_speed\\n        0  falcon    bird      389.0\\n        1  monkey  mammal        NaN\\n\\n        Take elements at indices 1 and 2 along the axis 1 (column selection).\\n\\n        >>> df.take([1, 2], axis=1)\\n            class  max_speed\\n        0    bird      389.0\\n        2    bird       24.0\\n        3  mammal       80.5\\n        1  mammal        NaN\\n\\n        We may take elements using negative integers for positive indices,\\n        starting from the end of the object, just like with Python lists.\\n\\n        >>> df.take([-1, -2])\\n             name   class  max_speed\\n        1  monkey  mammal        NaN\\n        3    lion  mammal       80.5\\n        \"\"\"\\n        if convert is not None:\\n            msg = (\"The \\'convert\\' parameter is deprecated \"\\n                   \"and will be removed in a future version.\")\\n            warnings.warn(msg, FutureWarning, stacklevel=2)\\n\\n        nv.validate_take(tuple(), kwargs)\\n        return self._take(indices, axis=axis, is_copy=is_copy)',\n 'def xs(self, key, axis=0, level=None, drop_level=True):\\n        \"\"\"\\n        Return cross-section from the Series/DataFrame.\\n\\n        This method takes a `key` argument to select data at a particular\\n        level of a MultiIndex.\\n\\n        Parameters\\n        ----------\\n        key : label or tuple of label\\n            Label contained in the index, or partially in a MultiIndex.\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n            Axis to retrieve cross-section on.\\n        level : object, defaults to first n levels (n=1 or len(key))\\n            In case of a key partially contained in a MultiIndex, indicate\\n            which levels are used. Levels can be referred by label or position.\\n        drop_level : bool, default True\\n            If False, returns object with same levels as self.\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            Cross-section from the original Series or DataFrame\\n            corresponding to the selected index levels.\\n\\n        See Also\\n        --------\\n        DataFrame.loc : Access a group of rows and columns\\n            by label(s) or a boolean array.\\n        DataFrame.iloc : Purely integer-location based indexing\\n            for selection by position.\\n\\n        Notes\\n        -----\\n        `xs` can not be used to set values.\\n\\n        MultiIndex Slicers is a generic way to get/set values on\\n        any level or levels.\\n        It is a superset of `xs` functionality, see\\n        :ref:`MultiIndex Slicers <advanced.mi_slicers>`.\\n\\n        Examples\\n        --------\\n        >>> d = {\\'num_legs\\': [4, 4, 2, 2],\\n        ...      \\'num_wings\\': [0, 0, 2, 2],\\n        ...      \\'class\\': [\\'mammal\\', \\'mammal\\', \\'mammal\\', \\'bird\\'],\\n        ...      \\'animal\\': [\\'cat\\', \\'dog\\', \\'bat\\', \\'penguin\\'],\\n        ...      \\'locomotion\\': [\\'walks\\', \\'walks\\', \\'flies\\', \\'walks\\']}\\n        >>> df = pd.DataFrame(data=d)\\n        >>> df = df.set_index([\\'class\\', \\'animal\\', \\'locomotion\\'])\\n        >>> df\\n                                   num_legs  num_wings\\n        class  animal  locomotion\\n        mammal cat     walks              4          0\\n               dog     walks              4          0\\n               bat     flies              2          2\\n        bird   penguin walks              2          2\\n\\n        Get values at specified index\\n\\n        >>> df.xs(\\'mammal\\')\\n                           num_legs  num_wings\\n        animal locomotion\\n        cat    walks              4          0\\n        dog    walks              4          0\\n        bat    flies              2          2\\n\\n        Get values at several indexes\\n\\n        >>> df.xs((\\'mammal\\', \\'dog\\'))\\n                    num_legs  num_wings\\n        locomotion\\n        walks              4          0\\n\\n        Get values at specified index and level\\n\\n        >>> df.xs(\\'cat\\', level=1)\\n                           num_legs  num_wings\\n        class  locomotion\\n        mammal walks              4          0\\n\\n        Get values at several indexes and levels\\n\\n        >>> df.xs((\\'bird\\', \\'walks\\'),\\n        ...       level=[0, \\'locomotion\\'])\\n                 num_legs  num_wings\\n        animal\\n        penguin         2          2\\n\\n        Get values at specified column and axis\\n\\n        >>> df.xs(\\'num_wings\\', axis=1)\\n        class   animal   locomotion\\n        mammal  cat      walks         0\\n                dog      walks         0\\n                bat      flies         2\\n        bird    penguin  walks         2\\n        Name: num_wings, dtype: int64\\n        \"\"\"\\n        axis = self._get_axis_number(axis)\\n        labels = self._get_axis(axis)\\n        if level is not None:\\n            loc, new_ax = labels.get_loc_level(key, level=level,\\n                                               drop_level=drop_level)\\n\\n            # create the tuple of the indexer\\n            indexer = [slice(None)] * self.ndim\\n            indexer[axis] = loc\\n            indexer = tuple(indexer)\\n\\n            result = self.iloc[indexer]\\n            setattr(result, result._get_axis_name(axis), new_ax)\\n            return result\\n\\n        if axis == 1:\\n            return self[key]\\n\\n        self._consolidate_inplace()\\n\\n        index = self.index\\n        if isinstance(index, MultiIndex):\\n            loc, new_index = self.index.get_loc_level(key,\\n                                                      drop_level=drop_level)\\n        else:\\n            loc = self.index.get_loc(key)\\n\\n            if isinstance(loc, np.ndarray):\\n                if loc.dtype == np.bool_:\\n                    inds, = loc.nonzero()\\n                    return self._take(inds, axis=axis)\\n                else:\\n                    return self._take(loc, axis=axis)\\n\\n            if not is_scalar(loc):\\n                new_index = self.index[loc]\\n\\n        if is_scalar(loc):\\n            new_values = self._data.fast_xs(loc)\\n\\n            # may need to box a datelike-scalar\\n            #\\n            # if we encounter an array-like and we only have 1 dim\\n            # that means that their are list/ndarrays inside the Series!\\n            # so just return them (GH 6394)\\n            if not is_list_like(new_values) or self.ndim == 1:\\n                return com.maybe_box_datetimelike(new_values)\\n\\n            result = self._constructor_sliced(\\n                new_values, index=self.columns,\\n                name=self.index[loc], dtype=new_values.dtype)\\n\\n        else:\\n            result = self.iloc[loc]\\n            result.index = new_index\\n\\n        # this could be a view\\n        # but only in a single-dtyped view slicable case\\n        result._set_is_copy(self, copy=not result._is_view)\\n        return result',\n 'def select(self, crit, axis=0):\\n        \"\"\"\\n        Return data corresponding to axis labels matching criteria.\\n\\n        .. deprecated:: 0.21.0\\n            Use df.loc[df.index.map(crit)] to select via labels\\n\\n        Parameters\\n        ----------\\n        crit : function\\n            To be called on each index (label). Should return True or False\\n        axis : int\\n\\n        Returns\\n        -------\\n        selection : same type as caller\\n        \"\"\"\\n        warnings.warn(\"\\'select\\' is deprecated and will be removed in a \"\\n                      \"future release. You can use \"\\n                      \".loc[labels.map(crit)] as a replacement\",\\n                      FutureWarning, stacklevel=2)\\n\\n        axis = self._get_axis_number(axis)\\n        axis_name = self._get_axis_name(axis)\\n        axis_values = self._get_axis(axis)\\n\\n        if len(axis_values) > 0:\\n            new_axis = axis_values[\\n                np.asarray([bool(crit(label)) for label in axis_values])]\\n        else:\\n            new_axis = axis_values\\n\\n        return self.reindex(**{axis_name: new_axis})',\n 'def reindex_like(self, other, method=None, copy=True, limit=None,\\n                     tolerance=None):\\n        \"\"\"\\n        Return an object with matching indices as other object.\\n\\n        Conform the object to the same index on all axes. Optional\\n        filling logic, placing NaN in locations having no value\\n        in the previous index. A new object is produced unless the\\n        new index is equivalent to the current one and copy=False.\\n\\n        Parameters\\n        ----------\\n        other : Object of the same data type\\n            Its row and column indices are used to define the new indices\\n            of this object.\\n        method : {None, \\'backfill\\'/\\'bfill\\', \\'pad\\'/\\'ffill\\', \\'nearest\\'}\\n            Method to use for filling holes in reindexed DataFrame.\\n            Please note: this is only applicable to DataFrames/Series with a\\n            monotonically increasing/decreasing index.\\n\\n            * None (default): don\\'t fill gaps\\n            * pad / ffill: propagate last valid observation forward to next\\n              valid\\n            * backfill / bfill: use next valid observation to fill gap\\n            * nearest: use nearest valid observations to fill gap\\n\\n        copy : bool, default True\\n            Return a new object, even if the passed indexes are the same.\\n        limit : int, default None\\n            Maximum number of consecutive labels to fill for inexact matches.\\n        tolerance : optional\\n            Maximum distance between original and new labels for inexact\\n            matches. The values of the index at the matching locations most\\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\\n\\n            Tolerance may be a scalar value, which applies the same tolerance\\n            to all values, or list-like, which applies variable tolerance per\\n            element. List-like includes list, tuple, array, Series, and must be\\n            the same size as the index and its dtype must exactly match the\\n            index\\'s type.\\n\\n            .. versionadded:: 0.21.0 (list-like tolerance)\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            Same type as caller, but with changed indices on each axis.\\n\\n        See Also\\n        --------\\n        DataFrame.set_index : Set row labels.\\n        DataFrame.reset_index : Remove row labels or move them to new columns.\\n        DataFrame.reindex : Change to new indices or expand indices.\\n\\n        Notes\\n        -----\\n        Same as calling\\n        ``.reindex(index=other.index, columns=other.columns,...)``.\\n\\n        Examples\\n        --------\\n        >>> df1 = pd.DataFrame([[24.3, 75.7, \\'high\\'],\\n        ...                     [31, 87.8, \\'high\\'],\\n        ...                     [22, 71.6, \\'medium\\'],\\n        ...                     [35, 95, \\'medium\\']],\\n        ...     columns=[\\'temp_celsius\\', \\'temp_fahrenheit\\', \\'windspeed\\'],\\n        ...     index=pd.date_range(start=\\'2014-02-12\\',\\n        ...                         end=\\'2014-02-15\\', freq=\\'D\\'))\\n\\n        >>> df1\\n                    temp_celsius  temp_fahrenheit windspeed\\n        2014-02-12          24.3             75.7      high\\n        2014-02-13          31.0             87.8      high\\n        2014-02-14          22.0             71.6    medium\\n        2014-02-15          35.0             95.0    medium\\n\\n        >>> df2 = pd.DataFrame([[28, \\'low\\'],\\n        ...                     [30, \\'low\\'],\\n        ...                     [35.1, \\'medium\\']],\\n        ...     columns=[\\'temp_celsius\\', \\'windspeed\\'],\\n        ...     index=pd.DatetimeIndex([\\'2014-02-12\\', \\'2014-02-13\\',\\n        ...                             \\'2014-02-15\\']))\\n\\n        >>> df2\\n                    temp_celsius windspeed\\n        2014-02-12          28.0       low\\n        2014-02-13          30.0       low\\n        2014-02-15          35.1    medium\\n\\n        >>> df2.reindex_like(df1)\\n                    temp_celsius  temp_fahrenheit windspeed\\n        2014-02-12          28.0              NaN       low\\n        2014-02-13          30.0              NaN       low\\n        2014-02-14           NaN              NaN       NaN\\n        2014-02-15          35.1              NaN    medium\\n        \"\"\"\\n        d = other._construct_axes_dict(axes=self._AXIS_ORDERS, method=method,\\n                                       copy=copy, limit=limit,\\n                                       tolerance=tolerance)\\n\\n        return self.reindex(**d)',\n 'def _drop_axis(self, labels, axis, level=None, errors=\\'raise\\'):\\n        \"\"\"\\n        Drop labels from specified axis. Used in the ``drop`` method\\n        internally.\\n\\n        Parameters\\n        ----------\\n        labels : single label or list-like\\n        axis : int or axis name\\n        level : int or level name, default None\\n            For MultiIndex\\n        errors : {\\'ignore\\', \\'raise\\'}, default \\'raise\\'\\n            If \\'ignore\\', suppress error and existing labels are dropped.\\n\\n        \"\"\"\\n        axis = self._get_axis_number(axis)\\n        axis_name = self._get_axis_name(axis)\\n        axis = self._get_axis(axis)\\n\\n        if axis.is_unique:\\n            if level is not None:\\n                if not isinstance(axis, MultiIndex):\\n                    raise AssertionError(\\'axis must be a MultiIndex\\')\\n                new_axis = axis.drop(labels, level=level, errors=errors)\\n            else:\\n                new_axis = axis.drop(labels, errors=errors)\\n            result = self.reindex(**{axis_name: new_axis})\\n\\n        # Case for non-unique axis\\n        else:\\n            labels = ensure_object(com.index_labels_to_array(labels))\\n            if level is not None:\\n                if not isinstance(axis, MultiIndex):\\n                    raise AssertionError(\\'axis must be a MultiIndex\\')\\n                indexer = ~axis.get_level_values(level).isin(labels)\\n\\n                # GH 18561 MultiIndex.drop should raise if label is absent\\n                if errors == \\'raise\\' and indexer.all():\\n                    raise KeyError(\\'{} not found in axis\\'.format(labels))\\n            else:\\n                indexer = ~axis.isin(labels)\\n                # Check if label doesn\\'t exist along axis\\n                labels_missing = (axis.get_indexer_for(labels) == -1).any()\\n                if errors == \\'raise\\' and labels_missing:\\n                    raise KeyError(\\'{} not found in axis\\'.format(labels))\\n\\n            slicer = [slice(None)] * self.ndim\\n            slicer[self._get_axis_number(axis_name)] = indexer\\n\\n            result = self.loc[tuple(slicer)]\\n\\n        return result',\n 'def _update_inplace(self, result, verify_is_copy=True):\\n        \"\"\"\\n        Replace self internals with result.\\n\\n        Parameters\\n        ----------\\n        verify_is_copy : boolean, default True\\n            provide is_copy checks\\n\\n        \"\"\"\\n        # NOTE: This does *not* call __finalize__ and that\\'s an explicit\\n        # decision that we may revisit in the future.\\n\\n        self._reset_cache()\\n        self._clear_item_cache()\\n        self._data = getattr(result, \\'_data\\', result)\\n        self._maybe_update_cacher(verify_is_copy=verify_is_copy)',\n 'def add_prefix(self, prefix):\\n        \"\"\"\\n        Prefix labels with string `prefix`.\\n\\n        For Series, the row labels are prefixed.\\n        For DataFrame, the column labels are prefixed.\\n\\n        Parameters\\n        ----------\\n        prefix : str\\n            The string to add before each label.\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            New Series or DataFrame with updated labels.\\n\\n        See Also\\n        --------\\n        Series.add_suffix: Suffix row labels with string `suffix`.\\n        DataFrame.add_suffix: Suffix column labels with string `suffix`.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series([1, 2, 3, 4])\\n        >>> s\\n        0    1\\n        1    2\\n        2    3\\n        3    4\\n        dtype: int64\\n\\n        >>> s.add_prefix(\\'item_\\')\\n        item_0    1\\n        item_1    2\\n        item_2    3\\n        item_3    4\\n        dtype: int64\\n\\n        >>> df = pd.DataFrame({\\'A\\': [1, 2, 3, 4],  \\'B\\': [3, 4, 5, 6]})\\n        >>> df\\n           A  B\\n        0  1  3\\n        1  2  4\\n        2  3  5\\n        3  4  6\\n\\n        >>> df.add_prefix(\\'col_\\')\\n             col_A  col_B\\n        0       1       3\\n        1       2       4\\n        2       3       5\\n        3       4       6\\n        \"\"\"\\n        f = functools.partial(\\'{prefix}{}\\'.format, prefix=prefix)\\n\\n        mapper = {self._info_axis_name: f}\\n        return self.rename(**mapper)',\n 'def add_suffix(self, suffix):\\n        \"\"\"\\n        Suffix labels with string `suffix`.\\n\\n        For Series, the row labels are suffixed.\\n        For DataFrame, the column labels are suffixed.\\n\\n        Parameters\\n        ----------\\n        suffix : str\\n            The string to add after each label.\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            New Series or DataFrame with updated labels.\\n\\n        See Also\\n        --------\\n        Series.add_prefix: Prefix row labels with string `prefix`.\\n        DataFrame.add_prefix: Prefix column labels with string `prefix`.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series([1, 2, 3, 4])\\n        >>> s\\n        0    1\\n        1    2\\n        2    3\\n        3    4\\n        dtype: int64\\n\\n        >>> s.add_suffix(\\'_item\\')\\n        0_item    1\\n        1_item    2\\n        2_item    3\\n        3_item    4\\n        dtype: int64\\n\\n        >>> df = pd.DataFrame({\\'A\\': [1, 2, 3, 4],  \\'B\\': [3, 4, 5, 6]})\\n        >>> df\\n           A  B\\n        0  1  3\\n        1  2  4\\n        2  3  5\\n        3  4  6\\n\\n        >>> df.add_suffix(\\'_col\\')\\n             A_col  B_col\\n        0       1       3\\n        1       2       4\\n        2       3       5\\n        3       4       6\\n        \"\"\"\\n        f = functools.partial(\\'{}{suffix}\\'.format, suffix=suffix)\\n\\n        mapper = {self._info_axis_name: f}\\n        return self.rename(**mapper)',\n 'def sort_values(self, by=None, axis=0, ascending=True, inplace=False,\\n                    kind=\\'quicksort\\', na_position=\\'last\\'):\\n        \"\"\"\\n        Sort by the values along either axis.\\n\\n        Parameters\\n        ----------%(optional_by)s\\n        axis : %(axes_single_arg)s, default 0\\n             Axis to be sorted.\\n        ascending : bool or list of bool, default True\\n             Sort ascending vs. descending. Specify list for multiple sort\\n             orders.  If this is a list of bools, must match the length of\\n             the by.\\n        inplace : bool, default False\\n             If True, perform operation in-place.\\n        kind : {\\'quicksort\\', \\'mergesort\\', \\'heapsort\\'}, default \\'quicksort\\'\\n             Choice of sorting algorithm. See also ndarray.np.sort for more\\n             information.  `mergesort` is the only stable algorithm. For\\n             DataFrames, this option is only applied when sorting on a single\\n             column or label.\\n        na_position : {\\'first\\', \\'last\\'}, default \\'last\\'\\n             Puts NaNs at the beginning if `first`; `last` puts NaNs at the\\n             end.\\n\\n        Returns\\n        -------\\n        sorted_obj : DataFrame or None\\n            DataFrame with sorted values if inplace=False, None otherwise.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\n        ...     \\'col1\\': [\\'A\\', \\'A\\', \\'B\\', np.nan, \\'D\\', \\'C\\'],\\n        ...     \\'col2\\': [2, 1, 9, 8, 7, 4],\\n        ...     \\'col3\\': [0, 1, 9, 4, 2, 3],\\n        ... })\\n        >>> df\\n            col1 col2 col3\\n        0   A    2    0\\n        1   A    1    1\\n        2   B    9    9\\n        3   NaN  8    4\\n        4   D    7    2\\n        5   C    4    3\\n\\n        Sort by col1\\n\\n        >>> df.sort_values(by=[\\'col1\\'])\\n            col1 col2 col3\\n        0   A    2    0\\n        1   A    1    1\\n        2   B    9    9\\n        5   C    4    3\\n        4   D    7    2\\n        3   NaN  8    4\\n\\n        Sort by multiple columns\\n\\n        >>> df.sort_values(by=[\\'col1\\', \\'col2\\'])\\n            col1 col2 col3\\n        1   A    1    1\\n        0   A    2    0\\n        2   B    9    9\\n        5   C    4    3\\n        4   D    7    2\\n        3   NaN  8    4\\n\\n        Sort Descending\\n\\n        >>> df.sort_values(by=\\'col1\\', ascending=False)\\n            col1 col2 col3\\n        4   D    7    2\\n        5   C    4    3\\n        2   B    9    9\\n        0   A    2    0\\n        1   A    1    1\\n        3   NaN  8    4\\n\\n        Putting NAs first\\n\\n        >>> df.sort_values(by=\\'col1\\', ascending=False, na_position=\\'first\\')\\n            col1 col2 col3\\n        3   NaN  8    4\\n        4   D    7    2\\n        5   C    4    3\\n        2   B    9    9\\n        0   A    2    0\\n        1   A    1    1\\n        \"\"\"\\n        raise NotImplementedError(\"sort_values has not been implemented \"\\n                                  \"on Panel or Panel4D objects.\")',\n 'def sort_index(self, axis=0, level=None, ascending=True, inplace=False,\\n                   kind=\\'quicksort\\', na_position=\\'last\\', sort_remaining=True):\\n        \"\"\"\\n        Sort object by labels (along an axis).\\n\\n        Parameters\\n        ----------\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n            The axis along which to sort.  The value 0 identifies the rows,\\n            and 1 identifies the columns.\\n        level : int or level name or list of ints or list of level names\\n            If not None, sort on values in specified index level(s).\\n        ascending : bool, default True\\n            Sort ascending vs. descending.\\n        inplace : bool, default False\\n            If True, perform operation in-place.\\n        kind : {\\'quicksort\\', \\'mergesort\\', \\'heapsort\\'}, default \\'quicksort\\'\\n            Choice of sorting algorithm. See also ndarray.np.sort for more\\n            information.  `mergesort` is the only stable algorithm. For\\n            DataFrames, this option is only applied when sorting on a single\\n            column or label.\\n        na_position : {\\'first\\', \\'last\\'}, default \\'last\\'\\n            Puts NaNs at the beginning if `first`; `last` puts NaNs at the end.\\n            Not implemented for MultiIndex.\\n        sort_remaining : bool, default True\\n            If True and sorting by level and index is multilevel, sort by other\\n            levels too (in order) after sorting by specified level.\\n\\n        Returns\\n        -------\\n        sorted_obj : DataFrame or None\\n            DataFrame with sorted index if inplace=False, None otherwise.\\n        \"\"\"\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        axis = self._get_axis_number(axis)\\n        axis_name = self._get_axis_name(axis)\\n        labels = self._get_axis(axis)\\n\\n        if level is not None:\\n            raise NotImplementedError(\"level is not implemented\")\\n        if inplace:\\n            raise NotImplementedError(\"inplace is not implemented\")\\n\\n        sort_index = labels.argsort()\\n        if not ascending:\\n            sort_index = sort_index[::-1]\\n\\n        new_axis = labels.take(sort_index)\\n        return self.reindex(**{axis_name: new_axis})',\n 'def reindex(self, *args, **kwargs):\\n        \"\"\"\\n        Conform %(klass)s to new index with optional filling logic, placing\\n        NA/NaN in locations having no value in the previous index. A new object\\n        is produced unless the new index is equivalent to the current one and\\n        ``copy=False``.\\n\\n        Parameters\\n        ----------\\n        %(optional_labels)s\\n        %(axes)s : array-like, optional\\n            New labels / index to conform to, should be specified using\\n            keywords. Preferably an Index object to avoid duplicating data\\n        %(optional_axis)s\\n        method : {None, \\'backfill\\'/\\'bfill\\', \\'pad\\'/\\'ffill\\', \\'nearest\\'}\\n            Method to use for filling holes in reindexed DataFrame.\\n            Please note: this is only applicable to DataFrames/Series with a\\n            monotonically increasing/decreasing index.\\n\\n            * None (default): don\\'t fill gaps\\n            * pad / ffill: propagate last valid observation forward to next\\n              valid\\n            * backfill / bfill: use next valid observation to fill gap\\n            * nearest: use nearest valid observations to fill gap\\n\\n        copy : bool, default True\\n            Return a new object, even if the passed indexes are the same.\\n        level : int or name\\n            Broadcast across a level, matching Index values on the\\n            passed MultiIndex level.\\n        fill_value : scalar, default np.NaN\\n            Value to use for missing values. Defaults to NaN, but can be any\\n            \"compatible\" value.\\n        limit : int, default None\\n            Maximum number of consecutive elements to forward or backward fill.\\n        tolerance : optional\\n            Maximum distance between original and new labels for inexact\\n            matches. The values of the index at the matching locations most\\n            satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\\n\\n            Tolerance may be a scalar value, which applies the same tolerance\\n            to all values, or list-like, which applies variable tolerance per\\n            element. List-like includes list, tuple, array, Series, and must be\\n            the same size as the index and its dtype must exactly match the\\n            index\\'s type.\\n\\n            .. versionadded:: 0.21.0 (list-like tolerance)\\n\\n        Returns\\n        -------\\n        %(klass)s with changed index.\\n\\n        See Also\\n        --------\\n        DataFrame.set_index : Set row labels.\\n        DataFrame.reset_index : Remove row labels or move them to new columns.\\n        DataFrame.reindex_like : Change to same indices as other DataFrame.\\n\\n        Examples\\n        --------\\n\\n        ``DataFrame.reindex`` supports two calling conventions\\n\\n        * ``(index=index_labels, columns=column_labels, ...)``\\n        * ``(labels, axis={\\'index\\', \\'columns\\'}, ...)``\\n\\n        We *highly* recommend using keyword arguments to clarify your\\n        intent.\\n\\n        Create a dataframe with some fictional data.\\n\\n        >>> index = [\\'Firefox\\', \\'Chrome\\', \\'Safari\\', \\'IE10\\', \\'Konqueror\\']\\n        >>> df = pd.DataFrame({\\n        ...      \\'http_status\\': [200,200,404,404,301],\\n        ...      \\'response_time\\': [0.04, 0.02, 0.07, 0.08, 1.0]},\\n        ...       index=index)\\n        >>> df\\n                   http_status  response_time\\n        Firefox            200           0.04\\n        Chrome             200           0.02\\n        Safari             404           0.07\\n        IE10               404           0.08\\n        Konqueror          301           1.00\\n\\n        Create a new index and reindex the dataframe. By default\\n        values in the new index that do not have corresponding\\n        records in the dataframe are assigned ``NaN``.\\n\\n        >>> new_index= [\\'Safari\\', \\'Iceweasel\\', \\'Comodo Dragon\\', \\'IE10\\',\\n        ...             \\'Chrome\\']\\n        >>> df.reindex(new_index)\\n                       http_status  response_time\\n        Safari               404.0           0.07\\n        Iceweasel              NaN            NaN\\n        Comodo Dragon          NaN            NaN\\n        IE10                 404.0           0.08\\n        Chrome               200.0           0.02\\n\\n        We can fill in the missing values by passing a value to\\n        the keyword ``fill_value``. Because the index is not monotonically\\n        increasing or decreasing, we cannot use arguments to the keyword\\n        ``method`` to fill the ``NaN`` values.\\n\\n        >>> df.reindex(new_index, fill_value=0)\\n                       http_status  response_time\\n        Safari                 404           0.07\\n        Iceweasel                0           0.00\\n        Comodo Dragon            0           0.00\\n        IE10                   404           0.08\\n        Chrome                 200           0.02\\n\\n        >>> df.reindex(new_index, fill_value=\\'missing\\')\\n                      http_status response_time\\n        Safari                404          0.07\\n        Iceweasel         missing       missing\\n        Comodo Dragon     missing       missing\\n        IE10                  404          0.08\\n        Chrome                200          0.02\\n\\n        We can also reindex the columns.\\n\\n        >>> df.reindex(columns=[\\'http_status\\', \\'user_agent\\'])\\n                   http_status  user_agent\\n        Firefox            200         NaN\\n        Chrome             200         NaN\\n        Safari             404         NaN\\n        IE10               404         NaN\\n        Konqueror          301         NaN\\n\\n        Or we can use \"axis-style\" keyword arguments\\n\\n        >>> df.reindex([\\'http_status\\', \\'user_agent\\'], axis=\"columns\")\\n                   http_status  user_agent\\n        Firefox            200         NaN\\n        Chrome             200         NaN\\n        Safari             404         NaN\\n        IE10               404         NaN\\n        Konqueror          301         NaN\\n\\n        To further illustrate the filling functionality in\\n        ``reindex``, we will create a dataframe with a\\n        monotonically increasing index (for example, a sequence\\n        of dates).\\n\\n        >>> date_index = pd.date_range(\\'1/1/2010\\', periods=6, freq=\\'D\\')\\n        >>> df2 = pd.DataFrame({\"prices\": [100, 101, np.nan, 100, 89, 88]},\\n        ...                    index=date_index)\\n        >>> df2\\n                    prices\\n        2010-01-01   100.0\\n        2010-01-02   101.0\\n        2010-01-03     NaN\\n        2010-01-04   100.0\\n        2010-01-05    89.0\\n        2010-01-06    88.0\\n\\n        Suppose we decide to expand the dataframe to cover a wider\\n        date range.\\n\\n        >>> date_index2 = pd.date_range(\\'12/29/2009\\', periods=10, freq=\\'D\\')\\n        >>> df2.reindex(date_index2)\\n                    prices\\n        2009-12-29     NaN\\n        2009-12-30     NaN\\n        2009-12-31     NaN\\n        2010-01-01   100.0\\n        2010-01-02   101.0\\n        2010-01-03     NaN\\n        2010-01-04   100.0\\n        2010-01-05    89.0\\n        2010-01-06    88.0\\n        2010-01-07     NaN\\n\\n        The index entries that did not have a value in the original data frame\\n        (for example, \\'2009-12-29\\') are by default filled with ``NaN``.\\n        If desired, we can fill in the missing values using one of several\\n        options.\\n\\n        For example, to back-propagate the last valid value to fill the ``NaN``\\n        values, pass ``bfill`` as an argument to the ``method`` keyword.\\n\\n        >>> df2.reindex(date_index2, method=\\'bfill\\')\\n                    prices\\n        2009-12-29   100.0\\n        2009-12-30   100.0\\n        2009-12-31   100.0\\n        2010-01-01   100.0\\n        2010-01-02   101.0\\n        2010-01-03     NaN\\n        2010-01-04   100.0\\n        2010-01-05    89.0\\n        2010-01-06    88.0\\n        2010-01-07     NaN\\n\\n        Please note that the ``NaN`` value present in the original dataframe\\n        (at index value 2010-01-03) will not be filled by any of the\\n        value propagation schemes. This is because filling while reindexing\\n        does not look at dataframe values, but only compares the original and\\n        desired indexes. If you do want to fill in the ``NaN`` values present\\n        in the original dataframe, use the ``fillna()`` method.\\n\\n        See the :ref:`user guide <basics.reindexing>` for more.\\n        \"\"\"\\n        # TODO: Decide if we care about having different examples for different\\n        # kinds\\n\\n        # construct the args\\n        axes, kwargs = self._construct_axes_from_arguments(args, kwargs)\\n        method = missing.clean_reindex_fill_method(kwargs.pop(\\'method\\', None))\\n        level = kwargs.pop(\\'level\\', None)\\n        copy = kwargs.pop(\\'copy\\', True)\\n        limit = kwargs.pop(\\'limit\\', None)\\n        tolerance = kwargs.pop(\\'tolerance\\', None)\\n        fill_value = kwargs.pop(\\'fill_value\\', None)\\n\\n        # Series.reindex doesn\\'t use / need the axis kwarg\\n        # We pop and ignore it here, to make writing Series/Frame generic code\\n        # easier\\n        kwargs.pop(\"axis\", None)\\n\\n        if kwargs:\\n            raise TypeError(\\'reindex() got an unexpected keyword \\'\\n                            \\'argument \"{0}\"\\'.format(list(kwargs.keys())[0]))\\n\\n        self._consolidate_inplace()\\n\\n        # if all axes that are requested to reindex are equal, then only copy\\n        # if indicated must have index names equal here as well as values\\n        if all(self._get_axis(axis).identical(ax)\\n               for axis, ax in axes.items() if ax is not None):\\n            if copy:\\n                return self.copy()\\n            return self\\n\\n        # check if we are a multi reindex\\n        if self._needs_reindex_multi(axes, method, level):\\n            try:\\n                return self._reindex_multi(axes, copy, fill_value)\\n            except Exception:\\n                pass\\n\\n        # perform the reindex on the axes\\n        return self._reindex_axes(axes, level, limit, tolerance, method,\\n                                  fill_value, copy).__finalize__(self)',\n 'def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,\\n                      copy):\\n        \"\"\"Perform the reindex for all the axes.\"\"\"\\n        obj = self\\n        for a in self._AXIS_ORDERS:\\n            labels = axes[a]\\n            if labels is None:\\n                continue\\n\\n            ax = self._get_axis(a)\\n            new_index, indexer = ax.reindex(labels, level=level, limit=limit,\\n                                            tolerance=tolerance, method=method)\\n\\n            axis = self._get_axis_number(a)\\n            obj = obj._reindex_with_indexers({axis: [new_index, indexer]},\\n                                             fill_value=fill_value,\\n                                             copy=copy, allow_dups=False)\\n\\n        return obj',\n 'def _needs_reindex_multi(self, axes, method, level):\\n        \"\"\"Check if we do need a multi reindex.\"\"\"\\n        return ((com.count_not_none(*axes.values()) == self._AXIS_LEN) and\\n                method is None and level is None and not self._is_mixed_type)',\n 'def _reindex_with_indexers(self, reindexers, fill_value=None, copy=False,\\n                               allow_dups=False):\\n        \"\"\"allow_dups indicates an internal call here \"\"\"\\n\\n        # reindex doing multiple operations on different axes if indicated\\n        new_data = self._data\\n        for axis in sorted(reindexers.keys()):\\n            index, indexer = reindexers[axis]\\n            baxis = self._get_block_manager_axis(axis)\\n\\n            if index is None:\\n                continue\\n\\n            index = ensure_index(index)\\n            if indexer is not None:\\n                indexer = ensure_int64(indexer)\\n\\n            # TODO: speed up on homogeneous DataFrame objects\\n            new_data = new_data.reindex_indexer(index, indexer, axis=baxis,\\n                                                fill_value=fill_value,\\n                                                allow_dups=allow_dups,\\n                                                copy=copy)\\n\\n        if copy and new_data is self._data:\\n            new_data = new_data.copy()\\n\\n        return self._constructor(new_data).__finalize__(self)',\n 'def filter(self, items=None, like=None, regex=None, axis=None):\\n        \"\"\"\\n        Subset rows or columns of dataframe according to labels in\\n        the specified index.\\n\\n        Note that this routine does not filter a dataframe on its\\n        contents. The filter is applied to the labels of the index.\\n\\n        Parameters\\n        ----------\\n        items : list-like\\n            Keep labels from axis which are in items.\\n        like : string\\n            Keep labels from axis for which \"like in label == True\".\\n        regex : string (regular expression)\\n            Keep labels from axis for which re.search(regex, label) == True.\\n        axis : int or string axis name\\n            The axis to filter on.  By default this is the info axis,\\n            \\'index\\' for Series, \\'columns\\' for DataFrame.\\n\\n        Returns\\n        -------\\n        same type as input object\\n\\n        See Also\\n        --------\\n        DataFrame.loc\\n\\n        Notes\\n        -----\\n        The ``items``, ``like``, and ``regex`` parameters are\\n        enforced to be mutually exclusive.\\n\\n        ``axis`` defaults to the info axis that is used when indexing\\n        with ``[]``.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),\\n        ...                   index=[\\'mouse\\', \\'rabbit\\'],\\n        ...                   columns=[\\'one\\', \\'two\\', \\'three\\'])\\n\\n        >>> # select columns by name\\n        >>> df.filter(items=[\\'one\\', \\'three\\'])\\n                 one  three\\n        mouse     1      3\\n        rabbit    4      6\\n\\n        >>> # select columns by regular expression\\n        >>> df.filter(regex=\\'e$\\', axis=1)\\n                 one  three\\n        mouse     1      3\\n        rabbit    4      6\\n\\n        >>> # select rows containing \\'bbi\\'\\n        >>> df.filter(like=\\'bbi\\', axis=0)\\n                 one  two  three\\n        rabbit    4    5      6\\n        \"\"\"\\n        import re\\n\\n        nkw = com.count_not_none(items, like, regex)\\n        if nkw > 1:\\n            raise TypeError(\\'Keyword arguments `items`, `like`, or `regex` \\'\\n                            \\'are mutually exclusive\\')\\n\\n        if axis is None:\\n            axis = self._info_axis_name\\n        labels = self._get_axis(axis)\\n\\n        if items is not None:\\n            name = self._get_axis_name(axis)\\n            return self.reindex(\\n                **{name: [r for r in items if r in labels]})\\n        elif like:\\n            def f(x):\\n                return like in to_str(x)\\n            values = labels.map(f)\\n            return self.loc(axis=axis)[values]\\n        elif regex:\\n            def f(x):\\n                return matcher.search(to_str(x)) is not None\\n            matcher = re.compile(regex)\\n            values = labels.map(f)\\n            return self.loc(axis=axis)[values]\\n        else:\\n            raise TypeError(\\'Must pass either `items`, `like`, or `regex`\\')',\n 'def sample(self, n=None, frac=None, replace=False, weights=None,\\n               random_state=None, axis=None):\\n        \"\"\"\\n        Return a random sample of items from an axis of object.\\n\\n        You can use `random_state` for reproducibility.\\n\\n        Parameters\\n        ----------\\n        n : int, optional\\n            Number of items from axis to return. Cannot be used with `frac`.\\n            Default = 1 if `frac` = None.\\n        frac : float, optional\\n            Fraction of axis items to return. Cannot be used with `n`.\\n        replace : bool, default False\\n            Sample with or without replacement.\\n        weights : str or ndarray-like, optional\\n            Default \\'None\\' results in equal probability weighting.\\n            If passed a Series, will align with target object on index. Index\\n            values in weights not found in sampled object will be ignored and\\n            index values in sampled object not in weights will be assigned\\n            weights of zero.\\n            If called on a DataFrame, will accept the name of a column\\n            when axis = 0.\\n            Unless weights are a Series, weights must be same length as axis\\n            being sampled.\\n            If weights do not sum to 1, they will be normalized to sum to 1.\\n            Missing values in the weights column will be treated as zero.\\n            Infinite values not allowed.\\n        random_state : int or numpy.random.RandomState, optional\\n            Seed for the random number generator (if int), or numpy RandomState\\n            object.\\n        axis : int or string, optional\\n            Axis to sample. Accepts axis number or name. Default is stat axis\\n            for given data type (0 for Series and DataFrames, 1 for Panels).\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            A new object of same type as caller containing `n` items randomly\\n            sampled from the caller object.\\n\\n        See Also\\n        --------\\n        numpy.random.choice: Generates a random sample from a given 1-D numpy\\n            array.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'num_legs\\': [2, 4, 8, 0],\\n        ...                    \\'num_wings\\': [2, 0, 0, 0],\\n        ...                    \\'num_specimen_seen\\': [10, 2, 1, 8]},\\n        ...                   index=[\\'falcon\\', \\'dog\\', \\'spider\\', \\'fish\\'])\\n        >>> df\\n                num_legs  num_wings  num_specimen_seen\\n        falcon         2          2                 10\\n        dog            4          0                  2\\n        spider         8          0                  1\\n        fish           0          0                  8\\n\\n        Extract 3 random elements from the ``Series`` ``df[\\'num_legs\\']``:\\n        Note that we use `random_state` to ensure the reproducibility of\\n        the examples.\\n\\n        >>> df[\\'num_legs\\'].sample(n=3, random_state=1)\\n        fish      0\\n        spider    8\\n        falcon    2\\n        Name: num_legs, dtype: int64\\n\\n        A random 50% sample of the ``DataFrame`` with replacement:\\n\\n        >>> df.sample(frac=0.5, replace=True, random_state=1)\\n              num_legs  num_wings  num_specimen_seen\\n        dog          4          0                  2\\n        fish         0          0                  8\\n\\n        Using a DataFrame column as weights. Rows with larger value in the\\n        `num_specimen_seen` column are more likely to be sampled.\\n\\n        >>> df.sample(n=2, weights=\\'num_specimen_seen\\', random_state=1)\\n                num_legs  num_wings  num_specimen_seen\\n        falcon         2          2                 10\\n        fish           0          0                  8\\n        \"\"\"\\n\\n        if axis is None:\\n            axis = self._stat_axis_number\\n\\n        axis = self._get_axis_number(axis)\\n        axis_length = self.shape[axis]\\n\\n        # Process random_state argument\\n        rs = com.random_state(random_state)\\n\\n        # Check weights for compliance\\n        if weights is not None:\\n\\n            # If a series, align with frame\\n            if isinstance(weights, pd.Series):\\n                weights = weights.reindex(self.axes[axis])\\n\\n            # Strings acceptable if a dataframe and axis = 0\\n            if isinstance(weights, str):\\n                if isinstance(self, pd.DataFrame):\\n                    if axis == 0:\\n                        try:\\n                            weights = self[weights]\\n                        except KeyError:\\n                            raise KeyError(\"String passed to weights not a \"\\n                                           \"valid column\")\\n                    else:\\n                        raise ValueError(\"Strings can only be passed to \"\\n                                         \"weights when sampling from rows on \"\\n                                         \"a DataFrame\")\\n                else:\\n                    raise ValueError(\"Strings cannot be passed as weights \"\\n                                     \"when sampling from a Series or Panel.\")\\n\\n            weights = pd.Series(weights, dtype=\\'float64\\')\\n\\n            if len(weights) != axis_length:\\n                raise ValueError(\"Weights and axis to be sampled must be of \"\\n                                 \"same length\")\\n\\n            if (weights == np.inf).any() or (weights == -np.inf).any():\\n                raise ValueError(\"weight vector may not include `inf` values\")\\n\\n            if (weights < 0).any():\\n                raise ValueError(\"weight vector many not include negative \"\\n                                 \"values\")\\n\\n            # If has nan, set to zero.\\n            weights = weights.fillna(0)\\n\\n            # Renormalize if don\\'t sum to 1\\n            if weights.sum() != 1:\\n                if weights.sum() != 0:\\n                    weights = weights / weights.sum()\\n                else:\\n                    raise ValueError(\"Invalid weights: weights sum to zero\")\\n\\n            weights = weights.values\\n\\n        # If no frac or n, default to n=1.\\n        if n is None and frac is None:\\n            n = 1\\n        elif n is not None and frac is None and n % 1 != 0:\\n            raise ValueError(\"Only integers accepted as `n` values\")\\n        elif n is None and frac is not None:\\n            n = int(round(frac * axis_length))\\n        elif n is not None and frac is not None:\\n            raise ValueError(\\'Please enter a value for `frac` OR `n`, not \\'\\n                             \\'both\\')\\n\\n        # Check for negative sizes\\n        if n < 0:\\n            raise ValueError(\"A negative number of rows requested. Please \"\\n                             \"provide positive value.\")\\n\\n        locs = rs.choice(axis_length, size=n, replace=replace, p=weights)\\n        return self.take(locs, axis=axis, is_copy=False)',\n 'def _dir_additions(self):\\n        \"\"\" add the string-like attributes from the info_axis.\\n        If info_axis is a MultiIndex, it\\'s first level values are used.\\n        \"\"\"\\n        additions = {c for c in self._info_axis.unique(level=0)[:100]\\n                     if isinstance(c, str) and c.isidentifier()}\\n        return super()._dir_additions().union(additions)',\n 'def _protect_consolidate(self, f):\\n        \"\"\"Consolidate _data -- if the blocks have changed, then clear the\\n        cache\\n        \"\"\"\\n        blocks_before = len(self._data.blocks)\\n        result = f()\\n        if len(self._data.blocks) != blocks_before:\\n            self._clear_item_cache()\\n        return result',\n 'def _consolidate_inplace(self):\\n        \"\"\"Consolidate data in place and return None\"\"\"\\n\\n        def f():\\n            self._data = self._data.consolidate()\\n\\n        self._protect_consolidate(f)',\n 'def _consolidate(self, inplace=False):\\n        \"\"\"\\n        Compute NDFrame with \"consolidated\" internals (data of each dtype\\n        grouped together in a single ndarray).\\n\\n        Parameters\\n        ----------\\n        inplace : boolean, default False\\n            If False return new object, otherwise modify existing object\\n\\n        Returns\\n        -------\\n        consolidated : same type as caller\\n        \"\"\"\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        if inplace:\\n            self._consolidate_inplace()\\n        else:\\n            f = lambda: self._data.consolidate()\\n            cons_data = self._protect_consolidate(f)\\n            return self._constructor(cons_data).__finalize__(self)',\n 'def _check_inplace_setting(self, value):\\n        \"\"\" check whether we allow in-place setting with this type of value \"\"\"\\n\\n        if self._is_mixed_type:\\n            if not self._is_numeric_mixed_type:\\n\\n                # allow an actual np.nan thru\\n                try:\\n                    if np.isnan(value):\\n                        return True\\n                except Exception:\\n                    pass\\n\\n                raise TypeError(\\'Cannot do inplace boolean setting on \\'\\n                                \\'mixed-types with a non np.nan value\\')\\n\\n        return True',\n 'def as_matrix(self, columns=None):\\n        \"\"\"\\n        Convert the frame to its Numpy-array representation.\\n\\n        .. deprecated:: 0.23.0\\n            Use :meth:`DataFrame.values` instead.\\n\\n        Parameters\\n        ----------\\n        columns : list, optional, default:None\\n            If None, return all columns, otherwise, returns specified columns.\\n\\n        Returns\\n        -------\\n        values : ndarray\\n            If the caller is heterogeneous and contains booleans or objects,\\n            the result will be of dtype=object. See Notes.\\n\\n        See Also\\n        --------\\n        DataFrame.values\\n\\n        Notes\\n        -----\\n        Return is NOT a Numpy-matrix, rather, a Numpy-array.\\n\\n        The dtype will be a lower-common-denominator dtype (implicit\\n        upcasting); that is to say if the dtypes (even of numeric types)\\n        are mixed, the one that accommodates all will be chosen. Use this\\n        with care if you are not dealing with the blocks.\\n\\n        e.g. If the dtypes are float16 and float32, dtype will be upcast to\\n        float32.  If dtypes are int32 and uint8, dtype will be upcase to\\n        int32. By numpy.find_common_type convention, mixing int64 and uint64\\n        will result in a float64 dtype.\\n\\n        This method is provided for backwards compatibility. Generally,\\n        it is recommended to use \\'.values\\'.\\n        \"\"\"\\n        warnings.warn(\"Method .as_matrix will be removed in a future version. \"\\n                      \"Use .values instead.\", FutureWarning, stacklevel=2)\\n        self._consolidate_inplace()\\n        return self._data.as_array(transpose=self._AXIS_REVERSED,\\n                                   items=columns)',\n 'def values(self):\\n        \"\"\"\\n        Return a Numpy representation of the DataFrame.\\n\\n        .. warning::\\n\\n           We recommend using :meth:`DataFrame.to_numpy` instead.\\n\\n        Only the values in the DataFrame will be returned, the axes labels\\n        will be removed.\\n\\n        Returns\\n        -------\\n        numpy.ndarray\\n            The values of the DataFrame.\\n\\n        See Also\\n        --------\\n        DataFrame.to_numpy : Recommended alternative to this method.\\n        DataFrame.index : Retrieve the index labels.\\n        DataFrame.columns : Retrieving the column names.\\n\\n        Notes\\n        -----\\n        The dtype will be a lower-common-denominator dtype (implicit\\n        upcasting); that is to say if the dtypes (even of numeric types)\\n        are mixed, the one that accommodates all will be chosen. Use this\\n        with care if you are not dealing with the blocks.\\n\\n        e.g. If the dtypes are float16 and float32, dtype will be upcast to\\n        float32.  If dtypes are int32 and uint8, dtype will be upcast to\\n        int32. By :func:`numpy.find_common_type` convention, mixing int64\\n        and uint64 will result in a float64 dtype.\\n\\n        Examples\\n        --------\\n        A DataFrame where all columns are the same type (e.g., int64) results\\n        in an array of the same type.\\n\\n        >>> df = pd.DataFrame({\\'age\\':    [ 3,  29],\\n        ...                    \\'height\\': [94, 170],\\n        ...                    \\'weight\\': [31, 115]})\\n        >>> df\\n           age  height  weight\\n        0    3      94      31\\n        1   29     170     115\\n        >>> df.dtypes\\n        age       int64\\n        height    int64\\n        weight    int64\\n        dtype: object\\n        >>> df.values\\n        array([[  3,  94,  31],\\n               [ 29, 170, 115]], dtype=int64)\\n\\n        A DataFrame with mixed type columns(e.g., str/object, int64, float32)\\n        results in an ndarray of the broadest type that accommodates these\\n        mixed types (e.g., object).\\n\\n        >>> df2 = pd.DataFrame([(\\'parrot\\',   24.0, \\'second\\'),\\n        ...                     (\\'lion\\',     80.5, 1),\\n        ...                     (\\'monkey\\', np.nan, None)],\\n        ...                   columns=(\\'name\\', \\'max_speed\\', \\'rank\\'))\\n        >>> df2.dtypes\\n        name          object\\n        max_speed    float64\\n        rank          object\\n        dtype: object\\n        >>> df2.values\\n        array([[\\'parrot\\', 24.0, \\'second\\'],\\n               [\\'lion\\', 80.5, 1],\\n               [\\'monkey\\', nan, None]], dtype=object)\\n        \"\"\"\\n        self._consolidate_inplace()\\n        return self._data.as_array(transpose=self._AXIS_REVERSED)',\n 'def get_ftype_counts(self):\\n        \"\"\"\\n        Return counts of unique ftypes in this object.\\n\\n        .. deprecated:: 0.23.0\\n\\n        This is useful for SparseDataFrame or for DataFrames containing\\n        sparse arrays.\\n\\n        Returns\\n        -------\\n        dtype : Series\\n            Series with the count of columns with each type and\\n            sparsity (dense/sparse).\\n\\n        See Also\\n        --------\\n        ftypes : Return ftypes (indication of sparse/dense and dtype) in\\n            this object.\\n\\n        Examples\\n        --------\\n        >>> a = [[\\'a\\', 1, 1.0], [\\'b\\', 2, 2.0], [\\'c\\', 3, 3.0]]\\n        >>> df = pd.DataFrame(a, columns=[\\'str\\', \\'int\\', \\'float\\'])\\n        >>> df\\n          str  int  float\\n        0   a    1    1.0\\n        1   b    2    2.0\\n        2   c    3    3.0\\n\\n        >>> df.get_ftype_counts()  # doctest: +SKIP\\n        float64:dense    1\\n        int64:dense      1\\n        object:dense     1\\n        dtype: int64\\n        \"\"\"\\n        warnings.warn(\"get_ftype_counts is deprecated and will \"\\n                      \"be removed in a future version\",\\n                      FutureWarning, stacklevel=2)\\n\\n        from pandas import Series\\n        return Series(self._data.get_ftype_counts())',\n 'def dtypes(self):\\n        \"\"\"\\n        Return the dtypes in the DataFrame.\\n\\n        This returns a Series with the data type of each column.\\n        The result\\'s index is the original DataFrame\\'s columns. Columns\\n        with mixed types are stored with the ``object`` dtype. See\\n        :ref:`the User Guide <basics.dtypes>` for more.\\n\\n        Returns\\n        -------\\n        pandas.Series\\n            The data type of each column.\\n\\n        See Also\\n        --------\\n        DataFrame.ftypes : Dtype and sparsity information.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'float\\': [1.0],\\n        ...                    \\'int\\': [1],\\n        ...                    \\'datetime\\': [pd.Timestamp(\\'20180310\\')],\\n        ...                    \\'string\\': [\\'foo\\']})\\n        >>> df.dtypes\\n        float              float64\\n        int                  int64\\n        datetime    datetime64[ns]\\n        string              object\\n        dtype: object\\n        \"\"\"\\n        from pandas import Series\\n        return Series(self._data.get_dtypes(), index=self._info_axis,\\n                      dtype=np.object_)',\n 'def ftypes(self):\\n        \"\"\"\\n        Return the ftypes (indication of sparse/dense and dtype) in DataFrame.\\n\\n        This returns a Series with the data type of each column.\\n        The result\\'s index is the original DataFrame\\'s columns. Columns\\n        with mixed types are stored with the ``object`` dtype.  See\\n        :ref:`the User Guide <basics.dtypes>` for more.\\n\\n        Returns\\n        -------\\n        pandas.Series\\n            The data type and indication of sparse/dense of each column.\\n\\n        See Also\\n        --------\\n        DataFrame.dtypes: Series with just dtype information.\\n        SparseDataFrame : Container for sparse tabular data.\\n\\n        Notes\\n        -----\\n        Sparse data should have the same dtypes as its dense representation.\\n\\n        Examples\\n        --------\\n        >>> arr = np.random.RandomState(0).randn(100, 4)\\n        >>> arr[arr < .8] = np.nan\\n        >>> pd.DataFrame(arr).ftypes\\n        0    float64:dense\\n        1    float64:dense\\n        2    float64:dense\\n        3    float64:dense\\n        dtype: object\\n\\n        >>> pd.SparseDataFrame(arr).ftypes\\n        0    float64:sparse\\n        1    float64:sparse\\n        2    float64:sparse\\n        3    float64:sparse\\n        dtype: object\\n        \"\"\"\\n        from pandas import Series\\n        return Series(self._data.get_ftypes(), index=self._info_axis,\\n                      dtype=np.object_)',\n 'def as_blocks(self, copy=True):\\n        \"\"\"\\n        Convert the frame to a dict of dtype -> Constructor Types that each has\\n        a homogeneous dtype.\\n\\n        .. deprecated:: 0.21.0\\n\\n        NOTE: the dtypes of the blocks WILL BE PRESERVED HERE (unlike in\\n              as_matrix)\\n\\n        Parameters\\n        ----------\\n        copy : boolean, default True\\n\\n        Returns\\n        -------\\n        values : a dict of dtype -> Constructor Types\\n        \"\"\"\\n        warnings.warn(\"as_blocks is deprecated and will \"\\n                      \"be removed in a future version\",\\n                      FutureWarning, stacklevel=2)\\n        return self._to_dict_of_blocks(copy=copy)',\n 'def _to_dict_of_blocks(self, copy=True):\\n        \"\"\"\\n        Return a dict of dtype -> Constructor Types that\\n        each is a homogeneous dtype.\\n\\n        Internal ONLY\\n        \"\"\"\\n        return {k: self._constructor(v).__finalize__(self)\\n                for k, v, in self._data.to_dict(copy=copy).items()}',\n 'def astype(self, dtype, copy=True, errors=\\'raise\\', **kwargs):\\n        \"\"\"\\n        Cast a pandas object to a specified dtype ``dtype``.\\n\\n        Parameters\\n        ----------\\n        dtype : data type, or dict of column name -> data type\\n            Use a numpy.dtype or Python type to cast entire pandas object to\\n            the same type. Alternatively, use {col: dtype, ...}, where col is a\\n            column label and dtype is a numpy.dtype or Python type to cast one\\n            or more of the DataFrame\\'s columns to column-specific types.\\n        copy : bool, default True\\n            Return a copy when ``copy=True`` (be very careful setting\\n            ``copy=False`` as changes to values then may propagate to other\\n            pandas objects).\\n        errors : {\\'raise\\', \\'ignore\\'}, default \\'raise\\'\\n            Control raising of exceptions on invalid data for provided dtype.\\n\\n            - ``raise`` : allow exceptions to be raised\\n            - ``ignore`` : suppress exceptions. On error return original object\\n\\n            .. versionadded:: 0.20.0\\n\\n        kwargs : keyword arguments to pass on to the constructor\\n\\n        Returns\\n        -------\\n        casted : same type as caller\\n\\n        See Also\\n        --------\\n        to_datetime : Convert argument to datetime.\\n        to_timedelta : Convert argument to timedelta.\\n        to_numeric : Convert argument to a numeric type.\\n        numpy.ndarray.astype : Cast a numpy array to a specified type.\\n\\n        Examples\\n        --------\\n        >>> ser = pd.Series([1, 2], dtype=\\'int32\\')\\n        >>> ser\\n        0    1\\n        1    2\\n        dtype: int32\\n        >>> ser.astype(\\'int64\\')\\n        0    1\\n        1    2\\n        dtype: int64\\n\\n        Convert to categorical type:\\n\\n        >>> ser.astype(\\'category\\')\\n        0    1\\n        1    2\\n        dtype: category\\n        Categories (2, int64): [1, 2]\\n\\n        Convert to ordered categorical type with custom ordering:\\n\\n        >>> cat_dtype = pd.api.types.CategoricalDtype(\\n        ...                     categories=[2, 1], ordered=True)\\n        >>> ser.astype(cat_dtype)\\n        0    1\\n        1    2\\n        dtype: category\\n        Categories (2, int64): [2 < 1]\\n\\n        Note that using ``copy=False`` and changing data on a new\\n        pandas object may propagate changes:\\n\\n        >>> s1 = pd.Series([1,2])\\n        >>> s2 = s1.astype(\\'int64\\', copy=False)\\n        >>> s2[0] = 10\\n        >>> s1  # note that s1[0] has changed too\\n        0    10\\n        1     2\\n        dtype: int64\\n        \"\"\"\\n        if is_dict_like(dtype):\\n            if self.ndim == 1:  # i.e. Series\\n                if len(dtype) > 1 or self.name not in dtype:\\n                    raise KeyError(\\'Only the Series name can be used for \\'\\n                                   \\'the key in Series dtype mappings.\\')\\n                new_type = dtype[self.name]\\n                return self.astype(new_type, copy, errors, **kwargs)\\n            elif self.ndim > 2:\\n                raise NotImplementedError(\\n                    \\'astype() only accepts a dtype arg of type dict when \\'\\n                    \\'invoked on Series and DataFrames. A single dtype must be \\'\\n                    \\'specified when invoked on a Panel.\\'\\n                )\\n            for col_name in dtype.keys():\\n                if col_name not in self:\\n                    raise KeyError(\\'Only a column name can be used for the \\'\\n                                   \\'key in a dtype mappings argument.\\')\\n            results = []\\n            for col_name, col in self.iteritems():\\n                if col_name in dtype:\\n                    results.append(col.astype(dtype=dtype[col_name], copy=copy,\\n                                              errors=errors, **kwargs))\\n                else:\\n                    results.append(results.append(col.copy() if copy else col))\\n\\n        elif is_extension_array_dtype(dtype) and self.ndim > 1:\\n            # GH 18099/22869: columnwise conversion to extension dtype\\n            # GH 24704: use iloc to handle duplicate column names\\n            results = (self.iloc[:, i].astype(dtype, copy=copy)\\n                       for i in range(len(self.columns)))\\n\\n        else:\\n            # else, only a single dtype is given\\n            new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors,\\n                                         **kwargs)\\n            return self._constructor(new_data).__finalize__(self)\\n\\n        # GH 19920: retain column metadata after concat\\n        result = pd.concat(results, axis=1, copy=False)\\n        result.columns = self.columns\\n        return result',\n 'def copy(self, deep=True):\\n        \"\"\"\\n        Make a copy of this object\\'s indices and data.\\n\\n        When ``deep=True`` (default), a new object will be created with a\\n        copy of the calling object\\'s data and indices. Modifications to\\n        the data or indices of the copy will not be reflected in the\\n        original object (see notes below).\\n\\n        When ``deep=False``, a new object will be created without copying\\n        the calling object\\'s data or index (only references to the data\\n        and index are copied). Any changes to the data of the original\\n        will be reflected in the shallow copy (and vice versa).\\n\\n        Parameters\\n        ----------\\n        deep : bool, default True\\n            Make a deep copy, including a copy of the data and the indices.\\n            With ``deep=False`` neither the indices nor the data are copied.\\n\\n        Returns\\n        -------\\n        copy : Series, DataFrame or Panel\\n            Object type matches caller.\\n\\n        Notes\\n        -----\\n        When ``deep=True``, data is copied but actual Python objects\\n        will not be copied recursively, only the reference to the object.\\n        This is in contrast to `copy.deepcopy` in the Standard Library,\\n        which recursively copies object data (see examples below).\\n\\n        While ``Index`` objects are copied when ``deep=True``, the underlying\\n        numpy array is not copied for performance reasons. Since ``Index`` is\\n        immutable, the underlying data can be safely shared and a copy\\n        is not needed.\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series([1, 2], index=[\"a\", \"b\"])\\n        >>> s\\n        a    1\\n        b    2\\n        dtype: int64\\n\\n        >>> s_copy = s.copy()\\n        >>> s_copy\\n        a    1\\n        b    2\\n        dtype: int64\\n\\n        **Shallow copy versus default (deep) copy:**\\n\\n        >>> s = pd.Series([1, 2], index=[\"a\", \"b\"])\\n        >>> deep = s.copy()\\n        >>> shallow = s.copy(deep=False)\\n\\n        Shallow copy shares data and index with original.\\n\\n        >>> s is shallow\\n        False\\n        >>> s.values is shallow.values and s.index is shallow.index\\n        True\\n\\n        Deep copy has own copy of data and index.\\n\\n        >>> s is deep\\n        False\\n        >>> s.values is deep.values or s.index is deep.index\\n        False\\n\\n        Updates to the data shared by shallow copy and original is reflected\\n        in both; deep copy remains unchanged.\\n\\n        >>> s[0] = 3\\n        >>> shallow[1] = 4\\n        >>> s\\n        a    3\\n        b    4\\n        dtype: int64\\n        >>> shallow\\n        a    3\\n        b    4\\n        dtype: int64\\n        >>> deep\\n        a    1\\n        b    2\\n        dtype: int64\\n\\n        Note that when copying an object containing Python objects, a deep copy\\n        will copy the data, but will not do so recursively. Updating a nested\\n        data object will be reflected in the deep copy.\\n\\n        >>> s = pd.Series([[1, 2], [3, 4]])\\n        >>> deep = s.copy()\\n        >>> s[0][0] = 10\\n        >>> s\\n        0    [10, 2]\\n        1     [3, 4]\\n        dtype: object\\n        >>> deep\\n        0    [10, 2]\\n        1     [3, 4]\\n        dtype: object\\n        \"\"\"\\n        data = self._data.copy(deep=deep)\\n        return self._constructor(data).__finalize__(self)',\n 'def _convert(self, datetime=False, numeric=False, timedelta=False,\\n                 coerce=False, copy=True):\\n        \"\"\"\\n        Attempt to infer better dtype for object columns\\n\\n        Parameters\\n        ----------\\n        datetime : boolean, default False\\n            If True, convert to date where possible.\\n        numeric : boolean, default False\\n            If True, attempt to convert to numbers (including strings), with\\n            unconvertible values becoming NaN.\\n        timedelta : boolean, default False\\n            If True, convert to timedelta where possible.\\n        coerce : boolean, default False\\n            If True, force conversion with unconvertible values converted to\\n            nulls (NaN or NaT)\\n        copy : boolean, default True\\n            If True, return a copy even if no copy is necessary (e.g. no\\n            conversion was done). Note: This is meant for internal use, and\\n            should not be confused with inplace.\\n\\n        Returns\\n        -------\\n        converted : same as input object\\n        \"\"\"\\n        return self._constructor(\\n            self._data.convert(datetime=datetime, numeric=numeric,\\n                               timedelta=timedelta, coerce=coerce,\\n                               copy=copy)).__finalize__(self)',\n 'def convert_objects(self, convert_dates=True, convert_numeric=False,\\n                        convert_timedeltas=True, copy=True):\\n        \"\"\"\\n        Attempt to infer better dtype for object columns.\\n\\n        .. deprecated:: 0.21.0\\n\\n        Parameters\\n        ----------\\n        convert_dates : boolean, default True\\n            If True, convert to date where possible. If \\'coerce\\', force\\n            conversion, with unconvertible values becoming NaT.\\n        convert_numeric : boolean, default False\\n            If True, attempt to coerce to numbers (including strings), with\\n            unconvertible values becoming NaN.\\n        convert_timedeltas : boolean, default True\\n            If True, convert to timedelta where possible. If \\'coerce\\', force\\n            conversion, with unconvertible values becoming NaT.\\n        copy : boolean, default True\\n            If True, return a copy even if no copy is necessary (e.g. no\\n            conversion was done). Note: This is meant for internal use, and\\n            should not be confused with inplace.\\n\\n        Returns\\n        -------\\n        converted : same as input object\\n\\n        See Also\\n        --------\\n        to_datetime : Convert argument to datetime.\\n        to_timedelta : Convert argument to timedelta.\\n        to_numeric : Convert argument to numeric type.\\n        \"\"\"\\n        msg = (\"convert_objects is deprecated.  To re-infer data dtypes for \"\\n               \"object columns, use {klass}.infer_objects()\\\\nFor all \"\\n               \"other conversions use the data-type specific converters \"\\n               \"pd.to_datetime, pd.to_timedelta and pd.to_numeric.\"\\n               ).format(klass=self.__class__.__name__)\\n        warnings.warn(msg, FutureWarning, stacklevel=2)\\n\\n        return self._constructor(\\n            self._data.convert(convert_dates=convert_dates,\\n                               convert_numeric=convert_numeric,\\n                               convert_timedeltas=convert_timedeltas,\\n                               copy=copy)).__finalize__(self)',\n 'def infer_objects(self):\\n        \"\"\"\\n        Attempt to infer better dtypes for object columns.\\n\\n        Attempts soft conversion of object-dtyped\\n        columns, leaving non-object and unconvertible\\n        columns unchanged. The inference rules are the\\n        same as during normal Series/DataFrame construction.\\n\\n        .. versionadded:: 0.21.0\\n\\n        Returns\\n        -------\\n        converted : same type as input object\\n\\n        See Also\\n        --------\\n        to_datetime : Convert argument to datetime.\\n        to_timedelta : Convert argument to timedelta.\\n        to_numeric : Convert argument to numeric type.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\"A\": [\"a\", 1, 2, 3]})\\n        >>> df = df.iloc[1:]\\n        >>> df\\n           A\\n        1  1\\n        2  2\\n        3  3\\n\\n        >>> df.dtypes\\n        A    object\\n        dtype: object\\n\\n        >>> df.infer_objects().dtypes\\n        A    int64\\n        dtype: object\\n        \"\"\"\\n        # numeric=False necessary to only soft convert;\\n        # python objects will still be converted to\\n        # native numpy numeric types\\n        return self._constructor(\\n            self._data.convert(datetime=True, numeric=False,\\n                               timedelta=True, coerce=False,\\n                               copy=True)).__finalize__(self)',\n 'def fillna(self, value=None, method=None, axis=None, inplace=False,\\n               limit=None, downcast=None):\\n        \"\"\"\\n        Fill NA/NaN values using the specified method.\\n\\n        Parameters\\n        ----------\\n        value : scalar, dict, Series, or DataFrame\\n            Value to use to fill holes (e.g. 0), alternately a\\n            dict/Series/DataFrame of values specifying which value to use for\\n            each index (for a Series) or column (for a DataFrame).  Values not\\n            in the dict/Series/DataFrame will not be filled. This value cannot\\n            be a list.\\n        method : {\\'backfill\\', \\'bfill\\', \\'pad\\', \\'ffill\\', None}, default None\\n            Method to use for filling holes in reindexed Series\\n            pad / ffill: propagate last valid observation forward to next valid\\n            backfill / bfill: use next valid observation to fill gap.\\n        axis : %(axes_single_arg)s\\n            Axis along which to fill missing values.\\n        inplace : bool, default False\\n            If True, fill in-place. Note: this will modify any\\n            other views on this object (e.g., a no-copy slice for a column in a\\n            DataFrame).\\n        limit : int, default None\\n            If method is specified, this is the maximum number of consecutive\\n            NaN values to forward/backward fill. In other words, if there is\\n            a gap with more than this number of consecutive NaNs, it will only\\n            be partially filled. If method is not specified, this is the\\n            maximum number of entries along the entire axis where NaNs will be\\n            filled. Must be greater than 0 if not None.\\n        downcast : dict, default is None\\n            A dict of item->dtype of what to downcast if possible,\\n            or the string \\'infer\\' which will try to downcast to an appropriate\\n            equal type (e.g. float64 to int64 if possible).\\n\\n        Returns\\n        -------\\n        %(klass)s\\n            Object with missing values filled.\\n\\n        See Also\\n        --------\\n        interpolate : Fill NaN values using interpolation.\\n        reindex : Conform object to new index.\\n        asfreq : Convert TimeSeries to specified frequency.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame([[np.nan, 2, np.nan, 0],\\n        ...                    [3, 4, np.nan, 1],\\n        ...                    [np.nan, np.nan, np.nan, 5],\\n        ...                    [np.nan, 3, np.nan, 4]],\\n        ...                   columns=list(\\'ABCD\\'))\\n        >>> df\\n             A    B   C  D\\n        0  NaN  2.0 NaN  0\\n        1  3.0  4.0 NaN  1\\n        2  NaN  NaN NaN  5\\n        3  NaN  3.0 NaN  4\\n\\n        Replace all NaN elements with 0s.\\n\\n        >>> df.fillna(0)\\n            A   B   C   D\\n        0   0.0 2.0 0.0 0\\n        1   3.0 4.0 0.0 1\\n        2   0.0 0.0 0.0 5\\n        3   0.0 3.0 0.0 4\\n\\n        We can also propagate non-null values forward or backward.\\n\\n        >>> df.fillna(method=\\'ffill\\')\\n            A   B   C   D\\n        0   NaN 2.0 NaN 0\\n        1   3.0 4.0 NaN 1\\n        2   3.0 4.0 NaN 5\\n        3   3.0 3.0 NaN 4\\n\\n        Replace all NaN elements in column \\'A\\', \\'B\\', \\'C\\', and \\'D\\', with 0, 1,\\n        2, and 3 respectively.\\n\\n        >>> values = {\\'A\\': 0, \\'B\\': 1, \\'C\\': 2, \\'D\\': 3}\\n        >>> df.fillna(value=values)\\n            A   B   C   D\\n        0   0.0 2.0 2.0 0\\n        1   3.0 4.0 2.0 1\\n        2   0.0 1.0 2.0 5\\n        3   0.0 3.0 2.0 4\\n\\n        Only replace the first NaN element.\\n\\n        >>> df.fillna(value=values, limit=1)\\n            A   B   C   D\\n        0   0.0 2.0 2.0 0\\n        1   3.0 4.0 NaN 1\\n        2   NaN 1.0 NaN 5\\n        3   NaN 3.0 NaN 4\\n        \"\"\"\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n        value, method = validate_fillna_kwargs(value, method)\\n\\n        self._consolidate_inplace()\\n\\n        # set the default here, so functions examining the signaure\\n        # can detect if something was set (e.g. in groupby) (GH9221)\\n        if axis is None:\\n            axis = 0\\n        axis = self._get_axis_number(axis)\\n\\n        from pandas import DataFrame\\n        if value is None:\\n\\n            if self._is_mixed_type and axis == 1:\\n                if inplace:\\n                    raise NotImplementedError()\\n                result = self.T.fillna(method=method, limit=limit).T\\n\\n                # need to downcast here because of all of the transposes\\n                result._data = result._data.downcast()\\n\\n                return result\\n\\n            # > 3d\\n            if self.ndim > 3:\\n                raise NotImplementedError(\\'Cannot fillna with a method for > \\'\\n                                          \\'3dims\\')\\n\\n            # 3d\\n            elif self.ndim == 3:\\n                # fill in 2d chunks\\n                result = {col: s.fillna(method=method, value=value)\\n                          for col, s in self.iteritems()}\\n                prelim_obj = self._constructor.from_dict(result)\\n                new_obj = prelim_obj.__finalize__(self)\\n                new_data = new_obj._data\\n\\n            else:\\n                # 2d or less\\n                new_data = self._data.interpolate(method=method, axis=axis,\\n                                                  limit=limit, inplace=inplace,\\n                                                  coerce=True,\\n                                                  downcast=downcast)\\n        else:\\n            if len(self._get_axis(axis)) == 0:\\n                return self\\n\\n            if self.ndim == 1:\\n                if isinstance(value, (dict, ABCSeries)):\\n                    from pandas import Series\\n                    value = Series(value)\\n                elif not is_list_like(value):\\n                    pass\\n                else:\\n                    raise TypeError(\\'\"value\" parameter must be a scalar, dict \\'\\n                                    \\'or Series, but you passed a \\'\\n                                    \\'\"{0}\"\\'.format(type(value).__name__))\\n\\n                new_data = self._data.fillna(value=value, limit=limit,\\n                                             inplace=inplace,\\n                                             downcast=downcast)\\n\\n            elif isinstance(value, (dict, ABCSeries)):\\n                if axis == 1:\\n                    raise NotImplementedError(\\'Currently only can fill \\'\\n                                              \\'with dict/Series column \\'\\n                                              \\'by column\\')\\n\\n                result = self if inplace else self.copy()\\n                for k, v in value.items():\\n                    if k not in result:\\n                        continue\\n                    obj = result[k]\\n                    obj.fillna(v, limit=limit, inplace=True, downcast=downcast)\\n                return result if not inplace else None\\n\\n            elif not is_list_like(value):\\n                new_data = self._data.fillna(value=value, limit=limit,\\n                                             inplace=inplace,\\n                                             downcast=downcast)\\n            elif isinstance(value, DataFrame) and self.ndim == 2:\\n                new_data = self.where(self.notna(), value)\\n            else:\\n                raise ValueError(\"invalid fill value with a %s\" % type(value))\\n\\n        if inplace:\\n            self._update_inplace(new_data)\\n        else:\\n            return self._constructor(new_data).__finalize__(self)',\n 'def interpolate(self, method=\\'linear\\', axis=0, limit=None, inplace=False,\\n                    limit_direction=\\'forward\\', limit_area=None,\\n                    downcast=None, **kwargs):\\n        \"\"\"\\n        Interpolate values according to different methods.\\n        \"\"\"\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n\\n        if self.ndim > 2:\\n            raise NotImplementedError(\"Interpolate has not been implemented \"\\n                                      \"on Panel and Panel 4D objects.\")\\n\\n        if axis == 0:\\n            ax = self._info_axis_name\\n            _maybe_transposed_self = self\\n        elif axis == 1:\\n            _maybe_transposed_self = self.T\\n            ax = 1\\n        else:\\n            _maybe_transposed_self = self\\n        ax = _maybe_transposed_self._get_axis_number(ax)\\n\\n        if _maybe_transposed_self.ndim == 2:\\n            alt_ax = 1 - ax\\n        else:\\n            alt_ax = ax\\n\\n        if (isinstance(_maybe_transposed_self.index, MultiIndex) and\\n                method != \\'linear\\'):\\n            raise ValueError(\"Only `method=linear` interpolation is supported \"\\n                             \"on MultiIndexes.\")\\n\\n        if _maybe_transposed_self._data.get_dtype_counts().get(\\n                \\'object\\') == len(_maybe_transposed_self.T):\\n            raise TypeError(\"Cannot interpolate with all object-dtype columns \"\\n                            \"in the DataFrame. Try setting at least one \"\\n                            \"column to a numeric dtype.\")\\n\\n        # create/use the index\\n        if method == \\'linear\\':\\n            # prior default\\n            index = np.arange(len(_maybe_transposed_self._get_axis(alt_ax)))\\n        else:\\n            index = _maybe_transposed_self._get_axis(alt_ax)\\n            methods = {\"index\", \"values\", \"nearest\", \"time\"}\\n            is_numeric_or_datetime = (\\n                is_numeric_dtype(index) or\\n                is_datetime64_dtype(index) or\\n                is_timedelta64_dtype(index)\\n            )\\n            if method not in methods and not is_numeric_or_datetime:\\n                raise ValueError(\\n                    \"Index column must be numeric or datetime type when \"\\n                    \"using {method} method other than linear. \"\\n                    \"Try setting a numeric or datetime index column before \"\\n                    \"interpolating.\".format(method=method))\\n\\n        if isna(index).any():\\n            raise NotImplementedError(\"Interpolation with NaNs in the index \"\\n                                      \"has not been implemented. Try filling \"\\n                                      \"those NaNs before interpolating.\")\\n        data = _maybe_transposed_self._data\\n        new_data = data.interpolate(method=method, axis=ax, index=index,\\n                                    values=_maybe_transposed_self, limit=limit,\\n                                    limit_direction=limit_direction,\\n                                    limit_area=limit_area,\\n                                    inplace=inplace, downcast=downcast,\\n                                    **kwargs)\\n\\n        if inplace:\\n            if axis == 1:\\n                new_data = self._constructor(new_data).T._data\\n            self._update_inplace(new_data)\\n        else:\\n            res = self._constructor(new_data).__finalize__(self)\\n            if axis == 1:\\n                res = res.T\\n            return res',\n 'def asof(self, where, subset=None):\\n        \"\"\"\\n        Return the last row(s) without any NaNs before `where`.\\n\\n        The last row (for each element in `where`, if list) without any\\n        NaN is taken.\\n        In case of a :class:`~pandas.DataFrame`, the last row without NaN\\n        considering only the subset of columns (if not `None`)\\n\\n        .. versionadded:: 0.19.0 For DataFrame\\n\\n        If there is no good value, NaN is returned for a Series or\\n        a Series of NaN values for a DataFrame\\n\\n        Parameters\\n        ----------\\n        where : date or array-like of dates\\n            Date(s) before which the last row(s) are returned.\\n        subset : str or array-like of str, default `None`\\n            For DataFrame, if not `None`, only use these columns to\\n            check for NaNs.\\n\\n        Returns\\n        -------\\n        scalar, Series, or DataFrame\\n\\n            The return can be:\\n\\n            * scalar : when `self` is a Series and `where` is a scalar\\n            * Series: when `self` is a Series and `where` is an array-like,\\n              or when `self` is a DataFrame and `where` is a scalar\\n            * DataFrame : when `self` is a DataFrame and `where` is an\\n              array-like\\n\\n            Return scalar, Series, or DataFrame.\\n\\n        See Also\\n        --------\\n        merge_asof : Perform an asof merge. Similar to left join.\\n\\n        Notes\\n        -----\\n        Dates are assumed to be sorted. Raises if this is not the case.\\n\\n        Examples\\n        --------\\n        A Series and a scalar `where`.\\n\\n        >>> s = pd.Series([1, 2, np.nan, 4], index=[10, 20, 30, 40])\\n        >>> s\\n        10    1.0\\n        20    2.0\\n        30    NaN\\n        40    4.0\\n        dtype: float64\\n\\n        >>> s.asof(20)\\n        2.0\\n\\n        For a sequence `where`, a Series is returned. The first value is\\n        NaN, because the first element of `where` is before the first\\n        index value.\\n\\n        >>> s.asof([5, 20])\\n        5     NaN\\n        20    2.0\\n        dtype: float64\\n\\n        Missing values are not considered. The following is ``2.0``, not\\n        NaN, even though NaN is at the index location for ``30``.\\n\\n        >>> s.asof(30)\\n        2.0\\n\\n        Take all columns into consideration\\n\\n        >>> df = pd.DataFrame({\\'a\\': [10, 20, 30, 40, 50],\\n        ...                    \\'b\\': [None, None, None, None, 500]},\\n        ...                   index=pd.DatetimeIndex([\\'2018-02-27 09:01:00\\',\\n        ...                                           \\'2018-02-27 09:02:00\\',\\n        ...                                           \\'2018-02-27 09:03:00\\',\\n        ...                                           \\'2018-02-27 09:04:00\\',\\n        ...                                           \\'2018-02-27 09:05:00\\']))\\n        >>> df.asof(pd.DatetimeIndex([\\'2018-02-27 09:03:30\\',\\n        ...                           \\'2018-02-27 09:04:30\\']))\\n                              a   b\\n        2018-02-27 09:03:30 NaN NaN\\n        2018-02-27 09:04:30 NaN NaN\\n\\n        Take a single column into consideration\\n\\n        >>> df.asof(pd.DatetimeIndex([\\'2018-02-27 09:03:30\\',\\n        ...                           \\'2018-02-27 09:04:30\\']),\\n        ...         subset=[\\'a\\'])\\n                                 a   b\\n        2018-02-27 09:03:30   30.0 NaN\\n        2018-02-27 09:04:30   40.0 NaN\\n        \"\"\"\\n        if isinstance(where, str):\\n            from pandas import to_datetime\\n            where = to_datetime(where)\\n\\n        if not self.index.is_monotonic:\\n            raise ValueError(\"asof requires a sorted index\")\\n\\n        is_series = isinstance(self, ABCSeries)\\n        if is_series:\\n            if subset is not None:\\n                raise ValueError(\"subset is not valid for Series\")\\n        elif self.ndim > 2:\\n            raise NotImplementedError(\"asof is not implemented \"\\n                                      \"for {type}\".format(type=type(self)))\\n        else:\\n            if subset is None:\\n                subset = self.columns\\n            if not is_list_like(subset):\\n                subset = [subset]\\n\\n        is_list = is_list_like(where)\\n        if not is_list:\\n            start = self.index[0]\\n            if isinstance(self.index, PeriodIndex):\\n                where = Period(where, freq=self.index.freq).ordinal\\n                start = start.ordinal\\n\\n            if where < start:\\n                if not is_series:\\n                    from pandas import Series\\n                    return Series(index=self.columns, name=where)\\n                return np.nan\\n\\n            # It\\'s always much faster to use a *while* loop here for\\n            # Series than pre-computing all the NAs. However a\\n            # *while* loop is extremely expensive for DataFrame\\n            # so we later pre-compute all the NAs and use the same\\n            # code path whether *where* is a scalar or list.\\n            # See PR: https://github.com/pandas-dev/pandas/pull/14476\\n            if is_series:\\n                loc = self.index.searchsorted(where, side=\\'right\\')\\n                if loc > 0:\\n                    loc -= 1\\n\\n                values = self._values\\n                while loc > 0 and isna(values[loc]):\\n                    loc -= 1\\n                return values[loc]\\n\\n        if not isinstance(where, Index):\\n            where = Index(where) if is_list else Index([where])\\n\\n        nulls = self.isna() if is_series else self[subset].isna().any(1)\\n        if nulls.all():\\n            if is_series:\\n                return self._constructor(np.nan, index=where, name=self.name)\\n            elif is_list:\\n                from pandas import DataFrame\\n                return DataFrame(np.nan, index=where, columns=self.columns)\\n            else:\\n                from pandas import Series\\n                return Series(np.nan, index=self.columns, name=where[0])\\n\\n        locs = self.index.asof_locs(where, ~(nulls.values))\\n\\n        # mask the missing\\n        missing = locs == -1\\n        data = self.take(locs, is_copy=False)\\n        data.index = where\\n        data.loc[missing] = np.nan\\n        return data if is_list else data.iloc[-1]',\n 'def clip(self, lower=None, upper=None, axis=None, inplace=False,\\n             *args, **kwargs):\\n        \"\"\"\\n        Trim values at input threshold(s).\\n\\n        Assigns values outside boundary to boundary values. Thresholds\\n        can be singular values or array like, and in the latter case\\n        the clipping is performed element-wise in the specified axis.\\n\\n        Parameters\\n        ----------\\n        lower : float or array_like, default None\\n            Minimum threshold value. All values below this\\n            threshold will be set to it.\\n        upper : float or array_like, default None\\n            Maximum threshold value. All values above this\\n            threshold will be set to it.\\n        axis : int or str axis name, optional\\n            Align object with lower and upper along the given axis.\\n        inplace : bool, default False\\n            Whether to perform the operation in place on the data.\\n\\n            .. versionadded:: 0.21.0\\n        *args, **kwargs\\n            Additional keywords have no effect but might be accepted\\n            for compatibility with numpy.\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            Same type as calling object with the values outside the\\n            clip boundaries replaced.\\n\\n        Examples\\n        --------\\n        >>> data = {\\'col_0\\': [9, -3, 0, -1, 5], \\'col_1\\': [-2, -7, 6, 8, -5]}\\n        >>> df = pd.DataFrame(data)\\n        >>> df\\n           col_0  col_1\\n        0      9     -2\\n        1     -3     -7\\n        2      0      6\\n        3     -1      8\\n        4      5     -5\\n\\n        Clips per column using lower and upper thresholds:\\n\\n        >>> df.clip(-4, 6)\\n           col_0  col_1\\n        0      6     -2\\n        1     -3     -4\\n        2      0      6\\n        3     -1      6\\n        4      5     -4\\n\\n        Clips using specific lower and upper thresholds per column element:\\n\\n        >>> t = pd.Series([2, -4, -1, 6, 3])\\n        >>> t\\n        0    2\\n        1   -4\\n        2   -1\\n        3    6\\n        4    3\\n        dtype: int64\\n\\n        >>> df.clip(t, t + 4, axis=0)\\n           col_0  col_1\\n        0      6      2\\n        1     -3     -4\\n        2      0      3\\n        3      6      8\\n        4      5      3\\n        \"\"\"\\n        if isinstance(self, ABCPanel):\\n            raise NotImplementedError(\"clip is not supported yet for panels\")\\n\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n\\n        axis = nv.validate_clip_with_axis(axis, args, kwargs)\\n        if axis is not None:\\n            axis = self._get_axis_number(axis)\\n\\n        # GH 17276\\n        # numpy doesn\\'t like NaN as a clip value\\n        # so ignore\\n        # GH 19992\\n        # numpy doesn\\'t drop a list-like bound containing NaN\\n        if not is_list_like(lower) and np.any(pd.isnull(lower)):\\n            lower = None\\n        if not is_list_like(upper) and np.any(pd.isnull(upper)):\\n            upper = None\\n\\n        # GH 2747 (arguments were reversed)\\n        if lower is not None and upper is not None:\\n            if is_scalar(lower) and is_scalar(upper):\\n                lower, upper = min(lower, upper), max(lower, upper)\\n\\n        # fast-path for scalars\\n        if ((lower is None or (is_scalar(lower) and is_number(lower))) and\\n                (upper is None or (is_scalar(upper) and is_number(upper)))):\\n            return self._clip_with_scalar(lower, upper, inplace=inplace)\\n\\n        result = self\\n        if lower is not None:\\n            result = result._clip_with_one_bound(lower, method=self.ge,\\n                                                 axis=axis, inplace=inplace)\\n        if upper is not None:\\n            if inplace:\\n                result = self\\n            result = result._clip_with_one_bound(upper, method=self.le,\\n                                                 axis=axis, inplace=inplace)\\n\\n        return result',\n 'def clip_upper(self, threshold, axis=None, inplace=False):\\n        \"\"\"\\n        Trim values above a given threshold.\\n\\n        .. deprecated:: 0.24.0\\n            Use clip(upper=threshold) instead.\\n\\n        Elements above the `threshold` will be changed to match the\\n        `threshold` value(s). Threshold can be a single value or an array,\\n        in the latter case it performs the truncation element-wise.\\n\\n        Parameters\\n        ----------\\n        threshold : numeric or array-like\\n            Maximum value allowed. All values above threshold will be set to\\n            this value.\\n\\n            * float : every value is compared to `threshold`.\\n            * array-like : The shape of `threshold` should match the object\\n              it\\'s compared to. When `self` is a Series, `threshold` should be\\n              the length. When `self` is a DataFrame, `threshold` should 2-D\\n              and the same shape as `self` for ``axis=None``, or 1-D and the\\n              same length as the axis being compared.\\n\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n            Align object with `threshold` along the given axis.\\n        inplace : bool, default False\\n            Whether to perform the operation in place on the data.\\n\\n            .. versionadded:: 0.21.0\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            Original data with values trimmed.\\n\\n        See Also\\n        --------\\n        Series.clip : General purpose method to trim Series values to given\\n            threshold(s).\\n        DataFrame.clip : General purpose method to trim DataFrame values to\\n            given threshold(s).\\n\\n        Examples\\n        --------\\n        >>> s = pd.Series([1, 2, 3, 4, 5])\\n        >>> s\\n        0    1\\n        1    2\\n        2    3\\n        3    4\\n        4    5\\n        dtype: int64\\n\\n        >>> s.clip(upper=3)\\n        0    1\\n        1    2\\n        2    3\\n        3    3\\n        4    3\\n        dtype: int64\\n\\n        >>> elemwise_thresholds = [5, 4, 3, 2, 1]\\n        >>> elemwise_thresholds\\n        [5, 4, 3, 2, 1]\\n\\n        >>> s.clip(upper=elemwise_thresholds)\\n        0    1\\n        1    2\\n        2    3\\n        3    2\\n        4    1\\n        dtype: int64\\n        \"\"\"\\n        warnings.warn(\\'clip_upper(threshold) is deprecated, \\'\\n                      \\'use clip(upper=threshold) instead\\',\\n                      FutureWarning, stacklevel=2)\\n        return self._clip_with_one_bound(threshold, method=self.le,\\n                                         axis=axis, inplace=inplace)',\n 'def clip_lower(self, threshold, axis=None, inplace=False):\\n        \"\"\"\\n        Trim values below a given threshold.\\n\\n        .. deprecated:: 0.24.0\\n            Use clip(lower=threshold) instead.\\n\\n        Elements below the `threshold` will be changed to match the\\n        `threshold` value(s). Threshold can be a single value or an array,\\n        in the latter case it performs the truncation element-wise.\\n\\n        Parameters\\n        ----------\\n        threshold : numeric or array-like\\n            Minimum value allowed. All values below threshold will be set to\\n            this value.\\n\\n            * float : every value is compared to `threshold`.\\n            * array-like : The shape of `threshold` should match the object\\n              it\\'s compared to. When `self` is a Series, `threshold` should be\\n              the length. When `self` is a DataFrame, `threshold` should 2-D\\n              and the same shape as `self` for ``axis=None``, or 1-D and the\\n              same length as the axis being compared.\\n\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n            Align `self` with `threshold` along the given axis.\\n\\n        inplace : bool, default False\\n            Whether to perform the operation in place on the data.\\n\\n            .. versionadded:: 0.21.0\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            Original data with values trimmed.\\n\\n        See Also\\n        --------\\n        Series.clip : General purpose method to trim Series values to given\\n            threshold(s).\\n        DataFrame.clip : General purpose method to trim DataFrame values to\\n            given threshold(s).\\n\\n        Examples\\n        --------\\n\\n        Series single threshold clipping:\\n\\n        >>> s = pd.Series([5, 6, 7, 8, 9])\\n        >>> s.clip(lower=8)\\n        0    8\\n        1    8\\n        2    8\\n        3    8\\n        4    9\\n        dtype: int64\\n\\n        Series clipping element-wise using an array of thresholds. `threshold`\\n        should be the same length as the Series.\\n\\n        >>> elemwise_thresholds = [4, 8, 7, 2, 5]\\n        >>> s.clip(lower=elemwise_thresholds)\\n        0    5\\n        1    8\\n        2    7\\n        3    8\\n        4    9\\n        dtype: int64\\n\\n        DataFrames can be compared to a scalar.\\n\\n        >>> df = pd.DataFrame({\"A\": [1, 3, 5], \"B\": [2, 4, 6]})\\n        >>> df\\n           A  B\\n        0  1  2\\n        1  3  4\\n        2  5  6\\n\\n        >>> df.clip(lower=3)\\n           A  B\\n        0  3  3\\n        1  3  4\\n        2  5  6\\n\\n        Or to an array of values. By default, `threshold` should be the same\\n        shape as the DataFrame.\\n\\n        >>> df.clip(lower=np.array([[3, 4], [2, 2], [6, 2]]))\\n           A  B\\n        0  3  4\\n        1  3  4\\n        2  6  6\\n\\n        Control how `threshold` is broadcast with `axis`. In this case\\n        `threshold` should be the same length as the axis specified by\\n        `axis`.\\n\\n        >>> df.clip(lower=[3, 3, 5], axis=\\'index\\')\\n           A  B\\n        0  3  3\\n        1  3  4\\n        2  5  6\\n\\n        >>> df.clip(lower=[4, 5], axis=\\'columns\\')\\n           A  B\\n        0  4  5\\n        1  4  5\\n        2  5  6\\n        \"\"\"\\n        warnings.warn(\\'clip_lower(threshold) is deprecated, \\'\\n                      \\'use clip(lower=threshold) instead\\',\\n                      FutureWarning, stacklevel=2)\\n        return self._clip_with_one_bound(threshold, method=self.ge,\\n                                         axis=axis, inplace=inplace)',\n 'def groupby(self, by=None, axis=0, level=None, as_index=True, sort=True,\\n                group_keys=True, squeeze=False, observed=False, **kwargs):\\n        \"\"\"\\n        Group DataFrame or Series using a mapper or by a Series of columns.\\n\\n        A groupby operation involves some combination of splitting the\\n        object, applying a function, and combining the results. This can be\\n        used to group large amounts of data and compute operations on these\\n        groups.\\n\\n        Parameters\\n        ----------\\n        by : mapping, function, label, or list of labels\\n            Used to determine the groups for the groupby.\\n            If ``by`` is a function, it\\'s called on each value of the object\\'s\\n            index. If a dict or Series is passed, the Series or dict VALUES\\n            will be used to determine the groups (the Series\\' values are first\\n            aligned; see ``.align()`` method). If an ndarray is passed, the\\n            values are used as-is determine the groups. A label or list of\\n            labels may be passed to group by the columns in ``self``. Notice\\n            that a tuple is interpreted a (single) key.\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n            Split along rows (0) or columns (1).\\n        level : int, level name, or sequence of such, default None\\n            If the axis is a MultiIndex (hierarchical), group by a particular\\n            level or levels.\\n        as_index : bool, default True\\n            For aggregated output, return object with group labels as the\\n            index. Only relevant for DataFrame input. as_index=False is\\n            effectively \"SQL-style\" grouped output.\\n        sort : bool, default True\\n            Sort group keys. Get better performance by turning this off.\\n            Note this does not influence the order of observations within each\\n            group. Groupby preserves the order of rows within each group.\\n        group_keys : bool, default True\\n            When calling apply, add group keys to index to identify pieces.\\n        squeeze : bool, default False\\n            Reduce the dimensionality of the return type if possible,\\n            otherwise return a consistent type.\\n        observed : bool, default False\\n            This only applies if any of the groupers are Categoricals.\\n            If True: only show observed values for categorical groupers.\\n            If False: show all values for categorical groupers.\\n\\n            .. versionadded:: 0.23.0\\n\\n        **kwargs\\n            Optional, only accepts keyword argument \\'mutated\\' and is passed\\n            to groupby.\\n\\n        Returns\\n        -------\\n        DataFrameGroupBy or SeriesGroupBy\\n            Depends on the calling object and returns groupby object that\\n            contains information about the groups.\\n\\n        See Also\\n        --------\\n        resample : Convenience method for frequency conversion and resampling\\n            of time series.\\n\\n        Notes\\n        -----\\n        See the `user guide\\n        <http://pandas.pydata.org/pandas-docs/stable/groupby.html>`_ for more.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'Animal\\': [\\'Falcon\\', \\'Falcon\\',\\n        ...                               \\'Parrot\\', \\'Parrot\\'],\\n        ...                    \\'Max Speed\\': [380., 370., 24., 26.]})\\n        >>> df\\n           Animal  Max Speed\\n        0  Falcon      380.0\\n        1  Falcon      370.0\\n        2  Parrot       24.0\\n        3  Parrot       26.0\\n        >>> df.groupby([\\'Animal\\']).mean()\\n                Max Speed\\n        Animal\\n        Falcon      375.0\\n        Parrot       25.0\\n\\n        **Hierarchical Indexes**\\n\\n        We can groupby different levels of a hierarchical index\\n        using the `level` parameter:\\n\\n        >>> arrays = [[\\'Falcon\\', \\'Falcon\\', \\'Parrot\\', \\'Parrot\\'],\\n        ...           [\\'Captive\\', \\'Wild\\', \\'Captive\\', \\'Wild\\']]\\n        >>> index = pd.MultiIndex.from_arrays(arrays, names=(\\'Animal\\', \\'Type\\'))\\n        >>> df = pd.DataFrame({\\'Max Speed\\': [390., 350., 30., 20.]},\\n        ...                   index=index)\\n        >>> df\\n                        Max Speed\\n        Animal Type\\n        Falcon Captive      390.0\\n               Wild         350.0\\n        Parrot Captive       30.0\\n               Wild          20.0\\n        >>> df.groupby(level=0).mean()\\n                Max Speed\\n        Animal\\n        Falcon      370.0\\n        Parrot       25.0\\n        >>> df.groupby(level=1).mean()\\n                 Max Speed\\n        Type\\n        Captive      210.0\\n        Wild         185.0\\n        \"\"\"\\n        from pandas.core.groupby.groupby import groupby\\n\\n        if level is None and by is None:\\n            raise TypeError(\"You have to supply one of \\'by\\' and \\'level\\'\")\\n        axis = self._get_axis_number(axis)\\n        return groupby(self, by=by, axis=axis, level=level, as_index=as_index,\\n                       sort=sort, group_keys=group_keys, squeeze=squeeze,\\n                       observed=observed, **kwargs)',\n 'def asfreq(self, freq, method=None, how=None, normalize=False,\\n               fill_value=None):\\n        \"\"\"\\n        Convert TimeSeries to specified frequency.\\n\\n        Optionally provide filling method to pad/backfill missing values.\\n\\n        Returns the original data conformed to a new index with the specified\\n        frequency. ``resample`` is more appropriate if an operation, such as\\n        summarization, is necessary to represent the data at the new frequency.\\n\\n        Parameters\\n        ----------\\n        freq : DateOffset object, or string\\n        method : {\\'backfill\\'/\\'bfill\\', \\'pad\\'/\\'ffill\\'}, default None\\n            Method to use for filling holes in reindexed Series (note this\\n            does not fill NaNs that already were present):\\n\\n            * \\'pad\\' / \\'ffill\\': propagate last valid observation forward to next\\n              valid\\n            * \\'backfill\\' / \\'bfill\\': use NEXT valid observation to fill\\n        how : {\\'start\\', \\'end\\'}, default end\\n            For PeriodIndex only, see PeriodIndex.asfreq\\n        normalize : bool, default False\\n            Whether to reset output index to midnight\\n        fill_value : scalar, optional\\n            Value to use for missing values, applied during upsampling (note\\n            this does not fill NaNs that already were present).\\n\\n            .. versionadded:: 0.20.0\\n\\n        Returns\\n        -------\\n        converted : same type as caller\\n\\n        See Also\\n        --------\\n        reindex\\n\\n        Notes\\n        -----\\n        To learn more about the frequency strings, please see `this link\\n        <http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases>`__.\\n\\n        Examples\\n        --------\\n\\n        Start by creating a series with 4 one minute timestamps.\\n\\n        >>> index = pd.date_range(\\'1/1/2000\\', periods=4, freq=\\'T\\')\\n        >>> series = pd.Series([0.0, None, 2.0, 3.0], index=index)\\n        >>> df = pd.DataFrame({\\'s\\':series})\\n        >>> df\\n                               s\\n        2000-01-01 00:00:00    0.0\\n        2000-01-01 00:01:00    NaN\\n        2000-01-01 00:02:00    2.0\\n        2000-01-01 00:03:00    3.0\\n\\n        Upsample the series into 30 second bins.\\n\\n        >>> df.asfreq(freq=\\'30S\\')\\n                               s\\n        2000-01-01 00:00:00    0.0\\n        2000-01-01 00:00:30    NaN\\n        2000-01-01 00:01:00    NaN\\n        2000-01-01 00:01:30    NaN\\n        2000-01-01 00:02:00    2.0\\n        2000-01-01 00:02:30    NaN\\n        2000-01-01 00:03:00    3.0\\n\\n        Upsample again, providing a ``fill value``.\\n\\n        >>> df.asfreq(freq=\\'30S\\', fill_value=9.0)\\n                               s\\n        2000-01-01 00:00:00    0.0\\n        2000-01-01 00:00:30    9.0\\n        2000-01-01 00:01:00    NaN\\n        2000-01-01 00:01:30    9.0\\n        2000-01-01 00:02:00    2.0\\n        2000-01-01 00:02:30    9.0\\n        2000-01-01 00:03:00    3.0\\n\\n        Upsample again, providing a ``method``.\\n\\n        >>> df.asfreq(freq=\\'30S\\', method=\\'bfill\\')\\n                               s\\n        2000-01-01 00:00:00    0.0\\n        2000-01-01 00:00:30    NaN\\n        2000-01-01 00:01:00    NaN\\n        2000-01-01 00:01:30    2.0\\n        2000-01-01 00:02:00    2.0\\n        2000-01-01 00:02:30    3.0\\n        2000-01-01 00:03:00    3.0\\n        \"\"\"\\n        from pandas.core.resample import asfreq\\n        return asfreq(self, freq, method=method, how=how, normalize=normalize,\\n                      fill_value=fill_value)',\n 'def at_time(self, time, asof=False, axis=None):\\n        \"\"\"\\n        Select values at particular time of day (e.g. 9:30AM).\\n\\n        Parameters\\n        ----------\\n        time : datetime.time or str\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n\\n            .. versionadded:: 0.24.0\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n\\n        Raises\\n        ------\\n        TypeError\\n            If the index is not  a :class:`DatetimeIndex`\\n\\n        See Also\\n        --------\\n        between_time : Select values between particular times of the day.\\n        first : Select initial periods of time series based on a date offset.\\n        last : Select final periods of time series based on a date offset.\\n        DatetimeIndex.indexer_at_time : Get just the index locations for\\n            values at particular time of the day.\\n\\n        Examples\\n        --------\\n        >>> i = pd.date_range(\\'2018-04-09\\', periods=4, freq=\\'12H\\')\\n        >>> ts = pd.DataFrame({\\'A\\': [1, 2, 3, 4]}, index=i)\\n        >>> ts\\n                             A\\n        2018-04-09 00:00:00  1\\n        2018-04-09 12:00:00  2\\n        2018-04-10 00:00:00  3\\n        2018-04-10 12:00:00  4\\n\\n        >>> ts.at_time(\\'12:00\\')\\n                             A\\n        2018-04-09 12:00:00  2\\n        2018-04-10 12:00:00  4\\n        \"\"\"\\n        if axis is None:\\n            axis = self._stat_axis_number\\n        axis = self._get_axis_number(axis)\\n\\n        index = self._get_axis(axis)\\n        try:\\n            indexer = index.indexer_at_time(time, asof=asof)\\n        except AttributeError:\\n            raise TypeError(\\'Index must be DatetimeIndex\\')\\n\\n        return self._take(indexer, axis=axis)',\n 'def between_time(self, start_time, end_time, include_start=True,\\n                     include_end=True, axis=None):\\n        \"\"\"\\n        Select values between particular times of the day (e.g., 9:00-9:30 AM).\\n\\n        By setting ``start_time`` to be later than ``end_time``,\\n        you can get the times that are *not* between the two times.\\n\\n        Parameters\\n        ----------\\n        start_time : datetime.time or str\\n        end_time : datetime.time or str\\n        include_start : bool, default True\\n        include_end : bool, default True\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n\\n            .. versionadded:: 0.24.0\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n\\n        Raises\\n        ------\\n        TypeError\\n            If the index is not  a :class:`DatetimeIndex`\\n\\n        See Also\\n        --------\\n        at_time : Select values at a particular time of the day.\\n        first : Select initial periods of time series based on a date offset.\\n        last : Select final periods of time series based on a date offset.\\n        DatetimeIndex.indexer_between_time : Get just the index locations for\\n            values between particular times of the day.\\n\\n        Examples\\n        --------\\n        >>> i = pd.date_range(\\'2018-04-09\\', periods=4, freq=\\'1D20min\\')\\n        >>> ts = pd.DataFrame({\\'A\\': [1, 2, 3, 4]}, index=i)\\n        >>> ts\\n                             A\\n        2018-04-09 00:00:00  1\\n        2018-04-10 00:20:00  2\\n        2018-04-11 00:40:00  3\\n        2018-04-12 01:00:00  4\\n\\n        >>> ts.between_time(\\'0:15\\', \\'0:45\\')\\n                             A\\n        2018-04-10 00:20:00  2\\n        2018-04-11 00:40:00  3\\n\\n        You get the times that are *not* between two times by setting\\n        ``start_time`` later than ``end_time``:\\n\\n        >>> ts.between_time(\\'0:45\\', \\'0:15\\')\\n                             A\\n        2018-04-09 00:00:00  1\\n        2018-04-12 01:00:00  4\\n        \"\"\"\\n        if axis is None:\\n            axis = self._stat_axis_number\\n        axis = self._get_axis_number(axis)\\n\\n        index = self._get_axis(axis)\\n        try:\\n            indexer = index.indexer_between_time(\\n                start_time, end_time, include_start=include_start,\\n                include_end=include_end)\\n        except AttributeError:\\n            raise TypeError(\\'Index must be DatetimeIndex\\')\\n\\n        return self._take(indexer, axis=axis)',\n 'def resample(self, rule, how=None, axis=0, fill_method=None, closed=None,\\n                 label=None, convention=\\'start\\', kind=None, loffset=None,\\n                 limit=None, base=0, on=None, level=None):\\n        \"\"\"\\n        Resample time-series data.\\n\\n        Convenience method for frequency conversion and resampling of time\\n        series. Object must have a datetime-like index (`DatetimeIndex`,\\n        `PeriodIndex`, or `TimedeltaIndex`), or pass datetime-like values\\n        to the `on` or `level` keyword.\\n\\n        Parameters\\n        ----------\\n        rule : str\\n            The offset string or object representing target conversion.\\n        how : str\\n            Method for down/re-sampling, default to \\'mean\\' for downsampling.\\n\\n            .. deprecated:: 0.18.0\\n               The new syntax is ``.resample(...).mean()``, or\\n               ``.resample(...).apply(<func>)``\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n            Which axis to use for up- or down-sampling. For `Series` this\\n            will default to 0, i.e. along the rows. Must be\\n            `DatetimeIndex`, `TimedeltaIndex` or `PeriodIndex`.\\n        fill_method : str, default None\\n            Filling method for upsampling.\\n\\n            .. deprecated:: 0.18.0\\n               The new syntax is ``.resample(...).<func>()``,\\n               e.g. ``.resample(...).pad()``\\n        closed : {\\'right\\', \\'left\\'}, default None\\n            Which side of bin interval is closed. The default is \\'left\\'\\n            for all frequency offsets except for \\'M\\', \\'A\\', \\'Q\\', \\'BM\\',\\n            \\'BA\\', \\'BQ\\', and \\'W\\' which all have a default of \\'right\\'.\\n        label : {\\'right\\', \\'left\\'}, default None\\n            Which bin edge label to label bucket with. The default is \\'left\\'\\n            for all frequency offsets except for \\'M\\', \\'A\\', \\'Q\\', \\'BM\\',\\n            \\'BA\\', \\'BQ\\', and \\'W\\' which all have a default of \\'right\\'.\\n        convention : {\\'start\\', \\'end\\', \\'s\\', \\'e\\'}, default \\'start\\'\\n            For `PeriodIndex` only, controls whether to use the start or\\n            end of `rule`.\\n        kind : {\\'timestamp\\', \\'period\\'}, optional, default None\\n            Pass \\'timestamp\\' to convert the resulting index to a\\n            `DateTimeIndex` or \\'period\\' to convert it to a `PeriodIndex`.\\n            By default the input representation is retained.\\n        loffset : timedelta, default None\\n            Adjust the resampled time labels.\\n        limit : int, default None\\n            Maximum size gap when reindexing with `fill_method`.\\n\\n            .. deprecated:: 0.18.0\\n        base : int, default 0\\n            For frequencies that evenly subdivide 1 day, the \"origin\" of the\\n            aggregated intervals. For example, for \\'5min\\' frequency, base could\\n            range from 0 through 4. Defaults to 0.\\n        on : str, optional\\n            For a DataFrame, column to use instead of index for resampling.\\n            Column must be datetime-like.\\n\\n            .. versionadded:: 0.19.0\\n\\n        level : str or int, optional\\n            For a MultiIndex, level (name or number) to use for\\n            resampling. `level` must be datetime-like.\\n\\n            .. versionadded:: 0.19.0\\n\\n        Returns\\n        -------\\n        Resampler object\\n\\n        See Also\\n        --------\\n        groupby : Group by mapping, function, label, or list of labels.\\n        Series.resample : Resample a Series.\\n        DataFrame.resample: Resample a DataFrame.\\n\\n        Notes\\n        -----\\n        See the `user guide\\n        <http://pandas.pydata.org/pandas-docs/stable/timeseries.html#resampling>`_\\n        for more.\\n\\n        To learn more about the offset strings, please see `this link\\n        <http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases>`__.\\n\\n        Examples\\n        --------\\n\\n        Start by creating a series with 9 one minute timestamps.\\n\\n        >>> index = pd.date_range(\\'1/1/2000\\', periods=9, freq=\\'T\\')\\n        >>> series = pd.Series(range(9), index=index)\\n        >>> series\\n        2000-01-01 00:00:00    0\\n        2000-01-01 00:01:00    1\\n        2000-01-01 00:02:00    2\\n        2000-01-01 00:03:00    3\\n        2000-01-01 00:04:00    4\\n        2000-01-01 00:05:00    5\\n        2000-01-01 00:06:00    6\\n        2000-01-01 00:07:00    7\\n        2000-01-01 00:08:00    8\\n        Freq: T, dtype: int64\\n\\n        Downsample the series into 3 minute bins and sum the values\\n        of the timestamps falling into a bin.\\n\\n        >>> series.resample(\\'3T\\').sum()\\n        2000-01-01 00:00:00     3\\n        2000-01-01 00:03:00    12\\n        2000-01-01 00:06:00    21\\n        Freq: 3T, dtype: int64\\n\\n        Downsample the series into 3 minute bins as above, but label each\\n        bin using the right edge instead of the left. Please note that the\\n        value in the bucket used as the label is not included in the bucket,\\n        which it labels. For example, in the original series the\\n        bucket ``2000-01-01 00:03:00`` contains the value 3, but the summed\\n        value in the resampled bucket with the label ``2000-01-01 00:03:00``\\n        does not include 3 (if it did, the summed value would be 6, not 3).\\n        To include this value close the right side of the bin interval as\\n        illustrated in the example below this one.\\n\\n        >>> series.resample(\\'3T\\', label=\\'right\\').sum()\\n        2000-01-01 00:03:00     3\\n        2000-01-01 00:06:00    12\\n        2000-01-01 00:09:00    21\\n        Freq: 3T, dtype: int64\\n\\n        Downsample the series into 3 minute bins as above, but close the right\\n        side of the bin interval.\\n\\n        >>> series.resample(\\'3T\\', label=\\'right\\', closed=\\'right\\').sum()\\n        2000-01-01 00:00:00     0\\n        2000-01-01 00:03:00     6\\n        2000-01-01 00:06:00    15\\n        2000-01-01 00:09:00    15\\n        Freq: 3T, dtype: int64\\n\\n        Upsample the series into 30 second bins.\\n\\n        >>> series.resample(\\'30S\\').asfreq()[0:5]   # Select first 5 rows\\n        2000-01-01 00:00:00   0.0\\n        2000-01-01 00:00:30   NaN\\n        2000-01-01 00:01:00   1.0\\n        2000-01-01 00:01:30   NaN\\n        2000-01-01 00:02:00   2.0\\n        Freq: 30S, dtype: float64\\n\\n        Upsample the series into 30 second bins and fill the ``NaN``\\n        values using the ``pad`` method.\\n\\n        >>> series.resample(\\'30S\\').pad()[0:5]\\n        2000-01-01 00:00:00    0\\n        2000-01-01 00:00:30    0\\n        2000-01-01 00:01:00    1\\n        2000-01-01 00:01:30    1\\n        2000-01-01 00:02:00    2\\n        Freq: 30S, dtype: int64\\n\\n        Upsample the series into 30 second bins and fill the\\n        ``NaN`` values using the ``bfill`` method.\\n\\n        >>> series.resample(\\'30S\\').bfill()[0:5]\\n        2000-01-01 00:00:00    0\\n        2000-01-01 00:00:30    1\\n        2000-01-01 00:01:00    1\\n        2000-01-01 00:01:30    2\\n        2000-01-01 00:02:00    2\\n        Freq: 30S, dtype: int64\\n\\n        Pass a custom function via ``apply``\\n\\n        >>> def custom_resampler(array_like):\\n        ...     return np.sum(array_like) + 5\\n        ...\\n        >>> series.resample(\\'3T\\').apply(custom_resampler)\\n        2000-01-01 00:00:00     8\\n        2000-01-01 00:03:00    17\\n        2000-01-01 00:06:00    26\\n        Freq: 3T, dtype: int64\\n\\n        For a Series with a PeriodIndex, the keyword `convention` can be\\n        used to control whether to use the start or end of `rule`.\\n\\n        Resample a year by quarter using \\'start\\' `convention`. Values are\\n        assigned to the first quarter of the period.\\n\\n        >>> s = pd.Series([1, 2], index=pd.period_range(\\'2012-01-01\\',\\n        ...                                             freq=\\'A\\',\\n        ...                                             periods=2))\\n        >>> s\\n        2012    1\\n        2013    2\\n        Freq: A-DEC, dtype: int64\\n        >>> s.resample(\\'Q\\', convention=\\'start\\').asfreq()\\n        2012Q1    1.0\\n        2012Q2    NaN\\n        2012Q3    NaN\\n        2012Q4    NaN\\n        2013Q1    2.0\\n        2013Q2    NaN\\n        2013Q3    NaN\\n        2013Q4    NaN\\n        Freq: Q-DEC, dtype: float64\\n\\n        Resample quarters by month using \\'end\\' `convention`. Values are\\n        assigned to the last month of the period.\\n\\n        >>> q = pd.Series([1, 2, 3, 4], index=pd.period_range(\\'2018-01-01\\',\\n        ...                                                   freq=\\'Q\\',\\n        ...                                                   periods=4))\\n        >>> q\\n        2018Q1    1\\n        2018Q2    2\\n        2018Q3    3\\n        2018Q4    4\\n        Freq: Q-DEC, dtype: int64\\n        >>> q.resample(\\'M\\', convention=\\'end\\').asfreq()\\n        2018-03    1.0\\n        2018-04    NaN\\n        2018-05    NaN\\n        2018-06    2.0\\n        2018-07    NaN\\n        2018-08    NaN\\n        2018-09    3.0\\n        2018-10    NaN\\n        2018-11    NaN\\n        2018-12    4.0\\n        Freq: M, dtype: float64\\n\\n        For DataFrame objects, the keyword `on` can be used to specify the\\n        column instead of the index for resampling.\\n\\n        >>> d = dict({\\'price\\': [10, 11, 9, 13, 14, 18, 17, 19],\\n        ...           \\'volume\\': [50, 60, 40, 100, 50, 100, 40, 50]})\\n        >>> df = pd.DataFrame(d)\\n        >>> df[\\'week_starting\\'] = pd.date_range(\\'01/01/2018\\',\\n        ...                                     periods=8,\\n        ...                                     freq=\\'W\\')\\n        >>> df\\n           price  volume week_starting\\n        0     10      50    2018-01-07\\n        1     11      60    2018-01-14\\n        2      9      40    2018-01-21\\n        3     13     100    2018-01-28\\n        4     14      50    2018-02-04\\n        5     18     100    2018-02-11\\n        6     17      40    2018-02-18\\n        7     19      50    2018-02-25\\n        >>> df.resample(\\'M\\', on=\\'week_starting\\').mean()\\n                       price  volume\\n        week_starting\\n        2018-01-31     10.75    62.5\\n        2018-02-28     17.00    60.0\\n\\n        For a DataFrame with MultiIndex, the keyword `level` can be used to\\n        specify on which level the resampling needs to take place.\\n\\n        >>> days = pd.date_range(\\'1/1/2000\\', periods=4, freq=\\'D\\')\\n        >>> d2 = dict({\\'price\\': [10, 11, 9, 13, 14, 18, 17, 19],\\n        ...            \\'volume\\': [50, 60, 40, 100, 50, 100, 40, 50]})\\n        >>> df2 = pd.DataFrame(d2,\\n        ...                    index=pd.MultiIndex.from_product([days,\\n        ...                                                     [\\'morning\\',\\n        ...                                                      \\'afternoon\\']]\\n        ...                                                     ))\\n        >>> df2\\n                              price  volume\\n        2000-01-01 morning       10      50\\n                   afternoon     11      60\\n        2000-01-02 morning        9      40\\n                   afternoon     13     100\\n        2000-01-03 morning       14      50\\n                   afternoon     18     100\\n        2000-01-04 morning       17      40\\n                   afternoon     19      50\\n        >>> df2.resample(\\'D\\', level=0).sum()\\n                    price  volume\\n        2000-01-01     21     110\\n        2000-01-02     22     140\\n        2000-01-03     32     150\\n        2000-01-04     36      90\\n        \"\"\"\\n\\n        from pandas.core.resample import (resample,\\n                                          _maybe_process_deprecations)\\n        axis = self._get_axis_number(axis)\\n        r = resample(self, freq=rule, label=label, closed=closed,\\n                     axis=axis, kind=kind, loffset=loffset,\\n                     convention=convention,\\n                     base=base, key=on, level=level)\\n        return _maybe_process_deprecations(r,\\n                                           how=how,\\n                                           fill_method=fill_method,\\n                                           limit=limit)',\n 'def first(self, offset):\\n        \"\"\"\\n        Convenience method for subsetting initial periods of time series data\\n        based on a date offset.\\n\\n        Parameters\\n        ----------\\n        offset : string, DateOffset, dateutil.relativedelta\\n\\n        Returns\\n        -------\\n        subset : same type as caller\\n\\n        Raises\\n        ------\\n        TypeError\\n            If the index is not  a :class:`DatetimeIndex`\\n\\n        See Also\\n        --------\\n        last : Select final periods of time series based on a date offset.\\n        at_time : Select values at a particular time of the day.\\n        between_time : Select values between particular times of the day.\\n\\n        Examples\\n        --------\\n        >>> i = pd.date_range(\\'2018-04-09\\', periods=4, freq=\\'2D\\')\\n        >>> ts = pd.DataFrame({\\'A\\': [1,2,3,4]}, index=i)\\n        >>> ts\\n                    A\\n        2018-04-09  1\\n        2018-04-11  2\\n        2018-04-13  3\\n        2018-04-15  4\\n\\n        Get the rows for the first 3 days:\\n\\n        >>> ts.first(\\'3D\\')\\n                    A\\n        2018-04-09  1\\n        2018-04-11  2\\n\\n        Notice the data for 3 first calender days were returned, not the first\\n        3 days observed in the dataset, and therefore data for 2018-04-13 was\\n        not returned.\\n        \"\"\"\\n        if not isinstance(self.index, DatetimeIndex):\\n            raise TypeError(\"\\'first\\' only supports a DatetimeIndex index\")\\n\\n        if len(self.index) == 0:\\n            return self\\n\\n        offset = to_offset(offset)\\n        end_date = end = self.index[0] + offset\\n\\n        # Tick-like, e.g. 3 weeks\\n        if not offset.isAnchored() and hasattr(offset, \\'_inc\\'):\\n            if end_date in self.index:\\n                end = self.index.searchsorted(end_date, side=\\'left\\')\\n                return self.iloc[:end]\\n\\n        return self.loc[:end]',\n 'def last(self, offset):\\n        \"\"\"\\n        Convenience method for subsetting final periods of time series data\\n        based on a date offset.\\n\\n        Parameters\\n        ----------\\n        offset : string, DateOffset, dateutil.relativedelta\\n\\n        Returns\\n        -------\\n        subset : same type as caller\\n\\n        Raises\\n        ------\\n        TypeError\\n            If the index is not  a :class:`DatetimeIndex`\\n\\n        See Also\\n        --------\\n        first : Select initial periods of time series based on a date offset.\\n        at_time : Select values at a particular time of the day.\\n        between_time : Select values between particular times of the day.\\n\\n        Examples\\n        --------\\n        >>> i = pd.date_range(\\'2018-04-09\\', periods=4, freq=\\'2D\\')\\n        >>> ts = pd.DataFrame({\\'A\\': [1,2,3,4]}, index=i)\\n        >>> ts\\n                    A\\n        2018-04-09  1\\n        2018-04-11  2\\n        2018-04-13  3\\n        2018-04-15  4\\n\\n        Get the rows for the last 3 days:\\n\\n        >>> ts.last(\\'3D\\')\\n                    A\\n        2018-04-13  3\\n        2018-04-15  4\\n\\n        Notice the data for 3 last calender days were returned, not the last\\n        3 observed days in the dataset, and therefore data for 2018-04-11 was\\n        not returned.\\n        \"\"\"\\n        if not isinstance(self.index, DatetimeIndex):\\n            raise TypeError(\"\\'last\\' only supports a DatetimeIndex index\")\\n\\n        if len(self.index) == 0:\\n            return self\\n\\n        offset = to_offset(offset)\\n\\n        start_date = self.index[-1] - offset\\n        start = self.index.searchsorted(start_date, side=\\'right\\')\\n        return self.iloc[start:]',\n 'def rank(self, axis=0, method=\\'average\\', numeric_only=None,\\n             na_option=\\'keep\\', ascending=True, pct=False):\\n        \"\"\"\\n        Compute numerical data ranks (1 through n) along axis. Equal values are\\n        assigned a rank that is the average of the ranks of those values.\\n\\n        Parameters\\n        ----------\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, default 0\\n            index to direct ranking\\n        method : {\\'average\\', \\'min\\', \\'max\\', \\'first\\', \\'dense\\'}\\n            * average: average rank of group\\n            * min: lowest rank in group\\n            * max: highest rank in group\\n            * first: ranks assigned in order they appear in the array\\n            * dense: like \\'min\\', but rank always increases by 1 between groups\\n        numeric_only : boolean, default None\\n            Include only float, int, boolean data. Valid only for DataFrame or\\n            Panel objects\\n        na_option : {\\'keep\\', \\'top\\', \\'bottom\\'}\\n            * keep: leave NA values where they are\\n            * top: smallest rank if ascending\\n            * bottom: smallest rank if descending\\n        ascending : boolean, default True\\n            False for ranks by high (1) to low (N)\\n        pct : boolean, default False\\n            Computes percentage rank of data\\n\\n        Returns\\n        -------\\n        ranks : same type as caller\\n        \"\"\"\\n        axis = self._get_axis_number(axis)\\n\\n        if self.ndim > 2:\\n            msg = \"rank does not make sense when ndim > 2\"\\n            raise NotImplementedError(msg)\\n\\n        if na_option not in {\\'keep\\', \\'top\\', \\'bottom\\'}:\\n            msg = \"na_option must be one of \\'keep\\', \\'top\\', or \\'bottom\\'\"\\n            raise ValueError(msg)\\n\\n        def ranker(data):\\n            ranks = algos.rank(data.values, axis=axis, method=method,\\n                               ascending=ascending, na_option=na_option,\\n                               pct=pct)\\n            ranks = self._constructor(ranks, **data._construct_axes_dict())\\n            return ranks.__finalize__(self)\\n\\n        # if numeric_only is None, and we can\\'t get anything, we try with\\n        # numeric_only=True\\n        if numeric_only is None:\\n            try:\\n                return ranker(self)\\n            except TypeError:\\n                numeric_only = True\\n\\n        if numeric_only:\\n            data = self._get_numeric_data()\\n        else:\\n            data = self\\n\\n        return ranker(data)',\n 'def _where(self, cond, other=np.nan, inplace=False, axis=None, level=None,\\n               errors=\\'raise\\', try_cast=False):\\n        \"\"\"\\n        Equivalent to public method `where`, except that `other` is not\\n        applied as a function even if callable. Used in __setitem__.\\n        \"\"\"\\n        inplace = validate_bool_kwarg(inplace, \\'inplace\\')\\n\\n        # align the cond to same shape as myself\\n        cond = com.apply_if_callable(cond, self)\\n        if isinstance(cond, NDFrame):\\n            cond, _ = cond.align(self, join=\\'right\\', broadcast_axis=1)\\n        else:\\n            if not hasattr(cond, \\'shape\\'):\\n                cond = np.asanyarray(cond)\\n            if cond.shape != self.shape:\\n                raise ValueError(\\'Array conditional must be same shape as \\'\\n                                 \\'self\\')\\n            cond = self._constructor(cond, **self._construct_axes_dict())\\n\\n        # make sure we are boolean\\n        fill_value = bool(inplace)\\n        cond = cond.fillna(fill_value)\\n\\n        msg = \"Boolean array expected for the condition, not {dtype}\"\\n\\n        if not isinstance(cond, pd.DataFrame):\\n            # This is a single-dimensional object.\\n            if not is_bool_dtype(cond):\\n                raise ValueError(msg.format(dtype=cond.dtype))\\n        elif not cond.empty:\\n            for dt in cond.dtypes:\\n                if not is_bool_dtype(dt):\\n                    raise ValueError(msg.format(dtype=dt))\\n\\n        cond = -cond if inplace else cond\\n\\n        # try to align with other\\n        try_quick = True\\n        if hasattr(other, \\'align\\'):\\n\\n            # align with me\\n            if other.ndim <= self.ndim:\\n\\n                _, other = self.align(other, join=\\'left\\', axis=axis,\\n                                      level=level, fill_value=np.nan)\\n\\n                # if we are NOT aligned, raise as we cannot where index\\n                if (axis is None and\\n                        not all(other._get_axis(i).equals(ax)\\n                                for i, ax in enumerate(self.axes))):\\n                    raise InvalidIndexError\\n\\n            # slice me out of the other\\n            else:\\n                raise NotImplementedError(\"cannot align with a higher \"\\n                                          \"dimensional NDFrame\")\\n\\n        if isinstance(other, np.ndarray):\\n\\n            if other.shape != self.shape:\\n\\n                if self.ndim == 1:\\n\\n                    icond = cond.values\\n\\n                    # GH 2745 / GH 4192\\n                    # treat like a scalar\\n                    if len(other) == 1:\\n                        other = np.array(other[0])\\n\\n                    # GH 3235\\n                    # match True cond to other\\n                    elif len(cond[icond]) == len(other):\\n\\n                        # try to not change dtype at first (if try_quick)\\n                        if try_quick:\\n\\n                            try:\\n                                new_other = com.values_from_object(self)\\n                                new_other = new_other.copy()\\n                                new_other[icond] = other\\n                                other = new_other\\n                            except Exception:\\n                                try_quick = False\\n\\n                        # let\\'s create a new (if we failed at the above\\n                        # or not try_quick\\n                        if not try_quick:\\n\\n                            dtype, fill_value = maybe_promote(other.dtype)\\n                            new_other = np.empty(len(icond), dtype=dtype)\\n                            new_other.fill(fill_value)\\n                            maybe_upcast_putmask(new_other, icond, other)\\n                            other = new_other\\n\\n                    else:\\n                        raise ValueError(\\'Length of replacements must equal \\'\\n                                         \\'series length\\')\\n\\n                else:\\n                    raise ValueError(\\'other must be the same shape as self \\'\\n                                     \\'when an ndarray\\')\\n\\n            # we are the same shape, so create an actual object for alignment\\n            else:\\n                other = self._constructor(other, **self._construct_axes_dict())\\n\\n        if axis is None:\\n            axis = 0\\n\\n        if self.ndim == getattr(other, \\'ndim\\', 0):\\n            align = True\\n        else:\\n            align = (self._get_axis_number(axis) == 1)\\n\\n        block_axis = self._get_block_manager_axis(axis)\\n\\n        if inplace:\\n            # we may have different type blocks come out of putmask, so\\n            # reconstruct the block manager\\n\\n            self._check_inplace_setting(other)\\n            new_data = self._data.putmask(mask=cond, new=other, align=align,\\n                                          inplace=True, axis=block_axis,\\n                                          transpose=self._AXIS_REVERSED)\\n            self._update_inplace(new_data)\\n\\n        else:\\n            new_data = self._data.where(other=other, cond=cond, align=align,\\n                                        errors=errors,\\n                                        try_cast=try_cast, axis=block_axis,\\n                                        transpose=self._AXIS_REVERSED)\\n\\n            return self._constructor(new_data).__finalize__(self)',\n 'def slice_shift(self, periods=1, axis=0):\\n        \"\"\"\\n        Equivalent to `shift` without copying data. The shifted data will\\n        not include the dropped periods and the shifted axis will be smaller\\n        than the original.\\n\\n        Parameters\\n        ----------\\n        periods : int\\n            Number of periods to move, can be positive or negative\\n\\n        Returns\\n        -------\\n        shifted : same type as caller\\n\\n        Notes\\n        -----\\n        While the `slice_shift` is faster than `shift`, you may pay for it\\n        later during alignment.\\n        \"\"\"\\n        if periods == 0:\\n            return self\\n\\n        if periods > 0:\\n            vslicer = slice(None, -periods)\\n            islicer = slice(periods, None)\\n        else:\\n            vslicer = slice(-periods, None)\\n            islicer = slice(None, periods)\\n\\n        new_obj = self._slice(vslicer, axis=axis)\\n        shifted_axis = self._get_axis(axis)[islicer]\\n        new_obj.set_axis(shifted_axis, axis=axis, inplace=True)\\n\\n        return new_obj.__finalize__(self)',\n 'def tshift(self, periods=1, freq=None, axis=0):\\n        \"\"\"\\n        Shift the time index, using the index\\'s frequency if available.\\n\\n        Parameters\\n        ----------\\n        periods : int\\n            Number of periods to move, can be positive or negative\\n        freq : DateOffset, timedelta, or time rule string, default None\\n            Increment to use from the tseries module or time rule (e.g. \\'EOM\\')\\n        axis : int or basestring\\n            Corresponds to the axis that contains the Index\\n\\n        Returns\\n        -------\\n        shifted : NDFrame\\n\\n        Notes\\n        -----\\n        If freq is not specified then tries to use the freq or inferred_freq\\n        attributes of the index. If neither of those attributes exist, a\\n        ValueError is thrown\\n        \"\"\"\\n\\n        index = self._get_axis(axis)\\n        if freq is None:\\n            freq = getattr(index, \\'freq\\', None)\\n\\n        if freq is None:\\n            freq = getattr(index, \\'inferred_freq\\', None)\\n\\n        if freq is None:\\n            msg = \\'Freq was not given and was not set in the index\\'\\n            raise ValueError(msg)\\n\\n        if periods == 0:\\n            return self\\n\\n        if isinstance(freq, str):\\n            freq = to_offset(freq)\\n\\n        block_axis = self._get_block_manager_axis(axis)\\n        if isinstance(index, PeriodIndex):\\n            orig_freq = to_offset(index.freq)\\n            if freq == orig_freq:\\n                new_data = self._data.copy()\\n                new_data.axes[block_axis] = index.shift(periods)\\n            else:\\n                msg = (\\'Given freq %s does not match PeriodIndex freq %s\\' %\\n                       (freq.rule_code, orig_freq.rule_code))\\n                raise ValueError(msg)\\n        else:\\n            new_data = self._data.copy()\\n            new_data.axes[block_axis] = index.shift(periods, freq)\\n\\n        return self._constructor(new_data).__finalize__(self)',\n 'def truncate(self, before=None, after=None, axis=None, copy=True):\\n        \"\"\"\\n        Truncate a Series or DataFrame before and after some index value.\\n\\n        This is a useful shorthand for boolean indexing based on index\\n        values above or below certain thresholds.\\n\\n        Parameters\\n        ----------\\n        before : date, string, int\\n            Truncate all rows before this index value.\\n        after : date, string, int\\n            Truncate all rows after this index value.\\n        axis : {0 or \\'index\\', 1 or \\'columns\\'}, optional\\n            Axis to truncate. Truncates the index (rows) by default.\\n        copy : boolean, default is True,\\n            Return a copy of the truncated section.\\n\\n        Returns\\n        -------\\n        type of caller\\n            The truncated Series or DataFrame.\\n\\n        See Also\\n        --------\\n        DataFrame.loc : Select a subset of a DataFrame by label.\\n        DataFrame.iloc : Select a subset of a DataFrame by position.\\n\\n        Notes\\n        -----\\n        If the index being truncated contains only datetime values,\\n        `before` and `after` may be specified as strings instead of\\n        Timestamps.\\n\\n        Examples\\n        --------\\n        >>> df = pd.DataFrame({\\'A\\': [\\'a\\', \\'b\\', \\'c\\', \\'d\\', \\'e\\'],\\n        ...                    \\'B\\': [\\'f\\', \\'g\\', \\'h\\', \\'i\\', \\'j\\'],\\n        ...                    \\'C\\': [\\'k\\', \\'l\\', \\'m\\', \\'n\\', \\'o\\']},\\n        ...                    index=[1, 2, 3, 4, 5])\\n        >>> df\\n           A  B  C\\n        1  a  f  k\\n        2  b  g  l\\n        3  c  h  m\\n        4  d  i  n\\n        5  e  j  o\\n\\n        >>> df.truncate(before=2, after=4)\\n           A  B  C\\n        2  b  g  l\\n        3  c  h  m\\n        4  d  i  n\\n\\n        The columns of a DataFrame can be truncated.\\n\\n        >>> df.truncate(before=\"A\", after=\"B\", axis=\"columns\")\\n           A  B\\n        1  a  f\\n        2  b  g\\n        3  c  h\\n        4  d  i\\n        5  e  j\\n\\n        For Series, only rows can be truncated.\\n\\n        >>> df[\\'A\\'].truncate(before=2, after=4)\\n        2    b\\n        3    c\\n        4    d\\n        Name: A, dtype: object\\n\\n        The index values in ``truncate`` can be datetimes or string\\n        dates.\\n\\n        >>> dates = pd.date_range(\\'2016-01-01\\', \\'2016-02-01\\', freq=\\'s\\')\\n        >>> df = pd.DataFrame(index=dates, data={\\'A\\': 1})\\n        >>> df.tail()\\n                             A\\n        2016-01-31 23:59:56  1\\n        2016-01-31 23:59:57  1\\n        2016-01-31 23:59:58  1\\n        2016-01-31 23:59:59  1\\n        2016-02-01 00:00:00  1\\n\\n        >>> df.truncate(before=pd.Timestamp(\\'2016-01-05\\'),\\n        ...             after=pd.Timestamp(\\'2016-01-10\\')).tail()\\n                             A\\n        2016-01-09 23:59:56  1\\n        2016-01-09 23:59:57  1\\n        2016-01-09 23:59:58  1\\n        2016-01-09 23:59:59  1\\n        2016-01-10 00:00:00  1\\n\\n        Because the index is a DatetimeIndex containing only dates, we can\\n        specify `before` and `after` as strings. They will be coerced to\\n        Timestamps before truncation.\\n\\n        >>> df.truncate(\\'2016-01-05\\', \\'2016-01-10\\').tail()\\n                             A\\n        2016-01-09 23:59:56  1\\n        2016-01-09 23:59:57  1\\n        2016-01-09 23:59:58  1\\n        2016-01-09 23:59:59  1\\n        2016-01-10 00:00:00  1\\n\\n        Note that ``truncate`` assumes a 0 value for any unspecified time\\n        component (midnight). This differs from partial string slicing, which\\n        returns any partially matching dates.\\n\\n        >>> df.loc[\\'2016-01-05\\':\\'2016-01-10\\', :].tail()\\n                             A\\n        2016-01-10 23:59:55  1\\n        2016-01-10 23:59:56  1\\n        2016-01-10 23:59:57  1\\n        2016-01-10 23:59:58  1\\n        2016-01-10 23:59:59  1\\n        \"\"\"\\n\\n        if axis is None:\\n            axis = self._stat_axis_number\\n        axis = self._get_axis_number(axis)\\n        ax = self._get_axis(axis)\\n\\n        # GH 17935\\n        # Check that index is sorted\\n        if not ax.is_monotonic_increasing and not ax.is_monotonic_decreasing:\\n            raise ValueError(\"truncate requires a sorted index\")\\n\\n        # if we have a date index, convert to dates, otherwise\\n        # treat like a slice\\n        if ax.is_all_dates:\\n            from pandas.core.tools.datetimes import to_datetime\\n            before = to_datetime(before)\\n            after = to_datetime(after)\\n\\n        if before is not None and after is not None:\\n            if before > after:\\n                raise ValueError(\\'Truncate: %s must be after %s\\' %\\n                                 (after, before))\\n\\n        slicer = [slice(None, None)] * self._AXIS_LEN\\n        slicer[axis] = slice(before, after)\\n        result = self.loc[tuple(slicer)]\\n\\n        if isinstance(ax, MultiIndex):\\n            setattr(result, self._get_axis_name(axis),\\n                    ax.truncate(before, after))\\n\\n        if copy:\\n            result = result.copy()\\n\\n        return result',\n 'def tz_convert(self, tz, axis=0, level=None, copy=True):\\n        \"\"\"\\n        Convert tz-aware axis to target time zone.\\n\\n        Parameters\\n        ----------\\n        tz : string or pytz.timezone object\\n        axis : the axis to convert\\n        level : int, str, default None\\n            If axis ia a MultiIndex, convert a specific level. Otherwise\\n            must be None\\n        copy : boolean, default True\\n            Also make a copy of the underlying data\\n\\n        Returns\\n        -------\\n\\n        Raises\\n        ------\\n        TypeError\\n            If the axis is tz-naive.\\n        \"\"\"\\n        axis = self._get_axis_number(axis)\\n        ax = self._get_axis(axis)\\n\\n        def _tz_convert(ax, tz):\\n            if not hasattr(ax, \\'tz_convert\\'):\\n                if len(ax) > 0:\\n                    ax_name = self._get_axis_name(axis)\\n                    raise TypeError(\\'%s is not a valid DatetimeIndex or \\'\\n                                    \\'PeriodIndex\\' % ax_name)\\n                else:\\n                    ax = DatetimeIndex([], tz=tz)\\n            else:\\n                ax = ax.tz_convert(tz)\\n            return ax\\n\\n        # if a level is given it must be a MultiIndex level or\\n        # equivalent to the axis name\\n        if isinstance(ax, MultiIndex):\\n            level = ax._get_level_number(level)\\n            new_level = _tz_convert(ax.levels[level], tz)\\n            ax = ax.set_levels(new_level, level=level)\\n        else:\\n            if level not in (None, 0, ax.name):\\n                raise ValueError(\"The level {0} is not valid\".format(level))\\n            ax = _tz_convert(ax, tz)\\n\\n        result = self._constructor(self._data, copy=copy)\\n        result = result.set_axis(ax, axis=axis, inplace=False)\\n        return result.__finalize__(self)',\n 'def tz_localize(self, tz, axis=0, level=None, copy=True,\\n                    ambiguous=\\'raise\\', nonexistent=\\'raise\\'):\\n        \"\"\"\\n        Localize tz-naive index of a Series or DataFrame to target time zone.\\n\\n        This operation localizes the Index. To localize the values in a\\n        timezone-naive Series, use :meth:`Series.dt.tz_localize`.\\n\\n        Parameters\\n        ----------\\n        tz : string or pytz.timezone object\\n        axis : the axis to localize\\n        level : int, str, default None\\n            If axis ia a MultiIndex, localize a specific level. Otherwise\\n            must be None\\n        copy : boolean, default True\\n            Also make a copy of the underlying data\\n        ambiguous : \\'infer\\', bool-ndarray, \\'NaT\\', default \\'raise\\'\\n            When clocks moved backward due to DST, ambiguous times may arise.\\n            For example in Central European Time (UTC+01), when going from\\n            03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at\\n            00:30:00 UTC and at 01:30:00 UTC. In such a situation, the\\n            `ambiguous` parameter dictates how ambiguous times should be\\n            handled.\\n\\n            - \\'infer\\' will attempt to infer fall dst-transition hours based on\\n              order\\n            - bool-ndarray where True signifies a DST time, False designates\\n              a non-DST time (note that this flag is only applicable for\\n              ambiguous times)\\n            - \\'NaT\\' will return NaT where there are ambiguous times\\n            - \\'raise\\' will raise an AmbiguousTimeError if there are ambiguous\\n              times\\n        nonexistent : str, default \\'raise\\'\\n            A nonexistent time does not exist in a particular timezone\\n            where clocks moved forward due to DST. Valid values are:\\n\\n            - \\'shift_forward\\' will shift the nonexistent time forward to the\\n              closest existing time\\n            - \\'shift_backward\\' will shift the nonexistent time backward to the\\n              closest existing time\\n            - \\'NaT\\' will return NaT where there are nonexistent times\\n            - timedelta objects will shift nonexistent times by the timedelta\\n            - \\'raise\\' will raise an NonExistentTimeError if there are\\n              nonexistent times\\n\\n            .. versionadded:: 0.24.0\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            Same type as the input.\\n\\n        Raises\\n        ------\\n        TypeError\\n            If the TimeSeries is tz-aware and tz is not None.\\n\\n        Examples\\n        --------\\n\\n        Localize local times:\\n\\n        >>> s = pd.Series([1],\\n        ... index=pd.DatetimeIndex([\\'2018-09-15 01:30:00\\']))\\n        >>> s.tz_localize(\\'CET\\')\\n        2018-09-15 01:30:00+02:00    1\\n        dtype: int64\\n\\n        Be careful with DST changes. When there is sequential data, pandas\\n        can infer the DST time:\\n\\n        >>> s = pd.Series(range(7), index=pd.DatetimeIndex([\\n        ... \\'2018-10-28 01:30:00\\',\\n        ... \\'2018-10-28 02:00:00\\',\\n        ... \\'2018-10-28 02:30:00\\',\\n        ... \\'2018-10-28 02:00:00\\',\\n        ... \\'2018-10-28 02:30:00\\',\\n        ... \\'2018-10-28 03:00:00\\',\\n        ... \\'2018-10-28 03:30:00\\']))\\n        >>> s.tz_localize(\\'CET\\', ambiguous=\\'infer\\')\\n        2018-10-28 01:30:00+02:00    0\\n        2018-10-28 02:00:00+02:00    1\\n        2018-10-28 02:30:00+02:00    2\\n        2018-10-28 02:00:00+01:00    3\\n        2018-10-28 02:30:00+01:00    4\\n        2018-10-28 03:00:00+01:00    5\\n        2018-10-28 03:30:00+01:00    6\\n        dtype: int64\\n\\n        In some cases, inferring the DST is impossible. In such cases, you can\\n        pass an ndarray to the ambiguous parameter to set the DST explicitly\\n\\n        >>> s = pd.Series(range(3), index=pd.DatetimeIndex([\\n        ... \\'2018-10-28 01:20:00\\',\\n        ... \\'2018-10-28 02:36:00\\',\\n        ... \\'2018-10-28 03:46:00\\']))\\n        >>> s.tz_localize(\\'CET\\', ambiguous=np.array([True, True, False]))\\n        2018-10-28 01:20:00+02:00    0\\n        2018-10-28 02:36:00+02:00    1\\n        2018-10-28 03:46:00+01:00    2\\n        dtype: int64\\n\\n        If the DST transition causes nonexistent times, you can shift these\\n        dates forward or backwards with a timedelta object or `\\'shift_forward\\'`\\n        or `\\'shift_backwards\\'`.\\n        >>> s = pd.Series(range(2), index=pd.DatetimeIndex([\\n        ... \\'2015-03-29 02:30:00\\',\\n        ... \\'2015-03-29 03:30:00\\']))\\n        >>> s.tz_localize(\\'Europe/Warsaw\\', nonexistent=\\'shift_forward\\')\\n        2015-03-29 03:00:00+02:00    0\\n        2015-03-29 03:30:00+02:00    1\\n        dtype: int64\\n        >>> s.tz_localize(\\'Europe/Warsaw\\', nonexistent=\\'shift_backward\\')\\n        2015-03-29 01:59:59.999999999+01:00    0\\n        2015-03-29 03:30:00+02:00              1\\n        dtype: int64\\n        >>> s.tz_localize(\\'Europe/Warsaw\\', nonexistent=pd.Timedelta(\\'1H\\'))\\n        2015-03-29 03:30:00+02:00    0\\n        2015-03-29 03:30:00+02:00    1\\n        dtype: int64\\n        \"\"\"\\n        nonexistent_options = (\\'raise\\', \\'NaT\\', \\'shift_forward\\',\\n                               \\'shift_backward\\')\\n        if nonexistent not in nonexistent_options and not isinstance(\\n                nonexistent, timedelta):\\n            raise ValueError(\"The nonexistent argument must be one of \\'raise\\',\"\\n                             \" \\'NaT\\', \\'shift_forward\\', \\'shift_backward\\' or\"\\n                             \" a timedelta object\")\\n\\n        axis = self._get_axis_number(axis)\\n        ax = self._get_axis(axis)\\n\\n        def _tz_localize(ax, tz, ambiguous, nonexistent):\\n            if not hasattr(ax, \\'tz_localize\\'):\\n                if len(ax) > 0:\\n                    ax_name = self._get_axis_name(axis)\\n                    raise TypeError(\\'%s is not a valid DatetimeIndex or \\'\\n                                    \\'PeriodIndex\\' % ax_name)\\n                else:\\n                    ax = DatetimeIndex([], tz=tz)\\n            else:\\n                ax = ax.tz_localize(\\n                    tz, ambiguous=ambiguous, nonexistent=nonexistent\\n                )\\n            return ax\\n\\n        # if a level is given it must be a MultiIndex level or\\n        # equivalent to the axis name\\n        if isinstance(ax, MultiIndex):\\n            level = ax._get_level_number(level)\\n            new_level = _tz_localize(\\n                ax.levels[level], tz, ambiguous, nonexistent\\n            )\\n            ax = ax.set_levels(new_level, level=level)\\n        else:\\n            if level not in (None, 0, ax.name):\\n                raise ValueError(\"The level {0} is not valid\".format(level))\\n            ax = _tz_localize(ax, tz, ambiguous, nonexistent)\\n\\n        result = self._constructor(self._data, copy=copy)\\n        result = result.set_axis(ax, axis=axis, inplace=False)\\n        return result.__finalize__(self)',\n 'def describe(self, percentiles=None, include=None, exclude=None):\\n        \"\"\"\\n        Generate descriptive statistics that summarize the central tendency,\\n        dispersion and shape of a dataset\\'s distribution, excluding\\n        ``NaN`` values.\\n\\n        Analyzes both numeric and object series, as well\\n        as ``DataFrame`` column sets of mixed data types. The output\\n        will vary depending on what is provided. Refer to the notes\\n        below for more detail.\\n\\n        Parameters\\n        ----------\\n        percentiles : list-like of numbers, optional\\n            The percentiles to include in the output. All should\\n            fall between 0 and 1. The default is\\n            ``[.25, .5, .75]``, which returns the 25th, 50th, and\\n            75th percentiles.\\n        include : \\'all\\', list-like of dtypes or None (default), optional\\n            A white list of data types to include in the result. Ignored\\n            for ``Series``. Here are the options:\\n\\n            - \\'all\\' : All columns of the input will be included in the output.\\n            - A list-like of dtypes : Limits the results to the\\n              provided data types.\\n              To limit the result to numeric types submit\\n              ``numpy.number``. To limit it instead to object columns submit\\n              the ``numpy.object`` data type. Strings\\n              can also be used in the style of\\n              ``select_dtypes`` (e.g. ``df.describe(include=[\\'O\\'])``). To\\n              select pandas categorical columns, use ``\\'category\\'``\\n            - None (default) : The result will include all numeric columns.\\n        exclude : list-like of dtypes or None (default), optional,\\n            A black list of data types to omit from the result. Ignored\\n            for ``Series``. Here are the options:\\n\\n            - A list-like of dtypes : Excludes the provided data types\\n              from the result. To exclude numeric types submit\\n              ``numpy.number``. To exclude object columns submit the data\\n              type ``numpy.object``. Strings can also be used in the style of\\n              ``select_dtypes`` (e.g. ``df.describe(include=[\\'O\\'])``). To\\n              exclude pandas categorical columns, use ``\\'category\\'``\\n            - None (default) : The result will exclude nothing.\\n\\n        Returns\\n        -------\\n        Series or DataFrame\\n            Summary statistics of the Series or Dataframe provided.\\n\\n        See Also\\n        --------\\n        DataFrame.count: Count number of non-NA/null observations.\\n        DataFrame.max: Maximum of the values in the object.\\n        DataFrame.min: Minimum of the values in the object.\\n        DataFrame.mean: Mean of the values.\\n        DataFrame.std: Standard deviation of the obersvations.\\n        DataFrame.select_dtypes: Subset of a DataFrame including/excluding\\n            columns based on their dtype.\\n\\n        Notes\\n        -----\\n        For numeric data, the result\\'s index will include ``count``,\\n        ``mean``, ``std``, ``min``, ``max`` as well as lower, ``50`` and\\n        upper percentiles. By default the lower percentile is ``25`` and the\\n        upper percentile is ``75``. The ``50`` percentile is the\\n        same as the median.\\n\\n        For object data (e.g. strings or timestamps), the result\\'s index\\n        will include ``count``, ``unique``, ``top``, and ``freq``. The ``top``\\n        is the most common value. The ``freq`` is the most common value\\'s\\n        frequency. Timestamps also include the ``first`` and ``last`` items.\\n\\n        If multiple object values have the highest count, then the\\n        ``count`` and ``top`` results will be arbitrarily chosen from\\n        among those with the highest count.\\n\\n        For mixed data types provided via a ``DataFrame``, the default is to\\n        return only an analysis of numeric columns. If the dataframe consists\\n        only of object and categorical data without any numeric columns, the\\n        default is to return an analysis of both the object and categorical\\n        columns. If ``include=\\'all\\'`` is provided as an option, the result\\n        will include a union of attributes of each type.\\n\\n        The `include` and `exclude` parameters can be used to limit\\n        which columns in a ``DataFrame`` are analyzed for the output.\\n        The parameters are ignored when analyzing a ``Series``.\\n\\n        Examples\\n        --------\\n        Describing a numeric ``Series``.\\n\\n        >>> s = pd.Series([1, 2, 3])\\n        >>> s.describe()\\n        count    3.0\\n        mean     2.0\\n        std      1.0\\n        min      1.0\\n        25%      1.5\\n        50%      2.0\\n        75%      2.5\\n        max      3.0\\n        dtype: float64\\n\\n        Describing a categorical ``Series``.\\n\\n        >>> s = pd.Series([\\'a\\', \\'a\\', \\'b\\', \\'c\\'])\\n        >>> s.describe()\\n        count     4\\n        unique    3\\n        top       a\\n        freq      2\\n        dtype: object\\n\\n        Describing a timestamp ``Series``.\\n\\n        >>> s = pd.Series([\\n        ...   np.datetime64(\"2000-01-01\"),\\n        ...   np.datetime64(\"2010-01-01\"),\\n        ...   np.datetime64(\"2010-01-01\")\\n        ... ])\\n        >>> s.describe()\\n        count                       3\\n        unique                      2\\n        top       2010-01-01 00:00:00\\n        freq                        2\\n        first     2000-01-01 00:00:00\\n        last      2010-01-01 00:00:00\\n        dtype: object\\n\\n        Describing a ``DataFrame``. By default only numeric fields\\n        are returned.\\n\\n        >>> df = pd.DataFrame({\\'categorical\\': pd.Categorical([\\'d\\',\\'e\\',\\'f\\']),\\n        ...                    \\'numeric\\': [1, 2, 3],\\n        ...                    \\'object\\': [\\'a\\', \\'b\\', \\'c\\']\\n        ...                   })\\n        >>> df.describe()\\n               numeric\\n        count      3.0\\n        mean       2.0\\n        std        1.0\\n        min        1.0\\n        25%        1.5\\n        50%        2.0\\n        75%        2.5\\n        max        3.0\\n\\n        Describing all columns of a ``DataFrame`` regardless of data type.\\n\\n        >>> df.describe(include=\\'all\\')\\n                categorical  numeric object\\n        count            3      3.0      3\\n        unique           3      NaN      3\\n        top              f      NaN      c\\n        freq             1      NaN      1\\n        mean           NaN      2.0    NaN\\n        std            NaN      1.0    NaN\\n        min            NaN      1.0    NaN\\n        25%            NaN      1.5    NaN\\n        50%            NaN      2.0    NaN\\n        75%            NaN      2.5    NaN\\n        max            NaN      3.0    NaN\\n\\n        Describing a column from a ``DataFrame`` by accessing it as\\n        an attribute.\\n\\n        >>> df.numeric.describe()\\n        count    3.0\\n        mean     2.0\\n        std      1.0\\n        min      1.0\\n        25%      1.5\\n        50%      2.0\\n        75%      2.5\\n        max      3.0\\n        Name: numeric, dtype: float64\\n\\n        Including only numeric columns in a ``DataFrame`` description.\\n\\n        >>> df.describe(include=[np.number])\\n               numeric\\n        count      3.0\\n        mean       2.0\\n        std        1.0\\n        min        1.0\\n        25%        1.5\\n        50%        2.0\\n        75%        2.5\\n        max        3.0\\n\\n        Including only string columns in a ``DataFrame`` description.\\n\\n        >>> df.describe(include=[np.object])\\n               object\\n        count       3\\n        unique      3\\n        top         c\\n        freq        1\\n\\n        Including only categorical columns from a ``DataFrame`` description.\\n\\n        >>> df.describe(include=[\\'category\\'])\\n               categorical\\n        count            3\\n        unique           3\\n        top              f\\n        freq             1\\n\\n        Excluding numeric columns from a ``DataFrame`` description.\\n\\n        >>> df.describe(exclude=[np.number])\\n               categorical object\\n        count            3      3\\n        unique           3      3\\n        top              f      c\\n        freq             1      1\\n\\n        Excluding object columns from a ``DataFrame`` description.\\n\\n        >>> df.describe(exclude=[np.object])\\n               categorical  numeric\\n        count            3      3.0\\n        unique           3      NaN\\n        top              f      NaN\\n        freq             1      NaN\\n        mean           NaN      2.0\\n        std            NaN      1.0\\n        min            NaN      1.0\\n        25%            NaN      1.5\\n        50%            NaN      2.0\\n        75%            NaN      2.5\\n        max            NaN      3.0\\n        \"\"\"\\n        if self.ndim >= 3:\\n            msg = \"describe is not implemented on Panel objects.\"\\n            raise NotImplementedError(msg)\\n        elif self.ndim == 2 and self.columns.size == 0:\\n            raise ValueError(\"Cannot describe a DataFrame without columns\")\\n\\n        if percentiles is not None:\\n            # explicit conversion of `percentiles` to list\\n            percentiles = list(percentiles)\\n\\n            # get them all to be in [0, 1]\\n            self._check_percentile(percentiles)\\n\\n            # median should always be included\\n            if 0.5 not in percentiles:\\n                percentiles.append(0.5)\\n            percentiles = np.asarray(percentiles)\\n        else:\\n            percentiles = np.array([0.25, 0.5, 0.75])\\n\\n        # sort and check for duplicates\\n        unique_pcts = np.unique(percentiles)\\n        if len(unique_pcts) < len(percentiles):\\n            raise ValueError(\"percentiles cannot contain duplicates\")\\n        percentiles = unique_pcts\\n\\n        formatted_percentiles = format_percentiles(percentiles)\\n\\n        def describe_numeric_1d(series):\\n            stat_index = ([\\'count\\', \\'mean\\', \\'std\\', \\'min\\'] +\\n                          formatted_percentiles + [\\'max\\'])\\n            d = ([series.count(), series.mean(), series.std(), series.min()] +\\n                 series.quantile(percentiles).tolist() + [series.max()])\\n            return pd.Series(d, index=stat_index, name=series.name)\\n\\n        def describe_categorical_1d(data):\\n            names = [\\'count\\', \\'unique\\']\\n            objcounts = data.value_counts()\\n            count_unique = len(objcounts[objcounts != 0])\\n            result = [data.count(), count_unique]\\n            if result[1] > 0:\\n                top, freq = objcounts.index[0], objcounts.iloc[0]\\n\\n                if is_datetime64_any_dtype(data):\\n                    tz = data.dt.tz\\n                    asint = data.dropna().values.view(\\'i8\\')\\n                    top = Timestamp(top)\\n                    if top.tzinfo is not None and tz is not None:\\n                        # Don\\'t tz_localize(None) if key is already tz-aware\\n                        top = top.tz_convert(tz)\\n                    else:\\n                        top = top.tz_localize(tz)\\n                    names += [\\'top\\', \\'freq\\', \\'first\\', \\'last\\']\\n                    result += [top, freq,\\n                               Timestamp(asint.min(), tz=tz),\\n                               Timestamp(asint.max(), tz=tz)]\\n                else:\\n                    names += [\\'top\\', \\'freq\\']\\n                    result += [top, freq]\\n\\n            return pd.Series(result, index=names, name=data.name)\\n\\n        def describe_1d(data):\\n            if is_bool_dtype(data):\\n                return describe_categorical_1d(data)\\n            elif is_numeric_dtype(data):\\n                return describe_numeric_1d(data)\\n            elif is_timedelta64_dtype(data):\\n                return describe_numeric_1d(data)\\n            else:\\n                return describe_categorical_1d(data)\\n\\n        if self.ndim == 1:\\n            return describe_1d(self)\\n        elif (include is None) and (exclude is None):\\n            # when some numerics are found, keep only numerics\\n            data = self.select_dtypes(include=[np.number])\\n            if len(data.columns) == 0:\\n                data = self\\n        elif include == \\'all\\':\\n            if exclude is not None:\\n                msg = \"exclude must be None when include is \\'all\\'\"\\n                raise ValueError(msg)\\n            data = self\\n        else:\\n            data = self.select_dtypes(include=include, exclude=exclude)\\n\\n        ldesc = [describe_1d(s) for _, s in data.iteritems()]\\n        # set a convenient order for rows\\n        names = []\\n        ldesc_indexes = sorted((x.index for x in ldesc), key=len)\\n        for idxnames in ldesc_indexes:\\n            for name in idxnames:\\n                if name not in names:\\n                    names.append(name)\\n\\n        d = pd.concat(ldesc, join_axes=pd.Index([names]), axis=1)\\n        d.columns = data.columns.copy()\\n        return d',\n 'def _check_percentile(self, q):\\n        \"\"\"\\n        Validate percentiles (used by describe and quantile).\\n        \"\"\"\\n\\n        msg = (\"percentiles should all be in the interval [0, 1]. \"\\n               \"Try {0} instead.\")\\n        q = np.asarray(q)\\n        if q.ndim == 0:\\n            if not 0 <= q <= 1:\\n                raise ValueError(msg.format(q / 100.0))\\n        else:\\n            if not all(0 <= qs <= 1 for qs in q):\\n                raise ValueError(msg.format(q / 100.0))\\n        return q',\n 'def _add_numeric_operations(cls):\\n        \"\"\"\\n        Add the operations to the cls; evaluate the doc strings again\\n        \"\"\"\\n\\n        axis_descr, name, name2 = _doc_parms(cls)\\n\\n        cls.any = _make_logical_function(\\n            cls, \\'any\\', name, name2, axis_descr, _any_desc, nanops.nanany,\\n            _any_see_also, _any_examples, empty_value=False)\\n        cls.all = _make_logical_function(\\n            cls, \\'all\\', name, name2, axis_descr, _all_desc, nanops.nanall,\\n            _all_see_also, _all_examples, empty_value=True)\\n\\n        @Substitution(desc=\"Return the mean absolute deviation of the values \"\\n                           \"for the requested axis.\",\\n                      name1=name, name2=name2, axis_descr=axis_descr,\\n                      min_count=\\'\\', see_also=\\'\\', examples=\\'\\')\\n        @Appender(_num_doc)\\n        def mad(self, axis=None, skipna=None, level=None):\\n            if skipna is None:\\n                skipna = True\\n            if axis is None:\\n                axis = self._stat_axis_number\\n            if level is not None:\\n                return self._agg_by_level(\\'mad\\', axis=axis, level=level,\\n                                          skipna=skipna)\\n\\n            data = self._get_numeric_data()\\n            if axis == 0:\\n                demeaned = data - data.mean(axis=0)\\n            else:\\n                demeaned = data.sub(data.mean(axis=1), axis=0)\\n            return np.abs(demeaned).mean(axis=axis, skipna=skipna)\\n\\n        cls.mad = mad\\n\\n        cls.sem = _make_stat_function_ddof(\\n            cls, \\'sem\\', name, name2, axis_descr,\\n            \"Return unbiased standard error of the mean over requested \"\\n            \"axis.\\\\n\\\\nNormalized by N-1 by default. This can be changed \"\\n            \"using the ddof argument\",\\n            nanops.nansem)\\n        cls.var = _make_stat_function_ddof(\\n            cls, \\'var\\', name, name2, axis_descr,\\n            \"Return unbiased variance over requested axis.\\\\n\\\\nNormalized by \"\\n            \"N-1 by default. This can be changed using the ddof argument\",\\n            nanops.nanvar)\\n        cls.std = _make_stat_function_ddof(\\n            cls, \\'std\\', name, name2, axis_descr,\\n            \"Return sample standard deviation over requested axis.\"\\n            \"\\\\n\\\\nNormalized by N-1 by default. This can be changed using the \"\\n            \"ddof argument\",\\n            nanops.nanstd)\\n\\n        @Substitution(desc=\"Return the compound percentage of the values for \"\\n                      \"the requested axis.\", name1=name, name2=name2,\\n                      axis_descr=axis_descr,\\n                      min_count=\\'\\', see_also=\\'\\', examples=\\'\\')\\n        @Appender(_num_doc)\\n        def compound(self, axis=None, skipna=None, level=None):\\n            if skipna is None:\\n                skipna = True\\n            return (1 + self).prod(axis=axis, skipna=skipna, level=level) - 1\\n\\n        cls.compound = compound\\n\\n        cls.cummin = _make_cum_function(\\n            cls, \\'cummin\\', name, name2, axis_descr, \"minimum\",\\n            lambda y, axis: np.minimum.accumulate(y, axis), \"min\",\\n            np.inf, np.nan, _cummin_examples)\\n        cls.cumsum = _make_cum_function(\\n            cls, \\'cumsum\\', name, name2, axis_descr, \"sum\",\\n            lambda y, axis: y.cumsum(axis), \"sum\", 0.,\\n            np.nan, _cumsum_examples)\\n        cls.cumprod = _make_cum_function(\\n            cls, \\'cumprod\\', name, name2, axis_descr, \"product\",\\n            lambda y, axis: y.cumprod(axis), \"prod\", 1.,\\n            np.nan, _cumprod_examples)\\n        cls.cummax = _make_cum_function(\\n            cls, \\'cummax\\', name, name2, axis_descr, \"maximum\",\\n            lambda y, axis: np.maximum.accumulate(y, axis), \"max\",\\n            -np.inf, np.nan, _cummax_examples)\\n\\n        cls.sum = _make_min_count_stat_function(\\n            cls, \\'sum\\', name, name2, axis_descr,\\n            \"\"\"Return the sum of the values for the requested axis.\\\\n\\n            This is equivalent to the method ``numpy.sum``.\"\"\",\\n            nanops.nansum, _stat_func_see_also, _sum_examples)\\n        cls.mean = _make_stat_function(\\n            cls, \\'mean\\', name, name2, axis_descr,\\n            \\'Return the mean of the values for the requested axis.\\',\\n            nanops.nanmean)\\n        cls.skew = _make_stat_function(\\n            cls, \\'skew\\', name, name2, axis_descr,\\n            \\'Return unbiased skew over requested axis\\\\nNormalized by N-1.\\',\\n            nanops.nanskew)\\n        cls.kurt = _make_stat_function(\\n            cls, \\'kurt\\', name, name2, axis_descr,\\n            \"Return unbiased kurtosis over requested axis using Fisher\\'s \"\\n            \"definition of\\\\nkurtosis (kurtosis of normal == 0.0). Normalized \"\\n            \"by N-1.\",\\n            nanops.nankurt)\\n        cls.kurtosis = cls.kurt\\n        cls.prod = _make_min_count_stat_function(\\n            cls, \\'prod\\', name, name2, axis_descr,\\n            \\'Return the product of the values for the requested axis.\\',\\n            nanops.nanprod, examples=_prod_examples)\\n        cls.product = cls.prod\\n        cls.median = _make_stat_function(\\n            cls, \\'median\\', name, name2, axis_descr,\\n            \\'Return the median of the values for the requested axis.\\',\\n            nanops.nanmedian)\\n        cls.max = _make_stat_function(\\n            cls, \\'max\\', name, name2, axis_descr,\\n            \"\"\"Return the maximum of the values for the requested axis.\\\\n\\n            If you want the *index* of the maximum, use ``idxmax``. This is\\n            the equivalent of the ``numpy.ndarray`` method ``argmax``.\"\"\",\\n            nanops.nanmax, _stat_func_see_also, _max_examples)\\n        cls.min = _make_stat_function(\\n            cls, \\'min\\', name, name2, axis_descr,\\n            \"\"\"Return the minimum of the values for the requested axis.\\\\n\\n            If you want the *index* of the minimum, use ``idxmin``. This is\\n            the equivalent of the ``numpy.ndarray`` method ``argmin``.\"\"\",\\n            nanops.nanmin, _stat_func_see_also, _min_examples)',\n 'def _add_series_only_operations(cls):\\n        \"\"\"\\n        Add the series only operations to the cls; evaluate the doc\\n        strings again.\\n        \"\"\"\\n\\n        axis_descr, name, name2 = _doc_parms(cls)\\n\\n        def nanptp(values, axis=0, skipna=True):\\n            nmax = nanops.nanmax(values, axis, skipna)\\n            nmin = nanops.nanmin(values, axis, skipna)\\n            warnings.warn(\"Method .ptp is deprecated and will be removed \"\\n                          \"in a future version. Use numpy.ptp instead.\",\\n                          FutureWarning, stacklevel=4)\\n            return nmax - nmin\\n\\n        cls.ptp = _make_stat_function(\\n            cls, \\'ptp\\', name, name2, axis_descr,\\n            \"\"\"Return the difference between the maximum value and the\\n            minimum value in the object. This is the equivalent of the\\n            ``numpy.ndarray`` method ``ptp``.\\\\n\\\\n.. deprecated:: 0.24.0\\n                Use numpy.ptp instead\"\"\",\\n            nanptp)',\n 'def _add_series_or_dataframe_operations(cls):\\n        \"\"\"\\n        Add the series or dataframe only operations to the cls; evaluate\\n        the doc strings again.\\n        \"\"\"\\n\\n        from pandas.core import window as rwindow\\n\\n        @Appender(rwindow.rolling.__doc__)\\n        def rolling(self, window, min_periods=None, center=False,\\n                    win_type=None, on=None, axis=0, closed=None):\\n            axis = self._get_axis_number(axis)\\n            return rwindow.rolling(self, window=window,\\n                                   min_periods=min_periods,\\n                                   center=center, win_type=win_type,\\n                                   on=on, axis=axis, closed=closed)\\n\\n        cls.rolling = rolling\\n\\n        @Appender(rwindow.expanding.__doc__)\\n        def expanding(self, min_periods=1, center=False, axis=0):\\n            axis = self._get_axis_number(axis)\\n            return rwindow.expanding(self, min_periods=min_periods,\\n                                     center=center, axis=axis)\\n\\n        cls.expanding = expanding\\n\\n        @Appender(rwindow.ewm.__doc__)\\n        def ewm(self, com=None, span=None, halflife=None, alpha=None,\\n                min_periods=0, adjust=True, ignore_na=False,\\n                axis=0):\\n            axis = self._get_axis_number(axis)\\n            return rwindow.ewm(self, com=com, span=span, halflife=halflife,\\n                               alpha=alpha, min_periods=min_periods,\\n                               adjust=adjust, ignore_na=ignore_na, axis=axis)\\n\\n        cls.ewm = ewm',\n 'def _find_valid_index(self, how):\\n        \"\"\"\\n        Retrieves the index of the first valid value.\\n\\n        Parameters\\n        ----------\\n        how : {\\'first\\', \\'last\\'}\\n            Use this parameter to change between the first or last valid index.\\n\\n        Returns\\n        -------\\n        idx_first_valid : type of index\\n        \"\"\"\\n        assert how in [\\'first\\', \\'last\\']\\n\\n        if len(self) == 0:  # early stop\\n            return None\\n        is_valid = ~self.isna()\\n\\n        if self.ndim == 2:\\n            is_valid = is_valid.any(1)  # reduce axis 1\\n\\n        if how == \\'first\\':\\n            idxpos = is_valid.values[::].argmax()\\n\\n        if how == \\'last\\':\\n            idxpos = len(self) - 1 - is_valid.values[::-1].argmax()\\n\\n        chk_notna = is_valid.iat[idxpos]\\n        idx = self.index[idxpos]\\n\\n        if not chk_notna:\\n            return None\\n        return idx',\n 'def _reset_cache(self, key=None):\\n        \"\"\"\\n        Reset cached properties. If ``key`` is passed, only clears that key.\\n        \"\"\"\\n        if getattr(self, \\'_cache\\', None) is None:\\n            return\\n        if key is None:\\n            self._cache.clear()\\n        else:\\n            self._cache.pop(key, None)',\n 'def _try_aggregate_string_function(self, arg, *args, **kwargs):\\n        \"\"\"\\n        if arg is a string, then try to operate on it:\\n        - try to find a function (or attribute) on ourselves\\n        - try to find a numpy function\\n        - raise\\n\\n        \"\"\"\\n        assert isinstance(arg, str)\\n\\n        f = getattr(self, arg, None)\\n        if f is not None:\\n            if callable(f):\\n                return f(*args, **kwargs)\\n\\n            # people may try to aggregate on a non-callable attribute\\n            # but don\\'t let them think they can pass args to it\\n            assert len(args) == 0\\n            assert len([kwarg for kwarg in kwargs\\n                        if kwarg not in [\\'axis\\', \\'_level\\']]) == 0\\n            return f\\n\\n        f = getattr(np, arg, None)\\n        if f is not None:\\n            return f(self, *args, **kwargs)\\n\\n        raise ValueError(\"{arg} is an unknown string function\".format(arg=arg))',\n 'def _aggregate(self, arg, *args, **kwargs):\\n        \"\"\"\\n        provide an implementation for the aggregators\\n\\n        Parameters\\n        ----------\\n        arg : string, dict, function\\n        *args : args to pass on to the function\\n        **kwargs : kwargs to pass on to the function\\n\\n        Returns\\n        -------\\n        tuple of result, how\\n\\n        Notes\\n        -----\\n        how can be a string describe the required post-processing, or\\n        None if not required\\n        \"\"\"\\n        is_aggregator = lambda x: isinstance(x, (list, tuple, dict))\\n        is_nested_renamer = False\\n\\n        _axis = kwargs.pop(\\'_axis\\', None)\\n        if _axis is None:\\n            _axis = getattr(self, \\'axis\\', 0)\\n        _level = kwargs.pop(\\'_level\\', None)\\n\\n        if isinstance(arg, str):\\n            return self._try_aggregate_string_function(arg, *args,\\n                                                       **kwargs), None\\n\\n        if isinstance(arg, dict):\\n\\n            # aggregate based on the passed dict\\n            if _axis != 0:  # pragma: no cover\\n                raise ValueError(\\'Can only pass dict with axis=0\\')\\n\\n            obj = self._selected_obj\\n\\n            def nested_renaming_depr(level=4):\\n                # deprecation of nested renaming\\n                # GH 15931\\n                warnings.warn(\\n                    (\"using a dict with renaming \"\\n                     \"is deprecated and will be removed in a future \"\\n                     \"version\"),\\n                    FutureWarning, stacklevel=level)\\n\\n            # if we have a dict of any non-scalars\\n            # eg. {\\'A\\' : [\\'mean\\']}, normalize all to\\n            # be list-likes\\n            if any(is_aggregator(x) for x in arg.values()):\\n                new_arg = OrderedDict()\\n                for k, v in arg.items():\\n                    if not isinstance(v, (tuple, list, dict)):\\n                        new_arg[k] = [v]\\n                    else:\\n                        new_arg[k] = v\\n\\n                    # the keys must be in the columns\\n                    # for ndim=2, or renamers for ndim=1\\n\\n                    # ok for now, but deprecated\\n                    # {\\'A\\': { \\'ra\\': \\'mean\\' }}\\n                    # {\\'A\\': { \\'ra\\': [\\'mean\\'] }}\\n                    # {\\'ra\\': [\\'mean\\']}\\n\\n                    # not ok\\n                    # {\\'ra\\' : { \\'A\\' : \\'mean\\' }}\\n                    if isinstance(v, dict):\\n                        is_nested_renamer = True\\n\\n                        if k not in obj.columns:\\n                            msg = (\\'cannot perform renaming for {key} with a \\'\\n                                   \\'nested dictionary\\').format(key=k)\\n                            raise SpecificationError(msg)\\n                        nested_renaming_depr(4 + (_level or 0))\\n\\n                    elif isinstance(obj, ABCSeries):\\n                        nested_renaming_depr()\\n                    elif (isinstance(obj, ABCDataFrame) and\\n                          k not in obj.columns):\\n                        raise KeyError(\\n                            \"Column \\'{col}\\' does not exist!\".format(col=k))\\n\\n                arg = new_arg\\n\\n            else:\\n                # deprecation of renaming keys\\n                # GH 15931\\n                keys = list(arg.keys())\\n                if (isinstance(obj, ABCDataFrame) and\\n                        len(obj.columns.intersection(keys)) != len(keys)):\\n                    nested_renaming_depr()\\n\\n            from pandas.core.reshape.concat import concat\\n\\n            def _agg_1dim(name, how, subset=None):\\n                \"\"\"\\n                aggregate a 1-dim with how\\n                \"\"\"\\n                colg = self._gotitem(name, ndim=1, subset=subset)\\n                if colg.ndim != 1:\\n                    raise SpecificationError(\"nested dictionary is ambiguous \"\\n                                             \"in aggregation\")\\n                return colg.aggregate(how, _level=(_level or 0) + 1)\\n\\n            def _agg_2dim(name, how):\\n                \"\"\"\\n                aggregate a 2-dim with how\\n                \"\"\"\\n                colg = self._gotitem(self._selection, ndim=2,\\n                                     subset=obj)\\n                return colg.aggregate(how, _level=None)\\n\\n            def _agg(arg, func):\\n                \"\"\"\\n                run the aggregations over the arg with func\\n                return an OrderedDict\\n                \"\"\"\\n                result = OrderedDict()\\n                for fname, agg_how in arg.items():\\n                    result[fname] = func(fname, agg_how)\\n                return result\\n\\n            # set the final keys\\n            keys = list(arg.keys())\\n            result = OrderedDict()\\n\\n            # nested renamer\\n            if is_nested_renamer:\\n                result = list(_agg(arg, _agg_1dim).values())\\n\\n                if all(isinstance(r, dict) for r in result):\\n\\n                    result, results = OrderedDict(), result\\n                    for r in results:\\n                        result.update(r)\\n                    keys = list(result.keys())\\n\\n                else:\\n\\n                    if self._selection is not None:\\n                        keys = None\\n\\n            # some selection on the object\\n            elif self._selection is not None:\\n\\n                sl = set(self._selection_list)\\n\\n                # we are a Series like object,\\n                # but may have multiple aggregations\\n                if len(sl) == 1:\\n\\n                    result = _agg(arg, lambda fname,\\n                                  agg_how: _agg_1dim(self._selection, agg_how))\\n\\n                # we are selecting the same set as we are aggregating\\n                elif not len(sl - set(keys)):\\n\\n                    result = _agg(arg, _agg_1dim)\\n\\n                # we are a DataFrame, with possibly multiple aggregations\\n                else:\\n\\n                    result = _agg(arg, _agg_2dim)\\n\\n            # no selection\\n            else:\\n\\n                try:\\n                    result = _agg(arg, _agg_1dim)\\n                except SpecificationError:\\n\\n                    # we are aggregating expecting all 1d-returns\\n                    # but we have 2d\\n                    result = _agg(arg, _agg_2dim)\\n\\n            # combine results\\n\\n            def is_any_series():\\n                # return a boolean if we have *any* nested series\\n                return any(isinstance(r, ABCSeries) for r in result.values())\\n\\n            def is_any_frame():\\n                # return a boolean if we have *any* nested series\\n                return any(isinstance(r, ABCDataFrame)\\n                           for r in result.values())\\n\\n            if isinstance(result, list):\\n                return concat(result, keys=keys, axis=1, sort=True), True\\n\\n            elif is_any_frame():\\n                # we have a dict of DataFrames\\n                # return a MI DataFrame\\n\\n                return concat([result[k] for k in keys],\\n                              keys=keys, axis=1), True\\n\\n            elif isinstance(self, ABCSeries) and is_any_series():\\n\\n                # we have a dict of Series\\n                # return a MI Series\\n                try:\\n                    result = concat(result)\\n                except TypeError:\\n                    # we want to give a nice error here if\\n                    # we have non-same sized objects, so\\n                    # we don\\'t automatically broadcast\\n\\n                    raise ValueError(\"cannot perform both aggregation \"\\n                                     \"and transformation operations \"\\n                                     \"simultaneously\")\\n\\n                return result, True\\n\\n            # fall thru\\n            from pandas import DataFrame, Series\\n            try:\\n                result = DataFrame(result)\\n            except ValueError:\\n\\n                # we have a dict of scalars\\n                result = Series(result,\\n                                name=getattr(self, \\'name\\', None))\\n\\n            return result, True\\n        elif is_list_like(arg):\\n            # we require a list, but not an \\'str\\'\\n            return self._aggregate_multiple_funcs(arg,\\n                                                  _level=_level,\\n                                                  _axis=_axis), None\\n        else:\\n            result = None\\n\\n        f = self._is_cython_func(arg)\\n        if f and not args and not kwargs:\\n            return getattr(self, f)(), None\\n\\n        # caller can react\\n        return result, True',\n 'def _shallow_copy(self, obj=None, obj_type=None, **kwargs):\\n        \"\"\"\\n        return a new object with the replacement attributes\\n        \"\"\"\\n        if obj is None:\\n            obj = self._selected_obj.copy()\\n        if obj_type is None:\\n            obj_type = self._constructor\\n        if isinstance(obj, obj_type):\\n            obj = obj.obj\\n        for attr in self._attributes:\\n            if attr not in kwargs:\\n                kwargs[attr] = getattr(self, attr)\\n        return obj_type(obj, **kwargs)',\n 'def itemsize(self):\\n        \"\"\"\\n        Return the size of the dtype of the item of the underlying data.\\n\\n        .. deprecated:: 0.23.0\\n        \"\"\"\\n        warnings.warn(\"{obj}.itemsize is deprecated and will be removed \"\\n                      \"in a future version\".format(obj=type(self).__name__),\\n                      FutureWarning, stacklevel=2)\\n        return self._ndarray_values.itemsize',\n 'def base(self):\\n        \"\"\"\\n        Return the base object if the memory of the underlying data is shared.\\n\\n        .. deprecated:: 0.23.0\\n        \"\"\"\\n        warnings.warn(\"{obj}.base is deprecated and will be removed \"\\n                      \"in a future version\".format(obj=type(self).__name__),\\n                      FutureWarning, stacklevel=2)\\n        return self.values.base',\n 'def array(self) -> ExtensionArray:\\n        \"\"\"\\n        The ExtensionArray of the data backing this Series or Index.\\n\\n        .. versionadded:: 0.24.0\\n\\n        Returns\\n        -------\\n        ExtensionArray\\n            An ExtensionArray of the values stored within. For extension\\n            types, this is the actual array. For NumPy native types, this\\n            is a thin (no copy) wrapper around :class:`numpy.ndarray`.\\n\\n            ``.array`` differs ``.values`` which may require converting the\\n            data to a different form.\\n\\n        See Also\\n        --------\\n        Index.to_numpy : Similar method that always returns a NumPy array.\\n        Series.to_numpy : Similar method that always returns a NumPy array.\\n\\n        Notes\\n        -----\\n        This table lays out the different array types for each extension\\n        dtype within pandas.\\n\\n        ================== =============================\\n        dtype              array type\\n        ================== =============================\\n        category           Categorical\\n        period             PeriodArray\\n        interval           IntervalArray\\n        IntegerNA          IntegerArray\\n        datetime64[ns, tz] DatetimeArray\\n        ================== =============================\\n\\n        For any 3rd-party extension types, the array type will be an\\n        ExtensionArray.\\n\\n        For all remaining dtypes ``.array`` will be a\\n        :class:`arrays.NumpyExtensionArray` wrapping the actual ndarray\\n        stored within. If you absolutely need a NumPy array (possibly with\\n        copying / coercing data), then use :meth:`Series.to_numpy` instead.\\n\\n        Examples\\n        --------\\n\\n        For regular NumPy types like int, and float, a PandasArray\\n        is returned.\\n\\n        >>> pd.Series([1, 2, 3]).array\\n        <PandasArray>\\n        [1, 2, 3]\\n        Length: 3, dtype: int64\\n\\n        For extension types, like Categorical, the actual ExtensionArray\\n        is returned\\n\\n        >>> ser = pd.Series(pd.Categorical([\\'a\\', \\'b\\', \\'a\\']))\\n        >>> ser.array\\n        [a, b, a]\\n        Categories (2, object): [a, b]\\n        \"\"\"\\n        result = self._values\\n\\n        if is_datetime64_ns_dtype(result.dtype):\\n            from pandas.arrays import DatetimeArray\\n            result = DatetimeArray(result)\\n        elif is_timedelta64_ns_dtype(result.dtype):\\n            from pandas.arrays import TimedeltaArray\\n            result = TimedeltaArray(result)\\n\\n        elif not is_extension_array_dtype(result.dtype):\\n            from pandas.core.arrays.numpy_ import PandasArray\\n            result = PandasArray(result)\\n\\n        return result',\n 'def to_numpy(self, dtype=None, copy=False):\\n        \"\"\"\\n        A NumPy ndarray representing the values in this Series or Index.\\n\\n        .. versionadded:: 0.24.0\\n\\n        Parameters\\n        ----------\\n        dtype : str or numpy.dtype, optional\\n            The dtype to pass to :meth:`numpy.asarray`\\n        copy : bool, default False\\n            Whether to ensure that the returned value is a not a view on\\n            another array. Note that ``copy=False`` does not *ensure* that\\n            ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\\n            a copy is made, even if not strictly necessary.\\n\\n        Returns\\n        -------\\n        numpy.ndarray\\n\\n        See Also\\n        --------\\n        Series.array : Get the actual data stored within.\\n        Index.array : Get the actual data stored within.\\n        DataFrame.to_numpy : Similar method for DataFrame.\\n\\n        Notes\\n        -----\\n        The returned array will be the same up to equality (values equal\\n        in `self` will be equal in the returned array; likewise for values\\n        that are not equal). When `self` contains an ExtensionArray, the\\n        dtype may be different. For example, for a category-dtype Series,\\n        ``to_numpy()`` will return a NumPy array and the categorical dtype\\n        will be lost.\\n\\n        For NumPy dtypes, this will be a reference to the actual data stored\\n        in this Series or Index (assuming ``copy=False``). Modifying the result\\n        in place will modify the data stored in the Series or Index (not that\\n        we recommend doing that).\\n\\n        For extension types, ``to_numpy()`` *may* require copying data and\\n        coercing the result to a NumPy type (possibly object), which may be\\n        expensive. When you need a no-copy reference to the underlying data,\\n        :attr:`Series.array` should be used instead.\\n\\n        This table lays out the different dtypes and default return types of\\n        ``to_numpy()`` for various dtypes within pandas.\\n\\n        ================== ================================\\n        dtype              array type\\n        ================== ================================\\n        category[T]        ndarray[T] (same dtype as input)\\n        period             ndarray[object] (Periods)\\n        interval           ndarray[object] (Intervals)\\n        IntegerNA          ndarray[object]\\n        datetime64[ns]     datetime64[ns]\\n        datetime64[ns, tz] ndarray[object] (Timestamps)\\n        ================== ================================\\n\\n        Examples\\n        --------\\n        >>> ser = pd.Series(pd.Categorical([\\'a\\', \\'b\\', \\'a\\']))\\n        >>> ser.to_numpy()\\n        array([\\'a\\', \\'b\\', \\'a\\'], dtype=object)\\n\\n        Specify the `dtype` to control how datetime-aware data is represented.\\n        Use ``dtype=object`` to return an ndarray of pandas :class:`Timestamp`\\n        objects, each with the correct ``tz``.\\n\\n        >>> ser = pd.Series(pd.date_range(\\'2000\\', periods=2, tz=\"CET\"))\\n        >>> ser.to_numpy(dtype=object)\\n        array([Timestamp(\\'2000-01-01 00:00:00+0100\\', tz=\\'CET\\', freq=\\'D\\'),\\n               Timestamp(\\'2000-01-02 00:00:00+0100\\', tz=\\'CET\\', freq=\\'D\\')],\\n              dtype=object)\\n\\n        Or ``dtype=\\'datetime64[ns]\\'`` to return an ndarray of native\\n        datetime64 values. The values are converted to UTC and the timezone\\n        info is dropped.\\n\\n        >>> ser.to_numpy(dtype=\"datetime64[ns]\")\\n        ... # doctest: +ELLIPSIS\\n        array([\\'1999-12-31T23:00:00.000000000\\', \\'2000-01-01T23:00:00...\\'],\\n              dtype=\\'datetime64[ns]\\')\\n        \"\"\"\\n        if is_datetime64tz_dtype(self.dtype) and dtype is None:\\n            # note: this is going to change very soon.\\n            # I have a WIP PR making this unnecessary, but it\\'s\\n            # a bit out of scope for the DatetimeArray PR.\\n            dtype = \"object\"\\n\\n        result = np.asarray(self._values, dtype=dtype)\\n        # TODO(GH-24345): Avoid potential double copy\\n        if copy:\\n            result = result.copy()\\n        return result',\n 'def _ndarray_values(self) -> np.ndarray:\\n        \"\"\"\\n        The data as an ndarray, possibly losing information.\\n\\n        The expectation is that this is cheap to compute, and is primarily\\n        used for interacting with our indexers.\\n\\n        - categorical -> codes\\n        \"\"\"\\n        if is_extension_array_dtype(self):\\n            return self.array._ndarray_values\\n        return self.values']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T14:07:56.049587Z",
     "start_time": "2023-11-09T14:07:55.969753400Z"
    }
   },
   "id": "6f2b891f26ab9366"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7c9ef48c827fb5cf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
